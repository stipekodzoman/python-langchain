{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation using Runnables and Chains w/ LangChain\n",
    "\n",
    "Enhance generation with specialized knowledge.\n",
    "\n",
    "**Purpose**:\n",
    "This notebook's purpose is to teach you how to build your own custom `Runnable`s from the `LangChain` ecosystem to build your own RAG app.\n",
    "\n",
    "## Definitions: `Runnables` and `Chains`\n",
    "\n",
    "### *Runnables*:\n",
    "\n",
    "• A Runnable represents a unit of work that can be executed.\n",
    "\n",
    "• It can perform a specific task or action, such as making an API call, processing data, or running a machine learning model.\n",
    "\n",
    "• Runnables can have input and output types specified, and they can be composed together to form more complex workflows.\n",
    "\n",
    "• They are designed to be flexible and reusable components that can be easily combined and configured\n",
    "\n",
    "• Require an `invoke` method, which is used to execute the Runnable.\n",
    "\n",
    "• Examples of Runnables include API calls, data processing functions, and machine learning models.\n",
    "\n",
    "### *Chains*:\n",
    "\n",
    "• A Chain is a sequence of Runnables that are executed in a specific order.\n",
    "\n",
    "• Chains provide a way to string together multiple Runnables to create a workflow or pipeline.\n",
    "\n",
    "• Each Runnable in the Chain takes the output of the previous Runnable as its input.\n",
    "\n",
    "• Chains can be used to build complex applications by combining and orchestrating the execution of multiple Runnables.\n",
    "\n",
    "• They provide a higher-level abstraction for organizing and structuring the flow of data and operations.\n",
    "\n",
    "• Examples of Chains include data processing pipelines, machine learning workflows, and API request/response sequences.\n",
    "\n",
    "## **Deeper explanation**:\n",
    "\n",
    "In the process of building an AI chatbot, we often need to connect different components together to create a functional system.\n",
    "\n",
    "One way to achieve this is by *chaining* these components, ensuring that the output of one component is properly passed to the next component for further processing. To accomplish this, we can directly call the functions or methods of each component and pass the output as arguments to the next component. \n",
    "\n",
    "- This straightforward approach works well when we only need to pass the output from one component to another ***without any*** additional processing or transformations in between.\n",
    "\n",
    "However, in more complex scenarios where we require intermediate processing or transformations on the output, we can use a concept called \"runnables.\" Runnables provide a flexible and modular way to encapsulate and compose these processing steps within a chain.\n",
    "\n",
    "By using runnables, we can easily add additional functionality, such as filtering or modifying the output, before passing it to the next component. \n",
    "\n",
    "- This allows us to *customize the behavior* of the chatbot and *ensure* that the output is properly prepared for the subsequent steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"How are `Runnable`s different than normal classes?\"\n",
    "\n",
    "*Similarities*:\n",
    "\n",
    "• Runnables can have methods and attributes, just like normal classes.\n",
    "\n",
    "• They can define and implement their own logic and functionality.\n",
    "\n",
    "• Runnables can have constructor arguments and can be instantiated with different configurations.\n",
    "\n",
    "*Differences*:\n",
    "\n",
    "• Runnables are designed to be executed as part of a larger system or workflow, often in a distributed or parallelized manner.\n",
    "\n",
    "• They are typically used for data processing, transformation, or analysis tasks.\n",
    "• Runnables have specific interfaces and methods that define how they interact with other runnables and the overall system.\n",
    "\n",
    "• They can be composed and combined with other runnables to create complex workflows.\n",
    "\n",
    "• Runnables often have additional features and capabilities specific to the Langchain platform, such as input and output type validation, configuration management, and error handling.\n",
    "\n",
    "• They can be executed asynchronously and in parallel, taking advantage of distributed computing resources.\n",
    "\n",
    "• Runnables can be versioned and deployed as part of a larger system, allowing for easy updates and maintenance.\n",
    "\n",
    "### \"How do I decide to use either a `Runnable` or a `Chain`?\"\n",
    "Ultimately, the decision to use runnables or a more straightforward sequential approach depends on the specific requirements and complexity of the chatbot system. You might find yourself using one, both, or neither based on your needs.\n",
    "\n",
    "In summary, \n",
    "1. Chains, which are sequences of interconnected tasks, can operate effectively on their own, without the need for Runnables. They are designed to link various components of a system in a specific order, allowing for the smooth execution of a workflow or pipeline. This makes them particularly useful in scenarios where a straightforward, sequential process is sufficient and where the complexity of Runnables is not required.\n",
    "\n",
    "2. Runnables resemble traditional classes but offer enhanced functionality, particularly in complex AI chatbot systems. They facilitate the integration and processing of outputs between different components, allowing for customization and increased flexibility in system design. This makes Runnables ideal for scenarios requiring more than just sequential processing, such as when intermediate steps or specific transformations of data are necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -Uq openai tiktoken chromadb langchain langchain-openai faiss-cpu beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set API Key Directly\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "\n",
    "# Load from an .env file\n",
    "# %pip install -Uq python-dotenv\n",
    "# import os\n",
    "# import dotenv\n",
    "# dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating and Executing a Custom Runnable\n",
    "\n",
    "In this section, we will put into practice the concepts of `Runnables` that we've discussed earlier. We will create a custom `Runnable` called `AddNumbersRunnable` and demonstrate how to execute it within our application.\n",
    "\n",
    "#### Step 1: Define the Runnable\n",
    "\n",
    "First, we instantiate our `AddNumbersRunnable`. This is a class that we've designed to perform a specific task—in this case, adding two numbers. The design of this class follows the principles of `Runnables` in the LangChain ecosystem, making it a modular and reusable component.\n",
    "\n",
    "#### Step 2: Prepare the Input\n",
    "\n",
    "Next, we prepare the input for our runnable using the `InputType` class. This class is a Pydantic model that ensures our input data is structured and typed correctly. By creating an instance of `InputType`, we are packaging our data (the two numbers we want to add) in a way that our `Runnable` can easily understand and process.\n",
    "\n",
    "#### Step 3: Execute the Runnable\n",
    "\n",
    "With our input ready, we call the `run` method of our `AddNumbersRunnable` instance. This method encapsulates the logic of our task and is responsible for executing the work defined by the `Runnable`. It takes our structured input, performs the addition, and returns an output in the form of an `OutputType` instance.\n",
    "\n",
    "#### Step 4: Display the Result\n",
    "\n",
    "Finally, we display the result of our runnable's execution. The `OutputType` class defines the expected structure of the output from our `Runnable`. By accessing the `result` attribute, we can retrieve the sum of the two numbers and present it to the user.\n",
    "\n",
    "This simple example illustrates the power of `Runnables` and how they can be used to build clean, organized, and reusable components within your applications. By following this pattern, you can create more complex workflows and applications using the LangChain library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.base import Runnable\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class AddNumbersRunnable(Runnable):\n",
    "    \"\"\"\n",
    "    A class representing a runnable that adds two numbers together.\n",
    "\n",
    "    Attributes:\n",
    "        InputType (BaseModel): A Pydantic model representing the input to the runnable. It has two attributes: num1 and num2, both integers.\n",
    "        OutputType (BaseModel): A Pydantic model representing the output of the runnable. It has one attribute: result, an integer.\n",
    "\n",
    "    Methods:\n",
    "        run(input: InputType) -> OutputType: This method takes an instance of InputType as input, adds the two numbers together, and returns an instance of OutputType containing the sum.\n",
    "        invoke(input: InputType) -> OutputType: This method is a wrapper around the run method. It takes an instance of InputType as input and returns the result of calling the run method with the same input.\n",
    "    \"\"\"\n",
    "    class InputType(BaseModel):\n",
    "        num1: int\n",
    "        num2: int\n",
    "\n",
    "    class OutputType(BaseModel):\n",
    "        result: int\n",
    "\n",
    "    def run(self, input: InputType) -> OutputType:\n",
    "        result = input.num1 + input.num2\n",
    "        output = self.OutputType(result=result)\n",
    "        return output\n",
    "    \n",
    "    def invoke(self, input: InputType) -> OutputType:\n",
    "        return self.run(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of adding 5 and 3 is: 8\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the AddNumbersRunnable class.\n",
    "# This class is an example of a Runnable, which is a core concept in LangChain for creating modular and reusable units of work.\n",
    "# As we've learned, Runnables like this one encapsulate specific tasks—in this case, adding two numbers.\n",
    "add_numbers = AddNumbersRunnable()\n",
    "\n",
    "# Prepare the input data for the runnable.\n",
    "# The InputType class is a Pydantic model that defines the expected structure of the input, ensuring type safety and validation.\n",
    "# Here, we're creating an instance of InputType with two numbers, demonstrating how inputs are structured for Runnables.\n",
    "input_data = AddNumbersRunnable.InputType(num1=5, num2=3)\n",
    "\n",
    "# Execute the runnable with the provided input data.\n",
    "# The run method is where the logic of the Runnable is executed. This method is a clear example of how a Runnable performs its task.\n",
    "# By calling this method, we're following the pattern of Runnables where they take an input, process it, and produce an output.\n",
    "output_data = add_numbers.run(input_data)\n",
    "\n",
    "# Display the result of the runnable's execution.\n",
    "# The OutputType class defines the structure of the output, which is another aspect of Runnables that promotes consistency and predictability.\n",
    "# This print statement not only shows the result but also reinforces the concept of Runnables having defined inputs and outputs.\n",
    "print(f\"The result of adding {input_data.num1} and {input_data.num2} is: {output_data.result}\")  # Output: 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LCEL: Creating and Executing a Chain\n",
    "\n",
    "## Conversation Retrieval Chain\n",
    "Below we'll use a FAISS index as our vectorstore retriever so that we can embed context and augment the LLM.\n",
    "\n",
    "### Step 1: Aquire Documents for Retrieval Augmented Generation\n",
    "We start with importing all necessary classes, and loading the documents we want to use for our retrieval chain. For this notebook, we'll use the `WebBaseLoader` to learn about LangChain Expression Language, a root concept for runnables and chains. To produce embeddings which produce accurate generations down the line, we'll chunk our document using smaller chunks, which can increase the focus of retrieved context, but may also require multiple rounds of retrieval which increases processing time.\n",
    "\n",
    "### Step 2: Create a set of prompt templates for a Chain\n",
    "This part is pretty straight forward. We use a couple classes that help us format a string of prompts that the chat model will have to answer. The first template takes in a single question and \"chat_history\" as parameters, which is then condensed by the following prompt, and finally\n",
    "\n",
    "\n",
    "SUMMARY:(This simple example illustrates the power of `Runnables` and how they can be used to build clean, organized, and reusable components within your applications. By following this pattern, you can create more complex workflows and applications using the LangChain library.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "from langchain.schema import format_document\n",
    "from langchain_core.messages import AIMessage, HumanMessage, get_buffer_string\n",
    "from langchain_core.runnables import RunnableParallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load/Split Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"\\n\\n\\n\\n\\nLangChain Expression Language (LCEL) | 🦜️🔗 Langchain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main content🦜️🔗 LangChainDocsUse casesIntegrationsGuidesAPIMoreVersioningChangelogDeveloper's guideTemplatesCookbooksTutorialsYouTube videos🦜️🔗LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS DocsChatSearchGet startedIntroductionInstallationQuickstartSecurityLangChain Expression LanguageGet startedWhy use LCELInterfaceStreamingHow toCookbookLangChain Expression Language (LCEL)ModulesModel I/ORetrievalAgentsChainsMoreLangServeLangSmithLangGraphLangChain Expression LanguageLangChain Expression Language (LCEL)LangChain Expression Language, or LCEL, is a declarative way to easily compose chains together.\\nLCEL was designed from day 1 to support putting prototypes in production, with no code changes, from the simplest “prompt + LLM” chain to the most complex chains (we’ve seen folks successfully run LCEL chains with 100s of steps in production). To highlight a few of the reasons you might want to use LCEL:Streaming support\\nWhen you build your chains with LCEL you get the best possible time-to-first-token (time elapsed until the first chunk of output comes out). For some chains this means eg. we stream tokens straight from an LLM to a streaming output parser, and you get back parsed, incremental chunks of output at the same rate as the LLM provider outputs the raw tokens.Async support\\nAny chain built with LCEL can be called both with the synchronous API (eg. in your Jupyter notebook while prototyping) as well as with the asynchronous API (eg. in a LangServe server). This enables using the same code for prototypes and in production, with great performance, and the ability to handle many concurrent requests in the same server.Optimized parallel execution\\nWhenever your LCEL chains have steps that can be executed in parallel (eg if you fetch documents from multiple retrievers) we automatically do it, both in the sync and the async interfaces, for the smallest possible latency.Retries and fallbacks\\nConfigure retries and fallbacks for any part of your LCEL chain. This is a great way to make your chains more reliable at scale. We’re currently working on adding streaming support for retries/fallbacks, so you can get the added reliability without any latency cost.Access intermediate results\\nFor more complex chains it’s often very useful to access the results of intermediate steps even before the final output is produced. This can be used to let end-users know something is happening, or even just to debug your chain. You can stream intermediate results, and it’s available on every LangServe server.Input and output schemas\\nInput and output schemas give every LCEL chain Pydantic and JSONSchema schemas inferred from the structure of your chain. This can be used for validation of inputs and outputs, and is an integral part of LangServe.Seamless LangSmith tracing integration\\nAs your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.\\nWith LCEL, all steps are automatically logged to LangSmith for maximum observability and debuggability.Seamless LangServe deployment integration\\nAny chain created with LCEL can be easily deployed using LangServe.PreviousSecurityNextGet startedCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright © 2024 LangChain, Inc.\\n\\n\\n\\n\", metadata={'source': 'https://python.langchain.com/docs/expression_language/', 'title': 'LangChain Expression Language (LCEL) | 🦜️🔗 Langchain', 'description': 'LangChain Expression Language, or LCEL, is a declarative way to easily compose chains together.', 'language': 'en'})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a loader, and load documents. \\\n",
    "    # We'll use the WebBaseLoader to load documents from the web.\n",
    "loader = WebBaseLoader(\"https://python.langchain.com/docs/expression_language/\")\n",
    "\n",
    "documents = loader.load()\n",
    "\n",
    "# Check documents contents\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='LangChain Expression Language (LCEL) | 🦜️🔗 Langchain', metadata={'source': 'https://python.langchain.com/docs/expression_language/', 'title': 'LangChain Expression Language (LCEL) | 🦜️🔗 Langchain', 'description': 'LangChain Expression Language, or LCEL, is a declarative way to easily compose chains together.', 'language': 'en'}),\n",
       " Document(page_content=\"Skip to main content🦜️🔗 LangChainDocsUse casesIntegrationsGuidesAPIMoreVersioningChangelogDeveloper's guideTemplatesCookbooksTutorialsYouTube videos🦜️🔗LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS DocsChatSearchGet startedIntroductionInstallationQuickstartSecurityLangChain Expression LanguageGet startedWhy use LCELInterfaceStreamingHow toCookbookLangChain Expression Language (LCEL)ModulesModel I/ORetrievalAgentsChainsMoreLangServeLangSmithLangGraphLangChain Expression LanguageLangChain Expression Language (LCEL)LangChain Expression Language, or LCEL, is a declarative way to easily compose chains together.\\nLCEL was designed from day 1 to support putting prototypes in production, with no code changes, from the simplest “prompt + LLM” chain to the most complex chains (we’ve seen folks successfully run LCEL chains with 100s of steps in production). To highlight a few of the reasons you might want to use LCEL:Streaming support\\nWhen you build your chains with LCEL you get the best possible time-to-first-token (time elapsed until the first chunk of output comes out). For some chains this means eg. we stream tokens straight from an LLM to a streaming output parser, and you get back parsed, incremental chunks of output at the same rate as the LLM provider outputs the raw tokens.Async support\\nAny chain built with LCEL can be called both with the synchronous API (eg. in your Jupyter notebook while prototyping) as well as with the asynchronous API (eg. in a LangServe server). This enables using the same code for prototypes and in production, with great performance, and the ability to handle many concurrent requests in the same server.Optimized parallel execution\\nWhenever your LCEL chains have steps that can be executed in parallel (eg if you fetch documents from multiple retrievers) we automatically do it, both in the sync and the async interfaces, for the smallest possible latency.Retries and fallbacks\\nConfigure retries and fallbacks for any part of your LCEL chain. This is a great way to make your chains more reliable at scale. We’re currently working on adding streaming support for retries/fallbacks, so you can get the added reliability without any latency cost.Access intermediate results\\nFor more complex chains it’s often very useful to access the results of intermediate steps even before the final output is produced. This can be used to let end-users know something is happening, or even just to debug your chain. You can stream intermediate results, and it’s available on every LangServe server.Input and output schemas\\nInput and output schemas give every LCEL chain Pydantic and JSONSchema schemas inferred from the structure of your chain. This can be used for validation of inputs and outputs, and is an integral part of LangServe.Seamless LangSmith tracing integration\\nAs your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.\\nWith LCEL, all steps are automatically logged to LangSmith for maximum observability and debuggability.Seamless LangServe deployment integration\\nAny chain created with LCEL can be easily deployed using LangServe.PreviousSecurityNextGet startedCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright © 2024 LangChain, Inc.\", metadata={'source': 'https://python.langchain.com/docs/expression_language/', 'title': 'LangChain Expression Language (LCEL) | 🦜️🔗 Langchain', 'description': 'LangChain Expression Language, or LCEL, is a declarative way to easily compose chains together.', 'language': 'en'})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(chunk_size=256, chunk_overlap=25)\n",
    "\n",
    "# Split our documents into chunks using the CharacterTextSplitter\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# Check docs contents\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Embeddings and Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the embeddings model\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Initialize a FAISS index with our embedded documents\n",
    "vectorstore = FAISS.from_texts([\"harrison worked at kensho\"], embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Retriever using Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001D9C67D5480>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set our vectorstore as our retriever\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Check retriever contents\n",
    "retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Templates & Chaining Functionality\n",
    "Let's start by building a simple prompt template that will take in a question and \"chat_history\" as parameters, which is then used to create a condensed question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import format_document\n",
    "from langchain_core.messages import AIMessage, HumanMessage, get_buffer_string\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a separate simple retrieval augmented generation prompt to generate an answer solely on the provided context and question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "ANSWER_PROMPT = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create another prompt template that will pass `page_content` through to whatever step follows it. We'll learn more about this later, once we start building and invoking our chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the document with question prompts\n",
    "def _combine_documents(\n",
    "    docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"\n",
    "):\n",
    "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
    "    return document_separator.join(doc_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got our combined documents and prompt templates, we can create a chain to ask a chat model about our context. The model will be responsible for processing all prompts at each step of the way.\n",
    "\n",
    "Please review our prompts if necessary for your learning, including `CONDENSE_QUESTION_PROMPT`, `ANSWER_PROMPT`, and `DEFAULT_DOCUMENT_PROMPT`.\n",
    "\n",
    "We'll use a RunnableParallel to pass the standalone question and chat history to the condense question prompt. Next, we create `_context` to pass the standalone question as a \"question\" to the retriever. Finally, we create our LangChain Expression Language \"Chain.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a RunnableParallel to pass the standalone question \\\n",
    "    # through the retriever with parallelism\n",
    "_inputs = RunnableParallel(\n",
    "    standalone_question=RunnablePassthrough.assign(\n",
    "        chat_history=lambda x: get_buffer_string(x[\"chat_history\"])\n",
    "    )\n",
    "    | CONDENSE_QUESTION_PROMPT\n",
    "    | ChatOpenAI(model_name=\"gpt-3.5-turbo-1106\", temperature=0)\n",
    "    | StrOutputParser(),\n",
    ")\n",
    "\n",
    "_context = {\n",
    "    \"context\": itemgetter(\"standalone_question\") | retriever | _combine_documents,\n",
    "    # Pass the standalone question as \"question\" to the retriever\n",
    "    \"question\": lambda x: x[\"standalone_question\"],\n",
    "}\n",
    "\n",
    "# Create a chain to enable interfacing with a chat model using our prompts and context\n",
    "conversational_qa_chain = _inputs | _context | ANSWER_PROMPT | ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Harrison worked at Kensho.')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the chain\n",
    "conversational_qa_chain.invoke(\n",
    "    {\n",
    "        \"question\": \"where did he work?\",\n",
    "        \"chat_history\": [\n",
    "            HumanMessage(content=\"Who wrote this notebook?\"),\n",
    "            AIMessage(content=\"Harrison\"),\n",
    "        ],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Initialize memory\n",
    "memory = ConversationBufferMemory(\n",
    "    return_messages=True, output_key=\"answer\", input_key=\"question\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we add a step to load memory\n",
    "# This adds a \"memory\" key to the input object\n",
    "loaded_memory = RunnablePassthrough.assign(\n",
    "    chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"),\n",
    ")\n",
    "# Now we calculate the standalone question\n",
    "standalone_question = {\n",
    "    \"standalone_question\": {\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "        \"chat_history\": lambda x: get_buffer_string(x[\"chat_history\"]),\n",
    "    }\n",
    "    | CONDENSE_QUESTION_PROMPT\n",
    "    | ChatOpenAI(temperature=0)\n",
    "    | StrOutputParser(),\n",
    "}\n",
    "# Now we retrieve the documents\n",
    "retrieved_documents = {\n",
    "    \"docs\": itemgetter(\"standalone_question\") | retriever,\n",
    "    \"question\": lambda x: x[\"standalone_question\"],\n",
    "}\n",
    "# Now we construct the inputs for the final prompt\n",
    "final_inputs = {\n",
    "    \"context\": lambda x: _combine_documents(x[\"docs\"]),\n",
    "    \"question\": itemgetter(\"question\"),\n",
    "}\n",
    "# And finally, we do the part that returns the answers\n",
    "answer = {\n",
    "    \"answer\": final_inputs | ANSWER_PROMPT | ChatOpenAI(),\n",
    "    \"docs\": itemgetter(\"docs\"),\n",
    "}\n",
    "# And now we put it all together!\n",
    "final_chain = loaded_memory | standalone_question | retrieved_documents | answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': AIMessage(content='Harrison was employed at Kensho.'),\n",
       " 'docs': [Document(page_content='harrison worked at kensho')]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\"question\": \"where did harrison work?\"}\n",
    "result = final_chain.invoke(inputs)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='where did harrison work?'),\n",
       "  AIMessage(content='Harrison was employed at Kensho.')]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Manually save context\n",
    "memory.save_context(inputs, {\"answer\": result[\"answer\"].content})\n",
    "\n",
    "# Load context\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Converational Chains\n",
    "\n",
    "### Lesson:\n",
    "Runnables are easily used to string multiple chains together which allows for more complex workflows. We will also learn about Branching and Merging RunnableParallels so that we can process multiple chains at the same time, while their outputs can be combined to synthesize a finalized response."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
