{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Privacy-friendly Retrieval Augmented Generation over code\n",
    "The purpose of this notebook is to teach a brand-new Python programmer to customize this notebook's code to build and execute their very own RAG script.\n",
    "\n",
    "Together we will, \n",
    "1. answer questions about code using a large language model, and \n",
    "2. learn how to do so in a privacy-respecting way, so that everything runs on *your* machine. \n",
    "\n",
    "## What's RAG?\n",
    "Retrieval Augmented Generation, or RAG, is a powerful tool in the field of artificial intelligence that combines the best of two worlds: information retrieval and text generation. \n",
    "\n",
    "In simple terms, imagine you're writing an essay on a topic you're not very familiar with. You would first search for relevant information on the internet (information retrieval), then you would use that information to write your essay (text generation). RAG does something similar, but in an automated way.\n",
    "\n",
    "When asked a question, RAG first retrieves relevant documents from a large database (like how you would search the internet). This is the \"Retrieval\" part. Then, it uses the information from these documents to generate a detailed, coherent response, which is the \"Augmented Generation\" part.\n",
    "\n",
    "### Why use RAG?\n",
    "The beauty of RAG is that it doesn't just blindly copy the information it retrieves. Instead, it understands the context of the question and generates a response that is not only accurate but also relevant and helpful. This makes RAG extremely useful in a variety of applications, such as answering complex questions, writing detailed summaries, or even creating content on specific topics.\n",
    "\n",
    "RAG is a sophisticated AI tool that retrieves relevant information to accurately and helpfully generate content, much like a well-informed and articulate writer.\n",
    "\n",
    "## Setup & Installation\n",
    "Begin by installing and import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: chromadb in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (0.4.22)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: langchainhub in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (0.1.14)\n",
      "Requirement already satisfied: GitPython in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (3.1.41)\n",
      "Requirement already satisfied: langchain-openai in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (0.0.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from langchain) (2.0.25)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from langchain) (3.9.1)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from langchain) (0.6.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.14 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from langchain) (0.0.14)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.14 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from langchain) (0.1.14)\n",
      "Requirement already satisfied: langsmith<0.0.84,>=0.0.83 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from langchain) (0.0.83)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from langchain) (1.26.3)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from langchain) (2.5.3)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: build>=1.0.3 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from chromadb) (1.0.3)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.3 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from chromadb) (0.7.3)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from chromadb) (0.109.0)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.26.0)\n",
      "Requirement already satisfied: posthog>=2.4.0 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from chromadb) (3.3.2)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from chromadb) (4.9.0)\n",
      "Requirement already satisfied: pulsar-client>=3.1.0 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from chromadb) (3.4.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from chromadb) (1.16.3)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from chromadb) (1.22.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from chromadb) (1.22.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from chromadb) (0.43b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from chromadb) (1.22.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from chromadb) (0.15.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from chromadb) (4.66.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from chromadb) (7.6.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from chromadb) (6.1.1)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from chromadb) (1.60.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from chromadb) (4.1.2)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from chromadb) (0.9.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from chromadb) (29.0.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from chromadb) (4.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from sentence-transformers) (4.37.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from sentence-transformers) (2.1.2)\n",
      "Requirement already satisfied: torchvision in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from sentence-transformers) (0.16.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from sentence-transformers) (1.4.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from sentence-transformers) (1.12.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from sentence-transformers) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from sentence-transformers) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from sentence-transformers) (0.20.3)\n",
      "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from langchainhub) (2.31.0.20240106)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from GitPython) (4.0.11)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.6.1 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: tiktoken<0.6.0,>=0.5.2 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from langchain-openai) (0.5.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: packaging>=19.0 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from build>=1.0.3->chromadb) (23.2)\n",
      "Requirement already satisfied: pyproject_hooks in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from build>=1.0.3->chromadb) (1.0.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Requirement already satisfied: tomli>=1.1.0 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from build>=1.0.3->chromadb) (2.0.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: starlette<0.36.0,>=0.35.0 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from fastapi>=0.95.2->chromadb) (0.35.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from gitdb<5,>=4.0.1->GitPython) (5.0.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.12.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: certifi>=14.05.14 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2023.11.17)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.26.2)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.7.0)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.1.0)\n",
      "Requirement already satisfied: anyio<5,>=3 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from langchain-core<0.2,>=0.1.14->langchain) (4.2.0)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
      "Requirement already satisfied: protobuf in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (4.25.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.6.1->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.6.1->langchain-openai) (0.26.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.6.1->langchain-openai) (1.3.0)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=6.0 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (6.11.0)\n",
      "Requirement already satisfied: backoff<3.0.0,>=1.10.0 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.62.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.22.0 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.22.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.22.0 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.22.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.43b0 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.43b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.43b0 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.43b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.43b0 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.43b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.43b0 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.43b0)\n",
      "Requirement already satisfied: setuptools>=16.0 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from opentelemetry-instrumentation==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (65.5.0)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from opentelemetry-instrumentation==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: asgiref~=3.0 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from opentelemetry-instrumentation-asgi==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.7.2)\n",
      "Requirement already satisfied: monotonic>=1.5 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from pydantic<3,>=1->langchain) (2.14.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.6)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from tiktoken<0.6.0,>=0.5.2->langchain-openai) (2023.12.25)\n",
      "Requirement already satisfied: networkx in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (3.1.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.4.1)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
      "Requirement already satisfied: httptools>=0.5.0 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (12.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from nltk->sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from torchvision->sentence-transformers) (10.2.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.14->langchain) (1.2.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.6.1->langchain-openai) (1.0.2)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.4.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\dae\\.vscode\\software\\build-ai\\.venv\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.5.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain chromadb sentence-transformers langchainhub GitPython langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangSmith\n",
    "If you have access to LangSmith, it will be extremely helpful as projects become increasingly more complex because it allows the tracing of action steps made throughout execution. \n",
    "\n",
    "This is especially true for us since we're building custom tools, creating an agent using Hugging Face's Model Hub, and tying everything together inside a runnable chain.\n",
    "\n",
    "It becomes even more true as you add memory, vectorstore searching and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"rag-over-code\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clone a Repo\n",
    "Hopefully you already have a GitHub repository you'd like to interrogate. \n",
    "\n",
    "Otherwise, you can use the LangChain repo as a default. Plus, once we've learned how to query our files, you can ask clarifying questions about LangChain code you used here today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from git import Repo\n",
    "\n",
    "# Clone\n",
    "repo_path = \"./langchain-library\" # directory to clone the repository to \\\n",
    "    # `./` will create a sub-directory in the current working directory, named langchain-library\n",
    "\n",
    "# Clone the repo if the sub-directory above doesn't exist\n",
    "if not os.path.exists(repo_path):\n",
    "    repo = Repo.clone_from(\"https://github.com/langchain-ai/langchain\", to_path=repo_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You have 1563 document(s) in your data\n",
      "\n",
      "There are 217 characters in your sample document\n",
      "\n",
      "Here is a sample: \n",
      "\n",
      "```\n",
      "\n",
      "\"\"\"Deprecated module for BaseLanguageModel class, kept for backwards compatibility.\"\"\"\n",
      "from __future__ import annotations\n",
      "\n",
      "from langchain_core.language_models import BaseLanguageModel\n",
      "\n",
      "__all__ = [\"Bas\n",
      "\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from langchain.text_splitter import Language\n",
    "\n",
    "# Use `GenericLoader` to load Python files from the cloned repository\n",
    "load_langchain_api_docs = GenericLoader.from_filesystem(\n",
    "    repo_path + \"/libs/langchain/langchain\",\n",
    "    glob=\"**/*\",\n",
    "    suffixes=[\".py\"], # Specify a list of file types\n",
    "    parser=LanguageParser(language=Language.PYTHON, parser_threshold=500),\n",
    ")\n",
    "\n",
    "# The usual syntax for the following line is, \\\n",
    "# `data = loader.load()`, but our `GenericLoader` \\\n",
    "# is assigned to a more specific variable than 'loader'\n",
    "documents = load_langchain_api_docs.load()\n",
    "\n",
    "# Note: If you're using PyPDFLoader then it will split by page for you already\n",
    "print (f'\\nYou have {len(documents)} document(s) in your data')\n",
    "print (f'\\nThere are {len(documents[0].page_content)} characters in your sample document')\n",
    "print (f'\\nHere is a sample: \\n\\n```\\n\\n{documents[0].page_content[:200]}\\n\\n```')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Documents into Chunks\n",
    "To make effective use of our loaded documents (files) we need to split them into manageable chunks.\n",
    "\n",
    "Generally speaking, smaller chunks warrant more accurate results, but may take longer to process.\n",
    "\n",
    "### Go Deeper\n",
    "\n",
    "#### Accuracy with Smaller Chunks\n",
    "* **Increased Focus**: Smaller chunks of text or queries allow the system to focus on a more specific set of information. This specificity can lead to more accurate and relevant results because the system is not overwhelmed by too much or too broad information.\n",
    "* **Contextual Relevance**: With a narrower focus, the likelihood of retrieving information that is contextually relevant to more specific queries, enhancing the accuracy of the response.\n",
    "\n",
    "#### Processing Time\n",
    "* **Multiple Queries**: Smaller chunks might require multiple queries to cover a topic comprehensively. Each query involves a separate retrieval process, which cumulatively can take more time.\n",
    "* **Trade-off Between Depth and Breadth**: While smaller queries allow for a depth in a specific area, they might necessitate multiple rounds of retrieval to get a broad understanding, thus increasing overall processing time.\n",
    "\n",
    "#### System Limitations and Efficiency:\n",
    "* **Computational Load**: Smaller chunks means more frequent calls to the retrieval system. Depending on the efficiency of the system, this can either slow down the process due to computational load or, if the system is highly efficient, might not significantly impact the processing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Set `text_splitter` as `RecursiveCharacterTextSplitter`\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2048, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the documents\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='\"\"\"Deprecated module for BaseLanguageModel class, kept for backwards compatibility.\"\"\"\\nfrom __future__ import annotations\\n\\nfrom langchain_core.language_models import BaseLanguageModel\\n\\n__all__ = [\"BaseLanguageModel\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\base_language.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.cache import (\\n    AstraDBCache,\\n    AstraDBSemanticCache,\\n    CassandraCache,\\n    CassandraSemanticCache,\\n    FullLLMCache,\\n    FullMd5LLMCache,\\n    GPTCache,\\n    InMemoryCache,\\n    MomentoCache,\\n    RedisCache,\\n    RedisSemanticCache,\\n    SQLAlchemyCache,\\n    SQLAlchemyMd5Cache,\\n    SQLiteCache,\\n    UpstashRedisCache,\\n)\\n\\n__all__ = [\\n    \"InMemoryCache\",\\n    \"FullLLMCache\",\\n    \"SQLAlchemyCache\",\\n    \"SQLiteCache\",\\n    \"UpstashRedisCache\",\\n    \"RedisCache\",\\n    \"RedisSemanticCache\",\\n    \"GPTCache\",\\n    \"MomentoCache\",\\n    \"CassandraCache\",\\n    \"CassandraSemanticCache\",\\n    \"FullMd5LLMCache\",\\n    \"SQLAlchemyMd5Cache\",\\n    \"AstraDBCache\",\\n    \"AstraDBSemanticCache\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\cache.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='import platform\\nfrom functools import lru_cache\\n\\n\\n@lru_cache(maxsize=1)\\ndef get_runtime_environment() -> dict:\\n    \"\"\"Get information about the LangChain runtime environment.\"\"\"\\n    # Lazy import to avoid circular imports\\n    from langchain import __version__\\n\\n    return {\\n        \"library_version\": __version__,\\n        \"library\": \"langchain\",\\n        \"platform\": platform.platform(),\\n        \"runtime\": \"python\",\\n        \"runtime_version\": platform.python_version(),\\n    }' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\env.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Keep here for backwards compatibility.\"\"\"\\nfrom langchain.chains.example_generator import generate_example\\n\\n__all__ = [\"generate_example\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\example_generator.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"DEPRECATED: Kept for backwards compatibility.\"\"\"\\nfrom langchain_core.utils.formatting import StrictFormatter, formatter\\n\\n__all__ = [\"StrictFormatter\", \"formatter\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\formatting.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Interface with the LangChain Hub.\"\"\"\\nfrom __future__ import annotations\\n\\nfrom typing import TYPE_CHECKING, Any, Optional\\n\\nfrom langchain_core.load.dump import dumps\\nfrom langchain_core.load.load import loads\\n\\nif TYPE_CHECKING:\\n    from langchainhub import Client\\n\\n\\ndef _get_client(api_url: Optional[str] = None, api_key: Optional[str] = None) -> Client:\\n    try:\\n        from langchainhub import Client\\n    except ImportError as e:\\n        raise ImportError(\\n            \"Could not import langchainhub, please install with `pip install \"\\n            \"langchainhub`.\"\\n        ) from e\\n\\n    # Client logic will also attempt to load URL/key from environment variables\\n    return Client(api_url, api_key=api_key)\\n\\n\\ndef push(\\n    repo_full_name: str,\\n    object: Any,\\n    *,\\n    api_url: Optional[str] = None,\\n    api_key: Optional[str] = None,\\n    parent_commit_hash: Optional[str] = \"latest\",\\n    new_repo_is_public: bool = True,\\n    new_repo_description: str = \"\",\\n) -> str:\\n    \"\"\"\\n    Pushes an object to the hub and returns the URL it can be viewed at in a browser.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\hub.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content=':param repo_full_name: The full name of the repo to push to in the format of\\n        `owner/repo`.\\n    :param object: The LangChain to serialize and push to the hub.\\n    :param api_url: The URL of the LangChain Hub API. Defaults to the hosted API service\\n        if you have an api key set, or a localhost instance if not.\\n    :param api_key: The API key to use to authenticate with the LangChain Hub API.\\n    :param parent_commit_hash: The commit hash of the parent commit to push to. Defaults\\n        to the latest commit automatically.\\n    :param new_repo_is_public: Whether the repo should be public. Defaults to\\n        True (Public by default).\\n    :param new_repo_description: The description of the repo. Defaults to an empty\\n        string.\\n    \"\"\"\\n    client = _get_client(api_url=api_url, api_key=api_key)\\n    manifest_json = dumps(object)\\n    message = client.push(\\n        repo_full_name,\\n        manifest_json,\\n        parent_commit_hash=parent_commit_hash,\\n        new_repo_is_public=new_repo_is_public,\\n        new_repo_description=new_repo_description,\\n    )\\n    return message\\n\\n\\ndef pull(\\n    owner_repo_commit: str,\\n    *,\\n    api_url: Optional[str] = None,\\n    api_key: Optional[str] = None,\\n) -> Any:\\n    \"\"\"\\n    Pulls an object from the hub and returns it as a LangChain object.\\n\\n    :param owner_repo_commit: The full name of the repo to pull from in the format of\\n        `owner/repo:commit_hash`.\\n    :param api_url: The URL of the LangChain Hub API. Defaults to the hosted API service\\n        if you have an api key set, or a localhost instance if not.\\n    :param api_key: The API key to use to authenticate with the LangChain Hub API.\\n    \"\"\"\\n    client = _get_client(api_url=api_url, api_key=api_key)\\n    resp: str = client.pull(owner_repo_commit)\\n    return loads(resp)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\hub.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"DEPRECATED: Kept for backwards compatibility.\"\"\"\\nfrom langchain_core.utils.input import (\\n    get_bolded_text,\\n    get_color_mapping,\\n    get_colored_text,\\n    print_text,\\n)\\n\\n__all__ = [\\n    \"get_bolded_text\",\\n    \"get_color_mapping\",\\n    \"get_colored_text\",\\n    \"print_text\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\input.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Experiment with different models.\"\"\"\\nfrom __future__ import annotations\\n\\nfrom typing import List, Optional, Sequence\\n\\nfrom langchain_core.language_models.llms import BaseLLM\\nfrom langchain_core.prompts.prompt import PromptTemplate\\nfrom langchain_core.utils.input import get_color_mapping, print_text\\n\\nfrom langchain.chains.base import Chain\\nfrom langchain.chains.llm import LLMChain\\n\\n\\nclass ModelLaboratory:\\n    \"\"\"Experiment with different models.\"\"\"\\n\\n    def __init__(self, chains: Sequence[Chain], names: Optional[List[str]] = None):\\n        \"\"\"Initialize with chains to experiment with.\\n\\n        Args:\\n            chains: list of chains to experiment with.\\n        \"\"\"\\n        for chain in chains:\\n            if not isinstance(chain, Chain):\\n                raise ValueError(\\n                    \"ModelLaboratory should now be initialized with Chains. \"\\n                    \"If you want to initialize with LLMs, use the `from_llms` method \"\\n                    \"instead (`ModelLaboratory.from_llms(...)`)\"\\n                )\\n            if len(chain.input_keys) != 1:\\n                raise ValueError(\\n                    \"Currently only support chains with one input variable, \"\\n                    f\"got {chain.input_keys}\"\\n                )\\n            if len(chain.output_keys) != 1:\\n                raise ValueError(\\n                    \"Currently only support chains with one output variable, \"\\n                    f\"got {chain.output_keys}\"\\n                )\\n        if names is not None:\\n            if len(names) != len(chains):\\n                raise ValueError(\"Length of chains does not match length of names.\")\\n        self.chains = chains\\n        chain_range = [str(i) for i in range(len(self.chains))]\\n        self.chain_colors = get_color_mapping(chain_range)\\n        self.names = names\\n\\n    @classmethod\\n    def from_llms(\\n        cls, llms: List[BaseLLM], prompt: Optional[PromptTemplate] = None\\n    ) -> ModelLaboratory:\\n        \"\"\"Initialize with LLMs to experiment with and optional prompt.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\model_laboratory.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Args:\\n            llms: list of LLMs to experiment with\\n            prompt: Optional prompt to use to prompt the LLMs. Defaults to None.\\n                If a prompt was provided, it should only have one input variable.\\n        \"\"\"\\n        if prompt is None:\\n            prompt = PromptTemplate(input_variables=[\"_input\"], template=\"{_input}\")\\n        chains = [LLMChain(llm=llm, prompt=prompt) for llm in llms]\\n        names = [str(llm) for llm in llms]\\n        return cls(chains, names=names)\\n\\n    def compare(self, text: str) -> None:\\n        \"\"\"Compare model outputs on an input text.\\n\\n        If a prompt was provided with starting the laboratory, then this text will be\\n        fed into the prompt. If no prompt was provided, then the input text is the\\n        entire prompt.\\n\\n        Args:\\n            text: input text to run all models on.\\n        \"\"\"\\n        print(f\"\\\\033[1mInput:\\\\033[0m\\\\n{text}\\\\n\")\\n        for i, chain in enumerate(self.chains):\\n            if self.names is not None:\\n                name = self.names[i]\\n            else:\\n                name = str(chain)\\n            print_text(name, end=\"\\\\n\")\\n            output = chain.run(text)\\n            print_text(output, color=self.chain_colors[str(i)], end=\"\\\\n\\\\n\")' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\model_laboratory.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"For backwards compatibility.\"\"\"\\nfrom langchain_community.utilities.python import PythonREPL\\n\\n__all__ = [\"PythonREPL\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\python.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"DEPRECATED: Kept for backwards compatibility.\"\"\"\\nfrom langchain_community.utilities import Requests, RequestsWrapper, TextRequestsWrapper\\n\\n__all__ = [\\n    \"Requests\",\\n    \"RequestsWrapper\",\\n    \"TextRequestsWrapper\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\requests.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"For backwards compatibility.\"\"\"\\nfrom langchain_community.utilities.serpapi import SerpAPIWrapper\\n\\n__all__ = [\"SerpAPIWrapper\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\serpapi.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Keep here for backwards compatibility.\"\"\"\\nfrom langchain_community.utilities.sql_database import SQLDatabase\\n\\n__all__ = [\"SQLDatabase\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\sql_database.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _make_spacy_pipeline_for_splitting(\\n    pipeline: str, *, max_length: int = 1_000_000\\n) -> Any:  # avoid importing spacy\\n    try:\\n        import spacy\\n    except ImportError:\\n        raise ImportError(\\n            \"Spacy is not installed, please install it with `pip install spacy`.\"\\n        )\\n    if pipeline == \"sentencizer\":\\n        from spacy.lang.en import English\\n\\n        sentencizer = English()\\n        sentencizer.add_pipe(\"sentencizer\")\\n    else:\\n        sentencizer = spacy.load(pipeline, exclude=[\"ner\", \"tagger\"])\\n        sentencizer.max_length = max_length\\n    return sentencizer' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\text_splitter.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _split_text_with_regex(\\n    text: str, separator: str, keep_separator: bool\\n) -> List[str]:\\n    # Now that we have the separator, split the text\\n    if separator:\\n        if keep_separator:\\n            # The parentheses in the pattern keep the delimiters in the result.\\n            _splits = re.split(f\"({separator})\", text)\\n            splits = [_splits[i] + _splits[i + 1] for i in range(1, len(_splits), 2)]\\n            if len(_splits) % 2 == 0:\\n                splits += _splits[-1:]\\n            splits = [_splits[0]] + splits\\n        else:\\n            splits = re.split(separator, text)\\n    else:\\n        splits = list(text)\\n    return [s for s in splits if s != \"\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\text_splitter.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='class TextSplitter(BaseDocumentTransformer, ABC):\\n    \"\"\"Interface for splitting text into chunks.\"\"\"\\n\\n    def __init__(\\n        self,\\n        chunk_size: int = 4000,\\n        chunk_overlap: int = 200,\\n        length_function: Callable[[str], int] = len,\\n        keep_separator: bool = False,\\n        add_start_index: bool = False,\\n        strip_whitespace: bool = True,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\\n\\n        Args:\\n            chunk_size: Maximum size of chunks to return\\n            chunk_overlap: Overlap in characters between chunks\\n            length_function: Function that measures the length of given chunks\\n            keep_separator: Whether to keep the separator in the chunks\\n            add_start_index: If `True`, includes chunk\\'s start index in metadata\\n            strip_whitespace: If `True`, strips whitespace from the start and end of\\n                              every document\\n        \"\"\"\\n        if chunk_overlap > chunk_size:\\n            raise ValueError(\\n                f\"Got a larger chunk overlap ({chunk_overlap}) than chunk size \"\\n                f\"({chunk_size}), should be smaller.\"\\n            )\\n        self._chunk_size = chunk_size\\n        self._chunk_overlap = chunk_overlap\\n        self._length_function = length_function\\n        self._keep_separator = keep_separator\\n        self._add_start_index = add_start_index\\n        self._strip_whitespace = strip_whitespace\\n\\n    @abstractmethod\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split text into multiple components.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\text_splitter.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def create_documents(\\n        self, texts: List[str], metadatas: Optional[List[dict]] = None\\n    ) -> List[Document]:\\n        \"\"\"Create documents from a list of texts.\"\"\"\\n        _metadatas = metadatas or [{}] * len(texts)\\n        documents = []\\n        for i, text in enumerate(texts):\\n            index = -1\\n            for chunk in self.split_text(text):\\n                metadata = copy.deepcopy(_metadatas[i])\\n                if self._add_start_index:\\n                    index = text.find(chunk, index + 1)\\n                    metadata[\"start_index\"] = index\\n                new_doc = Document(page_content=chunk, metadata=metadata)\\n                documents.append(new_doc)\\n        return documents\\n\\n    def split_documents(self, documents: Iterable[Document]) -> List[Document]:\\n        \"\"\"Split documents.\"\"\"\\n        texts, metadatas = [], []\\n        for doc in documents:\\n            texts.append(doc.page_content)\\n            metadatas.append(doc.metadata)\\n        return self.create_documents(texts, metadatas=metadatas)\\n\\n    def _join_docs(self, docs: List[str], separator: str) -> Optional[str]:\\n        text = separator.join(docs)\\n        if self._strip_whitespace:\\n            text = text.strip()\\n        if text == \"\":\\n            return None\\n        else:\\n            return text\\n\\n    def _merge_splits(self, splits: Iterable[str], separator: str) -> List[str]:\\n        # We now want to combine these smaller pieces into medium size\\n        # chunks to send to the LLM.\\n        separator_len = self._length_function(separator)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\text_splitter.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='docs = []\\n        current_doc: List[str] = []\\n        total = 0\\n        for d in splits:\\n            _len = self._length_function(d)\\n            if (\\n                total + _len + (separator_len if len(current_doc) > 0 else 0)\\n                > self._chunk_size\\n            ):\\n                if total > self._chunk_size:\\n                    logger.warning(\\n                        f\"Created a chunk of size {total}, \"\\n                        f\"which is longer than the specified {self._chunk_size}\"\\n                    )\\n                if len(current_doc) > 0:\\n                    doc = self._join_docs(current_doc, separator)\\n                    if doc is not None:\\n                        docs.append(doc)\\n                    # Keep on popping if:\\n                    # - we have a larger chunk than in the chunk overlap\\n                    # - or if we still have any chunks and the length is long\\n                    while total > self._chunk_overlap or (\\n                        total + _len + (separator_len if len(current_doc) > 0 else 0)\\n                        > self._chunk_size\\n                        and total > 0\\n                    ):\\n                        total -= self._length_function(current_doc[0]) + (\\n                            separator_len if len(current_doc) > 1 else 0\\n                        )\\n                        current_doc = current_doc[1:]\\n            current_doc.append(d)\\n            total += _len + (separator_len if len(current_doc) > 1 else 0)\\n        doc = self._join_docs(current_doc, separator)\\n        if doc is not None:\\n            docs.append(doc)\\n        return docs\\n\\n    @classmethod\\n    def from_huggingface_tokenizer(cls, tokenizer: Any, **kwargs: Any) -> TextSplitter:\\n        \"\"\"Text splitter that uses HuggingFace tokenizer to count length.\"\"\"\\n        try:\\n            from transformers import PreTrainedTokenizerBase' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\text_splitter.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='if not isinstance(tokenizer, PreTrainedTokenizerBase):\\n                raise ValueError(\\n                    \"Tokenizer received was not an instance of PreTrainedTokenizerBase\"\\n                )\\n\\n            def _huggingface_tokenizer_length(text: str) -> int:\\n                return len(tokenizer.encode(text))\\n\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import transformers python package. \"\\n                \"Please install it with `pip install transformers`.\"\\n            )\\n        return cls(length_function=_huggingface_tokenizer_length, **kwargs)\\n\\n    @classmethod\\n    def from_tiktoken_encoder(\\n        cls: Type[TS],\\n        encoding_name: str = \"gpt2\",\\n        model_name: Optional[str] = None,\\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\\n        **kwargs: Any,\\n    ) -> TS:\\n        \"\"\"Text splitter that uses tiktoken encoder to count length.\"\"\"\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to calculate max_tokens_for_prompt. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        if model_name is not None:\\n            enc = tiktoken.encoding_for_model(model_name)\\n        else:\\n            enc = tiktoken.get_encoding(encoding_name)\\n\\n        def _tiktoken_encoder(text: str) -> int:\\n            return len(\\n                enc.encode(\\n                    text,\\n                    allowed_special=allowed_special,\\n                    disallowed_special=disallowed_special,\\n                )\\n            )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\text_splitter.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='if issubclass(cls, TokenTextSplitter):\\n            extra_kwargs = {\\n                \"encoding_name\": encoding_name,\\n                \"model_name\": model_name,\\n                \"allowed_special\": allowed_special,\\n                \"disallowed_special\": disallowed_special,\\n            }\\n            kwargs = {**kwargs, **extra_kwargs}\\n\\n        return cls(length_function=_tiktoken_encoder, **kwargs)\\n\\n    def transform_documents(\\n        self, documents: Sequence[Document], **kwargs: Any\\n    ) -> Sequence[Document]:\\n        \"\"\"Transform sequence of documents by splitting them.\"\"\"\\n        return self.split_documents(list(documents))' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\text_splitter.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='class CharacterTextSplitter(TextSplitter):\\n    \"\"\"Splitting text that looks at characters.\"\"\"\\n\\n    def __init__(\\n        self, separator: str = \"\\\\n\\\\n\", is_separator_regex: bool = False, **kwargs: Any\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._separator = separator\\n        self._is_separator_regex = is_separator_regex\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        # First we naively split the large input into a bunch of smaller ones.\\n        separator = (\\n            self._separator if self._is_separator_regex else re.escape(self._separator)\\n        )\\n        splits = _split_text_with_regex(text, separator, self._keep_separator)\\n        _separator = \"\" if self._keep_separator else self._separator\\n        return self._merge_splits(splits, _separator)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\text_splitter.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='class LineType(TypedDict):\\n    \"\"\"Line type as typed dict.\"\"\"\\n\\n    metadata: Dict[str, str]\\n    content: str' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\text_splitter.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='class HeaderType(TypedDict):\\n    \"\"\"Header type as typed dict.\"\"\"\\n\\n    level: int\\n    name: str\\n    data: str' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\text_splitter.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='class MarkdownHeaderTextSplitter:\\n    \"\"\"Splitting markdown files based on specified headers.\"\"\"\\n\\n    def __init__(\\n        self,\\n        headers_to_split_on: List[Tuple[str, str]],\\n        return_each_line: bool = False,\\n        strip_headers: bool = True,\\n    ):\\n        \"\"\"Create a new MarkdownHeaderTextSplitter.\\n\\n        Args:\\n            headers_to_split_on: Headers we want to track\\n            return_each_line: Return each line w/ associated headers\\n            strip_headers: Strip split headers from the content of the chunk\\n        \"\"\"\\n        # Output line-by-line or aggregated into chunks w/ common headers\\n        self.return_each_line = return_each_line\\n        # Given the headers we want to split on,\\n        # (e.g., \"#, ##, etc\") order by length\\n        self.headers_to_split_on = sorted(\\n            headers_to_split_on, key=lambda split: len(split[0]), reverse=True\\n        )\\n        # Strip headers split headers from the content of the chunk\\n        self.strip_headers = strip_headers\\n\\n    def aggregate_lines_to_chunks(self, lines: List[LineType]) -> List[Document]:\\n        \"\"\"Combine lines with common metadata into chunks\\n        Args:\\n            lines: Line of text / associated header metadata\\n        \"\"\"\\n        aggregated_chunks: List[LineType] = []' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\text_splitter.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='for line in lines:\\n            if (\\n                aggregated_chunks\\n                and aggregated_chunks[-1][\"metadata\"] == line[\"metadata\"]\\n            ):\\n                # If the last line in the aggregated list\\n                # has the same metadata as the current line,\\n                # append the current content to the last lines\\'s content\\n                aggregated_chunks[-1][\"content\"] += \"  \\\\n\" + line[\"content\"]\\n            elif (\\n                aggregated_chunks\\n                and aggregated_chunks[-1][\"metadata\"] != line[\"metadata\"]\\n                # may be issues if other metadata is present\\n                and len(aggregated_chunks[-1][\"metadata\"]) < len(line[\"metadata\"])\\n                and aggregated_chunks[-1][\"content\"].split(\"\\\\n\")[-1][0] == \"#\"\\n                and not self.strip_headers\\n            ):\\n                # If the last line in the aggregated list\\n                # has different metadata as the current line,\\n                # and has shallower header level than the current line,\\n                # and the last line is a header,\\n                # and we are not stripping headers,\\n                # append the current content to the last line\\'s content\\n                aggregated_chunks[-1][\"content\"] += \"  \\\\n\" + line[\"content\"]\\n                # and update the last line\\'s metadata\\n                aggregated_chunks[-1][\"metadata\"] = line[\"metadata\"]\\n            else:\\n                # Otherwise, append the current line to the aggregated list\\n                aggregated_chunks.append(line)\\n\\n        return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]\\n\\n    def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split markdown file\\n        Args:\\n            text: Markdown file\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\text_splitter.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# Split the input text by newline character (\"\\\\n\").\\n        lines = text.split(\"\\\\n\")\\n        # Final output\\n        lines_with_metadata: List[LineType] = []\\n        # Content and metadata of the chunk currently being processed\\n        current_content: List[str] = []\\n        current_metadata: Dict[str, str] = {}\\n        # Keep track of the nested header structure\\n        # header_stack: List[Dict[str, Union[int, str]]] = []\\n        header_stack: List[HeaderType] = []\\n        initial_metadata: Dict[str, str] = {}\\n\\n        in_code_block = False\\n        opening_fence = \"\"\\n\\n        for line in lines:\\n            stripped_line = line.strip()\\n\\n            if not in_code_block:\\n                # Exclude inline code spans\\n                if stripped_line.startswith(\"```\") and stripped_line.count(\"```\") == 1:\\n                    in_code_block = True\\n                    opening_fence = \"```\"\\n                elif stripped_line.startswith(\"~~~\"):\\n                    in_code_block = True\\n                    opening_fence = \"~~~\"\\n            else:\\n                if stripped_line.startswith(opening_fence):\\n                    in_code_block = False\\n                    opening_fence = \"\"\\n\\n            if in_code_block:\\n                current_content.append(stripped_line)\\n                continue' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\text_splitter.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# Check each line against each of the header types (e.g., #, ##)\\n            for sep, name in self.headers_to_split_on:\\n                # Check if line starts with a header that we intend to split on\\n                if stripped_line.startswith(sep) and (\\n                    # Header with no text OR header is followed by space\\n                    # Both are valid conditions that sep is being used a header\\n                    len(stripped_line) == len(sep) or stripped_line[len(sep)] == \" \"\\n                ):\\n                    # Ensure we are tracking the header as metadata\\n                    if name is not None:\\n                        # Get the current header level\\n                        current_header_level = sep.count(\"#\")\\n\\n                        # Pop out headers of lower or same level from the stack\\n                        while (\\n                            header_stack\\n                            and header_stack[-1][\"level\"] >= current_header_level\\n                        ):\\n                            # We have encountered a new header\\n                            # at the same or higher level\\n                            popped_header = header_stack.pop()\\n                            # Clear the metadata for the\\n                            # popped header in initial_metadata\\n                            if popped_header[\"name\"] in initial_metadata:\\n                                initial_metadata.pop(popped_header[\"name\"])\\n\\n                        # Push the current header to the stack\\n                        header: HeaderType = {\\n                            \"level\": current_header_level,\\n                            \"name\": name,\\n                            \"data\": stripped_line[len(sep) :].strip(),\\n                        }\\n                        header_stack.append(header)\\n                        # Update initial_metadata with the current header\\n                        initial_metadata[name] = header[\"data\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\text_splitter.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# Add the previous line to the lines_with_metadata\\n                    # only if current_content is not empty\\n                    if current_content:\\n                        lines_with_metadata.append(\\n                            {\\n                                \"content\": \"\\\\n\".join(current_content),\\n                                \"metadata\": current_metadata.copy(),\\n                            }\\n                        )\\n                        current_content.clear()\\n\\n                    if not self.strip_headers:\\n                        current_content.append(stripped_line)\\n\\n                    break\\n            else:\\n                if stripped_line:\\n                    current_content.append(stripped_line)\\n                elif current_content:\\n                    lines_with_metadata.append(\\n                        {\\n                            \"content\": \"\\\\n\".join(current_content),\\n                            \"metadata\": current_metadata.copy(),\\n                        }\\n                    )\\n                    current_content.clear()\\n\\n            current_metadata = initial_metadata.copy()\\n\\n        if current_content:\\n            lines_with_metadata.append(\\n                {\"content\": \"\\\\n\".join(current_content), \"metadata\": current_metadata}\\n            )\\n\\n        # lines_with_metadata has each line with associated header metadata\\n        # aggregate these into chunks based on common metadata\\n        if not self.return_each_line:\\n            return self.aggregate_lines_to_chunks(lines_with_metadata)\\n        else:\\n            return [\\n                Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n                for chunk in lines_with_metadata\\n            ]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\text_splitter.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='class ElementType(TypedDict):\\n    \"\"\"Element type as typed dict.\"\"\"\\n\\n    url: str\\n    xpath: str\\n    content: str\\n    metadata: Dict[str, str]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\text_splitter.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='class HTMLHeaderTextSplitter:\\n    \"\"\"\\n    Splitting HTML files based on specified headers.\\n    Requires lxml package.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        headers_to_split_on: List[Tuple[str, str]],\\n        return_each_element: bool = False,\\n    ):\\n        \"\"\"Create a new HTMLHeaderTextSplitter.\\n\\n        Args:\\n            headers_to_split_on: list of tuples of headers we want to track mapped to\\n                (arbitrary) keys for metadata. Allowed header values: h1, h2, h3, h4,\\n                h5, h6 e.g. [(\"h1\", \"Header 1\"), (\"h2\", \"Header 2)].\\n            return_each_element: Return each element w/ associated headers.\\n        \"\"\"\\n        # Output element-by-element or aggregated into chunks w/ common headers\\n        self.return_each_element = return_each_element\\n        self.headers_to_split_on = sorted(headers_to_split_on)\\n\\n    def aggregate_elements_to_chunks(\\n        self, elements: List[ElementType]\\n    ) -> List[Document]:\\n        \"\"\"Combine elements with common metadata into chunks\\n\\n        Args:\\n            elements: HTML element content with associated identifying info and metadata\\n        \"\"\"\\n        aggregated_chunks: List[ElementType] = []\\n\\n        for element in elements:\\n            if (\\n                aggregated_chunks\\n                and aggregated_chunks[-1][\"metadata\"] == element[\"metadata\"]\\n            ):\\n                # If the last element in the aggregated list\\n                # has the same metadata as the current element,\\n                # append the current content to the last element\\'s content\\n                aggregated_chunks[-1][\"content\"] += \"  \\\\n\" + element[\"content\"]\\n            else:\\n                # Otherwise, append the current element to the aggregated list\\n                aggregated_chunks.append(element)\\n\\n        return [\\n            Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n            for chunk in aggregated_chunks\\n        ]\\n\\n    def split_text_from_url(self, url: str) -> List[Document]:\\n        \"\"\"Split HTML from web URL' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\text_splitter.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Args:\\n            url: web URL\\n        \"\"\"\\n        r = requests.get(url)\\n        return self.split_text_from_file(BytesIO(r.content))\\n\\n    def split_text(self, text: str) -> List[Document]:\\n        \"\"\"Split HTML text string\\n\\n        Args:\\n            text: HTML text\\n        \"\"\"\\n        return self.split_text_from_file(StringIO(text))\\n\\n    def split_text_from_file(self, file: Any) -> List[Document]:\\n        \"\"\"Split HTML file\\n\\n        Args:\\n            file: HTML file\\n        \"\"\"\\n        try:\\n            from lxml import etree\\n        except ImportError as e:\\n            raise ImportError(\\n                \"Unable to import lxml, please install with `pip install lxml`.\"\\n            ) from e\\n        # use lxml library to parse html document and return xml ElementTree\\n        parser = etree.HTMLParser()\\n        tree = etree.parse(file, parser)\\n\\n        # document transformation for \"structure-aware\" chunking is handled with xsl.\\n        # see comments in html_chunks_with_headers.xslt for more detailed information.\\n        xslt_path = (\\n            pathlib.Path(__file__).parent\\n            / \"document_transformers/xsl/html_chunks_with_headers.xslt\"\\n        )\\n        xslt_tree = etree.parse(xslt_path)\\n        transform = etree.XSLT(xslt_tree)\\n        result = transform(tree)\\n        result_dom = etree.fromstring(str(result))\\n\\n        # create filter and mapping for header metadata\\n        header_filter = [header[0] for header in self.headers_to_split_on]\\n        header_mapping = dict(self.headers_to_split_on)\\n\\n        # map xhtml namespace prefix\\n        ns_map = {\"h\": \"http://www.w3.org/1999/xhtml\"}' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\text_splitter.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# build list of elements from DOM\\n        elements = []\\n        for element in result_dom.findall(\"*//*\", ns_map):\\n            if element.findall(\"*[@class=\\'headers\\']\") or element.findall(\\n                \"*[@class=\\'chunk\\']\"\\n            ):\\n                elements.append(\\n                    ElementType(\\n                        url=file,\\n                        xpath=\"\".join(\\n                            [\\n                                node.text\\n                                for node in element.findall(\"*[@class=\\'xpath\\']\", ns_map)\\n                            ]\\n                        ),\\n                        content=\"\".join(\\n                            [\\n                                node.text\\n                                for node in element.findall(\"*[@class=\\'chunk\\']\", ns_map)\\n                            ]\\n                        ),\\n                        metadata={\\n                            # Add text of specified headers to metadata using header\\n                            # mapping.\\n                            header_mapping[node.tag]: node.text\\n                            for node in filter(\\n                                lambda x: x.tag in header_filter,\\n                                element.findall(\"*[@class=\\'headers\\']/*\", ns_map),\\n                            )\\n                        },\\n                    )\\n                )\\n\\n        if not self.return_each_element:\\n            return self.aggregate_elements_to_chunks(elements)\\n        else:\\n            return [\\n                Document(page_content=chunk[\"content\"], metadata=chunk[\"metadata\"])\\n                for chunk in elements\\n            ]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\text_splitter.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='class Tokenizer:\\n    \"\"\"Tokenizer data class.\"\"\"\\n\\n    chunk_overlap: int\\n    \"\"\"Overlap in tokens between chunks\"\"\"\\n    tokens_per_chunk: int\\n    \"\"\"Maximum number of tokens per chunk\"\"\"\\n    decode: Callable[[List[int]], str]\\n    \"\"\" Function to decode a list of token ids to a string\"\"\"\\n    encode: Callable[[str], List[int]]\\n    \"\"\" Function to encode a string to a list of token ids\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\text_splitter.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def split_text_on_tokens(*, text: str, tokenizer: Tokenizer) -> List[str]:\\n    \"\"\"Split incoming text and return chunks using tokenizer.\"\"\"\\n    splits: List[str] = []\\n    input_ids = tokenizer.encode(text)\\n    start_idx = 0\\n    cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\\n    chunk_ids = input_ids[start_idx:cur_idx]\\n    while start_idx < len(input_ids):\\n        splits.append(tokenizer.decode(chunk_ids))\\n        if cur_idx == len(input_ids):\\n            break\\n        start_idx += tokenizer.tokens_per_chunk - tokenizer.chunk_overlap\\n        cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\\n        chunk_ids = input_ids[start_idx:cur_idx]\\n    return splits' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\text_splitter.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='class TokenTextSplitter(TextSplitter):\\n    \"\"\"Splitting text to tokens using model tokenizer.\"\"\"\\n\\n    def __init__(\\n        self,\\n        encoding_name: str = \"gpt2\",\\n        model_name: Optional[str] = None,\\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            import tiktoken\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import tiktoken python package. \"\\n                \"This is needed in order to for TokenTextSplitter. \"\\n                \"Please install it with `pip install tiktoken`.\"\\n            )\\n\\n        if model_name is not None:\\n            enc = tiktoken.encoding_for_model(model_name)\\n        else:\\n            enc = tiktoken.get_encoding(encoding_name)\\n        self._tokenizer = enc\\n        self._allowed_special = allowed_special\\n        self._disallowed_special = disallowed_special\\n\\n    def split_text(self, text: str) -> List[str]:\\n        def _encode(_text: str) -> List[int]:\\n            return self._tokenizer.encode(\\n                _text,\\n                allowed_special=self._allowed_special,\\n                disallowed_special=self._disallowed_special,\\n            )\\n\\n        tokenizer = Tokenizer(\\n            chunk_overlap=self._chunk_overlap,\\n            tokens_per_chunk=self._chunk_size,\\n            decode=self._tokenizer.decode,\\n            encode=_encode,\\n        )\\n\\n        return split_text_on_tokens(text=text, tokenizer=tokenizer)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\text_splitter.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='class SentenceTransformersTokenTextSplitter(TextSplitter):\\n    \"\"\"Splitting text to tokens using sentence model tokenizer.\"\"\"\\n\\n    def __init__(\\n        self,\\n        chunk_overlap: int = 50,\\n        model_name: str = \"sentence-transformers/all-mpnet-base-v2\",\\n        tokens_per_chunk: Optional[int] = None,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(**kwargs, chunk_overlap=chunk_overlap)\\n\\n        try:\\n            from sentence_transformers import SentenceTransformer\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import sentence_transformer python package. \"\\n                \"This is needed in order to for SentenceTransformersTokenTextSplitter. \"\\n                \"Please install it with `pip install sentence-transformers`.\"\\n            )\\n\\n        self.model_name = model_name\\n        self._model = SentenceTransformer(self.model_name)\\n        self.tokenizer = self._model.tokenizer\\n        self._initialize_chunk_configuration(tokens_per_chunk=tokens_per_chunk)\\n\\n    def _initialize_chunk_configuration(\\n        self, *, tokens_per_chunk: Optional[int]\\n    ) -> None:\\n        self.maximum_tokens_per_chunk = cast(int, self._model.max_seq_length)\\n\\n        if tokens_per_chunk is None:\\n            self.tokens_per_chunk = self.maximum_tokens_per_chunk\\n        else:\\n            self.tokens_per_chunk = tokens_per_chunk\\n\\n        if self.tokens_per_chunk > self.maximum_tokens_per_chunk:\\n            raise ValueError(\\n                f\"The token limit of the models \\'{self.model_name}\\'\"\\n                f\" is: {self.maximum_tokens_per_chunk}.\"\\n                f\" Argument tokens_per_chunk={self.tokens_per_chunk}\"\\n                f\" > maximum token limit.\"\\n            )\\n\\n    def split_text(self, text: str) -> List[str]:\\n        def encode_strip_start_and_stop_token_ids(text: str) -> List[int]:\\n            return self._encode(text)[1:-1]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\text_splitter.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='tokenizer = Tokenizer(\\n            chunk_overlap=self._chunk_overlap,\\n            tokens_per_chunk=self.tokens_per_chunk,\\n            decode=self.tokenizer.decode,\\n            encode=encode_strip_start_and_stop_token_ids,\\n        )\\n\\n        return split_text_on_tokens(text=text, tokenizer=tokenizer)\\n\\n    def count_tokens(self, *, text: str) -> int:\\n        return len(self._encode(text))\\n\\n    _max_length_equal_32_bit_integer: int = 2**32\\n\\n    def _encode(self, text: str) -> List[int]:\\n        token_ids_with_start_and_end_token_ids = self.tokenizer.encode(\\n            text,\\n            max_length=self._max_length_equal_32_bit_integer,\\n            truncation=\"do_not_truncate\",\\n        )\\n        return token_ids_with_start_and_end_token_ids' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\text_splitter.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='class Language(str, Enum):\\n    \"\"\"Enum of the programming languages.\"\"\"\\n\\n    CPP = \"cpp\"\\n    GO = \"go\"\\n    JAVA = \"java\"\\n    KOTLIN = \"kotlin\"\\n    JS = \"js\"\\n    TS = \"ts\"\\n    PHP = \"php\"\\n    PROTO = \"proto\"\\n    PYTHON = \"python\"\\n    RST = \"rst\"\\n    RUBY = \"ruby\"\\n    RUST = \"rust\"\\n    SCALA = \"scala\"\\n    SWIFT = \"swift\"\\n    MARKDOWN = \"markdown\"\\n    LATEX = \"latex\"\\n    HTML = \"html\"\\n    SOL = \"sol\"\\n    CSHARP = \"csharp\"\\n    COBOL = \"cobol\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\text_splitter.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='class RecursiveCharacterTextSplitter(TextSplitter):\\n    \"\"\"Splitting text by recursively look at characters.\\n\\n    Recursively tries to split by different characters to find one\\n    that works.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        separators: Optional[List[str]] = None,\\n        keep_separator: bool = True,\\n        is_separator_regex: bool = False,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Create a new TextSplitter.\"\"\"\\n        super().__init__(keep_separator=keep_separator, **kwargs)\\n        self._separators = separators or [\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"]\\n        self._is_separator_regex = is_separator_regex\\n\\n    def _split_text(self, text: str, separators: List[str]) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        final_chunks = []\\n        # Get appropriate separator to use\\n        separator = separators[-1]\\n        new_separators = []\\n        for i, _s in enumerate(separators):\\n            _separator = _s if self._is_separator_regex else re.escape(_s)\\n            if _s == \"\":\\n                separator = _s\\n                break\\n            if re.search(_separator, text):\\n                separator = _s\\n                new_separators = separators[i + 1 :]\\n                break\\n\\n        _separator = separator if self._is_separator_regex else re.escape(separator)\\n        splits = _split_text_with_regex(text, _separator, self._keep_separator)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\text_splitter.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# Now go merging things, recursively splitting longer texts.\\n        _good_splits = []\\n        _separator = \"\" if self._keep_separator else separator\\n        for s in splits:\\n            if self._length_function(s) < self._chunk_size:\\n                _good_splits.append(s)\\n            else:\\n                if _good_splits:\\n                    merged_text = self._merge_splits(_good_splits, _separator)\\n                    final_chunks.extend(merged_text)\\n                    _good_splits = []\\n                if not new_separators:\\n                    final_chunks.append(s)\\n                else:\\n                    other_info = self._split_text(s, new_separators)\\n                    final_chunks.extend(other_info)\\n        if _good_splits:\\n            merged_text = self._merge_splits(_good_splits, _separator)\\n            final_chunks.extend(merged_text)\\n        return final_chunks\\n\\n    def split_text(self, text: str) -> List[str]:\\n        return self._split_text(text, self._separators)\\n\\n    @classmethod\\n    def from_language(\\n        cls, language: Language, **kwargs: Any\\n    ) -> RecursiveCharacterTextSplitter:\\n        separators = cls.get_separators_for_language(language)\\n        return cls(separators=separators, is_separator_regex=True, **kwargs)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\text_splitter.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@staticmethod\\n    def get_separators_for_language(language: Language) -> List[str]:\\n        if language == Language.CPP:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along function definitions\\n                \"\\\\nvoid \",\\n                \"\\\\nint \",\\n                \"\\\\nfloat \",\\n                \"\\\\ndouble \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.GO:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunc \",\\n                \"\\\\nvar \",\\n                \"\\\\nconst \",\\n                \"\\\\ntype \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.JAVA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\nstatic \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.KOTLIN:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\text_splitter.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\ninternal \",\\n                \"\\\\ncompanion \",\\n                \"\\\\nfun \",\\n                \"\\\\nval \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nwhen \",\\n                \"\\\\ncase \",\\n                \"\\\\nelse \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.JS:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                \"\\\\ndefault \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.TS:\\n            return [\\n                \"\\\\nenum \",\\n                \"\\\\ninterface \",\\n                \"\\\\nnamespace \",\\n                \"\\\\ntype \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                \"\\\\ndefault \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\text_splitter.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content=']\\n        elif language == Language.PHP:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunction \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nforeach \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PROTO:\\n            return [\\n                # Split along message definitions\\n                \"\\\\nmessage \",\\n                # Split along service definitions\\n                \"\\\\nservice \",\\n                # Split along enum definitions\\n                \"\\\\nenum \",\\n                # Split along option definitions\\n                \"\\\\noption \",\\n                # Split along import statements\\n                \"\\\\nimport \",\\n                # Split along syntax declarations\\n                \"\\\\nsyntax \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.PYTHON:\\n            return [\\n                # First, try to split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\ndef \",\\n                \"\\\\n\\\\tdef \",\\n                # Now split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RST:\\n            return [\\n                # Split along section titles\\n                \"\\\\n=+\\\\n\",\\n                \"\\\\n-+\\\\n\",\\n                \"\\\\n\\\\\\\\*+\\\\n\",\\n                # Split along directive markers\\n                \"\\\\n\\\\n.. *\\\\n\\\\n\",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\text_splitter.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\",\\n            ]\\n        elif language == Language.RUBY:\\n            return [\\n                # Split along method definitions\\n                \"\\\\ndef \",\\n                \"\\\\nclass \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nunless \",\\n                \"\\\\nwhile \",\\n                \"\\\\nfor \",\\n                \"\\\\ndo \",\\n                \"\\\\nbegin \",\\n                \"\\\\nrescue \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.RUST:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfn \",\\n                \"\\\\nconst \",\\n                \"\\\\nlet \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nwhile \",\\n                \"\\\\nfor \",\\n                \"\\\\nloop \",\\n                \"\\\\nmatch \",\\n                \"\\\\nconst \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SCALA:\\n            return [\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nobject \",\\n                # Split along method definitions\\n                \"\\\\ndef \",\\n                \"\\\\nval \",\\n                \"\\\\nvar \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\nmatch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SWIFT:\\n            return [\\n                # Split along function definitions\\n                \"\\\\nfunc \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nstruct \",' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\text_splitter.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\\\\nenum \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo \",\\n                \"\\\\nswitch \",\\n                \"\\\\ncase \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.MARKDOWN:\\n            return [\\n                # First, try to split along Markdown headings (starting with level 2)\\n                \"\\\\n#{1,6} \",\\n                # Note the alternative syntax for headings (below) is not handled here\\n                # Heading level 2\\n                # ---------------\\n                # End of code block\\n                \"```\\\\n\",\\n                # Horizontal lines\\n                \"\\\\n\\\\\\\\*\\\\\\\\*\\\\\\\\*+\\\\n\",\\n                \"\\\\n---+\\\\n\",\\n                \"\\\\n___+\\\\n\",\\n                # Note that this splitter doesn\\'t handle horizontal lines defined\\n                # by *three or more* of ***, ---, or ___, but this is not handled\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.LATEX:\\n            return [\\n                # First, try to split along Latex sections\\n                \"\\\\n\\\\\\\\\\\\\\\\chapter{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\section{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\subsection{\",\\n                \"\\\\n\\\\\\\\\\\\\\\\subsubsection{\",\\n                # Now split by environments\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{enumerate}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{itemize}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{description}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{list}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{quote}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{quotation}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{verse}\",\\n                \"\\\\n\\\\\\\\\\\\\\\\begin{verbatim}\",\\n                # Now split by math environments\\n                \"\\\\n\\\\\\\\\\\\begin{align}\",\\n                \"$$\",\\n                \"$\",\\n                # Now split by the normal type of lines\\n                \" \",\\n                \"\",' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\text_splitter.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content=']\\n        elif language == Language.HTML:\\n            return [\\n                # First, try to split along HTML tags\\n                \"<body\",\\n                \"<div\",\\n                \"<p\",\\n                \"<br\",\\n                \"<li\",\\n                \"<h1\",\\n                \"<h2\",\\n                \"<h3\",\\n                \"<h4\",\\n                \"<h5\",\\n                \"<h6\",\\n                \"<span\",\\n                \"<table\",\\n                \"<tr\",\\n                \"<td\",\\n                \"<th\",\\n                \"<ul\",\\n                \"<ol\",\\n                \"<header\",\\n                \"<footer\",\\n                \"<nav\",\\n                # Head\\n                \"<head\",\\n                \"<style\",\\n                \"<script\",\\n                \"<meta\",\\n                \"<title\",\\n                \"\",\\n            ]\\n        elif language == Language.CSHARP:\\n            return [\\n                \"\\\\ninterface \",\\n                \"\\\\nenum \",\\n                \"\\\\nimplements \",\\n                \"\\\\ndelegate \",\\n                \"\\\\nevent \",\\n                # Split along class definitions\\n                \"\\\\nclass \",\\n                \"\\\\nabstract \",\\n                # Split along method definitions\\n                \"\\\\npublic \",\\n                \"\\\\nprotected \",\\n                \"\\\\nprivate \",\\n                \"\\\\nstatic \",\\n                \"\\\\nreturn \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\ncontinue \",\\n                \"\\\\nfor \",\\n                \"\\\\nforeach \",\\n                \"\\\\nwhile \",\\n                \"\\\\nswitch \",\\n                \"\\\\nbreak \",\\n                \"\\\\ncase \",\\n                \"\\\\nelse \",\\n                # Split by exceptions\\n                \"\\\\ntry \",\\n                \"\\\\nthrow \",\\n                \"\\\\nfinally \",\\n                \"\\\\ncatch \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.SOL:\\n            return [' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\text_splitter.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# Split along compiler information definitions\\n                \"\\\\npragma \",\\n                \"\\\\nusing \",\\n                # Split along contract definitions\\n                \"\\\\ncontract \",\\n                \"\\\\ninterface \",\\n                \"\\\\nlibrary \",\\n                # Split along method definitions\\n                \"\\\\nconstructor \",\\n                \"\\\\ntype \",\\n                \"\\\\nfunction \",\\n                \"\\\\nevent \",\\n                \"\\\\nmodifier \",\\n                \"\\\\nerror \",\\n                \"\\\\nstruct \",\\n                \"\\\\nenum \",\\n                # Split along control flow statements\\n                \"\\\\nif \",\\n                \"\\\\nfor \",\\n                \"\\\\nwhile \",\\n                \"\\\\ndo while \",\\n                \"\\\\nassembly \",\\n                # Split by the normal type of lines\\n                \"\\\\n\\\\n\",\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]\\n        elif language == Language.COBOL:\\n            return [\\n                # Split along divisions\\n                \"\\\\nIDENTIFICATION DIVISION.\",\\n                \"\\\\nENVIRONMENT DIVISION.\",\\n                \"\\\\nDATA DIVISION.\",\\n                \"\\\\nPROCEDURE DIVISION.\",\\n                # Split along sections within DATA DIVISION\\n                \"\\\\nWORKING-STORAGE SECTION.\",\\n                \"\\\\nLINKAGE SECTION.\",\\n                \"\\\\nFILE SECTION.\",\\n                # Split along sections within PROCEDURE DIVISION\\n                \"\\\\nINPUT-OUTPUT SECTION.\",\\n                # Split along paragraphs and common statements\\n                \"\\\\nOPEN \",\\n                \"\\\\nCLOSE \",\\n                \"\\\\nREAD \",\\n                \"\\\\nWRITE \",\\n                \"\\\\nIF \",\\n                \"\\\\nELSE \",\\n                \"\\\\nMOVE \",\\n                \"\\\\nPERFORM \",\\n                \"\\\\nUNTIL \",\\n                \"\\\\nVARYING \",\\n                \"\\\\nACCEPT \",\\n                \"\\\\nDISPLAY \",\\n                \"\\\\nSTOP RUN.\",\\n                # Split by the normal type of lines\\n                \"\\\\n\",\\n                \" \",\\n                \"\",\\n            ]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\text_splitter.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='else:\\n            raise ValueError(\\n                f\"Language {language} is not supported! \"\\n                f\"Please choose from {list(Language)}\"\\n            )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\text_splitter.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='class NLTKTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using NLTK package.\"\"\"\\n\\n    def __init__(\\n        self, separator: str = \"\\\\n\\\\n\", language: str = \"english\", **kwargs: Any\\n    ) -> None:\\n        \"\"\"Initialize the NLTK splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        try:\\n            from nltk.tokenize import sent_tokenize\\n\\n            self._tokenizer = sent_tokenize\\n        except ImportError:\\n            raise ImportError(\\n                \"NLTK is not installed, please install it with `pip install nltk`.\"\\n            )\\n        self._separator = separator\\n        self._language = language\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        # First we naively split the large input into a bunch of smaller ones.\\n        splits = self._tokenizer(text, language=self._language)\\n        return self._merge_splits(splits, self._separator)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\text_splitter.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='class SpacyTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using Spacy package.\\n\\n\\n    Per default, Spacy\\'s `en_core_web_sm` model is used and\\n    its default max_length is 1000000 (it is the length of maximum character\\n    this model takes which can be increased for large files). For a faster, but\\n    potentially less accurate splitting, you can use `pipeline=\\'sentencizer\\'`.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        separator: str = \"\\\\n\\\\n\",\\n        pipeline: str = \"en_core_web_sm\",\\n        max_length: int = 1_000_000,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Initialize the spacy text splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._tokenizer = _make_spacy_pipeline_for_splitting(\\n            pipeline, max_length=max_length\\n        )\\n        self._separator = separator\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        splits = (s.text for s in self._tokenizer(text).sents)\\n        return self._merge_splits(splits, self._separator)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\text_splitter.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='class KonlpyTextSplitter(TextSplitter):\\n    \"\"\"Splitting text using Konlpy package.\\n\\n    It is good for splitting Korean text.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        separator: str = \"\\\\n\\\\n\",\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Initialize the Konlpy text splitter.\"\"\"\\n        super().__init__(**kwargs)\\n        self._separator = separator\\n        try:\\n            from konlpy.tag import Kkma\\n        except ImportError:\\n            raise ImportError(\\n                \"\"\"\\n                Konlpy is not installed, please install it with \\n                `pip install konlpy`\\n                \"\"\"\\n            )\\n        self.kkma = Kkma()\\n\\n    def split_text(self, text: str) -> List[str]:\\n        \"\"\"Split incoming text and return chunks.\"\"\"\\n        splits = self.kkma.sentences(text)\\n        return self._merge_splits(splits, self._separator)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\text_splitter.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='class PythonCodeTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Python syntax.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a PythonCodeTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.PYTHON)\\n        super().__init__(separators=separators, **kwargs)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\text_splitter.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='class MarkdownTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Markdown-formatted headings.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a MarkdownTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.MARKDOWN)\\n        super().__init__(separators=separators, **kwargs)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\text_splitter.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='class LatexTextSplitter(RecursiveCharacterTextSplitter):\\n    \"\"\"Attempts to split the text along Latex-formatted layout elements.\"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initialize a LatexTextSplitter.\"\"\"\\n        separators = self.get_separators_for_language(Language.LATEX)\\n        super().__init__(separators=separators, **kwargs)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\text_splitter.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"**Text Splitters** are classes for splitting text.\\n\\n\\n**Class hierarchy:**\\n\\n.. code-block::\\n\\n    BaseDocumentTransformer --> TextSplitter --> <name>TextSplitter  # Example: CharacterTextSplitter\\n                                                 RecursiveCharacterTextSplitter -->  <name>TextSplitter\\n\\nNote: **MarkdownHeaderTextSplitter** and **HTMLHeaderTextSplitter do not derive from TextSplitter.\\n\\n\\n**Main helpers:**\\n\\n.. code-block::\\n\\n    Document, Tokenizer, Language, LineType, HeaderType\\n\\n\"\"\"  # noqa: E501\\n\\nfrom __future__ import annotations\\n\\nimport copy\\nimport logging\\nimport pathlib\\nimport re\\nfrom abc import ABC, abstractmethod\\nfrom dataclasses import dataclass\\nfrom enum import Enum\\nfrom io import BytesIO, StringIO\\nfrom typing import (\\n    AbstractSet,\\n    Any,\\n    Callable,\\n    Collection,\\n    Dict,\\n    Iterable,\\n    List,\\n    Literal,\\n    Optional,\\n    Sequence,\\n    Tuple,\\n    Type,\\n    TypedDict,\\n    TypeVar,\\n    Union,\\n    cast,\\n)\\n\\nimport requests\\nfrom langchain_core.documents import BaseDocumentTransformer, Document\\n\\nlogger = logging.getLogger(__name__)\\n\\nTS = TypeVar(\"TS\", bound=\"TextSplitter\")\\n\\n\\n# Code for: def _make_spacy_pipeline_for_splitting(\\n\\n\\n# Code for: def _split_text_with_regex(\\n\\n\\n# Code for: class TextSplitter(BaseDocumentTransformer, ABC):\\n\\n\\n# Code for: class CharacterTextSplitter(TextSplitter):\\n\\n\\n# Code for: class LineType(TypedDict):\\n\\n\\n# Code for: class HeaderType(TypedDict):\\n\\n\\n# Code for: class MarkdownHeaderTextSplitter:\\n\\n\\n# Code for: class ElementType(TypedDict):\\n\\n\\n# Code for: class HTMLHeaderTextSplitter:\\n\\n\\n# should be in newer Python versions (3.10+)\\n# @dataclass(frozen=True, kw_only=True, slots=True)\\n@dataclass(frozen=True)\\n# Code for: class Tokenizer:\\n\\n\\n# Code for: def split_text_on_tokens(*, text: str, tokenizer: Tokenizer) -> List[str]:\\n\\n\\n# Code for: class TokenTextSplitter(TextSplitter):\\n\\n\\n# Code for: class SentenceTransformersTokenTextSplitter(TextSplitter):\\n\\n\\n# Code for: class Language(str, Enum):\\n\\n\\n# Code for: class RecursiveCharacterTextSplitter(TextSplitter):' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\text_splitter.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# Code for: class NLTKTextSplitter(TextSplitter):\\n\\n\\n# Code for: class SpacyTextSplitter(TextSplitter):\\n\\n\\n# Code for: class KonlpyTextSplitter(TextSplitter):\\n\\n\\n# For backwards compatibility\\n# Code for: class PythonCodeTextSplitter(RecursiveCharacterTextSplitter):\\n\\n\\n# Code for: class MarkdownTextSplitter(RecursiveCharacterTextSplitter):\\n\\n\\n# Code for: class LatexTextSplitter(RecursiveCharacterTextSplitter):' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\text_splitter.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# ruff: noqa: E402\\n\"\"\"Main entrypoint into package.\"\"\"\\nimport warnings\\nfrom importlib import metadata\\nfrom typing import Any, Optional\\n\\nfrom langchain_core._api.deprecation import surface_langchain_deprecation_warnings\\n\\ntry:\\n    __version__ = metadata.version(__package__)\\nexcept metadata.PackageNotFoundError:\\n    # Case where package metadata is not available.\\n    __version__ = \"\"\\ndel metadata  # optional, avoids polluting the results of dir(__package__)\\n\\n\\ndef _warn_on_import(name: str, replacement: Optional[str] = None) -> None:\\n    \"\"\"Warn on import of deprecated module.\"\"\"\\n    from langchain.utils.interactive_env import is_interactive_env\\n\\n    if is_interactive_env():\\n        # No warnings for interactive environments.\\n        # This is done to avoid polluting the output of interactive environments\\n        # where users rely on auto-complete and may trigger this warning\\n        # even if they are not using any deprecated modules\\n        return\\n\\n    if replacement:\\n        warnings.warn(\\n            f\"Importing {name} from langchain root module is no longer supported. \"\\n            f\"Please use {replacement} instead.\"\\n        )\\n    else:\\n        warnings.warn(\\n            f\"Importing {name} from langchain root module is no longer supported.\"\\n        )\\n\\n\\n# Surfaces Deprecation and Pending Deprecation warnings from langchain.\\nsurface_langchain_deprecation_warnings()\\n\\n\\ndef __getattr__(name: str) -> Any:\\n    if name == \"MRKLChain\":\\n        from langchain.agents import MRKLChain\\n\\n        _warn_on_import(name, replacement=\"langchain.agents.MRKLChain\")\\n\\n        return MRKLChain\\n    elif name == \"ReActChain\":\\n        from langchain.agents import ReActChain\\n\\n        _warn_on_import(name, replacement=\"langchain.agents.ReActChain\")\\n\\n        return ReActChain\\n    elif name == \"SelfAskWithSearchChain\":\\n        from langchain.agents import SelfAskWithSearchChain\\n\\n        _warn_on_import(name, replacement=\"langchain.agents.SelfAskWithSearchChain\")' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='return SelfAskWithSearchChain\\n    elif name == \"ConversationChain\":\\n        from langchain.chains import ConversationChain\\n\\n        _warn_on_import(name, replacement=\"langchain.chains.ConversationChain\")\\n\\n        return ConversationChain\\n    elif name == \"LLMBashChain\":\\n        raise ImportError(\\n            \"This module has been moved to langchain-experimental. \"\\n            \"For more details: \"\\n            \"https://github.com/langchain-ai/langchain/discussions/11352.\"\\n            \"To access this code, install it with `pip install langchain-experimental`.\"\\n            \"`from langchain_experimental.llm_bash.base \"\\n            \"import LLMBashChain`\"\\n        )\\n\\n    elif name == \"LLMChain\":\\n        from langchain.chains import LLMChain\\n\\n        _warn_on_import(name, replacement=\"langchain.chains.LLMChain\")\\n\\n        return LLMChain\\n    elif name == \"LLMCheckerChain\":\\n        from langchain.chains import LLMCheckerChain\\n\\n        _warn_on_import(name, replacement=\"langchain.chains.LLMCheckerChain\")\\n\\n        return LLMCheckerChain\\n    elif name == \"LLMMathChain\":\\n        from langchain.chains import LLMMathChain\\n\\n        _warn_on_import(name, replacement=\"langchain.chains.LLMMathChain\")\\n\\n        return LLMMathChain\\n    elif name == \"QAWithSourcesChain\":\\n        from langchain.chains import QAWithSourcesChain\\n\\n        _warn_on_import(name, replacement=\"langchain.chains.QAWithSourcesChain\")\\n\\n        return QAWithSourcesChain\\n    elif name == \"VectorDBQA\":\\n        from langchain.chains import VectorDBQA\\n\\n        _warn_on_import(name, replacement=\"langchain.chains.VectorDBQA\")\\n\\n        return VectorDBQA\\n    elif name == \"VectorDBQAWithSourcesChain\":\\n        from langchain.chains import VectorDBQAWithSourcesChain\\n\\n        _warn_on_import(name, replacement=\"langchain.chains.VectorDBQAWithSourcesChain\")\\n\\n        return VectorDBQAWithSourcesChain\\n    elif name == \"InMemoryDocstore\":\\n        from langchain.docstore import InMemoryDocstore\\n\\n        _warn_on_import(name, replacement=\"langchain.docstore.InMemoryDocstore\")' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='return InMemoryDocstore\\n    elif name == \"Wikipedia\":\\n        from langchain.docstore import Wikipedia\\n\\n        _warn_on_import(name, replacement=\"langchain.docstore.Wikipedia\")\\n\\n        return Wikipedia\\n    elif name == \"Anthropic\":\\n        from langchain_community.llms import Anthropic\\n\\n        _warn_on_import(name, replacement=\"langchain_community.llms.Anthropic\")\\n\\n        return Anthropic\\n    elif name == \"Banana\":\\n        from langchain_community.llms import Banana\\n\\n        _warn_on_import(name, replacement=\"langchain_community.llms.Banana\")\\n\\n        return Banana\\n    elif name == \"CerebriumAI\":\\n        from langchain_community.llms import CerebriumAI\\n\\n        _warn_on_import(name, replacement=\"langchain_community.llms.CerebriumAI\")\\n\\n        return CerebriumAI\\n    elif name == \"Cohere\":\\n        from langchain_community.llms import Cohere\\n\\n        _warn_on_import(name, replacement=\"langchain_community.llms.Cohere\")\\n\\n        return Cohere\\n    elif name == \"ForefrontAI\":\\n        from langchain_community.llms import ForefrontAI\\n\\n        _warn_on_import(name, replacement=\"langchain_community.llms.ForefrontAI\")\\n\\n        return ForefrontAI\\n    elif name == \"GooseAI\":\\n        from langchain_community.llms import GooseAI\\n\\n        _warn_on_import(name, replacement=\"langchain_community.llms.GooseAI\")\\n\\n        return GooseAI\\n    elif name == \"HuggingFaceHub\":\\n        from langchain_community.llms import HuggingFaceHub\\n\\n        _warn_on_import(name, replacement=\"langchain_community.llms.HuggingFaceHub\")\\n\\n        return HuggingFaceHub\\n    elif name == \"HuggingFaceTextGenInference\":\\n        from langchain_community.llms import HuggingFaceTextGenInference\\n\\n        _warn_on_import(\\n            name, replacement=\"langchain_community.llms.HuggingFaceTextGenInference\"\\n        )\\n\\n        return HuggingFaceTextGenInference\\n    elif name == \"LlamaCpp\":\\n        from langchain_community.llms import LlamaCpp\\n\\n        _warn_on_import(name, replacement=\"langchain_community.llms.LlamaCpp\")' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='return LlamaCpp\\n    elif name == \"Modal\":\\n        from langchain_community.llms import Modal\\n\\n        _warn_on_import(name, replacement=\"langchain_community.llms.Modal\")\\n\\n        return Modal\\n    elif name == \"OpenAI\":\\n        from langchain_community.llms import OpenAI\\n\\n        _warn_on_import(name, replacement=\"langchain_community.llms.OpenAI\")\\n\\n        return OpenAI\\n    elif name == \"Petals\":\\n        from langchain_community.llms import Petals\\n\\n        _warn_on_import(name, replacement=\"langchain_community.llms.Petals\")\\n\\n        return Petals\\n    elif name == \"PipelineAI\":\\n        from langchain_community.llms import PipelineAI\\n\\n        _warn_on_import(name, replacement=\"langchain_community.llms.PipelineAI\")\\n\\n        return PipelineAI\\n    elif name == \"SagemakerEndpoint\":\\n        from langchain_community.llms import SagemakerEndpoint\\n\\n        _warn_on_import(name, replacement=\"langchain_community.llms.SagemakerEndpoint\")\\n\\n        return SagemakerEndpoint\\n    elif name == \"StochasticAI\":\\n        from langchain_community.llms import StochasticAI\\n\\n        _warn_on_import(name, replacement=\"langchain_community.llms.StochasticAI\")\\n\\n        return StochasticAI\\n    elif name == \"Writer\":\\n        from langchain_community.llms import Writer\\n\\n        _warn_on_import(name, replacement=\"langchain_community.llms.Writer\")\\n\\n        return Writer\\n    elif name == \"HuggingFacePipeline\":\\n        from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\\n\\n        _warn_on_import(\\n            name,\\n            replacement=\"langchain_community.llms.huggingface_pipeline.HuggingFacePipeline\",\\n        )\\n\\n        return HuggingFacePipeline\\n    elif name == \"FewShotPromptTemplate\":\\n        from langchain_core.prompts import FewShotPromptTemplate\\n\\n        _warn_on_import(name, replacement=\"langchain.prompts.FewShotPromptTemplate\")\\n\\n        return FewShotPromptTemplate\\n    elif name == \"Prompt\":\\n        from langchain.prompts import Prompt\\n\\n        _warn_on_import(name, replacement=\"langchain.prompts.Prompt\")' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='return Prompt\\n    elif name == \"PromptTemplate\":\\n        from langchain_core.prompts import PromptTemplate\\n\\n        _warn_on_import(name, replacement=\"langchain.prompts.PromptTemplate\")\\n\\n        return PromptTemplate\\n    elif name == \"BasePromptTemplate\":\\n        from langchain_core.prompts import BasePromptTemplate\\n\\n        _warn_on_import(\\n            name, replacement=\"langchain.schema.prompt_template.BasePromptTemplate\"\\n        )\\n\\n        return BasePromptTemplate\\n    elif name == \"ArxivAPIWrapper\":\\n        from langchain_community.utilities import ArxivAPIWrapper\\n\\n        _warn_on_import(\\n            name, replacement=\"langchain_community.utilities.ArxivAPIWrapper\"\\n        )\\n\\n        return ArxivAPIWrapper\\n    elif name == \"GoldenQueryAPIWrapper\":\\n        from langchain_community.utilities import GoldenQueryAPIWrapper\\n\\n        _warn_on_import(\\n            name, replacement=\"langchain_community.utilities.GoldenQueryAPIWrapper\"\\n        )\\n\\n        return GoldenQueryAPIWrapper\\n    elif name == \"GoogleSearchAPIWrapper\":\\n        from langchain_community.utilities import GoogleSearchAPIWrapper\\n\\n        _warn_on_import(\\n            name, replacement=\"langchain_community.utilities.GoogleSearchAPIWrapper\"\\n        )\\n\\n        return GoogleSearchAPIWrapper\\n    elif name == \"GoogleSerperAPIWrapper\":\\n        from langchain_community.utilities import GoogleSerperAPIWrapper\\n\\n        _warn_on_import(\\n            name, replacement=\"langchain_community.utilities.GoogleSerperAPIWrapper\"\\n        )\\n\\n        return GoogleSerperAPIWrapper\\n    elif name == \"PowerBIDataset\":\\n        from langchain_community.utilities import PowerBIDataset\\n\\n        _warn_on_import(\\n            name, replacement=\"langchain_community.utilities.PowerBIDataset\"\\n        )\\n\\n        return PowerBIDataset\\n    elif name == \"SearxSearchWrapper\":\\n        from langchain_community.utilities import SearxSearchWrapper\\n\\n        _warn_on_import(\\n            name, replacement=\"langchain_community.utilities.SearxSearchWrapper\"\\n        )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='return SearxSearchWrapper\\n    elif name == \"WikipediaAPIWrapper\":\\n        from langchain_community.utilities import WikipediaAPIWrapper\\n\\n        _warn_on_import(\\n            name, replacement=\"langchain_community.utilities.WikipediaAPIWrapper\"\\n        )\\n\\n        return WikipediaAPIWrapper\\n    elif name == \"WolframAlphaAPIWrapper\":\\n        from langchain_community.utilities import WolframAlphaAPIWrapper\\n\\n        _warn_on_import(\\n            name, replacement=\"langchain_community.utilities.WolframAlphaAPIWrapper\"\\n        )\\n\\n        return WolframAlphaAPIWrapper\\n    elif name == \"SQLDatabase\":\\n        from langchain_community.utilities import SQLDatabase\\n\\n        _warn_on_import(name, replacement=\"langchain_community.utilities.SQLDatabase\")\\n\\n        return SQLDatabase\\n    elif name == \"FAISS\":\\n        from langchain_community.vectorstores import FAISS\\n\\n        _warn_on_import(name, replacement=\"langchain_community.vectorstores.FAISS\")\\n\\n        return FAISS\\n    elif name == \"ElasticVectorSearch\":\\n        from langchain_community.vectorstores import ElasticVectorSearch\\n\\n        _warn_on_import(\\n            name, replacement=\"langchain_community.vectorstores.ElasticVectorSearch\"\\n        )\\n\\n        return ElasticVectorSearch\\n    # For backwards compatibility\\n    elif name == \"SerpAPIChain\" or name == \"SerpAPIWrapper\":\\n        from langchain_community.utilities import SerpAPIWrapper\\n\\n        _warn_on_import(\\n            name, replacement=\"langchain_community.utilities.SerpAPIWrapper\"\\n        )\\n\\n        return SerpAPIWrapper\\n    elif name == \"verbose\":\\n        from langchain.globals import _verbose\\n\\n        _warn_on_import(\\n            name,\\n            replacement=(\\n                \"langchain.globals.set_verbose() / langchain.globals.get_verbose()\"\\n            ),\\n        )\\n\\n        return _verbose\\n    elif name == \"debug\":\\n        from langchain.globals import _debug' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='_warn_on_import(\\n            name,\\n            replacement=(\\n                \"langchain.globals.set_debug() / langchain.globals.get_debug()\"\\n            ),\\n        )\\n\\n        return _debug\\n    elif name == \"llm_cache\":\\n        from langchain.globals import _llm_cache\\n\\n        _warn_on_import(\\n            name,\\n            replacement=(\\n                \"langchain.globals.set_llm_cache() / langchain.globals.get_llm_cache()\"\\n            ),\\n        )\\n\\n        return _llm_cache\\n    else:\\n        raise AttributeError(f\"Could not find: {name}\")\\n\\n\\n__all__ = [\\n    \"LLMChain\",\\n    \"LLMCheckerChain\",\\n    \"LLMMathChain\",\\n    \"ArxivAPIWrapper\",\\n    \"GoldenQueryAPIWrapper\",\\n    \"SelfAskWithSearchChain\",\\n    \"SerpAPIWrapper\",\\n    \"SerpAPIChain\",\\n    \"SearxSearchWrapper\",\\n    \"GoogleSearchAPIWrapper\",\\n    \"GoogleSerperAPIWrapper\",\\n    \"WolframAlphaAPIWrapper\",\\n    \"WikipediaAPIWrapper\",\\n    \"Anthropic\",\\n    \"Banana\",\\n    \"CerebriumAI\",\\n    \"Cohere\",\\n    \"ForefrontAI\",\\n    \"GooseAI\",\\n    \"Modal\",\\n    \"OpenAI\",\\n    \"Petals\",\\n    \"PipelineAI\",\\n    \"StochasticAI\",\\n    \"Writer\",\\n    \"BasePromptTemplate\",\\n    \"Prompt\",\\n    \"FewShotPromptTemplate\",\\n    \"PromptTemplate\",\\n    \"ReActChain\",\\n    \"Wikipedia\",\\n    \"HuggingFaceHub\",\\n    \"SagemakerEndpoint\",\\n    \"HuggingFacePipeline\",\\n    \"SQLDatabase\",\\n    \"PowerBIDataset\",\\n    \"FAISS\",\\n    \"MRKLChain\",\\n    \"VectorDBQA\",\\n    \"ElasticVectorSearch\",\\n    \"InMemoryDocstore\",\\n    \"ConversationChain\",\\n    \"VectorDBQAWithSourcesChain\",\\n    \"QAWithSourcesChain\",\\n    \"LlamaCpp\",\\n    \"HuggingFaceTextGenInference\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.adapters.openai import (\\n    Chat,\\n    ChatCompletion,\\n    ChatCompletionChunk,\\n    ChatCompletions,\\n    Choice,\\n    ChoiceChunk,\\n    Completions,\\n    IndexableBaseModel,\\n    chat,\\n    convert_dict_to_message,\\n    convert_message_to_dict,\\n    convert_messages_for_finetuning,\\n    convert_openai_messages,\\n)\\n\\n__all__ = [\\n    \"IndexableBaseModel\",\\n    \"Choice\",\\n    \"ChatCompletions\",\\n    \"ChoiceChunk\",\\n    \"ChatCompletionChunk\",\\n    \"convert_dict_to_message\",\\n    \"convert_message_to_dict\",\\n    \"convert_openai_messages\",\\n    \"ChatCompletion\",\\n    \"convert_messages_for_finetuning\",\\n    \"Completions\",\\n    \"Chat\",\\n    \"chat\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\adapters\\\\openai.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='class BaseSingleActionAgent(BaseModel):\\n    \"\"\"Base Single Action Agent class.\"\"\"\\n\\n    @property\\n    def return_values(self) -> List[str]:\\n        \"\"\"Return values of the agent.\"\"\"\\n        return [\"output\"]\\n\\n    def get_allowed_tools(self) -> Optional[List[str]]:\\n        return None\\n\\n    @abstractmethod\\n    def plan(\\n        self,\\n        intermediate_steps: List[Tuple[AgentAction, str]],\\n        callbacks: Callbacks = None,\\n        **kwargs: Any,\\n    ) -> Union[AgentAction, AgentFinish]:\\n        \"\"\"Given input, decided what to do.\\n\\n        Args:\\n            intermediate_steps: Steps the LLM has taken to date,\\n                along with observations\\n            callbacks: Callbacks to run.\\n            **kwargs: User inputs.\\n\\n        Returns:\\n            Action specifying what tool to use.\\n        \"\"\"\\n\\n    @abstractmethod\\n    async def aplan(\\n        self,\\n        intermediate_steps: List[Tuple[AgentAction, str]],\\n        callbacks: Callbacks = None,\\n        **kwargs: Any,\\n    ) -> Union[AgentAction, AgentFinish]:\\n        \"\"\"Given input, decided what to do.\\n\\n        Args:\\n            intermediate_steps: Steps the LLM has taken to date,\\n                along with observations\\n            callbacks: Callbacks to run.\\n            **kwargs: User inputs.\\n\\n        Returns:\\n            Action specifying what tool to use.\\n        \"\"\"\\n\\n    @property\\n    @abstractmethod\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Return the input keys.\\n\\n        :meta private:\\n        \"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def return_stopped_response(\\n        self,\\n        early_stopping_method: str,\\n        intermediate_steps: List[Tuple[AgentAction, str]],\\n        **kwargs: Any,\\n    ) -> AgentFinish:\\n        \"\"\"Return response when agent has been stopped due to max iterations.\"\"\"\\n        if early_stopping_method == \"force\":\\n            # `force` just returns a constant string\\n            return AgentFinish(\\n                {\"output\": \"Agent stopped due to iteration limit or time limit.\"}, \"\"\\n            )\\n        else:\\n            raise ValueError(\\n                f\"Got unsupported early_stopping_method `{early_stopping_method}`\"\\n            )\\n\\n    @classmethod\\n    def from_llm_and_tools(\\n        cls,\\n        llm: BaseLanguageModel,\\n        tools: Sequence[BaseTool],\\n        callback_manager: Optional[BaseCallbackManager] = None,\\n        **kwargs: Any,\\n    ) -> BaseSingleActionAgent:\\n        raise NotImplementedError\\n\\n    @property\\n    def _agent_type(self) -> str:\\n        \"\"\"Return Identifier of agent type.\"\"\"\\n        raise NotImplementedError\\n\\n    def dict(self, **kwargs: Any) -> Dict:\\n        \"\"\"Return dictionary representation of agent.\"\"\"\\n        _dict = super().dict()\\n        try:\\n            _type = self._agent_type\\n        except NotImplementedError:\\n            _type = None\\n        if isinstance(_type, AgentType):\\n            _dict[\"_type\"] = str(_type.value)\\n        elif _type is not None:\\n            _dict[\"_type\"] = _type\\n        return _dict\\n\\n    def save(self, file_path: Union[Path, str]) -> None:\\n        \"\"\"Save the agent.\\n\\n        Args:\\n            file_path: Path to file to save the agent to.\\n\\n        Example:\\n        .. code-block:: python\\n\\n            # If working with agent executor\\n            agent.agent.save(file_path=\"path/agent.yaml\")\\n        \"\"\"\\n        # Convert file to Path object.\\n        if isinstance(file_path, str):\\n            save_path = Path(file_path)\\n        else:\\n            save_path = file_path' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='directory_path = save_path.parent\\n        directory_path.mkdir(parents=True, exist_ok=True)\\n\\n        # Fetch dictionary to save\\n        agent_dict = self.dict()\\n        if \"_type\" not in agent_dict:\\n            raise NotImplementedError(f\"Agent {self} does not support saving\")\\n\\n        if save_path.suffix == \".json\":\\n            with open(file_path, \"w\") as f:\\n                json.dump(agent_dict, f, indent=4)\\n        elif save_path.suffix == \".yaml\":\\n            with open(file_path, \"w\") as f:\\n                yaml.dump(agent_dict, f, default_flow_style=False)\\n        else:\\n            raise ValueError(f\"{save_path} must be json or yaml\")\\n\\n    def tool_run_logging_kwargs(self) -> Dict:\\n        return {}' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='class BaseMultiActionAgent(BaseModel):\\n    \"\"\"Base Multi Action Agent class.\"\"\"\\n\\n    @property\\n    def return_values(self) -> List[str]:\\n        \"\"\"Return values of the agent.\"\"\"\\n        return [\"output\"]\\n\\n    def get_allowed_tools(self) -> Optional[List[str]]:\\n        return None\\n\\n    @abstractmethod\\n    def plan(\\n        self,\\n        intermediate_steps: List[Tuple[AgentAction, str]],\\n        callbacks: Callbacks = None,\\n        **kwargs: Any,\\n    ) -> Union[List[AgentAction], AgentFinish]:\\n        \"\"\"Given input, decided what to do.\\n\\n        Args:\\n            intermediate_steps: Steps the LLM has taken to date,\\n                along with the observations.\\n            callbacks: Callbacks to run.\\n            **kwargs: User inputs.\\n\\n        Returns:\\n            Actions specifying what tool to use.\\n        \"\"\"\\n\\n    @abstractmethod\\n    async def aplan(\\n        self,\\n        intermediate_steps: List[Tuple[AgentAction, str]],\\n        callbacks: Callbacks = None,\\n        **kwargs: Any,\\n    ) -> Union[List[AgentAction], AgentFinish]:\\n        \"\"\"Given input, decided what to do.\\n\\n        Args:\\n            intermediate_steps: Steps the LLM has taken to date,\\n                along with the observations.\\n            callbacks: Callbacks to run.\\n            **kwargs: User inputs.\\n\\n        Returns:\\n            Actions specifying what tool to use.\\n        \"\"\"\\n\\n    @property\\n    @abstractmethod\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Return the input keys.\\n\\n        :meta private:\\n        \"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def return_stopped_response(\\n        self,\\n        early_stopping_method: str,\\n        intermediate_steps: List[Tuple[AgentAction, str]],\\n        **kwargs: Any,\\n    ) -> AgentFinish:\\n        \"\"\"Return response when agent has been stopped due to max iterations.\"\"\"\\n        if early_stopping_method == \"force\":\\n            # `force` just returns a constant string\\n            return AgentFinish({\"output\": \"Agent stopped due to max iterations.\"}, \"\")\\n        else:\\n            raise ValueError(\\n                f\"Got unsupported early_stopping_method `{early_stopping_method}`\"\\n            )\\n\\n    @property\\n    def _agent_type(self) -> str:\\n        \"\"\"Return Identifier of agent type.\"\"\"\\n        raise NotImplementedError\\n\\n    def dict(self, **kwargs: Any) -> Dict:\\n        \"\"\"Return dictionary representation of agent.\"\"\"\\n        _dict = super().dict()\\n        try:\\n            _dict[\"_type\"] = str(self._agent_type)\\n        except NotImplementedError:\\n            pass\\n        return _dict\\n\\n    def save(self, file_path: Union[Path, str]) -> None:\\n        \"\"\"Save the agent.\\n\\n        Args:\\n            file_path: Path to file to save the agent to.\\n\\n        Example:\\n        .. code-block:: python\\n\\n            # If working with agent executor\\n            agent.agent.save(file_path=\"path/agent.yaml\")\\n        \"\"\"\\n        # Convert file to Path object.\\n        if isinstance(file_path, str):\\n            save_path = Path(file_path)\\n        else:\\n            save_path = file_path\\n\\n        # Fetch dictionary to save\\n        agent_dict = self.dict()\\n        if \"_type\" not in agent_dict:\\n            raise NotImplementedError(f\"Agent {self} does not support saving.\")\\n\\n        directory_path = save_path.parent\\n        directory_path.mkdir(parents=True, exist_ok=True)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='if save_path.suffix == \".json\":\\n            with open(file_path, \"w\") as f:\\n                json.dump(agent_dict, f, indent=4)\\n        elif save_path.suffix == \".yaml\":\\n            with open(file_path, \"w\") as f:\\n                yaml.dump(agent_dict, f, default_flow_style=False)\\n        else:\\n            raise ValueError(f\"{save_path} must be json or yaml\")\\n\\n    def tool_run_logging_kwargs(self) -> Dict:\\n        return {}' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='class AgentOutputParser(BaseOutputParser[Union[AgentAction, AgentFinish]]):\\n    \"\"\"Base class for parsing agent output into agent action/finish.\"\"\"\\n\\n    @abstractmethod\\n    def parse(self, text: str) -> Union[AgentAction, AgentFinish]:\\n        \"\"\"Parse text into agent action/finish.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='class MultiActionAgentOutputParser(\\n    BaseOutputParser[Union[List[AgentAction], AgentFinish]]\\n):\\n    \"\"\"Base class for parsing agent output into agent actions/finish.\"\"\"\\n\\n    @abstractmethod\\n    def parse(self, text: str) -> Union[List[AgentAction], AgentFinish]:\\n        \"\"\"Parse text into agent actions/finish.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='class RunnableAgent(BaseSingleActionAgent):\\n    \"\"\"Agent powered by runnables.\"\"\"\\n\\n    runnable: Runnable[dict, Union[AgentAction, AgentFinish]]\\n    \"\"\"Runnable to call to get agent action.\"\"\"\\n    input_keys_arg: List[str] = []\\n    return_keys_arg: List[str] = []\\n\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n\\n        arbitrary_types_allowed = True\\n\\n    @property\\n    def return_values(self) -> List[str]:\\n        \"\"\"Return values of the agent.\"\"\"\\n        return self.return_keys_arg\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        return self.input_keys_arg\\n\\n    def plan(\\n        self,\\n        intermediate_steps: List[Tuple[AgentAction, str]],\\n        callbacks: Callbacks = None,\\n        **kwargs: Any,\\n    ) -> Union[AgentAction, AgentFinish]:\\n        \"\"\"Based on past history and current inputs, decide what to do.\\n\\n        Args:\\n            intermediate_steps: Steps the LLM has taken to date,\\n                along with the observations.\\n            callbacks: Callbacks to run.\\n            **kwargs: User inputs.\\n\\n        Returns:\\n            Action specifying what tool to use.\\n        \"\"\"\\n        inputs = {**kwargs, **{\"intermediate_steps\": intermediate_steps}}\\n        # Use streaming to make sure that the underlying LLM is invoked in a streaming\\n        # fashion to make it possible to get access to the individual LLM tokens\\n        # when using stream_log with the Agent Executor.\\n        # Because the response from the plan is not a generator, we need to\\n        # accumulate the output into final output and return that.\\n        final_output: Any = None\\n        for chunk in self.runnable.stream(inputs, config={\"callbacks\": callbacks}):\\n            if final_output is None:\\n                final_output = chunk\\n            else:\\n                final_output += chunk\\n\\n        return final_output' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='async def aplan(\\n        self,\\n        intermediate_steps: List[Tuple[AgentAction, str]],\\n        callbacks: Callbacks = None,\\n        **kwargs: Any,\\n    ) -> Union[\\n        AgentAction,\\n        AgentFinish,\\n    ]:\\n        \"\"\"Based on past history and current inputs, decide what to do.\\n\\n        Args:\\n            intermediate_steps: Steps the LLM has taken to date,\\n                along with observations\\n            callbacks: Callbacks to run.\\n            **kwargs: User inputs\\n\\n        Returns:\\n            Action specifying what tool to use.\\n        \"\"\"\\n        inputs = {**kwargs, **{\"intermediate_steps\": intermediate_steps}}\\n        final_output: Any = None\\n        # Use streaming to make sure that the underlying LLM is invoked in a streaming\\n        # fashion to make it possible to get access to the individual LLM tokens\\n        # when using stream_log with the Agent Executor.\\n        # Because the response from the plan is not a generator, we need to\\n        # accumulate the output into final output and return that.\\n        async for chunk in self.runnable.astream(\\n            inputs, config={\"callbacks\": callbacks}\\n        ):\\n            if final_output is None:\\n                final_output = chunk\\n            else:\\n                final_output += chunk\\n        return final_output' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='class RunnableMultiActionAgent(BaseMultiActionAgent):\\n    \"\"\"Agent powered by runnables.\"\"\"\\n\\n    runnable: Runnable[dict, Union[List[AgentAction], AgentFinish]]\\n    \"\"\"Runnable to call to get agent actions.\"\"\"\\n    input_keys_arg: List[str] = []\\n    return_keys_arg: List[str] = []\\n\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n\\n        arbitrary_types_allowed = True\\n\\n    @property\\n    def return_values(self) -> List[str]:\\n        \"\"\"Return values of the agent.\"\"\"\\n        return self.return_keys_arg\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Return the input keys.\\n\\n        Returns:\\n            List of input keys.\\n        \"\"\"\\n        return self.input_keys_arg\\n\\n    def plan(\\n        self,\\n        intermediate_steps: List[Tuple[AgentAction, str]],\\n        callbacks: Callbacks = None,\\n        **kwargs: Any,\\n    ) -> Union[\\n        List[AgentAction],\\n        AgentFinish,\\n    ]:\\n        \"\"\"Based on past history and current inputs, decide what to do.\\n\\n        Args:\\n            intermediate_steps: Steps the LLM has taken to date,\\n                along with the observations.\\n            callbacks: Callbacks to run.\\n            **kwargs: User inputs.\\n\\n        Returns:\\n            Action specifying what tool to use.\\n        \"\"\"\\n        inputs = {**kwargs, **{\"intermediate_steps\": intermediate_steps}}\\n        # Use streaming to make sure that the underlying LLM is invoked in a streaming\\n        # fashion to make it possible to get access to the individual LLM tokens\\n        # when using stream_log with the Agent Executor.\\n        # Because the response from the plan is not a generator, we need to\\n        # accumulate the output into final output and return that.\\n        final_output: Any = None\\n        for chunk in self.runnable.stream(inputs, config={\"callbacks\": callbacks}):\\n            if final_output is None:\\n                final_output = chunk\\n            else:\\n                final_output += chunk\\n\\n        return final_output' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='async def aplan(\\n        self,\\n        intermediate_steps: List[Tuple[AgentAction, str]],\\n        callbacks: Callbacks = None,\\n        **kwargs: Any,\\n    ) -> Union[\\n        List[AgentAction],\\n        AgentFinish,\\n    ]:\\n        \"\"\"Based on past history and current inputs, decide what to do.\\n\\n        Args:\\n            intermediate_steps: Steps the LLM has taken to date,\\n                along with observations\\n            callbacks: Callbacks to run.\\n            **kwargs: User inputs.\\n\\n        Returns:\\n            Action specifying what tool to use.\\n        \"\"\"\\n        inputs = {**kwargs, **{\"intermediate_steps\": intermediate_steps}}\\n        # Use streaming to make sure that the underlying LLM is invoked in a streaming\\n        # fashion to make it possible to get access to the individual LLM tokens\\n        # when using stream_log with the Agent Executor.\\n        # Because the response from the plan is not a generator, we need to\\n        # accumulate the output into final output and return that.\\n        final_output: Any = None\\n        async for chunk in self.runnable.astream(\\n            inputs, config={\"callbacks\": callbacks}\\n        ):\\n            if final_output is None:\\n                final_output = chunk\\n            else:\\n                final_output += chunk\\n\\n        return final_output' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='class LLMSingleActionAgent(BaseSingleActionAgent):\\n    \"\"\"Base class for single action agents.\"\"\"\\n\\n    llm_chain: LLMChain\\n    \"\"\"LLMChain to use for agent.\"\"\"\\n    output_parser: AgentOutputParser\\n    \"\"\"Output parser to use for agent.\"\"\"\\n    stop: List[str]\\n    \"\"\"List of strings to stop on.\"\"\"\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Return the input keys.\\n\\n        Returns:\\n            List of input keys.\\n        \"\"\"\\n        return list(set(self.llm_chain.input_keys) - {\"intermediate_steps\"})\\n\\n    def dict(self, **kwargs: Any) -> Dict:\\n        \"\"\"Return dictionary representation of agent.\"\"\"\\n        _dict = super().dict()\\n        del _dict[\"output_parser\"]\\n        return _dict\\n\\n    def plan(\\n        self,\\n        intermediate_steps: List[Tuple[AgentAction, str]],\\n        callbacks: Callbacks = None,\\n        **kwargs: Any,\\n    ) -> Union[AgentAction, AgentFinish]:\\n        \"\"\"Given input, decided what to do.\\n\\n        Args:\\n            intermediate_steps: Steps the LLM has taken to date,\\n                along with the observations.\\n            callbacks: Callbacks to run.\\n            **kwargs: User inputs.\\n\\n        Returns:\\n            Action specifying what tool to use.\\n        \"\"\"\\n        output = self.llm_chain.run(\\n            intermediate_steps=intermediate_steps,\\n            stop=self.stop,\\n            callbacks=callbacks,\\n            **kwargs,\\n        )\\n        return self.output_parser.parse(output)\\n\\n    async def aplan(\\n        self,\\n        intermediate_steps: List[Tuple[AgentAction, str]],\\n        callbacks: Callbacks = None,\\n        **kwargs: Any,\\n    ) -> Union[AgentAction, AgentFinish]:\\n        \"\"\"Given input, decided what to do.\\n\\n        Args:\\n            intermediate_steps: Steps the LLM has taken to date,\\n                along with observations\\n            callbacks: Callbacks to run.\\n            **kwargs: User inputs.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Returns:\\n            Action specifying what tool to use.\\n        \"\"\"\\n        output = await self.llm_chain.arun(\\n            intermediate_steps=intermediate_steps,\\n            stop=self.stop,\\n            callbacks=callbacks,\\n            **kwargs,\\n        )\\n        return self.output_parser.parse(output)\\n\\n    def tool_run_logging_kwargs(self) -> Dict:\\n        return {\\n            \"llm_prefix\": \"\",\\n            \"observation_prefix\": \"\" if len(self.stop) == 0 else self.stop[0],\\n        }' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='class Agent(BaseSingleActionAgent):\\n    \"\"\"Agent that calls the language model and deciding the action.\\n\\n    This is driven by an LLMChain. The prompt in the LLMChain MUST include\\n    a variable called \"agent_scratchpad\" where the agent can put its\\n    intermediary work.\\n    \"\"\"\\n\\n    llm_chain: LLMChain\\n    output_parser: AgentOutputParser\\n    allowed_tools: Optional[List[str]] = None\\n\\n    def dict(self, **kwargs: Any) -> Dict:\\n        \"\"\"Return dictionary representation of agent.\"\"\"\\n        _dict = super().dict()\\n        del _dict[\"output_parser\"]\\n        return _dict\\n\\n    def get_allowed_tools(self) -> Optional[List[str]]:\\n        return self.allowed_tools\\n\\n    @property\\n    def return_values(self) -> List[str]:\\n        return [\"output\"]\\n\\n    def _fix_text(self, text: str) -> str:\\n        \"\"\"Fix the text.\"\"\"\\n        raise ValueError(\"fix_text not implemented for this agent.\")\\n\\n    @property\\n    def _stop(self) -> List[str]:\\n        return [\\n            f\"\\\\n{self.observation_prefix.rstrip()}\",\\n            f\"\\\\n\\\\t{self.observation_prefix.rstrip()}\",\\n        ]\\n\\n    def _construct_scratchpad(\\n        self, intermediate_steps: List[Tuple[AgentAction, str]]\\n    ) -> Union[str, List[BaseMessage]]:\\n        \"\"\"Construct the scratchpad that lets the agent continue its thought process.\"\"\"\\n        thoughts = \"\"\\n        for action, observation in intermediate_steps:\\n            thoughts += action.log\\n            thoughts += f\"\\\\n{self.observation_prefix}{observation}\\\\n{self.llm_prefix}\"\\n        return thoughts\\n\\n    def plan(\\n        self,\\n        intermediate_steps: List[Tuple[AgentAction, str]],\\n        callbacks: Callbacks = None,\\n        **kwargs: Any,\\n    ) -> Union[AgentAction, AgentFinish]:\\n        \"\"\"Given input, decided what to do.\\n\\n        Args:\\n            intermediate_steps: Steps the LLM has taken to date,\\n                along with observations\\n            callbacks: Callbacks to run.\\n            **kwargs: User inputs.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Returns:\\n            Action specifying what tool to use.\\n        \"\"\"\\n        full_inputs = self.get_full_inputs(intermediate_steps, **kwargs)\\n        full_output = self.llm_chain.predict(callbacks=callbacks, **full_inputs)\\n        return self.output_parser.parse(full_output)\\n\\n    async def aplan(\\n        self,\\n        intermediate_steps: List[Tuple[AgentAction, str]],\\n        callbacks: Callbacks = None,\\n        **kwargs: Any,\\n    ) -> Union[AgentAction, AgentFinish]:\\n        \"\"\"Given input, decided what to do.\\n\\n        Args:\\n            intermediate_steps: Steps the LLM has taken to date,\\n                along with observations\\n            callbacks: Callbacks to run.\\n            **kwargs: User inputs.\\n\\n        Returns:\\n            Action specifying what tool to use.\\n        \"\"\"\\n        full_inputs = self.get_full_inputs(intermediate_steps, **kwargs)\\n        full_output = await self.llm_chain.apredict(callbacks=callbacks, **full_inputs)\\n        agent_output = await self.output_parser.aparse(full_output)\\n        return agent_output\\n\\n    def get_full_inputs(\\n        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any\\n    ) -> Dict[str, Any]:\\n        \"\"\"Create the full inputs for the LLMChain from intermediate steps.\"\"\"\\n        thoughts = self._construct_scratchpad(intermediate_steps)\\n        new_inputs = {\"agent_scratchpad\": thoughts, \"stop\": self._stop}\\n        full_inputs = {**kwargs, **new_inputs}\\n        return full_inputs\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Return the input keys.\\n\\n        :meta private:\\n        \"\"\"\\n        return list(set(self.llm_chain.input_keys) - {\"agent_scratchpad\"})' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@root_validator()\\n    def validate_prompt(cls, values: Dict) -> Dict:\\n        \"\"\"Validate that prompt matches format.\"\"\"\\n        prompt = values[\"llm_chain\"].prompt\\n        if \"agent_scratchpad\" not in prompt.input_variables:\\n            logger.warning(\\n                \"`agent_scratchpad` should be a variable in prompt.input_variables.\"\\n                \" Did not find it, so adding it at the end.\"\\n            )\\n            prompt.input_variables.append(\"agent_scratchpad\")\\n            if isinstance(prompt, PromptTemplate):\\n                prompt.template += \"\\\\n{agent_scratchpad}\"\\n            elif isinstance(prompt, FewShotPromptTemplate):\\n                prompt.suffix += \"\\\\n{agent_scratchpad}\"\\n            else:\\n                raise ValueError(f\"Got unexpected prompt type {type(prompt)}\")\\n        return values\\n\\n    @property\\n    @abstractmethod\\n    def observation_prefix(self) -> str:\\n        \"\"\"Prefix to append the observation with.\"\"\"\\n\\n    @property\\n    @abstractmethod\\n    def llm_prefix(self) -> str:\\n        \"\"\"Prefix to append the LLM call with.\"\"\"\\n\\n    @classmethod\\n    @abstractmethod\\n    def create_prompt(cls, tools: Sequence[BaseTool]) -> BasePromptTemplate:\\n        \"\"\"Create a prompt for this class.\"\"\"\\n\\n    @classmethod\\n    def _validate_tools(cls, tools: Sequence[BaseTool]) -> None:\\n        \"\"\"Validate that appropriate tools are passed in.\"\"\"\\n        pass\\n\\n    @classmethod\\n    @abstractmethod\\n    def _get_default_output_parser(cls, **kwargs: Any) -> AgentOutputParser:\\n        \"\"\"Get default output parser for this class.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@classmethod\\n    def from_llm_and_tools(\\n        cls,\\n        llm: BaseLanguageModel,\\n        tools: Sequence[BaseTool],\\n        callback_manager: Optional[BaseCallbackManager] = None,\\n        output_parser: Optional[AgentOutputParser] = None,\\n        **kwargs: Any,\\n    ) -> Agent:\\n        \"\"\"Construct an agent from an LLM and tools.\"\"\"\\n        cls._validate_tools(tools)\\n        llm_chain = LLMChain(\\n            llm=llm,\\n            prompt=cls.create_prompt(tools),\\n            callback_manager=callback_manager,\\n        )\\n        tool_names = [tool.name for tool in tools]\\n        _output_parser = output_parser or cls._get_default_output_parser()\\n        return cls(\\n            llm_chain=llm_chain,\\n            allowed_tools=tool_names,\\n            output_parser=_output_parser,\\n            **kwargs,\\n        )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def return_stopped_response(\\n        self,\\n        early_stopping_method: str,\\n        intermediate_steps: List[Tuple[AgentAction, str]],\\n        **kwargs: Any,\\n    ) -> AgentFinish:\\n        \"\"\"Return response when agent has been stopped due to max iterations.\"\"\"\\n        if early_stopping_method == \"force\":\\n            # `force` just returns a constant string\\n            return AgentFinish(\\n                {\"output\": \"Agent stopped due to iteration limit or time limit.\"}, \"\"\\n            )\\n        elif early_stopping_method == \"generate\":\\n            # Generate does one final forward pass\\n            thoughts = \"\"\\n            for action, observation in intermediate_steps:\\n                thoughts += action.log\\n                thoughts += (\\n                    f\"\\\\n{self.observation_prefix}{observation}\\\\n{self.llm_prefix}\"\\n                )\\n            # Adding to the previous steps, we now tell the LLM to make a final pred\\n            thoughts += (\\n                \"\\\\n\\\\nI now need to return a final answer based on the previous steps:\"\\n            )\\n            new_inputs = {\"agent_scratchpad\": thoughts, \"stop\": self._stop}\\n            full_inputs = {**kwargs, **new_inputs}\\n            full_output = self.llm_chain.predict(**full_inputs)\\n            # We try to extract a final answer\\n            parsed_output = self.output_parser.parse(full_output)\\n            if isinstance(parsed_output, AgentFinish):\\n                # If we can extract, we send the correct stuff\\n                return parsed_output\\n            else:\\n                # If we can extract, but the tool is not the final tool,\\n                # we just return the full output\\n                return AgentFinish({\"output\": full_output}, full_output)\\n        else:\\n            raise ValueError(\\n                \"early_stopping_method should be one of `force` or `generate`, \"\\n                f\"got {early_stopping_method}\"\\n            )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def tool_run_logging_kwargs(self) -> Dict:\\n        return {\\n            \"llm_prefix\": self.llm_prefix,\\n            \"observation_prefix\": self.observation_prefix,\\n        }' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='class ExceptionTool(BaseTool):\\n    \"\"\"Tool that just returns the query.\"\"\"\\n\\n    name: str = \"_Exception\"\\n    \"\"\"Name of the tool.\"\"\"\\n    description: str = \"Exception tool\"\\n    \"\"\"Description of the tool.\"\"\"\\n\\n    def _run(\\n        self,\\n        query: str,\\n        run_manager: Optional[CallbackManagerForToolRun] = None,\\n    ) -> str:\\n        return query\\n\\n    async def _arun(\\n        self,\\n        query: str,\\n        run_manager: Optional[AsyncCallbackManagerForToolRun] = None,\\n    ) -> str:\\n        return query' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='class AgentExecutor(Chain):\\n    \"\"\"Agent that is using tools.\"\"\"\\n\\n    agent: Union[BaseSingleActionAgent, BaseMultiActionAgent]\\n    \"\"\"The agent to run for creating a plan and determining actions\\n    to take at each step of the execution loop.\"\"\"\\n    tools: Sequence[BaseTool]\\n    \"\"\"The valid tools the agent can call.\"\"\"\\n    return_intermediate_steps: bool = False\\n    \"\"\"Whether to return the agent\\'s trajectory of intermediate steps\\n    at the end in addition to the final output.\"\"\"\\n    max_iterations: Optional[int] = 15\\n    \"\"\"The maximum number of steps to take before ending the execution\\n    loop.\\n    \\n    Setting to \\'None\\' could lead to an infinite loop.\"\"\"\\n    max_execution_time: Optional[float] = None\\n    \"\"\"The maximum amount of wall clock time to spend in the execution\\n    loop.\\n    \"\"\"\\n    early_stopping_method: str = \"force\"\\n    \"\"\"The method to use for early stopping if the agent never\\n    returns `AgentFinish`. Either \\'force\\' or \\'generate\\'.\\n\\n    `\"force\"` returns a string saying that it stopped because it met a\\n        time or iteration limit.\\n    \\n    `\"generate\"` calls the agent\\'s LLM Chain one final time to generate\\n        a final answer based on the previous steps.\\n    \"\"\"\\n    handle_parsing_errors: Union[\\n        bool, str, Callable[[OutputParserException], str]\\n    ] = False\\n    \"\"\"How to handle errors raised by the agent\\'s output parser.\\n    Defaults to `False`, which raises the error.\\n    If `true`, the error will be sent back to the LLM as an observation.\\n    If a string, the string itself will be sent to the LLM as an observation.\\n    If a callable function, the function will be called with the exception\\n     as an argument, and the result of that function will be passed to the agent\\n      as an observation.\\n    \"\"\"\\n    trim_intermediate_steps: Union[\\n        int, Callable[[List[Tuple[AgentAction, str]]], List[Tuple[AgentAction, str]]]\\n    ] = -1' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@classmethod\\n    def from_agent_and_tools(\\n        cls,\\n        agent: Union[BaseSingleActionAgent, BaseMultiActionAgent],\\n        tools: Sequence[BaseTool],\\n        callbacks: Callbacks = None,\\n        **kwargs: Any,\\n    ) -> AgentExecutor:\\n        \"\"\"Create from agent and tools.\"\"\"\\n        return cls(\\n            agent=agent,\\n            tools=tools,\\n            callbacks=callbacks,\\n            **kwargs,\\n        )\\n\\n    @root_validator()\\n    def validate_tools(cls, values: Dict) -> Dict:\\n        \"\"\"Validate that tools are compatible with agent.\"\"\"\\n        agent = values[\"agent\"]\\n        tools = values[\"tools\"]\\n        allowed_tools = agent.get_allowed_tools()\\n        if allowed_tools is not None:\\n            if set(allowed_tools) != set([tool.name for tool in tools]):\\n                raise ValueError(\\n                    f\"Allowed tools ({allowed_tools}) different than \"\\n                    f\"provided tools ({[tool.name for tool in tools]})\"\\n                )\\n        return values\\n\\n    @root_validator()\\n    def validate_return_direct_tool(cls, values: Dict) -> Dict:\\n        \"\"\"Validate that tools are compatible with agent.\"\"\"\\n        agent = values[\"agent\"]\\n        tools = values[\"tools\"]\\n        if isinstance(agent, BaseMultiActionAgent):\\n            for tool in tools:\\n                if tool.return_direct:\\n                    raise ValueError(\\n                        \"Tools that have `return_direct=True` are not allowed \"\\n                        \"in multi-action agents\"\\n                    )\\n        return values\\n\\n    @root_validator(pre=True)\\n    def validate_runnable_agent(cls, values: Dict) -> Dict:\\n        \"\"\"Convert runnable to agent if passed in.\"\"\"\\n        agent = values[\"agent\"]\\n        if isinstance(agent, Runnable):\\n            try:\\n                output_type = agent.OutputType\\n            except Exception as _:\\n                multi_action = False\\n            else:\\n                multi_action = output_type == Union[List[AgentAction], AgentFinish]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='if multi_action:\\n                values[\"agent\"] = RunnableMultiActionAgent(runnable=agent)\\n            else:\\n                values[\"agent\"] = RunnableAgent(runnable=agent)\\n        return values\\n\\n    def save(self, file_path: Union[Path, str]) -> None:\\n        \"\"\"Raise error - saving not supported for Agent Executors.\"\"\"\\n        raise ValueError(\\n            \"Saving not supported for agent executors. \"\\n            \"If you are trying to save the agent, please use the \"\\n            \"`.save_agent(...)`\"\\n        )\\n\\n    def save_agent(self, file_path: Union[Path, str]) -> None:\\n        \"\"\"Save the underlying agent.\"\"\"\\n        return self.agent.save(file_path)\\n\\n    def iter(\\n        self,\\n        inputs: Any,\\n        callbacks: Callbacks = None,\\n        *,\\n        include_run_info: bool = False,\\n        async_: bool = False,  # arg kept for backwards compat, but ignored\\n    ) -> AgentExecutorIterator:\\n        \"\"\"Enables iteration over steps taken to reach final output.\"\"\"\\n        return AgentExecutorIterator(\\n            self,\\n            inputs,\\n            callbacks,\\n            tags=self.tags,\\n            include_run_info=include_run_info,\\n        )\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Return the input keys.\\n\\n        :meta private:\\n        \"\"\"\\n        return self.agent.input_keys\\n\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Return the singular output key.\\n\\n        :meta private:\\n        \"\"\"\\n        if self.return_intermediate_steps:\\n            return self.agent.return_values + [\"intermediate_steps\"]\\n        else:\\n            return self.agent.return_values\\n\\n    def lookup_tool(self, name: str) -> BaseTool:\\n        \"\"\"Lookup tool by name.\"\"\"\\n        return {tool.name: tool for tool in self.tools}[name]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _should_continue(self, iterations: int, time_elapsed: float) -> bool:\\n        if self.max_iterations is not None and iterations >= self.max_iterations:\\n            return False\\n        if (\\n            self.max_execution_time is not None\\n            and time_elapsed >= self.max_execution_time\\n        ):\\n            return False\\n\\n        return True\\n\\n    def _return(\\n        self,\\n        output: AgentFinish,\\n        intermediate_steps: list,\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        if run_manager:\\n            run_manager.on_agent_finish(output, color=\"green\", verbose=self.verbose)\\n        final_output = output.return_values\\n        if self.return_intermediate_steps:\\n            final_output[\"intermediate_steps\"] = intermediate_steps\\n        return final_output\\n\\n    async def _areturn(\\n        self,\\n        output: AgentFinish,\\n        intermediate_steps: list,\\n        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        if run_manager:\\n            await run_manager.on_agent_finish(\\n                output, color=\"green\", verbose=self.verbose\\n            )\\n        final_output = output.return_values\\n        if self.return_intermediate_steps:\\n            final_output[\"intermediate_steps\"] = intermediate_steps\\n        return final_output\\n\\n    def _consume_next_step(\\n        self, values: NextStepOutput\\n    ) -> Union[AgentFinish, List[Tuple[AgentAction, str]]]:\\n        if isinstance(values[-1], AgentFinish):\\n            assert len(values) == 1\\n            return values[-1]\\n        else:\\n            return [\\n                (a.action, a.observation) for a in values if isinstance(a, AgentStep)\\n            ]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _take_next_step(\\n        self,\\n        name_to_tool_map: Dict[str, BaseTool],\\n        color_mapping: Dict[str, str],\\n        inputs: Dict[str, str],\\n        intermediate_steps: List[Tuple[AgentAction, str]],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Union[AgentFinish, List[Tuple[AgentAction, str]]]:\\n        return self._consume_next_step(\\n            [\\n                a\\n                for a in self._iter_next_step(\\n                    name_to_tool_map,\\n                    color_mapping,\\n                    inputs,\\n                    intermediate_steps,\\n                    run_manager,\\n                )\\n            ]\\n        )\\n\\n    def _iter_next_step(\\n        self,\\n        name_to_tool_map: Dict[str, BaseTool],\\n        color_mapping: Dict[str, str],\\n        inputs: Dict[str, str],\\n        intermediate_steps: List[Tuple[AgentAction, str]],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Iterator[Union[AgentFinish, AgentAction, AgentStep]]:\\n        \"\"\"Take a single step in the thought-action-observation loop.\\n\\n        Override this to take control of how the agent makes and acts on choices.\\n        \"\"\"\\n        try:\\n            intermediate_steps = self._prepare_intermediate_steps(intermediate_steps)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# Call the LLM to see what to do.\\n            output = self.agent.plan(\\n                intermediate_steps,\\n                callbacks=run_manager.get_child() if run_manager else None,\\n                **inputs,\\n            )\\n        except OutputParserException as e:\\n            if isinstance(self.handle_parsing_errors, bool):\\n                raise_error = not self.handle_parsing_errors\\n            else:\\n                raise_error = False\\n            if raise_error:\\n                raise ValueError(\\n                    \"An output parsing error occurred. \"\\n                    \"In order to pass this error back to the agent and have it try \"\\n                    \"again, pass `handle_parsing_errors=True` to the AgentExecutor. \"\\n                    f\"This is the error: {str(e)}\"\\n                )\\n            text = str(e)\\n            if isinstance(self.handle_parsing_errors, bool):\\n                if e.send_to_llm:\\n                    observation = str(e.observation)\\n                    text = str(e.llm_output)\\n                else:\\n                    observation = \"Invalid or incomplete response\"\\n            elif isinstance(self.handle_parsing_errors, str):\\n                observation = self.handle_parsing_errors\\n            elif callable(self.handle_parsing_errors):\\n                observation = self.handle_parsing_errors(e)\\n            else:\\n                raise ValueError(\"Got unexpected type of `handle_parsing_errors`\")\\n            output = AgentAction(\"_Exception\", observation, text)\\n            if run_manager:\\n                run_manager.on_agent_action(output, color=\"green\")\\n            tool_run_kwargs = self.agent.tool_run_logging_kwargs()\\n            observation = ExceptionTool().run(\\n                output.tool_input,\\n                verbose=self.verbose,\\n                color=None,\\n                callbacks=run_manager.get_child() if run_manager else None,\\n                **tool_run_kwargs,\\n            )\\n            yield AgentStep(action=output, observation=observation)\\n            return' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# If the tool chosen is the finishing tool, then we end and return.\\n        if isinstance(output, AgentFinish):\\n            yield output\\n            return\\n\\n        actions: List[AgentAction]\\n        if isinstance(output, AgentAction):\\n            actions = [output]\\n        else:\\n            actions = output\\n        for agent_action in actions:\\n            yield agent_action\\n        for agent_action in actions:\\n            if run_manager:\\n                run_manager.on_agent_action(agent_action, color=\"green\")\\n            # Otherwise we lookup the tool\\n            if agent_action.tool in name_to_tool_map:\\n                tool = name_to_tool_map[agent_action.tool]\\n                return_direct = tool.return_direct\\n                color = color_mapping[agent_action.tool]\\n                tool_run_kwargs = self.agent.tool_run_logging_kwargs()\\n                if return_direct:\\n                    tool_run_kwargs[\"llm_prefix\"] = \"\"\\n                # We then call the tool on the tool input to get an observation\\n                observation = tool.run(\\n                    agent_action.tool_input,\\n                    verbose=self.verbose,\\n                    color=color,\\n                    callbacks=run_manager.get_child() if run_manager else None,\\n                    **tool_run_kwargs,\\n                )\\n            else:\\n                tool_run_kwargs = self.agent.tool_run_logging_kwargs()\\n                observation = InvalidTool().run(\\n                    {\\n                        \"requested_tool_name\": agent_action.tool,\\n                        \"available_tool_names\": list(name_to_tool_map.keys()),\\n                    },\\n                    verbose=self.verbose,\\n                    color=None,\\n                    callbacks=run_manager.get_child() if run_manager else None,\\n                    **tool_run_kwargs,\\n                )\\n            yield AgentStep(action=agent_action, observation=observation)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='async def _atake_next_step(\\n        self,\\n        name_to_tool_map: Dict[str, BaseTool],\\n        color_mapping: Dict[str, str],\\n        inputs: Dict[str, str],\\n        intermediate_steps: List[Tuple[AgentAction, str]],\\n        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\\n    ) -> Union[AgentFinish, List[Tuple[AgentAction, str]]]:\\n        return self._consume_next_step(\\n            [\\n                a\\n                async for a in self._aiter_next_step(\\n                    name_to_tool_map,\\n                    color_mapping,\\n                    inputs,\\n                    intermediate_steps,\\n                    run_manager,\\n                )\\n            ]\\n        )\\n\\n    async def _aiter_next_step(\\n        self,\\n        name_to_tool_map: Dict[str, BaseTool],\\n        color_mapping: Dict[str, str],\\n        inputs: Dict[str, str],\\n        intermediate_steps: List[Tuple[AgentAction, str]],\\n        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\\n    ) -> AsyncIterator[Union[AgentFinish, AgentAction, AgentStep]]:\\n        \"\"\"Take a single step in the thought-action-observation loop.\\n\\n        Override this to take control of how the agent makes and acts on choices.\\n        \"\"\"\\n        try:\\n            intermediate_steps = self._prepare_intermediate_steps(intermediate_steps)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# Call the LLM to see what to do.\\n            output = await self.agent.aplan(\\n                intermediate_steps,\\n                callbacks=run_manager.get_child() if run_manager else None,\\n                **inputs,\\n            )\\n        except OutputParserException as e:\\n            if isinstance(self.handle_parsing_errors, bool):\\n                raise_error = not self.handle_parsing_errors\\n            else:\\n                raise_error = False\\n            if raise_error:\\n                raise ValueError(\\n                    \"An output parsing error occurred. \"\\n                    \"In order to pass this error back to the agent and have it try \"\\n                    \"again, pass `handle_parsing_errors=True` to the AgentExecutor. \"\\n                    f\"This is the error: {str(e)}\"\\n                )\\n            text = str(e)\\n            if isinstance(self.handle_parsing_errors, bool):\\n                if e.send_to_llm:\\n                    observation = str(e.observation)\\n                    text = str(e.llm_output)\\n                else:\\n                    observation = \"Invalid or incomplete response\"\\n            elif isinstance(self.handle_parsing_errors, str):\\n                observation = self.handle_parsing_errors\\n            elif callable(self.handle_parsing_errors):\\n                observation = self.handle_parsing_errors(e)\\n            else:\\n                raise ValueError(\"Got unexpected type of `handle_parsing_errors`\")\\n            output = AgentAction(\"_Exception\", observation, text)\\n            tool_run_kwargs = self.agent.tool_run_logging_kwargs()\\n            observation = await ExceptionTool().arun(\\n                output.tool_input,\\n                verbose=self.verbose,\\n                color=None,\\n                callbacks=run_manager.get_child() if run_manager else None,\\n                **tool_run_kwargs,\\n            )\\n            yield AgentStep(action=output, observation=observation)\\n            return' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# If the tool chosen is the finishing tool, then we end and return.\\n        if isinstance(output, AgentFinish):\\n            yield output\\n            return\\n\\n        actions: List[AgentAction]\\n        if isinstance(output, AgentAction):\\n            actions = [output]\\n        else:\\n            actions = output\\n        for agent_action in actions:\\n            yield agent_action' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='async def _aperform_agent_action(\\n            agent_action: AgentAction,\\n        ) -> AgentStep:\\n            if run_manager:\\n                await run_manager.on_agent_action(\\n                    agent_action, verbose=self.verbose, color=\"green\"\\n                )\\n            # Otherwise we lookup the tool\\n            if agent_action.tool in name_to_tool_map:\\n                tool = name_to_tool_map[agent_action.tool]\\n                return_direct = tool.return_direct\\n                color = color_mapping[agent_action.tool]\\n                tool_run_kwargs = self.agent.tool_run_logging_kwargs()\\n                if return_direct:\\n                    tool_run_kwargs[\"llm_prefix\"] = \"\"\\n                # We then call the tool on the tool input to get an observation\\n                observation = await tool.arun(\\n                    agent_action.tool_input,\\n                    verbose=self.verbose,\\n                    color=color,\\n                    callbacks=run_manager.get_child() if run_manager else None,\\n                    **tool_run_kwargs,\\n                )\\n            else:\\n                tool_run_kwargs = self.agent.tool_run_logging_kwargs()\\n                observation = await InvalidTool().arun(\\n                    {\\n                        \"requested_tool_name\": agent_action.tool,\\n                        \"available_tool_names\": list(name_to_tool_map.keys()),\\n                    },\\n                    verbose=self.verbose,\\n                    color=None,\\n                    callbacks=run_manager.get_child() if run_manager else None,\\n                    **tool_run_kwargs,\\n                )\\n            return AgentStep(action=agent_action, observation=observation)\\n\\n        # Use asyncio.gather to run multiple tool.arun() calls concurrently\\n        result = await asyncio.gather(\\n            *[_aperform_agent_action(agent_action) for agent_action in actions]\\n        )\\n\\n        # TODO This could yield each result as it becomes available\\n        for chunk in result:\\n            yield chunk' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _call(\\n        self,\\n        inputs: Dict[str, str],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        \"\"\"Run text through and get agent response.\"\"\"\\n        # Construct a mapping of tool name to tool for easy lookup\\n        name_to_tool_map = {tool.name: tool for tool in self.tools}\\n        # We construct a mapping from each tool to a color, used for logging.\\n        color_mapping = get_color_mapping(\\n            [tool.name for tool in self.tools], excluded_colors=[\"green\", \"red\"]\\n        )\\n        intermediate_steps: List[Tuple[AgentAction, str]] = []\\n        # Let\\'s start tracking the number of iterations and time elapsed\\n        iterations = 0\\n        time_elapsed = 0.0\\n        start_time = time.time()\\n        # We now enter the agent loop (until it returns something).\\n        while self._should_continue(iterations, time_elapsed):\\n            next_step_output = self._take_next_step(\\n                name_to_tool_map,\\n                color_mapping,\\n                inputs,\\n                intermediate_steps,\\n                run_manager=run_manager,\\n            )\\n            if isinstance(next_step_output, AgentFinish):\\n                return self._return(\\n                    next_step_output, intermediate_steps, run_manager=run_manager\\n                )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='intermediate_steps.extend(next_step_output)\\n            if len(next_step_output) == 1:\\n                next_step_action = next_step_output[0]\\n                # See if tool should return directly\\n                tool_return = self._get_tool_return(next_step_action)\\n                if tool_return is not None:\\n                    return self._return(\\n                        tool_return, intermediate_steps, run_manager=run_manager\\n                    )\\n            iterations += 1\\n            time_elapsed = time.time() - start_time\\n        output = self.agent.return_stopped_response(\\n            self.early_stopping_method, intermediate_steps, **inputs\\n        )\\n        return self._return(output, intermediate_steps, run_manager=run_manager)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='async def _acall(\\n        self,\\n        inputs: Dict[str, str],\\n        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\\n    ) -> Dict[str, str]:\\n        \"\"\"Run text through and get agent response.\"\"\"\\n        # Construct a mapping of tool name to tool for easy lookup\\n        name_to_tool_map = {tool.name: tool for tool in self.tools}\\n        # We construct a mapping from each tool to a color, used for logging.\\n        color_mapping = get_color_mapping(\\n            [tool.name for tool in self.tools], excluded_colors=[\"green\"]\\n        )\\n        intermediate_steps: List[Tuple[AgentAction, str]] = []\\n        # Let\\'s start tracking the number of iterations and time elapsed\\n        iterations = 0\\n        time_elapsed = 0.0\\n        start_time = time.time()\\n        # We now enter the agent loop (until it returns something).\\n        try:\\n            async with asyncio_timeout(self.max_execution_time):\\n                while self._should_continue(iterations, time_elapsed):\\n                    next_step_output = await self._atake_next_step(\\n                        name_to_tool_map,\\n                        color_mapping,\\n                        inputs,\\n                        intermediate_steps,\\n                        run_manager=run_manager,\\n                    )\\n                    if isinstance(next_step_output, AgentFinish):\\n                        return await self._areturn(\\n                            next_step_output,\\n                            intermediate_steps,\\n                            run_manager=run_manager,\\n                        )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='intermediate_steps.extend(next_step_output)\\n                    if len(next_step_output) == 1:\\n                        next_step_action = next_step_output[0]\\n                        # See if tool should return directly\\n                        tool_return = self._get_tool_return(next_step_action)\\n                        if tool_return is not None:\\n                            return await self._areturn(\\n                                tool_return, intermediate_steps, run_manager=run_manager\\n                            )\\n\\n                    iterations += 1\\n                    time_elapsed = time.time() - start_time\\n                output = self.agent.return_stopped_response(\\n                    self.early_stopping_method, intermediate_steps, **inputs\\n                )\\n                return await self._areturn(\\n                    output, intermediate_steps, run_manager=run_manager\\n                )\\n        except (TimeoutError, asyncio.TimeoutError):\\n            # stop early when interrupted by the async timeout\\n            output = self.agent.return_stopped_response(\\n                self.early_stopping_method, intermediate_steps, **inputs\\n            )\\n            return await self._areturn(\\n                output, intermediate_steps, run_manager=run_manager\\n            )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_tool_return(\\n        self, next_step_output: Tuple[AgentAction, str]\\n    ) -> Optional[AgentFinish]:\\n        \"\"\"Check if the tool is a returning tool.\"\"\"\\n        agent_action, observation = next_step_output\\n        name_to_tool_map = {tool.name: tool for tool in self.tools}\\n        return_value_key = \"output\"\\n        if len(self.agent.return_values) > 0:\\n            return_value_key = self.agent.return_values[0]\\n        # Invalid tools won\\'t be in the map, so we return False.\\n        if agent_action.tool in name_to_tool_map:\\n            if name_to_tool_map[agent_action.tool].return_direct:\\n                return AgentFinish(\\n                    {return_value_key: observation},\\n                    \"\",\\n                )\\n        return None\\n\\n    def _prepare_intermediate_steps(\\n        self, intermediate_steps: List[Tuple[AgentAction, str]]\\n    ) -> List[Tuple[AgentAction, str]]:\\n        if (\\n            isinstance(self.trim_intermediate_steps, int)\\n            and self.trim_intermediate_steps > 0\\n        ):\\n            return intermediate_steps[-self.trim_intermediate_steps :]\\n        elif callable(self.trim_intermediate_steps):\\n            return self.trim_intermediate_steps(intermediate_steps)\\n        else:\\n            return intermediate_steps\\n\\n    def stream(\\n        self,\\n        input: Union[Dict[str, Any], Any],\\n        config: Optional[RunnableConfig] = None,\\n        **kwargs: Any,\\n    ) -> Iterator[AddableDict]:\\n        \"\"\"Enables streaming over steps taken to reach final output.\"\"\"\\n        config = ensure_config(config)\\n        iterator = AgentExecutorIterator(\\n            self,\\n            input,\\n            config.get(\"callbacks\"),\\n            tags=config.get(\"tags\"),\\n            metadata=config.get(\"metadata\"),\\n            run_name=config.get(\"run_name\"),\\n            yield_actions=True,\\n            **kwargs,\\n        )\\n        for step in iterator:\\n            yield step' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='async def astream(\\n        self,\\n        input: Union[Dict[str, Any], Any],\\n        config: Optional[RunnableConfig] = None,\\n        **kwargs: Any,\\n    ) -> AsyncIterator[AddableDict]:\\n        \"\"\"Enables streaming over steps taken to reach final output.\"\"\"\\n        config = ensure_config(config)\\n        iterator = AgentExecutorIterator(\\n            self,\\n            input,\\n            config.get(\"callbacks\"),\\n            tags=config.get(\"tags\"),\\n            metadata=config.get(\"metadata\"),\\n            run_name=config.get(\"run_name\"),\\n            yield_actions=True,\\n            **kwargs,\\n        )\\n        async for step in iterator:\\n            yield step' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Chain that takes in an input and produces an action and action input.\"\"\"\\nfrom __future__ import annotations\\n\\nimport asyncio\\nimport json\\nimport logging\\nimport time\\nfrom abc import abstractmethod\\nfrom pathlib import Path\\nfrom typing import (\\n    Any,\\n    AsyncIterator,\\n    Callable,\\n    Dict,\\n    Iterator,\\n    List,\\n    Optional,\\n    Sequence,\\n    Tuple,\\n    Union,\\n)\\n\\nimport yaml\\nfrom langchain_core._api import deprecated\\nfrom langchain_core.agents import AgentAction, AgentFinish, AgentStep\\nfrom langchain_core.callbacks import (\\n    AsyncCallbackManagerForChainRun,\\n    AsyncCallbackManagerForToolRun,\\n    BaseCallbackManager,\\n    CallbackManagerForChainRun,\\n    CallbackManagerForToolRun,\\n    Callbacks,\\n)\\nfrom langchain_core.exceptions import OutputParserException\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.messages import BaseMessage\\nfrom langchain_core.output_parsers import BaseOutputParser\\nfrom langchain_core.prompts import BasePromptTemplate\\nfrom langchain_core.prompts.few_shot import FewShotPromptTemplate\\nfrom langchain_core.prompts.prompt import PromptTemplate\\nfrom langchain_core.pydantic_v1 import BaseModel, root_validator\\nfrom langchain_core.runnables import Runnable, RunnableConfig, ensure_config\\nfrom langchain_core.runnables.utils import AddableDict\\nfrom langchain_core.tools import BaseTool\\nfrom langchain_core.utils.input import get_color_mapping\\n\\nfrom langchain.agents.agent_iterator import AgentExecutorIterator\\nfrom langchain.agents.agent_types import AgentType\\nfrom langchain.agents.tools import InvalidTool\\nfrom langchain.chains.base import Chain\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.utilities.asyncio import asyncio_timeout\\n\\nlogger = logging.getLogger(__name__)\\n\\n\\n# Code for: class BaseSingleActionAgent(BaseModel):\\n\\n\\n# Code for: class BaseMultiActionAgent(BaseModel):\\n\\n\\n# Code for: class AgentOutputParser(BaseOutputParser[Union[AgentAction, AgentFinish]]):\\n\\n\\n# Code for: class MultiActionAgentOutputParser(' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# Code for: class RunnableAgent(BaseSingleActionAgent):\\n\\n\\n# Code for: class RunnableMultiActionAgent(BaseMultiActionAgent):\\n\\n\\n@deprecated(\\n    \"0.1.0\",\\n    alternative=(\\n        \"Use new agent constructor methods like create_react_agent, create_json_agent, \"\\n        \"create_structured_chat_agent, etc.\"\\n    ),\\n    removal=\"0.2.0\",\\n)\\n# Code for: class LLMSingleActionAgent(BaseSingleActionAgent):\\n\\n\\n@deprecated(\\n    \"0.1.0\",\\n    alternative=(\\n        \"Use new agent constructor methods like create_react_agent, create_json_agent, \"\\n        \"create_structured_chat_agent, etc.\"\\n    ),\\n    removal=\"0.2.0\",\\n)\\n# Code for: class Agent(BaseSingleActionAgent):\\n\\n\\n# Code for: class ExceptionTool(BaseTool):\\n\\n\\nNextStepOutput = List[Union[AgentFinish, AgentAction, AgentStep]]\\n\\n\\n# Code for: class AgentExecutor(Chain):' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from __future__ import annotations\\n\\nimport asyncio\\nimport logging\\nimport time\\nfrom typing import (\\n    TYPE_CHECKING,\\n    Any,\\n    AsyncIterator,\\n    Dict,\\n    Iterator,\\n    List,\\n    Optional,\\n    Tuple,\\n    Union,\\n)\\n\\nfrom langchain_core.agents import (\\n    AgentAction,\\n    AgentFinish,\\n    AgentStep,\\n)\\nfrom langchain_core.callbacks import (\\n    AsyncCallbackManager,\\n    AsyncCallbackManagerForChainRun,\\n    CallbackManager,\\n    CallbackManagerForChainRun,\\n    Callbacks,\\n)\\nfrom langchain_core.load.dump import dumpd\\nfrom langchain_core.outputs import RunInfo\\nfrom langchain_core.runnables.utils import AddableDict\\nfrom langchain_core.tools import BaseTool\\nfrom langchain_core.utils.input import get_color_mapping\\n\\nfrom langchain.schema import RUN_KEY\\nfrom langchain.utilities.asyncio import asyncio_timeout\\n\\nif TYPE_CHECKING:\\n    from langchain.agents.agent import AgentExecutor, NextStepOutput\\n\\nlogger = logging.getLogger(__name__)\\n\\n\\nclass AgentExecutorIterator:\\n    \"\"\"Iterator for AgentExecutor.\"\"\"\\n\\n    def __init__(\\n        self,\\n        agent_executor: AgentExecutor,\\n        inputs: Any,\\n        callbacks: Callbacks = None,\\n        *,\\n        tags: Optional[list[str]] = None,\\n        metadata: Optional[Dict[str, Any]] = None,\\n        run_name: Optional[str] = None,\\n        include_run_info: bool = False,\\n        yield_actions: bool = False,\\n    ):\\n        \"\"\"\\n        Initialize the AgentExecutorIterator with the given AgentExecutor,\\n        inputs, and optional callbacks.\\n        \"\"\"\\n        self._agent_executor = agent_executor\\n        self.inputs = inputs\\n        self.callbacks = callbacks\\n        self.tags = tags\\n        self.metadata = metadata\\n        self.run_name = run_name\\n        self.include_run_info = include_run_info\\n        self.yield_actions = yield_actions\\n        self.reset()\\n\\n    _inputs: Dict[str, str]\\n    callbacks: Callbacks\\n    tags: Optional[list[str]]\\n    metadata: Optional[Dict[str, Any]]\\n    run_name: Optional[str]\\n    include_run_info: bool\\n    yield_actions: bool' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_iterator.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@property\\n    def inputs(self) -> Dict[str, str]:\\n        return self._inputs\\n\\n    @inputs.setter\\n    def inputs(self, inputs: Any) -> None:\\n        self._inputs = self.agent_executor.prep_inputs(inputs)\\n\\n    @property\\n    def agent_executor(self) -> AgentExecutor:\\n        return self._agent_executor\\n\\n    @agent_executor.setter\\n    def agent_executor(self, agent_executor: AgentExecutor) -> None:\\n        self._agent_executor = agent_executor\\n        # force re-prep inputs in case agent_executor\\'s prep_inputs fn changed\\n        self.inputs = self.inputs\\n\\n    @property\\n    def name_to_tool_map(self) -> Dict[str, BaseTool]:\\n        return {tool.name: tool for tool in self.agent_executor.tools}\\n\\n    @property\\n    def color_mapping(self) -> Dict[str, str]:\\n        return get_color_mapping(\\n            [tool.name for tool in self.agent_executor.tools],\\n            excluded_colors=[\"green\", \"red\"],\\n        )\\n\\n    def reset(self) -> None:\\n        \"\"\"\\n        Reset the iterator to its initial state, clearing intermediate steps,\\n        iterations, and time elapsed.\\n        \"\"\"\\n        logger.debug(\"(Re)setting AgentExecutorIterator to fresh state\")\\n        self.intermediate_steps: list[tuple[AgentAction, str]] = []\\n        self.iterations = 0\\n        # maybe better to start these on the first __anext__ call?\\n        self.time_elapsed = 0.0\\n        self.start_time = time.time()\\n\\n    def update_iterations(self) -> None:\\n        \"\"\"\\n        Increment the number of iterations and update the time elapsed.\\n        \"\"\"\\n        self.iterations += 1\\n        self.time_elapsed = time.time() - self.start_time\\n        logger.debug(\\n            f\"Agent Iterations: {self.iterations} ({self.time_elapsed:.2f}s elapsed)\"\\n        )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_iterator.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def make_final_outputs(\\n        self,\\n        outputs: Dict[str, Any],\\n        run_manager: Union[CallbackManagerForChainRun, AsyncCallbackManagerForChainRun],\\n    ) -> AddableDict:\\n        # have access to intermediate steps by design in iterator,\\n        # so return only outputs may as well always be true.\\n\\n        prepared_outputs = AddableDict(\\n            self.agent_executor.prep_outputs(\\n                self.inputs, outputs, return_only_outputs=True\\n            )\\n        )\\n        if self.include_run_info:\\n            prepared_outputs[RUN_KEY] = RunInfo(run_id=run_manager.run_id)\\n        return prepared_outputs' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_iterator.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def __iter__(self: \"AgentExecutorIterator\") -> Iterator[AddableDict]:\\n        logger.debug(\"Initialising AgentExecutorIterator\")\\n        self.reset()\\n        callback_manager = CallbackManager.configure(\\n            self.callbacks,\\n            self.agent_executor.callbacks,\\n            self.agent_executor.verbose,\\n            self.tags,\\n            self.agent_executor.tags,\\n            self.metadata,\\n            self.agent_executor.metadata,\\n        )\\n        run_manager = callback_manager.on_chain_start(\\n            dumpd(self.agent_executor),\\n            self.inputs,\\n            name=self.run_name,\\n        )\\n        try:\\n            while self.agent_executor._should_continue(\\n                self.iterations, self.time_elapsed\\n            ):\\n                # take the next step: this plans next action, executes it,\\n                # yielding action and observation as they are generated\\n                next_step_seq: NextStepOutput = []\\n                for chunk in self.agent_executor._iter_next_step(\\n                    self.name_to_tool_map,\\n                    self.color_mapping,\\n                    self.inputs,\\n                    self.intermediate_steps,\\n                    run_manager,\\n                ):\\n                    next_step_seq.append(chunk)\\n                    # if we\\'re yielding actions, yield them as they come\\n                    # do not yield AgentFinish, which will be handled below\\n                    if self.yield_actions:\\n                        if isinstance(chunk, AgentAction):\\n                            yield AddableDict(actions=[chunk], messages=chunk.messages)\\n                        elif isinstance(chunk, AgentStep):\\n                            yield AddableDict(steps=[chunk], messages=chunk.messages)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_iterator.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# convert iterator output to format handled by _process_next_step_output\\n                next_step = self.agent_executor._consume_next_step(next_step_seq)\\n                # update iterations and time elapsed\\n                self.update_iterations()\\n                # decide if this is the final output\\n                output = self._process_next_step_output(next_step, run_manager)\\n                is_final = \"intermediate_step\" not in output\\n                # yield the final output always\\n                # for backwards compat, yield int. output if not yielding actions\\n                if not self.yield_actions or is_final:\\n                    yield output\\n                # if final output reached, stop iteration\\n                if is_final:\\n                    return\\n        except BaseException as e:\\n            run_manager.on_chain_error(e)\\n            raise\\n\\n        # if we got here means we exhausted iterations or time\\n        yield self._stop(run_manager)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_iterator.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='async def __aiter__(self) -> AsyncIterator[AddableDict]:\\n        \"\"\"\\n        N.B. __aiter__ must be a normal method, so need to initialize async run manager\\n        on first __anext__ call where we can await it\\n        \"\"\"\\n        logger.debug(\"Initialising AgentExecutorIterator (async)\")\\n        self.reset()\\n        callback_manager = AsyncCallbackManager.configure(\\n            self.callbacks,\\n            self.agent_executor.callbacks,\\n            self.agent_executor.verbose,\\n            self.tags,\\n            self.agent_executor.tags,\\n            self.metadata,\\n            self.agent_executor.metadata,\\n        )\\n        run_manager = await callback_manager.on_chain_start(\\n            dumpd(self.agent_executor),\\n            self.inputs,\\n            name=self.run_name,\\n        )\\n        try:\\n            async with asyncio_timeout(self.agent_executor.max_execution_time):\\n                while self.agent_executor._should_continue(\\n                    self.iterations, self.time_elapsed\\n                ):\\n                    # take the next step: this plans next action, executes it,\\n                    # yielding action and observation as they are generated\\n                    next_step_seq: NextStepOutput = []\\n                    async for chunk in self.agent_executor._aiter_next_step(\\n                        self.name_to_tool_map,\\n                        self.color_mapping,\\n                        self.inputs,\\n                        self.intermediate_steps,\\n                        run_manager,\\n                    ):\\n                        next_step_seq.append(chunk)\\n                        # if we\\'re yielding actions, yield them as they come\\n                        # do not yield AgentFinish, which will be handled below\\n                        if self.yield_actions:\\n                            if isinstance(chunk, AgentAction):\\n                                yield AddableDict(\\n                                    actions=[chunk], messages=chunk.messages\\n                                )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_iterator.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='elif isinstance(chunk, AgentStep):\\n                                yield AddableDict(\\n                                    steps=[chunk], messages=chunk.messages\\n                                )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_iterator.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# convert iterator output to format handled by _process_next_step\\n                    next_step = self.agent_executor._consume_next_step(next_step_seq)\\n                    # update iterations and time elapsed\\n                    self.update_iterations()\\n                    # decide if this is the final output\\n                    output = await self._aprocess_next_step_output(\\n                        next_step, run_manager\\n                    )\\n                    is_final = \"intermediate_step\" not in output\\n                    # yield the final output always\\n                    # for backwards compat, yield int. output if not yielding actions\\n                    if not self.yield_actions or is_final:\\n                        yield output\\n                    # if final output reached, stop iteration\\n                    if is_final:\\n                        return\\n        except (TimeoutError, asyncio.TimeoutError):\\n            yield await self._astop(run_manager)\\n            return\\n        except BaseException as e:\\n            await run_manager.on_chain_error(e)\\n            raise\\n\\n        # if we got here means we exhausted iterations or time\\n        yield await self._astop(run_manager)\\n\\n    def _process_next_step_output(\\n        self,\\n        next_step_output: Union[AgentFinish, List[Tuple[AgentAction, str]]],\\n        run_manager: CallbackManagerForChainRun,\\n    ) -> AddableDict:\\n        \"\"\"\\n        Process the output of the next step,\\n        handling AgentFinish and tool return cases.\\n        \"\"\"\\n        logger.debug(\"Processing output of Agent loop step\")\\n        if isinstance(next_step_output, AgentFinish):\\n            logger.debug(\\n                \"Hit AgentFinish: _return -> on_chain_end -> run final output logic\"\\n            )\\n            return self._return(next_step_output, run_manager=run_manager)\\n\\n        self.intermediate_steps.extend(next_step_output)\\n        logger.debug(\"Updated intermediate_steps with step output\")' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_iterator.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# Check for tool return\\n        if len(next_step_output) == 1:\\n            next_step_action = next_step_output[0]\\n            tool_return = self.agent_executor._get_tool_return(next_step_action)\\n            if tool_return is not None:\\n                return self._return(tool_return, run_manager=run_manager)\\n\\n        return AddableDict(intermediate_step=next_step_output)\\n\\n    async def _aprocess_next_step_output(\\n        self,\\n        next_step_output: Union[AgentFinish, List[Tuple[AgentAction, str]]],\\n        run_manager: AsyncCallbackManagerForChainRun,\\n    ) -> AddableDict:\\n        \"\"\"\\n        Process the output of the next async step,\\n        handling AgentFinish and tool return cases.\\n        \"\"\"\\n        logger.debug(\"Processing output of async Agent loop step\")\\n        if isinstance(next_step_output, AgentFinish):\\n            logger.debug(\\n                \"Hit AgentFinish: _areturn -> on_chain_end -> run final output logic\"\\n            )\\n            return await self._areturn(next_step_output, run_manager=run_manager)\\n\\n        self.intermediate_steps.extend(next_step_output)\\n        logger.debug(\"Updated intermediate_steps with step output\")\\n\\n        # Check for tool return\\n        if len(next_step_output) == 1:\\n            next_step_action = next_step_output[0]\\n            tool_return = self.agent_executor._get_tool_return(next_step_action)\\n            if tool_return is not None:\\n                return await self._areturn(tool_return, run_manager=run_manager)\\n\\n        return AddableDict(intermediate_step=next_step_output)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_iterator.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _stop(self, run_manager: CallbackManagerForChainRun) -> AddableDict:\\n        \"\"\"\\n        Stop the iterator and raise a StopIteration exception with the stopped response.\\n        \"\"\"\\n        logger.warning(\"Stopping agent prematurely due to triggering stop condition\")\\n        # this manually constructs agent finish with output key\\n        output = self.agent_executor.agent.return_stopped_response(\\n            self.agent_executor.early_stopping_method,\\n            self.intermediate_steps,\\n            **self.inputs,\\n        )\\n        return self._return(output, run_manager=run_manager)\\n\\n    async def _astop(self, run_manager: AsyncCallbackManagerForChainRun) -> AddableDict:\\n        \"\"\"\\n        Stop the async iterator and raise a StopAsyncIteration exception with\\n        the stopped response.\\n        \"\"\"\\n        logger.warning(\"Stopping agent prematurely due to triggering stop condition\")\\n        output = self.agent_executor.agent.return_stopped_response(\\n            self.agent_executor.early_stopping_method,\\n            self.intermediate_steps,\\n            **self.inputs,\\n        )\\n        return await self._areturn(output, run_manager=run_manager)\\n\\n    def _return(\\n        self, output: AgentFinish, run_manager: CallbackManagerForChainRun\\n    ) -> AddableDict:\\n        \"\"\"\\n        Return the final output of the iterator.\\n        \"\"\"\\n        returned_output = self.agent_executor._return(\\n            output, self.intermediate_steps, run_manager=run_manager\\n        )\\n        returned_output[\"messages\"] = output.messages\\n        run_manager.on_chain_end(returned_output)\\n        return self.make_final_outputs(returned_output, run_manager)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_iterator.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='async def _areturn(\\n        self, output: AgentFinish, run_manager: AsyncCallbackManagerForChainRun\\n    ) -> AddableDict:\\n        \"\"\"\\n        Return the final output of the async iterator.\\n        \"\"\"\\n        returned_output = await self.agent_executor._areturn(\\n            output, self.intermediate_steps, run_manager=run_manager\\n        )\\n        returned_output[\"messages\"] = output.messages\\n        await run_manager.on_chain_end(returned_output)\\n        return self.make_final_outputs(returned_output, run_manager)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_iterator.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Module definitions of agent types together with corresponding agents.\"\"\"\\nfrom enum import Enum\\n\\nfrom langchain_core._api import deprecated\\n\\n\\n@deprecated(\\n    \"0.1.0\",\\n    alternative=(\\n        \"Use new agent constructor methods like create_react_agent, create_json_agent, \"\\n        \"create_structured_chat_agent, etc.\"\\n    ),\\n    removal=\"0.2.0\",\\n)\\nclass AgentType(str, Enum):\\n    \"\"\"An enum for agent types.\\n\\n    See documentation: https://python.langchain.com/docs/modules/agents/agent_types/\\n    \"\"\"\\n\\n    ZERO_SHOT_REACT_DESCRIPTION = \"zero-shot-react-description\"\\n    \"\"\"A zero shot agent that does a reasoning step before acting.\"\"\"\\n\\n    REACT_DOCSTORE = \"react-docstore\"\\n    \"\"\"A zero shot agent that does a reasoning step before acting.\\n    \\n    This agent has access to a document store that allows it to look up \\n    relevant information to answering the question.\\n    \"\"\"\\n\\n    SELF_ASK_WITH_SEARCH = \"self-ask-with-search\"\\n    \"\"\"An agent that breaks down a complex question into a series of simpler questions.\\n    \\n    This agent uses a search tool to look up answers to the simpler questions\\n    in order to answer the original complex question.\\n    \"\"\"\\n    CONVERSATIONAL_REACT_DESCRIPTION = \"conversational-react-description\"\\n    CHAT_ZERO_SHOT_REACT_DESCRIPTION = \"chat-zero-shot-react-description\"\\n    \"\"\"A zero shot agent that does a reasoning step before acting.\\n    \\n    This agent is designed to be used in conjunction \\n    \"\"\"\\n\\n    CHAT_CONVERSATIONAL_REACT_DESCRIPTION = \"chat-conversational-react-description\"\\n\\n    STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION = (\\n        \"structured-chat-zero-shot-react-description\"\\n    )\\n    \"\"\"An zero-shot react agent optimized for chat models.\\n    \\n    This agent is capable of invoking tools that have multiple inputs.\\n    \"\"\"\\n\\n    OPENAI_FUNCTIONS = \"openai-functions\"\\n    \"\"\"An agent optimized for using open AI functions.\"\"\"\\n\\n    OPENAI_MULTI_FUNCTIONS = \"openai-multi-functions\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_types.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Load agent.\"\"\"\\nfrom typing import Any, Optional, Sequence\\n\\nfrom langchain_core._api import deprecated\\nfrom langchain_core.callbacks import BaseCallbackManager\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.tools import BaseTool\\n\\nfrom langchain.agents.agent import AgentExecutor\\nfrom langchain.agents.agent_types import AgentType\\nfrom langchain.agents.loading import AGENT_TO_CLASS, load_agent\\n\\n\\n@deprecated(\\n    \"0.1.0\",\\n    alternative=(\\n        \"Use new agent constructor methods like create_react_agent, create_json_agent, \"\\n        \"create_structured_chat_agent, etc.\"\\n    ),\\n    removal=\"0.2.0\",\\n)\\ndef initialize_agent(\\n    tools: Sequence[BaseTool],\\n    llm: BaseLanguageModel,\\n    agent: Optional[AgentType] = None,\\n    callback_manager: Optional[BaseCallbackManager] = None,\\n    agent_path: Optional[str] = None,\\n    agent_kwargs: Optional[dict] = None,\\n    *,\\n    tags: Optional[Sequence[str]] = None,\\n    **kwargs: Any,\\n) -> AgentExecutor:\\n    \"\"\"Load an agent executor given tools and LLM.\\n\\n    Args:\\n        tools: List of tools this agent has access to.\\n        llm: Language model to use as the agent.\\n        agent: Agent type to use. If None and agent_path is also None, will default to\\n            AgentType.ZERO_SHOT_REACT_DESCRIPTION.\\n        callback_manager: CallbackManager to use. Global callback manager is used if\\n            not provided. Defaults to None.\\n        agent_path: Path to serialized agent to use.\\n        agent_kwargs: Additional keyword arguments to pass to the underlying agent\\n        tags: Tags to apply to the traced runs.\\n        **kwargs: Additional keyword arguments passed to the agent executor' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\initialize.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Returns:\\n        An agent executor\\n    \"\"\"\\n    tags_ = list(tags) if tags else []\\n    if agent is None and agent_path is None:\\n        agent = AgentType.ZERO_SHOT_REACT_DESCRIPTION\\n    if agent is not None and agent_path is not None:\\n        raise ValueError(\\n            \"Both `agent` and `agent_path` are specified, \"\\n            \"but at most only one should be.\"\\n        )\\n    if agent is not None:\\n        if agent not in AGENT_TO_CLASS:\\n            raise ValueError(\\n                f\"Got unknown agent type: {agent}. \"\\n                f\"Valid types are: {AGENT_TO_CLASS.keys()}.\"\\n            )\\n        tags_.append(agent.value if isinstance(agent, AgentType) else agent)\\n        agent_cls = AGENT_TO_CLASS[agent]\\n        agent_kwargs = agent_kwargs or {}\\n        agent_obj = agent_cls.from_llm_and_tools(\\n            llm, tools, callback_manager=callback_manager, **agent_kwargs\\n        )\\n    elif agent_path is not None:\\n        agent_obj = load_agent(\\n            agent_path, llm=llm, tools=tools, callback_manager=callback_manager\\n        )\\n        try:\\n            # TODO: Add tags from the serialized object directly.\\n            tags_.append(agent_obj._agent_type)\\n        except NotImplementedError:\\n            pass\\n    else:\\n        raise ValueError(\\n            \"Somehow both `agent` and `agent_path` are None, \"\\n            \"this should never happen.\"\\n        )\\n    return AgentExecutor.from_agent_and_tools(\\n        agent=agent_obj,\\n        tools=tools,\\n        callback_manager=callback_manager,\\n        tags=tags_,\\n        **kwargs,\\n    )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\initialize.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Functionality for loading agents.\"\"\"\\nimport json\\nimport logging\\nfrom pathlib import Path\\nfrom typing import Any, List, Optional, Union\\n\\nimport yaml\\nfrom langchain_core._api import deprecated\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.tools import Tool\\nfrom langchain_core.utils.loading import try_load_from_hub\\n\\nfrom langchain.agents.agent import BaseMultiActionAgent, BaseSingleActionAgent\\nfrom langchain.agents.types import AGENT_TO_CLASS\\nfrom langchain.chains.loading import load_chain, load_chain_from_config\\n\\nlogger = logging.getLogger(__file__)\\n\\nURL_BASE = \"https://raw.githubusercontent.com/hwchase17/langchain-hub/master/agents/\"\\n\\n\\ndef _load_agent_from_tools(\\n    config: dict, llm: BaseLanguageModel, tools: List[Tool], **kwargs: Any\\n) -> Union[BaseSingleActionAgent, BaseMultiActionAgent]:\\n    config_type = config.pop(\"_type\")\\n    if config_type not in AGENT_TO_CLASS:\\n        raise ValueError(f\"Loading {config_type} agent not supported\")\\n\\n    agent_cls = AGENT_TO_CLASS[config_type]\\n    combined_config = {**config, **kwargs}\\n    return agent_cls.from_llm_and_tools(llm, tools, **combined_config)\\n\\n\\n@deprecated(\"0.1.0\", removal=\"0.2.0\")\\ndef load_agent_from_config(\\n    config: dict,\\n    llm: Optional[BaseLanguageModel] = None,\\n    tools: Optional[List[Tool]] = None,\\n    **kwargs: Any,\\n) -> Union[BaseSingleActionAgent, BaseMultiActionAgent]:\\n    \"\"\"Load agent from Config Dict.\\n\\n    Args:\\n        config: Config dict to load agent from.\\n        llm: Language model to use as the agent.\\n        tools: List of tools this agent has access to.\\n        **kwargs: Additional keyword arguments passed to the agent executor.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\loading.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Returns:\\n        An agent executor.\\n    \"\"\"\\n    if \"_type\" not in config:\\n        raise ValueError(\"Must specify an agent Type in config\")\\n    load_from_tools = config.pop(\"load_from_llm_and_tools\", False)\\n    if load_from_tools:\\n        if llm is None:\\n            raise ValueError(\\n                \"If `load_from_llm_and_tools` is set to True, \"\\n                \"then LLM must be provided\"\\n            )\\n        if tools is None:\\n            raise ValueError(\\n                \"If `load_from_llm_and_tools` is set to True, \"\\n                \"then tools must be provided\"\\n            )\\n        return _load_agent_from_tools(config, llm, tools, **kwargs)\\n    config_type = config.pop(\"_type\")\\n\\n    if config_type not in AGENT_TO_CLASS:\\n        raise ValueError(f\"Loading {config_type} agent not supported\")\\n\\n    agent_cls = AGENT_TO_CLASS[config_type]\\n    if \"llm_chain\" in config:\\n        config[\"llm_chain\"] = load_chain_from_config(config.pop(\"llm_chain\"))\\n    elif \"llm_chain_path\" in config:\\n        config[\"llm_chain\"] = load_chain(config.pop(\"llm_chain_path\"))\\n    else:\\n        raise ValueError(\"One of `llm_chain` and `llm_chain_path` should be specified.\")\\n    if \"output_parser\" in config:\\n        logger.warning(\\n            \"Currently loading output parsers on agent is not supported, \"\\n            \"will just use the default one.\"\\n        )\\n        del config[\"output_parser\"]\\n\\n    combined_config = {**config, **kwargs}\\n    return agent_cls(**combined_config)  # type: ignore\\n\\n\\n@deprecated(\"0.1.0\", removal=\"0.2.0\")\\ndef load_agent(\\n    path: Union[str, Path], **kwargs: Any\\n) -> Union[BaseSingleActionAgent, BaseMultiActionAgent]:\\n    \"\"\"Unified method for loading an agent from LangChainHub or local fs.\\n\\n    Args:\\n        path: Path to the agent file.\\n        **kwargs: Additional keyword arguments passed to the agent executor.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\loading.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Returns:\\n        An agent executor.\\n    \"\"\"\\n    valid_suffixes = {\"json\", \"yaml\"}\\n    if hub_result := try_load_from_hub(\\n        path, _load_agent_from_file, \"agents\", valid_suffixes\\n    ):\\n        return hub_result\\n    else:\\n        return _load_agent_from_file(path, **kwargs)\\n\\n\\ndef _load_agent_from_file(\\n    file: Union[str, Path], **kwargs: Any\\n) -> Union[BaseSingleActionAgent, BaseMultiActionAgent]:\\n    \"\"\"Load agent from file.\"\"\"\\n    valid_suffixes = {\"json\", \"yaml\"}\\n    # Convert file to Path object.\\n    if isinstance(file, str):\\n        file_path = Path(file)\\n    else:\\n        file_path = file\\n    # Load from either json or yaml.\\n    if file_path.suffix[1:] == \"json\":\\n        with open(file_path) as f:\\n            config = json.load(f)\\n    elif file_path.suffix[1:] == \"yaml\":\\n        with open(file_path, \"r\") as f:\\n            config = yaml.safe_load(f)\\n    else:\\n        raise ValueError(f\"Unsupported file type, must be one of {valid_suffixes}.\")\\n    # Load the agent from the config now.\\n    return load_agent_from_config(config, **kwargs)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\loading.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_tools_requests_get() -> BaseTool:\\n    return RequestsGetTool(requests_wrapper=TextRequestsWrapper())' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_tools_requests_post() -> BaseTool:\\n    return RequestsPostTool(requests_wrapper=TextRequestsWrapper())' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_tools_requests_patch() -> BaseTool:\\n    return RequestsPatchTool(requests_wrapper=TextRequestsWrapper())' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_tools_requests_put() -> BaseTool:\\n    return RequestsPutTool(requests_wrapper=TextRequestsWrapper())' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_tools_requests_delete() -> BaseTool:\\n    return RequestsDeleteTool(requests_wrapper=TextRequestsWrapper())' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_terminal() -> BaseTool:\\n    return ShellTool()' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_sleep() -> BaseTool:\\n    return SleepTool()' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_llm_math(llm: BaseLanguageModel) -> BaseTool:\\n    return Tool(\\n        name=\"Calculator\",\\n        description=\"Useful for when you need to answer questions about math.\",\\n        func=LLMMathChain.from_llm(llm=llm).run,\\n        coroutine=LLMMathChain.from_llm(llm=llm).arun,\\n    )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_open_meteo_api(llm: BaseLanguageModel) -> BaseTool:\\n    chain = APIChain.from_llm_and_api_docs(\\n        llm,\\n        open_meteo_docs.OPEN_METEO_DOCS,\\n        limit_to_domains=[\"https://api.open-meteo.com/\"],\\n    )\\n    return Tool(\\n        name=\"Open-Meteo-API\",\\n        description=\"Useful for when you want to get weather information from the OpenMeteo API. The input should be a question in natural language that this API can answer.\",\\n        func=chain.run,\\n    )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_news_api(llm: BaseLanguageModel, **kwargs: Any) -> BaseTool:\\n    news_api_key = kwargs[\"news_api_key\"]\\n    chain = APIChain.from_llm_and_api_docs(\\n        llm,\\n        news_docs.NEWS_DOCS,\\n        headers={\"X-Api-Key\": news_api_key},\\n        limit_to_domains=[\"https://newsapi.org/\"],\\n    )\\n    return Tool(\\n        name=\"News-API\",\\n        description=\"Use this when you want to get information about the top headlines of current news stories. The input should be a question in natural language that this API can answer.\",\\n        func=chain.run,\\n    )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_tmdb_api(llm: BaseLanguageModel, **kwargs: Any) -> BaseTool:\\n    tmdb_bearer_token = kwargs[\"tmdb_bearer_token\"]\\n    chain = APIChain.from_llm_and_api_docs(\\n        llm,\\n        tmdb_docs.TMDB_DOCS,\\n        headers={\"Authorization\": f\"Bearer {tmdb_bearer_token}\"},\\n        limit_to_domains=[\"https://api.themoviedb.org/\"],\\n    )\\n    return Tool(\\n        name=\"TMDB-API\",\\n        description=\"Useful for when you want to get information from The Movie Database. The input should be a question in natural language that this API can answer.\",\\n        func=chain.run,\\n    )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_podcast_api(llm: BaseLanguageModel, **kwargs: Any) -> BaseTool:\\n    listen_api_key = kwargs[\"listen_api_key\"]\\n    chain = APIChain.from_llm_and_api_docs(\\n        llm,\\n        podcast_docs.PODCAST_DOCS,\\n        headers={\"X-ListenAPI-Key\": listen_api_key},\\n        limit_to_domains=[\"https://listen-api.listennotes.com/\"],\\n    )\\n    return Tool(\\n        name=\"Podcast-API\",\\n        description=\"Use the Listen Notes Podcast API to search all podcasts or episodes. The input should be a question in natural language that this API can answer.\",\\n        func=chain.run,\\n    )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_lambda_api(**kwargs: Any) -> BaseTool:\\n    return Tool(\\n        name=kwargs[\"awslambda_tool_name\"],\\n        description=kwargs[\"awslambda_tool_description\"],\\n        func=LambdaWrapper(**kwargs).run,\\n    )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_wolfram_alpha(**kwargs: Any) -> BaseTool:\\n    return WolframAlphaQueryRun(api_wrapper=WolframAlphaAPIWrapper(**kwargs))' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_google_search(**kwargs: Any) -> BaseTool:\\n    return GoogleSearchRun(api_wrapper=GoogleSearchAPIWrapper(**kwargs))' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_merriam_webster(**kwargs: Any) -> BaseTool:\\n    return MerriamWebsterQueryRun(api_wrapper=MerriamWebsterAPIWrapper(**kwargs))' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_wikipedia(**kwargs: Any) -> BaseTool:\\n    return WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(**kwargs))' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_arxiv(**kwargs: Any) -> BaseTool:\\n    return ArxivQueryRun(api_wrapper=ArxivAPIWrapper(**kwargs))' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_golden_query(**kwargs: Any) -> BaseTool:\\n    return GoldenQueryRun(api_wrapper=GoldenQueryAPIWrapper(**kwargs))' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_pubmed(**kwargs: Any) -> BaseTool:\\n    return PubmedQueryRun(api_wrapper=PubMedAPIWrapper(**kwargs))' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_google_jobs(**kwargs: Any) -> BaseTool:\\n    return GoogleJobsQueryRun(api_wrapper=GoogleJobsAPIWrapper(**kwargs))' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_google_lens(**kwargs: Any) -> BaseTool:\\n    return GoogleLensQueryRun(api_wrapper=GoogleLensAPIWrapper(**kwargs))' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_google_serper(**kwargs: Any) -> BaseTool:\\n    return GoogleSerperRun(api_wrapper=GoogleSerperAPIWrapper(**kwargs))' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_google_scholar(**kwargs: Any) -> BaseTool:\\n    return GoogleScholarQueryRun(api_wrapper=GoogleScholarAPIWrapper(**kwargs))' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_google_finance(**kwargs: Any) -> BaseTool:\\n    return GoogleFinanceQueryRun(api_wrapper=GoogleFinanceAPIWrapper(**kwargs))' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_google_trends(**kwargs: Any) -> BaseTool:\\n    return GoogleTrendsQueryRun(api_wrapper=GoogleTrendsAPIWrapper(**kwargs))' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_google_serper_results_json(**kwargs: Any) -> BaseTool:\\n    return GoogleSerperResults(api_wrapper=GoogleSerperAPIWrapper(**kwargs))' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_google_search_results_json(**kwargs: Any) -> BaseTool:\\n    return GoogleSearchResults(api_wrapper=GoogleSearchAPIWrapper(**kwargs))' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_searchapi(**kwargs: Any) -> BaseTool:\\n    return SearchAPIRun(api_wrapper=SearchApiAPIWrapper(**kwargs))' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_searchapi_results_json(**kwargs: Any) -> BaseTool:\\n    return SearchAPIResults(api_wrapper=SearchApiAPIWrapper(**kwargs))' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_serpapi(**kwargs: Any) -> BaseTool:\\n    return Tool(\\n        name=\"Search\",\\n        description=\"A search engine. Useful for when you need to answer questions about current events. Input should be a search query.\",\\n        func=SerpAPIWrapper(**kwargs).run,\\n        coroutine=SerpAPIWrapper(**kwargs).arun,\\n    )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_stackexchange(**kwargs: Any) -> BaseTool:\\n    return StackExchangeTool(api_wrapper=StackExchangeAPIWrapper(**kwargs))' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_dalle_image_generator(**kwargs: Any) -> Tool:\\n    return Tool(\\n        \"Dall-E-Image-Generator\",\\n        DallEAPIWrapper(**kwargs).run,\\n        \"A wrapper around OpenAI DALL-E API. Useful for when you need to generate images from a text description. Input should be an image description.\",\\n    )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_twilio(**kwargs: Any) -> BaseTool:\\n    return Tool(\\n        name=\"Text-Message\",\\n        description=\"Useful for when you need to send a text message to a provided phone number.\",\\n        func=TwilioAPIWrapper(**kwargs).run,\\n    )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_searx_search(**kwargs: Any) -> BaseTool:\\n    return SearxSearchRun(wrapper=SearxSearchWrapper(**kwargs))' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_searx_search_results_json(**kwargs: Any) -> BaseTool:\\n    wrapper_kwargs = {k: v for k, v in kwargs.items() if k != \"num_results\"}\\n    return SearxSearchResults(wrapper=SearxSearchWrapper(**wrapper_kwargs), **kwargs)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_bing_search(**kwargs: Any) -> BaseTool:\\n    return BingSearchRun(api_wrapper=BingSearchAPIWrapper(**kwargs))' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_metaphor_search(**kwargs: Any) -> BaseTool:\\n    return MetaphorSearchResults(api_wrapper=MetaphorSearchAPIWrapper(**kwargs))' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_ddg_search(**kwargs: Any) -> BaseTool:\\n    return DuckDuckGoSearchRun(api_wrapper=DuckDuckGoSearchAPIWrapper(**kwargs))' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_human_tool(**kwargs: Any) -> BaseTool:\\n    return HumanInputRun(**kwargs)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_scenexplain(**kwargs: Any) -> BaseTool:\\n    return SceneXplainTool(**kwargs)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_graphql_tool(**kwargs: Any) -> BaseTool:\\n    graphql_endpoint = kwargs[\"graphql_endpoint\"]\\n    wrapper = GraphQLAPIWrapper(graphql_endpoint=graphql_endpoint)\\n    return BaseGraphQLTool(graphql_wrapper=wrapper)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_openweathermap(**kwargs: Any) -> BaseTool:\\n    return OpenWeatherMapQueryRun(api_wrapper=OpenWeatherMapAPIWrapper(**kwargs))' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_dataforseo_api_search(**kwargs: Any) -> BaseTool:\\n    return DataForSeoAPISearchRun(api_wrapper=DataForSeoAPIWrapper(**kwargs))' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_dataforseo_api_search_json(**kwargs: Any) -> BaseTool:\\n    return DataForSeoAPISearchResults(api_wrapper=DataForSeoAPIWrapper(**kwargs))' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_eleven_labs_text2speech(**kwargs: Any) -> BaseTool:\\n    return ElevenLabsText2SpeechTool(**kwargs)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_memorize(llm: BaseLanguageModel, **kwargs: Any) -> BaseTool:\\n    return Memorize(llm=llm)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_google_cloud_texttospeech(**kwargs: Any) -> BaseTool:\\n    return GoogleCloudTextToSpeechTool(**kwargs)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_reddit_search(**kwargs: Any) -> BaseTool:\\n    return RedditSearchRun(api_wrapper=RedditSearchAPIWrapper(**kwargs))' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _handle_callbacks(\\n    callback_manager: Optional[BaseCallbackManager], callbacks: Callbacks\\n) -> Callbacks:\\n    if callback_manager is not None:\\n        warnings.warn(\\n            \"callback_manager is deprecated. Please use callbacks instead.\",\\n            DeprecationWarning,\\n        )\\n        if callbacks is not None:\\n            raise ValueError(\\n                \"Cannot specify both callback_manager and callbacks arguments.\"\\n            )\\n        return callback_manager\\n    return callbacks' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def load_huggingface_tool(\\n    task_or_repo_id: str,\\n    model_repo_id: Optional[str] = None,\\n    token: Optional[str] = None,\\n    remote: bool = False,\\n    **kwargs: Any,\\n) -> BaseTool:\\n    \"\"\"Loads a tool from the HuggingFace Hub.\\n\\n    Args:\\n        task_or_repo_id: Task or model repo id.\\n        model_repo_id: Optional model repo id.\\n        token: Optional token.\\n        remote: Optional remote. Defaults to False.\\n        **kwargs:\\n\\n    Returns:\\n        A tool.\\n    \"\"\"\\n    try:\\n        from transformers import load_tool\\n    except ImportError:\\n        raise ImportError(\\n            \"HuggingFace tools require the libraries `transformers>=4.29.0`\"\\n            \" and `huggingface_hub>=0.14.1` to be installed.\"\\n            \" Please install it with\"\\n            \" `pip install --upgrade transformers huggingface_hub`.\"\\n        )\\n    hf_tool = load_tool(\\n        task_or_repo_id,\\n        model_repo_id=model_repo_id,\\n        token=token,\\n        remote=remote,\\n        **kwargs,\\n    )\\n    outputs = hf_tool.outputs\\n    if set(outputs) != {\"text\"}:\\n        raise NotImplementedError(\"Multimodal outputs not supported yet.\")\\n    inputs = hf_tool.inputs\\n    if set(inputs) != {\"text\"}:\\n        raise NotImplementedError(\"Multimodal inputs not supported yet.\")\\n    return Tool.from_function(\\n        hf_tool.__call__, name=hf_tool.name, description=hf_tool.description\\n    )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def load_tools(\\n    tool_names: List[str],\\n    llm: Optional[BaseLanguageModel] = None,\\n    callbacks: Callbacks = None,\\n    **kwargs: Any,\\n) -> List[BaseTool]:\\n    \"\"\"Load tools based on their name.\\n\\n    Tools allow agents to interact with various resources and services like\\n    APIs, databases, file systems, etc.\\n\\n    Please scope the permissions of each tools to the minimum required for the\\n    application.\\n\\n    For example, if an application only needs to read from a database,\\n    the database tool should not be given write permissions. Moreover\\n    consider scoping the permissions to only allow accessing specific\\n    tables and impose user-level quota for limiting resource usage.\\n\\n    Please read the APIs of the individual tools to determine which configuration\\n    they support.\\n\\n    See [Security](https://python.langchain.com/docs/security) for more information.\\n\\n    Args:\\n        tool_names: name of tools to load.\\n        llm: An optional language model, may be needed to initialize certain tools.\\n        callbacks: Optional callback manager or list of callback handlers.\\n            If not provided, default global callback manager will be used.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Returns:\\n        List of tools.\\n    \"\"\"\\n    tools = []\\n    callbacks = _handle_callbacks(\\n        callback_manager=kwargs.get(\"callback_manager\"), callbacks=callbacks\\n    )\\n    # print(_BASE_TOOLS)\\n    # print(1)\\n    for name in tool_names:\\n        if name == \"requests\":\\n            warnings.warn(\\n                \"tool name `requests` is deprecated - \"\\n                \"please use `requests_all` or specify the requests method\"\\n            )\\n        if name == \"requests_all\":\\n            # expand requests into various methods\\n            requests_method_tools = [\\n                _tool for _tool in _BASE_TOOLS if _tool.startswith(\"requests_\")\\n            ]\\n            tool_names.extend(requests_method_tools)\\n        elif name in _BASE_TOOLS:\\n            tools.append(_BASE_TOOLS[name]())\\n        elif name in _LLM_TOOLS:\\n            if llm is None:\\n                raise ValueError(f\"Tool {name} requires an LLM to be provided\")\\n            tool = _LLM_TOOLS[name](llm)\\n            tools.append(tool)\\n        elif name in _EXTRA_LLM_TOOLS:\\n            if llm is None:\\n                raise ValueError(f\"Tool {name} requires an LLM to be provided\")\\n            _get_llm_tool_func, extra_keys = _EXTRA_LLM_TOOLS[name]\\n            missing_keys = set(extra_keys).difference(kwargs)\\n            if missing_keys:\\n                raise ValueError(\\n                    f\"Tool {name} requires some parameters that were not \"\\n                    f\"provided: {missing_keys}\"\\n                )\\n            sub_kwargs = {k: kwargs[k] for k in extra_keys}\\n            tool = _get_llm_tool_func(llm=llm, **sub_kwargs)\\n            tools.append(tool)\\n        elif name in _EXTRA_OPTIONAL_TOOLS:\\n            _get_tool_func, extra_keys = _EXTRA_OPTIONAL_TOOLS[name]\\n            sub_kwargs = {k: kwargs[k] for k in extra_keys if k in kwargs}\\n            tool = _get_tool_func(**sub_kwargs)\\n            tools.append(tool)\\n        else:\\n            raise ValueError(f\"Got unknown tool {name}\")\\n    if callbacks is not None:\\n        for tool in tools:' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='tool.callbacks = callbacks\\n    return tools' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def get_all_tool_names() -> List[str]:\\n    \"\"\"Get a list of all possible tool names.\"\"\"\\n    return (\\n        list(_BASE_TOOLS)\\n        + list(_EXTRA_OPTIONAL_TOOLS)\\n        + list(_EXTRA_LLM_TOOLS)\\n        + list(_LLM_TOOLS)\\n    )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# flake8: noqa\\n\"\"\"Tools provide access to various resources and services.\\n\\nLangChain has a large ecosystem of integrations with various external resources\\nlike local and remote file systems, APIs and databases.\\n\\nThese integrations allow developers to create versatile applications that combine the\\npower of LLMs with the ability to access, interact with and manipulate external\\nresources.\\n\\nWhen developing an application, developers should inspect the capabilities and\\npermissions of the tools that underlie the given agent toolkit, and determine\\nwhether permissions of the given toolkit are appropriate for the application.\\n\\nSee [Security](https://python.langchain.com/docs/security) for more information.\\n\"\"\"\\nimport warnings\\nfrom typing import Any, Dict, List, Optional, Callable, Tuple\\nfrom mypy_extensions import Arg, KwArg' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.tools import Tool\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.callbacks import BaseCallbackManager\\nfrom langchain_core.callbacks import Callbacks\\nfrom langchain.chains.api import news_docs, open_meteo_docs, podcast_docs, tmdb_docs\\nfrom langchain.chains.api.base import APIChain\\nfrom langchain.chains.llm_math.base import LLMMathChain\\nfrom langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\\nfrom langchain_community.utilities.requests import TextRequestsWrapper\\nfrom langchain_community.tools.arxiv.tool import ArxivQueryRun\\nfrom langchain_community.tools.golden_query.tool import GoldenQueryRun\\nfrom langchain_community.tools.pubmed.tool import PubmedQueryRun\\nfrom langchain_core.tools import BaseTool\\nfrom langchain_community.tools.bing_search.tool import BingSearchRun\\nfrom langchain_community.tools.ddg_search.tool import DuckDuckGoSearchRun\\nfrom langchain_community.tools.google_cloud.texttospeech import (\\n    GoogleCloudTextToSpeechTool,\\n)\\nfrom langchain_community.tools.google_lens.tool import GoogleLensQueryRun\\nfrom langchain_community.tools.google_search.tool import (\\n    GoogleSearchResults,\\n    GoogleSearchRun,\\n)\\nfrom langchain_community.tools.google_scholar.tool import GoogleScholarQueryRun\\nfrom langchain_community.tools.google_finance.tool import GoogleFinanceQueryRun\\nfrom langchain_community.tools.google_trends.tool import GoogleTrendsQueryRun\\nfrom langchain_community.tools.metaphor_search.tool import MetaphorSearchResults\\nfrom langchain_community.tools.google_jobs.tool import GoogleJobsQueryRun\\nfrom langchain_community.tools.google_serper.tool import (\\n    GoogleSerperResults,\\n    GoogleSerperRun,\\n)\\nfrom langchain_community.tools.searchapi.tool import SearchAPIResults, SearchAPIRun\\nfrom langchain_community.tools.graphql.tool import BaseGraphQLTool\\nfrom langchain_community.tools.human.tool import HumanInputRun\\nfrom langchain_community.tools.requests.tool import (\\n    RequestsDeleteTool,\\n    RequestsGetTool,\\n    RequestsPatchTool,' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='RequestsPostTool,\\n    RequestsPutTool,\\n)\\nfrom langchain_community.tools.eleven_labs.text2speech import ElevenLabsText2SpeechTool\\nfrom langchain_community.tools.scenexplain.tool import SceneXplainTool\\nfrom langchain_community.tools.searx_search.tool import (\\n    SearxSearchResults,\\n    SearxSearchRun,\\n)\\nfrom langchain_community.tools.shell.tool import ShellTool\\nfrom langchain_community.tools.sleep.tool import SleepTool\\nfrom langchain_community.tools.stackexchange.tool import StackExchangeTool\\nfrom langchain_community.tools.merriam_webster.tool import MerriamWebsterQueryRun\\nfrom langchain_community.tools.wikipedia.tool import WikipediaQueryRun\\nfrom langchain_community.tools.wolfram_alpha.tool import WolframAlphaQueryRun\\nfrom langchain_community.tools.openweathermap.tool import OpenWeatherMapQueryRun\\nfrom langchain_community.tools.dataforseo_api_search import DataForSeoAPISearchRun\\nfrom langchain_community.tools.dataforseo_api_search import DataForSeoAPISearchResults\\nfrom langchain_community.tools.memorize.tool import Memorize\\nfrom langchain_community.tools.reddit_search.tool import RedditSearchRun\\nfrom langchain_community.utilities.arxiv import ArxivAPIWrapper\\nfrom langchain_community.utilities.golden_query import GoldenQueryAPIWrapper\\nfrom langchain_community.utilities.pubmed import PubMedAPIWrapper\\nfrom langchain_community.utilities.bing_search import BingSearchAPIWrapper\\nfrom langchain_community.utilities.duckduckgo_search import DuckDuckGoSearchAPIWrapper\\nfrom langchain_community.utilities.google_lens import GoogleLensAPIWrapper\\nfrom langchain_community.utilities.google_jobs import GoogleJobsAPIWrapper\\nfrom langchain_community.utilities.google_search import GoogleSearchAPIWrapper\\nfrom langchain_community.utilities.google_serper import GoogleSerperAPIWrapper\\nfrom langchain_community.utilities.google_scholar import GoogleScholarAPIWrapper\\nfrom langchain_community.utilities.google_finance import GoogleFinanceAPIWrapper\\nfrom langchain_community.utilities.google_trends import GoogleTrendsAPIWrapper' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utilities.metaphor_search import MetaphorSearchAPIWrapper\\nfrom langchain_community.utilities.awslambda import LambdaWrapper\\nfrom langchain_community.utilities.graphql import GraphQLAPIWrapper\\nfrom langchain_community.utilities.searchapi import SearchApiAPIWrapper\\nfrom langchain_community.utilities.searx_search import SearxSearchWrapper\\nfrom langchain_community.utilities.serpapi import SerpAPIWrapper\\nfrom langchain_community.utilities.stackexchange import StackExchangeAPIWrapper\\nfrom langchain_community.utilities.twilio import TwilioAPIWrapper\\nfrom langchain_community.utilities.merriam_webster import MerriamWebsterAPIWrapper\\nfrom langchain_community.utilities.wikipedia import WikipediaAPIWrapper\\nfrom langchain_community.utilities.wolfram_alpha import WolframAlphaAPIWrapper\\nfrom langchain_community.utilities.openweathermap import OpenWeatherMapAPIWrapper\\nfrom langchain_community.utilities.dataforseo_api_search import DataForSeoAPIWrapper\\nfrom langchain_community.utilities.reddit_search import RedditSearchAPIWrapper' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# Code for: def _get_tools_requests_get() -> BaseTool:\\n\\n\\n# Code for: def _get_tools_requests_post() -> BaseTool:\\n\\n\\n# Code for: def _get_tools_requests_patch() -> BaseTool:\\n\\n\\n# Code for: def _get_tools_requests_put() -> BaseTool:\\n\\n\\n# Code for: def _get_tools_requests_delete() -> BaseTool:\\n\\n\\n# Code for: def _get_terminal() -> BaseTool:\\n\\n\\n# Code for: def _get_sleep() -> BaseTool:\\n\\n\\n_BASE_TOOLS: Dict[str, Callable[[], BaseTool]] = {\\n    \"requests\": _get_tools_requests_get,  # preserved for backwards compatibility\\n    \"requests_get\": _get_tools_requests_get,\\n    \"requests_post\": _get_tools_requests_post,\\n    \"requests_patch\": _get_tools_requests_patch,\\n    \"requests_put\": _get_tools_requests_put,\\n    \"requests_delete\": _get_tools_requests_delete,\\n    \"terminal\": _get_terminal,\\n    \"sleep\": _get_sleep,\\n}\\n\\n\\n# Code for: def _get_llm_math(llm: BaseLanguageModel) -> BaseTool:\\n\\n\\n# Code for: def _get_open_meteo_api(llm: BaseLanguageModel) -> BaseTool:\\n\\n\\n_LLM_TOOLS: Dict[str, Callable[[BaseLanguageModel], BaseTool]] = {\\n    \"llm-math\": _get_llm_math,\\n    \"open-meteo-api\": _get_open_meteo_api,\\n}\\n\\n\\n# Code for: def _get_news_api(llm: BaseLanguageModel, **kwargs: Any) -> BaseTool:\\n\\n\\n# Code for: def _get_tmdb_api(llm: BaseLanguageModel, **kwargs: Any) -> BaseTool:\\n\\n\\n# Code for: def _get_podcast_api(llm: BaseLanguageModel, **kwargs: Any) -> BaseTool:\\n\\n\\n# Code for: def _get_lambda_api(**kwargs: Any) -> BaseTool:\\n\\n\\n# Code for: def _get_wolfram_alpha(**kwargs: Any) -> BaseTool:\\n\\n\\n# Code for: def _get_google_search(**kwargs: Any) -> BaseTool:\\n\\n\\n# Code for: def _get_merriam_webster(**kwargs: Any) -> BaseTool:\\n\\n\\n# Code for: def _get_wikipedia(**kwargs: Any) -> BaseTool:\\n\\n\\n# Code for: def _get_arxiv(**kwargs: Any) -> BaseTool:\\n\\n\\n# Code for: def _get_golden_query(**kwargs: Any) -> BaseTool:\\n\\n\\n# Code for: def _get_pubmed(**kwargs: Any) -> BaseTool:\\n\\n\\n# Code for: def _get_google_jobs(**kwargs: Any) -> BaseTool:\\n\\n\\n# Code for: def _get_google_lens(**kwargs: Any) -> BaseTool:\\n\\n\\n# Code for: def _get_google_serper(**kwargs: Any) -> BaseTool:' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# Code for: def _get_google_scholar(**kwargs: Any) -> BaseTool:\\n\\n\\n# Code for: def _get_google_finance(**kwargs: Any) -> BaseTool:\\n\\n\\n# Code for: def _get_google_trends(**kwargs: Any) -> BaseTool:\\n\\n\\n# Code for: def _get_google_serper_results_json(**kwargs: Any) -> BaseTool:\\n\\n\\n# Code for: def _get_google_search_results_json(**kwargs: Any) -> BaseTool:\\n\\n\\n# Code for: def _get_searchapi(**kwargs: Any) -> BaseTool:\\n\\n\\n# Code for: def _get_searchapi_results_json(**kwargs: Any) -> BaseTool:\\n\\n\\n# Code for: def _get_serpapi(**kwargs: Any) -> BaseTool:\\n\\n\\n# Code for: def _get_stackexchange(**kwargs: Any) -> BaseTool:\\n\\n\\n# Code for: def _get_dalle_image_generator(**kwargs: Any) -> Tool:\\n\\n\\n# Code for: def _get_twilio(**kwargs: Any) -> BaseTool:\\n\\n\\n# Code for: def _get_searx_search(**kwargs: Any) -> BaseTool:\\n\\n\\n# Code for: def _get_searx_search_results_json(**kwargs: Any) -> BaseTool:\\n\\n\\n# Code for: def _get_bing_search(**kwargs: Any) -> BaseTool:\\n\\n\\n# Code for: def _get_metaphor_search(**kwargs: Any) -> BaseTool:\\n\\n\\n# Code for: def _get_ddg_search(**kwargs: Any) -> BaseTool:\\n\\n\\n# Code for: def _get_human_tool(**kwargs: Any) -> BaseTool:\\n\\n\\n# Code for: def _get_scenexplain(**kwargs: Any) -> BaseTool:\\n\\n\\n# Code for: def _get_graphql_tool(**kwargs: Any) -> BaseTool:\\n\\n\\n# Code for: def _get_openweathermap(**kwargs: Any) -> BaseTool:\\n\\n\\n# Code for: def _get_dataforseo_api_search(**kwargs: Any) -> BaseTool:\\n\\n\\n# Code for: def _get_dataforseo_api_search_json(**kwargs: Any) -> BaseTool:\\n\\n\\n# Code for: def _get_eleven_labs_text2speech(**kwargs: Any) -> BaseTool:\\n\\n\\n# Code for: def _get_memorize(llm: BaseLanguageModel, **kwargs: Any) -> BaseTool:\\n\\n\\n# Code for: def _get_google_cloud_texttospeech(**kwargs: Any) -> BaseTool:\\n\\n\\n# Code for: def _get_reddit_search(**kwargs: Any) -> BaseTool:' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='_EXTRA_LLM_TOOLS: Dict[\\n    str,\\n    Tuple[Callable[[Arg(BaseLanguageModel, \"llm\"), KwArg(Any)], BaseTool], List[str]],\\n] = {\\n    \"news-api\": (_get_news_api, [\"news_api_key\"]),\\n    \"tmdb-api\": (_get_tmdb_api, [\"tmdb_bearer_token\"]),\\n    \"podcast-api\": (_get_podcast_api, [\"listen_api_key\"]),\\n    \"memorize\": (_get_memorize, []),\\n}\\n_EXTRA_OPTIONAL_TOOLS: Dict[str, Tuple[Callable[[KwArg(Any)], BaseTool], List[str]]] = {\\n    \"wolfram-alpha\": (_get_wolfram_alpha, [\"wolfram_alpha_appid\"]),\\n    \"google-search\": (_get_google_search, [\"google_api_key\", \"google_cse_id\"]),\\n    \"google-search-results-json\": (\\n        _get_google_search_results_json,\\n        [\"google_api_key\", \"google_cse_id\", \"num_results\"],\\n    ),\\n    \"searx-search-results-json\": (\\n        _get_searx_search_results_json,\\n        [\"searx_host\", \"engines\", \"num_results\", \"aiosession\"],\\n    ),\\n    \"bing-search\": (_get_bing_search, [\"bing_subscription_key\", \"bing_search_url\"]),\\n    \"metaphor-search\": (_get_metaphor_search, [\"metaphor_api_key\"]),\\n    \"ddg-search\": (_get_ddg_search, []),\\n    \"google-lens\": (_get_google_lens, [\"serp_api_key\"]),\\n    \"google-serper\": (_get_google_serper, [\"serper_api_key\", \"aiosession\"]),\\n    \"google-scholar\": (\\n        _get_google_scholar,\\n        [\"top_k_results\", \"hl\", \"lr\", \"serp_api_key\"],\\n    ),\\n    \"google-finance\": (\\n        _get_google_finance,\\n        [\"serp_api_key\"],\\n    ),\\n    \"google-trends\": (\\n        _get_google_trends,\\n        [\"serp_api_key\"],\\n    ),\\n    \"google-jobs\": (\\n        _get_google_jobs,\\n        [\"serp_api_key\"],\\n    ),\\n    \"google-serper-results-json\": (\\n        _get_google_serper_results_json,\\n        [\"serper_api_key\", \"aiosession\"],\\n    ),\\n    \"searchapi\": (_get_searchapi, [\"searchapi_api_key\", \"aiosession\"]),\\n    \"searchapi-results-json\": (\\n        _get_searchapi_results_json,\\n        [\"searchapi_api_key\", \"aiosession\"],\\n    ),\\n    \"serpapi\": (_get_serpapi, [\"serpapi_api_key\", \"aiosession\"]),\\n    \"dalle-image-generator\": (_get_dalle_image_generator, [\"openai_api_key\"]),' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"twilio\": (_get_twilio, [\"account_sid\", \"auth_token\", \"from_number\"]),\\n    \"searx-search\": (_get_searx_search, [\"searx_host\", \"engines\", \"aiosession\"]),\\n    \"merriam-webster\": (_get_merriam_webster, [\"merriam_webster_api_key\"]),\\n    \"wikipedia\": (_get_wikipedia, [\"top_k_results\", \"lang\"]),\\n    \"arxiv\": (\\n        _get_arxiv,\\n        [\"top_k_results\", \"load_max_docs\", \"load_all_available_meta\"],\\n    ),\\n    \"golden-query\": (_get_golden_query, [\"golden_api_key\"]),\\n    \"pubmed\": (_get_pubmed, [\"top_k_results\"]),\\n    \"human\": (_get_human_tool, [\"prompt_func\", \"input_func\"]),\\n    \"awslambda\": (\\n        _get_lambda_api,\\n        [\"awslambda_tool_name\", \"awslambda_tool_description\", \"function_name\"],\\n    ),\\n    \"stackexchange\": (_get_stackexchange, []),\\n    \"sceneXplain\": (_get_scenexplain, []),\\n    \"graphql\": (_get_graphql_tool, [\"graphql_endpoint\"]),\\n    \"openweathermap-api\": (_get_openweathermap, [\"openweathermap_api_key\"]),\\n    \"dataforseo-api-search\": (\\n        _get_dataforseo_api_search,\\n        [\"api_login\", \"api_password\", \"aiosession\"],\\n    ),\\n    \"dataforseo-api-search-json\": (\\n        _get_dataforseo_api_search_json,\\n        [\"api_login\", \"api_password\", \"aiosession\"],\\n    ),\\n    \"eleven_labs_text2speech\": (_get_eleven_labs_text2speech, [\"eleven_api_key\"]),\\n    \"google_cloud_texttospeech\": (_get_google_cloud_texttospeech, []),\\n    \"reddit_search\": (\\n        _get_reddit_search,\\n        [\"reddit_client_id\", \"reddit_client_secret\", \"reddit_user_agent\"],\\n    ),\\n}' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# Code for: def _handle_callbacks(\\n\\n\\n# Code for: def load_huggingface_tool(\\n\\n\\n# Code for: def load_tools(\\n\\n\\n# Code for: def get_all_tool_names() -> List[str]:' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from typing import Any, Dict, List, Tuple\\n\\nfrom langchain_core.agents import AgentAction\\nfrom langchain_core.prompts.chat import ChatPromptTemplate\\n\\n\\nclass AgentScratchPadChatPromptTemplate(ChatPromptTemplate):\\n    \"\"\"Chat prompt template for the agent scratchpad.\"\"\"\\n\\n    @classmethod\\n    def is_lc_serializable(cls) -> bool:\\n        return False\\n\\n    def _construct_agent_scratchpad(\\n        self, intermediate_steps: List[Tuple[AgentAction, str]]\\n    ) -> str:\\n        if len(intermediate_steps) == 0:\\n            return \"\"\\n        thoughts = \"\"\\n        for action, observation in intermediate_steps:\\n            thoughts += action.log\\n            thoughts += f\"\\\\nObservation: {observation}\\\\nThought: \"\\n        return (\\n            f\"This was your previous work \"\\n            f\"(but I haven\\'t seen any of it! I only see what \"\\n            f\"you return as final answer):\\\\n{thoughts}\"\\n        )\\n\\n    def _merge_partial_and_user_variables(self, **kwargs: Any) -> Dict[str, Any]:\\n        intermediate_steps = kwargs.pop(\"intermediate_steps\")\\n        kwargs[\"agent_scratchpad\"] = self._construct_agent_scratchpad(\\n            intermediate_steps\\n        )\\n        return kwargs' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\schema.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Interface for tools.\"\"\"\\nfrom typing import List, Optional\\n\\nfrom langchain_core.callbacks import (\\n    AsyncCallbackManagerForToolRun,\\n    CallbackManagerForToolRun,\\n)\\nfrom langchain_core.tools import BaseTool, Tool, tool\\n\\n\\nclass InvalidTool(BaseTool):\\n    \"\"\"Tool that is run when invalid tool name is encountered by agent.\"\"\"\\n\\n    name: str = \"invalid_tool\"\\n    description: str = \"Called when tool name is invalid. Suggests valid tool names.\"\\n\\n    def _run(\\n        self,\\n        requested_tool_name: str,\\n        available_tool_names: List[str],\\n        run_manager: Optional[CallbackManagerForToolRun] = None,\\n    ) -> str:\\n        \"\"\"Use the tool.\"\"\"\\n        available_tool_names_str = \", \".join([tool for tool in available_tool_names])\\n        return (\\n            f\"{requested_tool_name} is not a valid tool, \"\\n            f\"try one of [{available_tool_names_str}].\"\\n        )\\n\\n    async def _arun(\\n        self,\\n        requested_tool_name: str,\\n        available_tool_names: List[str],\\n        run_manager: Optional[AsyncCallbackManagerForToolRun] = None,\\n    ) -> str:\\n        \"\"\"Use the tool asynchronously.\"\"\"\\n        available_tool_names_str = \", \".join([tool for tool in available_tool_names])\\n        return (\\n            f\"{requested_tool_name} is not a valid tool, \"\\n            f\"try one of [{available_tool_names_str}].\"\\n        )\\n\\n\\n__all__ = [\"InvalidTool\", \"BaseTool\", \"tool\", \"Tool\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\tools.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from typing import Dict, Type, Union\\n\\nfrom langchain.agents.agent import BaseSingleActionAgent\\nfrom langchain.agents.agent_types import AgentType\\nfrom langchain.agents.chat.base import ChatAgent\\nfrom langchain.agents.conversational.base import ConversationalAgent\\nfrom langchain.agents.conversational_chat.base import ConversationalChatAgent\\nfrom langchain.agents.mrkl.base import ZeroShotAgent\\nfrom langchain.agents.openai_functions_agent.base import OpenAIFunctionsAgent\\nfrom langchain.agents.openai_functions_multi_agent.base import OpenAIMultiFunctionsAgent\\nfrom langchain.agents.react.base import ReActDocstoreAgent\\nfrom langchain.agents.self_ask_with_search.base import SelfAskWithSearchAgent\\nfrom langchain.agents.structured_chat.base import StructuredChatAgent\\n\\nAGENT_TYPE = Union[Type[BaseSingleActionAgent], Type[OpenAIMultiFunctionsAgent]]\\n\\nAGENT_TO_CLASS: Dict[AgentType, AGENT_TYPE] = {\\n    AgentType.ZERO_SHOT_REACT_DESCRIPTION: ZeroShotAgent,\\n    AgentType.REACT_DOCSTORE: ReActDocstoreAgent,\\n    AgentType.SELF_ASK_WITH_SEARCH: SelfAskWithSearchAgent,\\n    AgentType.CONVERSATIONAL_REACT_DESCRIPTION: ConversationalAgent,\\n    AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION: ChatAgent,\\n    AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION: ConversationalChatAgent,\\n    AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION: StructuredChatAgent,\\n    AgentType.OPENAI_FUNCTIONS: OpenAIFunctionsAgent,\\n    AgentType.OPENAI_MULTI_FUNCTIONS: OpenAIMultiFunctionsAgent,\\n}' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\types.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from typing import Sequence\\n\\nfrom langchain_core.tools import BaseTool\\n\\n\\ndef validate_tools_single_input(class_name: str, tools: Sequence[BaseTool]) -> None:\\n    \"\"\"Validate tools for single input.\"\"\"\\n    for tool in tools:\\n        if not tool.is_single_input:\\n            raise ValueError(\\n                f\"{class_name} does not support multi-input tool {tool.name}.\"\\n            )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\utils.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"\\n**Agent** is a class that uses an LLM to choose a sequence of actions to take.\\n\\nIn Chains, a sequence of actions is hardcoded. In Agents,\\na language model is used as a reasoning engine to determine which actions\\nto take and in which order.\\n\\nAgents select and use **Tools** and **Toolkits** for actions.\\n\\n**Class hierarchy:**\\n\\n.. code-block::\\n\\n    BaseSingleActionAgent --> LLMSingleActionAgent\\n                              OpenAIFunctionsAgent\\n                              XMLAgent\\n                              Agent --> <name>Agent  # Examples: ZeroShotAgent, ChatAgent\\n                                        \\n\\n    BaseMultiActionAgent  --> OpenAIMultiFunctionsAgent\\n    \\n    \\n**Main helpers:**\\n\\n.. code-block::\\n\\n    AgentType, AgentExecutor, AgentOutputParser, AgentExecutorIterator,\\n    AgentAction, AgentFinish\\n    \\n\"\"\"  # noqa: E501\\nfrom pathlib import Path\\nfrom typing import Any\\n\\nfrom langchain_community.agent_toolkits import (\\n    create_json_agent,\\n    create_openapi_agent,\\n    create_pbi_agent,\\n    create_pbi_chat_agent,\\n    create_spark_sql_agent,\\n    create_sql_agent,\\n)\\nfrom langchain_core._api.path import as_import_path' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain.agents.agent import (\\n    Agent,\\n    AgentExecutor,\\n    AgentOutputParser,\\n    BaseMultiActionAgent,\\n    BaseSingleActionAgent,\\n    LLMSingleActionAgent,\\n)\\nfrom langchain.agents.agent_iterator import AgentExecutorIterator\\nfrom langchain.agents.agent_toolkits.vectorstore.base import (\\n    create_vectorstore_agent,\\n    create_vectorstore_router_agent,\\n)\\nfrom langchain.agents.agent_types import AgentType\\nfrom langchain.agents.conversational.base import ConversationalAgent\\nfrom langchain.agents.conversational_chat.base import ConversationalChatAgent\\nfrom langchain.agents.initialize import initialize_agent\\nfrom langchain.agents.json_chat.base import create_json_chat_agent\\nfrom langchain.agents.load_tools import (\\n    get_all_tool_names,\\n    load_huggingface_tool,\\n    load_tools,\\n)\\nfrom langchain.agents.loading import load_agent\\nfrom langchain.agents.mrkl.base import MRKLChain, ZeroShotAgent\\nfrom langchain.agents.openai_functions_agent.base import (\\n    OpenAIFunctionsAgent,\\n    create_openai_functions_agent,\\n)\\nfrom langchain.agents.openai_functions_multi_agent.base import OpenAIMultiFunctionsAgent\\nfrom langchain.agents.openai_tools.base import create_openai_tools_agent\\nfrom langchain.agents.react.agent import create_react_agent\\nfrom langchain.agents.react.base import ReActChain, ReActTextWorldAgent\\nfrom langchain.agents.self_ask_with_search.base import (\\n    SelfAskWithSearchChain,\\n    create_self_ask_with_search_agent,\\n)\\nfrom langchain.agents.structured_chat.base import (\\n    StructuredChatAgent,\\n    create_structured_chat_agent,\\n)\\nfrom langchain.agents.tools import Tool, tool\\nfrom langchain.agents.xml.base import XMLAgent, create_xml_agent\\n\\nDEPRECATED_CODE = [\\n    \"create_csv_agent\",\\n    \"create_pandas_dataframe_agent\",\\n    \"create_spark_dataframe_agent\",\\n    \"create_xorbits_agent\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def __getattr__(name: str) -> Any:\\n    \"\"\"Get attr name.\"\"\"\\n    if name in DEPRECATED_CODE:\\n        # Get directory of langchain package\\n        HERE = Path(__file__).parents[1]\\n        relative_path = as_import_path(\\n            Path(__file__).parent, suffix=name, relative_to=HERE\\n        )\\n        old_path = \"langchain.\" + relative_path\\n        new_path = \"langchain_experimental.\" + relative_path\\n        raise ImportError(\\n            f\"{name} has been moved to langchain experimental. \"\\n            \"See https://github.com/langchain-ai/langchain/discussions/11680\"\\n            \"for more information.\\\\n\"\\n            f\"Please update your import statement from: `{old_path}` to `{new_path}`.\"\\n        )\\n    raise AttributeError(f\"{name} does not exist\")\\n\\n\\n__all__ = [\\n    \"Agent\",\\n    \"AgentExecutor\",\\n    \"AgentExecutorIterator\",\\n    \"AgentOutputParser\",\\n    \"AgentType\",\\n    \"BaseMultiActionAgent\",\\n    \"BaseSingleActionAgent\",\\n    \"ConversationalAgent\",\\n    \"ConversationalChatAgent\",\\n    \"LLMSingleActionAgent\",\\n    \"MRKLChain\",\\n    \"OpenAIFunctionsAgent\",\\n    \"OpenAIMultiFunctionsAgent\",\\n    \"ReActChain\",\\n    \"ReActTextWorldAgent\",\\n    \"SelfAskWithSearchChain\",\\n    \"StructuredChatAgent\",\\n    \"Tool\",\\n    \"ZeroShotAgent\",\\n    \"create_json_agent\",\\n    \"create_openapi_agent\",\\n    \"create_pbi_agent\",\\n    \"create_pbi_chat_agent\",\\n    \"create_spark_sql_agent\",\\n    \"create_sql_agent\",\\n    \"create_vectorstore_agent\",\\n    \"create_vectorstore_router_agent\",\\n    \"get_all_tool_names\",\\n    \"initialize_agent\",\\n    \"load_agent\",\\n    \"load_huggingface_tool\",\\n    \"load_tools\",\\n    \"tool\",\\n    \"XMLAgent\",\\n    \"create_openai_functions_agent\",\\n    \"create_xml_agent\",\\n    \"create_react_agent\",\\n    \"create_openai_tools_agent\",\\n    \"create_self_ask_with_search_agent\",\\n    \"create_json_chat_agent\",\\n    \"create_structured_chat_agent\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.agent_toolkits.azure_cognitive_services import (\\n    AzureCognitiveServicesToolkit,\\n)\\n\\n__all__ = [\"AzureCognitiveServicesToolkit\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\azure_cognitive_services.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.agent_toolkits.base import BaseToolkit\\n\\n__all__ = [\"BaseToolkit\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Agent toolkits contain integrations with various resources and services.\\n\\nLangChain has a large ecosystem of integrations with various external resources\\nlike local and remote file systems, APIs and databases.\\n\\nThese integrations allow developers to create versatile applications that combine the\\npower of LLMs with the ability to access, interact with and manipulate external\\nresources.\\n\\nWhen developing an application, developers should inspect the capabilities and\\npermissions of the tools that underlie the given agent toolkit, and determine\\nwhether permissions of the given toolkit are appropriate for the application.\\n\\nSee [Security](https://python.langchain.com/docs/security) for more information.\\n\"\"\"\\nimport warnings\\nfrom pathlib import Path\\nfrom typing import Any\\n\\nfrom langchain_core._api import LangChainDeprecationWarning\\nfrom langchain_core._api.path import as_import_path\\n\\nfrom langchain.agents.agent_toolkits.conversational_retrieval.openai_functions import (\\n    create_conversational_retrieval_agent,\\n)\\nfrom langchain.agents.agent_toolkits.vectorstore.base import (\\n    create_vectorstore_agent,\\n    create_vectorstore_router_agent,\\n)\\nfrom langchain.agents.agent_toolkits.vectorstore.toolkit import (\\n    VectorStoreInfo,\\n    VectorStoreRouterToolkit,\\n    VectorStoreToolkit,\\n)\\nfrom langchain.tools.retriever import create_retriever_tool\\nfrom langchain.utils.interactive_env import is_interactive_env\\n\\nDEPRECATED_AGENTS = [\\n    \"create_csv_agent\",\\n    \"create_pandas_dataframe_agent\",\\n    \"create_xorbits_agent\",\\n    \"create_python_agent\",\\n    \"create_spark_dataframe_agent\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def __getattr__(name: str) -> Any:\\n    \"\"\"Get attr name.\"\"\"\\n    if name in DEPRECATED_AGENTS:\\n        relative_path = as_import_path(Path(__file__).parent, suffix=name)\\n        old_path = \"langchain.\" + relative_path\\n        new_path = \"langchain_experimental.\" + relative_path\\n        raise ImportError(\\n            f\"{name} has been moved to langchain experimental. \"\\n            \"See https://github.com/langchain-ai/langchain/discussions/11680\"\\n            \"for more information.\\\\n\"\\n            f\"Please update your import statement from: `{old_path}` to `{new_path}`.\"\\n        )\\n\\n    from langchain_community import agent_toolkits\\n\\n    # If not in interactive env, raise warning.\\n    if not is_interactive_env():\\n        warnings.warn(\\n            \"Importing this agent toolkit from langchain is deprecated. Importing it \"\\n            \"from langchain will no longer be supported as of langchain==0.2.0. \"\\n            \"Please import from langchain-community instead:\\\\n\\\\n\"\\n            f\"`from langchain_community.agent_toolkits import {name}`.\\\\n\\\\n\"\\n            \"To install langchain-community run `pip install -U langchain-community`.\",\\n            category=LangChainDeprecationWarning,\\n        )\\n\\n    return getattr(agent_toolkits, name)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='__all__ = [\\n    \"AINetworkToolkit\",\\n    \"AmadeusToolkit\",\\n    \"AzureCognitiveServicesToolkit\",\\n    \"FileManagementToolkit\",\\n    \"GmailToolkit\",\\n    \"JiraToolkit\",\\n    \"JsonToolkit\",\\n    \"MultionToolkit\",\\n    \"NasaToolkit\",\\n    \"NLAToolkit\",\\n    \"O365Toolkit\",\\n    \"OpenAPIToolkit\",\\n    \"PlayWrightBrowserToolkit\",\\n    \"PowerBIToolkit\",\\n    \"SlackToolkit\",\\n    \"SteamToolkit\",\\n    \"SQLDatabaseToolkit\",\\n    \"SparkSQLToolkit\",\\n    \"VectorStoreInfo\",\\n    \"VectorStoreRouterToolkit\",\\n    \"VectorStoreToolkit\",\\n    \"ZapierToolkit\",\\n    \"create_json_agent\",\\n    \"create_openapi_agent\",\\n    \"create_pbi_agent\",\\n    \"create_pbi_chat_agent\",\\n    \"create_spark_sql_agent\",\\n    \"create_sql_agent\",\\n    \"create_vectorstore_agent\",\\n    \"create_vectorstore_router_agent\",\\n    \"create_conversational_retrieval_agent\",\\n    \"create_retriever_tool\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.agent_toolkits.ainetwork.toolkit import AINetworkToolkit\\n\\n__all__ = [\"AINetworkToolkit\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\ainetwork\\\\toolkit.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"AINetwork toolkit.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\ainetwork\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.agent_toolkits.amadeus.toolkit import AmadeusToolkit\\n\\n__all__ = [\"AmadeusToolkit\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\amadeus\\\\toolkit.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.agent_toolkits.clickup.toolkit import ClickupToolkit\\n\\n__all__ = [\"ClickupToolkit\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\clickup\\\\toolkit.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from typing import Any, List, Optional  # noqa: E501\\n\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.memory import BaseMemory\\nfrom langchain_core.messages import SystemMessage\\nfrom langchain_core.prompts.chat import MessagesPlaceholder\\n\\nfrom langchain.agents.agent import AgentExecutor\\nfrom langchain.agents.openai_functions_agent.agent_token_buffer_memory import (\\n    AgentTokenBufferMemory,\\n)\\nfrom langchain.agents.openai_functions_agent.base import OpenAIFunctionsAgent\\nfrom langchain.memory.token_buffer import ConversationTokenBufferMemory\\nfrom langchain.tools.base import BaseTool\\n\\n\\ndef _get_default_system_message() -> SystemMessage:\\n    return SystemMessage(\\n        content=(\\n            \"Do your best to answer the questions. \"\\n            \"Feel free to use any tools available to look up \"\\n            \"relevant information, only if necessary\"\\n        )\\n    )\\n\\n\\ndef create_conversational_retrieval_agent(\\n    llm: BaseLanguageModel,\\n    tools: List[BaseTool],\\n    remember_intermediate_steps: bool = True,\\n    memory_key: str = \"chat_history\",\\n    system_message: Optional[SystemMessage] = None,\\n    verbose: bool = False,\\n    max_token_limit: int = 2000,\\n    **kwargs: Any,\\n) -> AgentExecutor:\\n    \"\"\"A convenience method for creating a conversational retrieval agent.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\conversational_retrieval\\\\openai_functions.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Args:\\n        llm: The language model to use, should be ChatOpenAI\\n        tools: A list of tools the agent has access to\\n        remember_intermediate_steps: Whether the agent should remember intermediate\\n            steps or not. Intermediate steps refer to prior action/observation\\n            pairs from previous questions. The benefit of remembering these is if\\n            there is relevant information in there, the agent can use it to answer\\n            follow up questions. The downside is it will take up more tokens.\\n        memory_key: The name of the memory key in the prompt.\\n        system_message: The system message to use. By default, a basic one will\\n            be used.\\n        verbose: Whether or not the final AgentExecutor should be verbose or not,\\n            defaults to False.\\n        max_token_limit: The max number of tokens to keep around in memory.\\n            Defaults to 2000.\\n\\n    Returns:\\n        An agent executor initialized appropriately\\n    \"\"\"\\n\\n    if remember_intermediate_steps:\\n        memory: BaseMemory = AgentTokenBufferMemory(\\n            memory_key=memory_key, llm=llm, max_token_limit=max_token_limit\\n        )\\n    else:\\n        memory = ConversationTokenBufferMemory(\\n            memory_key=memory_key,\\n            return_messages=True,\\n            output_key=\"output\",\\n            llm=llm,\\n            max_token_limit=max_token_limit,\\n        )\\n\\n    _system_message = system_message or _get_default_system_message()\\n    prompt = OpenAIFunctionsAgent.create_prompt(\\n        system_message=_system_message,\\n        extra_prompt_messages=[MessagesPlaceholder(variable_name=memory_key)],\\n    )\\n    agent = OpenAIFunctionsAgent(llm=llm, tools=tools, prompt=prompt)\\n    return AgentExecutor(\\n        agent=agent,\\n        tools=tools,\\n        memory=memory,\\n        verbose=verbose,\\n        return_intermediate_steps=remember_intermediate_steps,\\n        **kwargs,\\n    )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\conversational_retrieval\\\\openai_functions.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain.tools.retriever import create_retriever_tool\\n\\n__all__ = [\"create_retriever_tool\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\conversational_retrieval\\\\tool.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from pathlib import Path\\nfrom typing import Any\\n\\nfrom langchain_core._api.path import as_import_path\\n\\n\\ndef __getattr__(name: str) -> Any:\\n    \"\"\"Get attr name.\"\"\"\\n\\n    if name == \"create_csv_agent\":\\n        # Get directory of langchain package\\n        HERE = Path(__file__).parents[3]\\n        here = as_import_path(Path(__file__).parent, relative_to=HERE)\\n\\n        old_path = \"langchain.\" + here + \".\" + name\\n        new_path = \"langchain_experimental.\" + here + \".\" + name\\n        raise ImportError(\\n            \"This agent has been moved to langchain experiment. \"\\n            \"This agent relies on python REPL tool under the hood, so to use it \"\\n            \"safely please sandbox the python REPL. \"\\n            \"Read https://github.com/langchain-ai/langchain/blob/master/SECURITY.md \"\\n            \"and https://github.com/langchain-ai/langchain/discussions/11680\"\\n            \"To keep using this code as is, install langchain experimental and \"\\n            f\"update your import statement from:\\\\n `{old_path}` to `{new_path}`.\"\\n        )\\n    raise AttributeError(f\"{name} does not exist\")' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\csv\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.agent_toolkits.file_management.toolkit import (\\n    FileManagementToolkit,\\n)\\n\\n__all__ = [\"FileManagementToolkit\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\file_management\\\\toolkit.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Local file management toolkit.\"\"\"\\n\\nfrom langchain_community.agent_toolkits.file_management.toolkit import (\\n    FileManagementToolkit,\\n)\\n\\n__all__ = [\"FileManagementToolkit\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\file_management\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.agent_toolkits.github.toolkit import (\\n    BranchName,\\n    CommentOnIssue,\\n    CreateFile,\\n    CreatePR,\\n    CreateReviewRequest,\\n    DeleteFile,\\n    DirectoryPath,\\n    GetIssue,\\n    GetPR,\\n    GitHubToolkit,\\n    NoInput,\\n    ReadFile,\\n    SearchCode,\\n    SearchIssuesAndPRs,\\n    UpdateFile,\\n)\\n\\n__all__ = [\\n    \"NoInput\",\\n    \"GetIssue\",\\n    \"CommentOnIssue\",\\n    \"GetPR\",\\n    \"CreatePR\",\\n    \"CreateFile\",\\n    \"ReadFile\",\\n    \"UpdateFile\",\\n    \"DeleteFile\",\\n    \"DirectoryPath\",\\n    \"BranchName\",\\n    \"SearchCode\",\\n    \"CreateReviewRequest\",\\n    \"SearchIssuesAndPRs\",\\n    \"GitHubToolkit\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\github\\\\toolkit.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"GitHub Toolkit.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\github\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.agent_toolkits.gitlab.toolkit import GitLabToolkit\\n\\n__all__ = [\"GitLabToolkit\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\gitlab\\\\toolkit.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"GitLab Toolkit.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\gitlab\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.agent_toolkits.gmail.toolkit import SCOPES, GmailToolkit\\n\\n__all__ = [\"SCOPES\", \"GmailToolkit\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\gmail\\\\toolkit.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Gmail toolkit.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\gmail\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.agent_toolkits.jira.toolkit import JiraToolkit\\n\\n__all__ = [\"JiraToolkit\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\jira\\\\toolkit.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Jira Toolkit.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\jira\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.agent_toolkits.json.base import create_json_agent\\n\\n__all__ = [\"create_json_agent\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\json\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.agent_toolkits.json.prompt import JSON_PREFIX, JSON_SUFFIX\\n\\n__all__ = [\"JSON_PREFIX\", \"JSON_SUFFIX\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\json\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.agent_toolkits.json.toolkit import JsonToolkit\\n\\n__all__ = [\"JsonToolkit\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\json\\\\toolkit.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Json agent.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\json\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.agent_toolkits.multion.toolkit import MultionToolkit\\n\\n__all__ = [\"MultionToolkit\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\multion\\\\toolkit.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"MultiOn Toolkit.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\multion\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.agent_toolkits.nasa.toolkit import NasaToolkit\\n\\n__all__ = [\"NasaToolkit\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\nasa\\\\toolkit.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"NASA Toolkit\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\nasa\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.agent_toolkits.nla.tool import NLATool\\n\\n__all__ = [\"NLATool\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\nla\\\\tool.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.agent_toolkits.nla.toolkit import NLAToolkit\\n\\n__all__ = [\"NLAToolkit\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\nla\\\\toolkit.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.agent_toolkits.office365.toolkit import O365Toolkit\\n\\n__all__ = [\"O365Toolkit\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\office365\\\\toolkit.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Office365 toolkit.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\office365\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.agent_toolkits.openapi.base import create_openapi_agent\\n\\n__all__ = [\"create_openapi_agent\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\openapi\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.agent_toolkits.openapi.planner import (\\n    MAX_RESPONSE_LENGTH,\\n    RequestsDeleteToolWithParsing,\\n    RequestsGetToolWithParsing,\\n    RequestsPatchToolWithParsing,\\n    RequestsPostToolWithParsing,\\n    RequestsPutToolWithParsing,\\n    create_openapi_agent,\\n)\\n\\n__all__ = [\\n    \"MAX_RESPONSE_LENGTH\",\\n    \"RequestsGetToolWithParsing\",\\n    \"RequestsPostToolWithParsing\",\\n    \"RequestsPatchToolWithParsing\",\\n    \"RequestsPutToolWithParsing\",\\n    \"RequestsDeleteToolWithParsing\",\\n    \"create_openapi_agent\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\openapi\\\\planner.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.agent_toolkits.openapi.planner_prompt import (\\n    API_CONTROLLER_PROMPT,\\n    API_CONTROLLER_TOOL_DESCRIPTION,\\n    API_CONTROLLER_TOOL_NAME,\\n    API_ORCHESTRATOR_PROMPT,\\n    API_PLANNER_PROMPT,\\n    API_PLANNER_TOOL_DESCRIPTION,\\n    API_PLANNER_TOOL_NAME,\\n    PARSING_DELETE_PROMPT,\\n    PARSING_GET_PROMPT,\\n    PARSING_PATCH_PROMPT,\\n    PARSING_POST_PROMPT,\\n    PARSING_PUT_PROMPT,\\n    REQUESTS_DELETE_TOOL_DESCRIPTION,\\n    REQUESTS_GET_TOOL_DESCRIPTION,\\n    REQUESTS_PATCH_TOOL_DESCRIPTION,\\n    REQUESTS_POST_TOOL_DESCRIPTION,\\n    REQUESTS_PUT_TOOL_DESCRIPTION,\\n)\\n\\n__all__ = [\\n    \"API_PLANNER_PROMPT\",\\n    \"API_PLANNER_TOOL_NAME\",\\n    \"API_PLANNER_TOOL_DESCRIPTION\",\\n    \"API_CONTROLLER_PROMPT\",\\n    \"API_CONTROLLER_TOOL_NAME\",\\n    \"API_CONTROLLER_TOOL_DESCRIPTION\",\\n    \"API_ORCHESTRATOR_PROMPT\",\\n    \"REQUESTS_GET_TOOL_DESCRIPTION\",\\n    \"PARSING_GET_PROMPT\",\\n    \"REQUESTS_POST_TOOL_DESCRIPTION\",\\n    \"PARSING_POST_PROMPT\",\\n    \"REQUESTS_PATCH_TOOL_DESCRIPTION\",\\n    \"PARSING_PATCH_PROMPT\",\\n    \"REQUESTS_PUT_TOOL_DESCRIPTION\",\\n    \"PARSING_PUT_PROMPT\",\\n    \"REQUESTS_DELETE_TOOL_DESCRIPTION\",\\n    \"PARSING_DELETE_PROMPT\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\openapi\\\\planner_prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.agent_toolkits.openapi.prompt import (\\n    DESCRIPTION,\\n    OPENAPI_PREFIX,\\n    OPENAPI_SUFFIX,\\n)\\n\\n__all__ = [\"OPENAPI_PREFIX\", \"OPENAPI_SUFFIX\", \"DESCRIPTION\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\openapi\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.agent_toolkits.openapi.spec import (\\n    ReducedOpenAPISpec,\\n    reduce_openapi_spec,\\n)\\n\\n__all__ = [\"ReducedOpenAPISpec\", \"reduce_openapi_spec\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\openapi\\\\spec.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.agent_toolkits.openapi.toolkit import (\\n    OpenAPIToolkit,\\n    RequestsToolkit,\\n)\\n\\n__all__ = [\"RequestsToolkit\", \"OpenAPIToolkit\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\openapi\\\\toolkit.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"OpenAPI spec agent.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\openapi\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from pathlib import Path\\nfrom typing import Any\\n\\nfrom langchain_core._api.path import as_import_path\\n\\n\\ndef __getattr__(name: str) -> Any:\\n    \"\"\"Get attr name.\"\"\"\\n\\n    if name == \"create_pandas_dataframe_agent\":\\n        # Get directory of langchain package\\n        HERE = Path(__file__).parents[3]\\n        here = as_import_path(Path(__file__).parent, relative_to=HERE)\\n\\n        old_path = \"langchain.\" + here + \".\" + name\\n        new_path = \"langchain_experimental.\" + here + \".\" + name\\n        raise ImportError(\\n            \"This agent has been moved to langchain experiment. \"\\n            \"This agent relies on python REPL tool under the hood, so to use it \"\\n            \"safely please sandbox the python REPL. \"\\n            \"Read https://github.com/langchain-ai/langchain/blob/master/SECURITY.md \"\\n            \"and https://github.com/langchain-ai/langchain/discussions/11680\"\\n            \"To keep using this code as is, install langchain experimental and \"\\n            f\"update your import statement from:\\\\n `{old_path}` to `{new_path}`.\"\\n        )\\n    raise AttributeError(f\"{name} does not exist\")' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\pandas\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.agent_toolkits.playwright.toolkit import (\\n    PlayWrightBrowserToolkit,\\n)\\n\\n__all__ = [\"PlayWrightBrowserToolkit\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\playwright\\\\toolkit.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Playwright browser toolkit.\"\"\"\\nfrom langchain_community.agent_toolkits.playwright.toolkit import (\\n    PlayWrightBrowserToolkit,\\n)\\n\\n__all__ = [\"PlayWrightBrowserToolkit\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\playwright\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.agent_toolkits.powerbi.base import create_pbi_agent\\n\\n__all__ = [\"create_pbi_agent\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\powerbi\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.agent_toolkits.powerbi.chat_base import create_pbi_chat_agent\\n\\n__all__ = [\"create_pbi_chat_agent\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\powerbi\\\\chat_base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.agent_toolkits.powerbi.prompt import (\\n    POWERBI_CHAT_PREFIX,\\n    POWERBI_CHAT_SUFFIX,\\n    POWERBI_PREFIX,\\n    POWERBI_SUFFIX,\\n)\\n\\n__all__ = [\\n    \"POWERBI_PREFIX\",\\n    \"POWERBI_SUFFIX\",\\n    \"POWERBI_CHAT_PREFIX\",\\n    \"POWERBI_CHAT_SUFFIX\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\powerbi\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.agent_toolkits.powerbi.toolkit import PowerBIToolkit\\n\\n__all__ = [\"PowerBIToolkit\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\powerbi\\\\toolkit.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Power BI agent.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\powerbi\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from pathlib import Path\\nfrom typing import Any\\n\\nfrom langchain_core._api.path import as_import_path\\n\\n\\ndef __getattr__(name: str) -> Any:\\n    \"\"\"Get attr name.\"\"\"\\n\\n    if name == \"create_python_agent\":\\n        # Get directory of langchain package\\n        HERE = Path(__file__).parents[3]\\n        here = as_import_path(Path(__file__).parent, relative_to=HERE)\\n\\n        old_path = \"langchain.\" + here + \".\" + name\\n        new_path = \"langchain_experimental.\" + here + \".\" + name\\n        raise ImportError(\\n            \"This agent has been moved to langchain experiment. \"\\n            \"This agent relies on python REPL tool under the hood, so to use it \"\\n            \"safely please sandbox the python REPL. \"\\n            \"Read https://github.com/langchain-ai/langchain/blob/master/SECURITY.md \"\\n            \"and https://github.com/langchain-ai/langchain/discussions/11680\"\\n            \"To keep using this code as is, install langchain experimental and \"\\n            f\"update your import statement from:\\\\n `{old_path}` to `{new_path}`.\"\\n        )\\n    raise AttributeError(f\"{name} does not exist\")' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\python\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.agent_toolkits.slack.toolkit import SlackToolkit\\n\\n__all__ = [\"SlackToolkit\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\slack\\\\toolkit.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Slack toolkit.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\slack\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from pathlib import Path\\nfrom typing import Any\\n\\nfrom langchain_core._api.path import as_import_path\\n\\n\\ndef __getattr__(name: str) -> Any:\\n    \"\"\"Get attr name.\"\"\"\\n\\n    if name == \"create_spark_dataframe_agent\":\\n        # Get directory of langchain package\\n        HERE = Path(__file__).parents[3]\\n        here = as_import_path(Path(__file__).parent, relative_to=HERE)\\n\\n        old_path = \"langchain.\" + here + \".\" + name\\n        new_path = \"langchain_experimental.\" + here + \".\" + name\\n        raise ImportError(\\n            \"This agent has been moved to langchain experiment. \"\\n            \"This agent relies on python REPL tool under the hood, so to use it \"\\n            \"safely please sandbox the python REPL. \"\\n            \"Read https://github.com/langchain-ai/langchain/blob/master/SECURITY.md \"\\n            \"and https://github.com/langchain-ai/langchain/discussions/11680\"\\n            \"To keep using this code as is, install langchain experimental and \"\\n            f\"update your import statement from:\\\\n `{old_path}` to `{new_path}`.\"\\n        )\\n    raise AttributeError(f\"{name} does not exist\")' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\spark\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.agent_toolkits.spark_sql.base import create_spark_sql_agent\\n\\n__all__ = [\"create_spark_sql_agent\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\spark_sql\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.agent_toolkits.spark_sql.prompt import SQL_PREFIX, SQL_SUFFIX\\n\\n__all__ = [\"SQL_PREFIX\", \"SQL_SUFFIX\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\spark_sql\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.agent_toolkits.spark_sql.toolkit import SparkSQLToolkit\\n\\n__all__ = [\"SparkSQLToolkit\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\spark_sql\\\\toolkit.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Spark SQL agent.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\spark_sql\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.agent_toolkits.sql.base import create_sql_agent\\n\\n__all__ = [\"create_sql_agent\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\sql\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.agent_toolkits.sql.prompt import (\\n    SQL_FUNCTIONS_SUFFIX,\\n    SQL_PREFIX,\\n    SQL_SUFFIX,\\n)\\n\\n__all__ = [\"SQL_PREFIX\", \"SQL_SUFFIX\", \"SQL_FUNCTIONS_SUFFIX\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\sql\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.agent_toolkits.sql.toolkit import SQLDatabaseToolkit\\n\\n__all__ = [\"SQLDatabaseToolkit\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\sql\\\\toolkit.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"SQL agent.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\sql\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.agent_toolkits.steam.toolkit import SteamToolkit\\n\\n__all__ = [\"SteamToolkit\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\steam\\\\toolkit.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Steam Toolkit.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\steam\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"VectorStore agent.\"\"\"\\nfrom typing import Any, Dict, Optional\\n\\nfrom langchain_core.language_models import BaseLanguageModel\\n\\nfrom langchain.agents.agent import AgentExecutor\\nfrom langchain.agents.agent_toolkits.vectorstore.prompt import PREFIX, ROUTER_PREFIX\\nfrom langchain.agents.agent_toolkits.vectorstore.toolkit import (\\n    VectorStoreRouterToolkit,\\n    VectorStoreToolkit,\\n)\\nfrom langchain.agents.mrkl.base import ZeroShotAgent\\nfrom langchain.callbacks.base import BaseCallbackManager\\nfrom langchain.chains.llm import LLMChain\\n\\n\\ndef create_vectorstore_agent(\\n    llm: BaseLanguageModel,\\n    toolkit: VectorStoreToolkit,\\n    callback_manager: Optional[BaseCallbackManager] = None,\\n    prefix: str = PREFIX,\\n    verbose: bool = False,\\n    agent_executor_kwargs: Optional[Dict[str, Any]] = None,\\n    **kwargs: Any,\\n) -> AgentExecutor:\\n    \"\"\"Construct a VectorStore agent from an LLM and tools.\\n\\n    Args:\\n        llm (BaseLanguageModel): LLM that will be used by the agent\\n        toolkit (VectorStoreToolkit): Set of tools for the agent\\n        callback_manager (Optional[BaseCallbackManager], optional): Object to handle the callback [ Defaults to None. ]\\n        prefix (str, optional): The prefix prompt for the agent. If not provided uses default PREFIX.\\n        verbose (bool, optional): If you want to see the content of the scratchpad. [ Defaults to False ]\\n        agent_executor_kwargs (Optional[Dict[str, Any]], optional): If there is any other parameter you want to send to the agent. [ Defaults to None ]\\n        **kwargs: Additional named parameters to pass to the ZeroShotAgent.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\vectorstore\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Returns:\\n        AgentExecutor: Returns a callable AgentExecutor object. Either you can call it or use run method with the query to get the response\\n    \"\"\"  # noqa: E501\\n    tools = toolkit.get_tools()\\n    prompt = ZeroShotAgent.create_prompt(tools, prefix=prefix)\\n    llm_chain = LLMChain(\\n        llm=llm,\\n        prompt=prompt,\\n        callback_manager=callback_manager,\\n    )\\n    tool_names = [tool.name for tool in tools]\\n    agent = ZeroShotAgent(llm_chain=llm_chain, allowed_tools=tool_names, **kwargs)\\n    return AgentExecutor.from_agent_and_tools(\\n        agent=agent,\\n        tools=tools,\\n        callback_manager=callback_manager,\\n        verbose=verbose,\\n        **(agent_executor_kwargs or {}),\\n    )\\n\\n\\ndef create_vectorstore_router_agent(\\n    llm: BaseLanguageModel,\\n    toolkit: VectorStoreRouterToolkit,\\n    callback_manager: Optional[BaseCallbackManager] = None,\\n    prefix: str = ROUTER_PREFIX,\\n    verbose: bool = False,\\n    agent_executor_kwargs: Optional[Dict[str, Any]] = None,\\n    **kwargs: Any,\\n) -> AgentExecutor:\\n    \"\"\"Construct a VectorStore router agent from an LLM and tools.\\n\\n    Args:\\n        llm (BaseLanguageModel): LLM that will be used by the agent\\n        toolkit (VectorStoreRouterToolkit): Set of tools for the agent which have routing capability with multiple vector stores\\n        callback_manager (Optional[BaseCallbackManager], optional): Object to handle the callback [ Defaults to None. ]\\n        prefix (str, optional): The prefix prompt for the router agent. If not provided uses default ROUTER_PREFIX.\\n        verbose (bool, optional): If you want to see the content of the scratchpad. [ Defaults to False ]\\n        agent_executor_kwargs (Optional[Dict[str, Any]], optional): If there is any other parameter you want to send to the agent. [ Defaults to None ]\\n        **kwargs: Additional named parameters to pass to the ZeroShotAgent.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\vectorstore\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Returns:\\n        AgentExecutor: Returns a callable AgentExecutor object. Either you can call it or use run method with the query to get the response.\\n    \"\"\"  # noqa: E501\\n    tools = toolkit.get_tools()\\n    prompt = ZeroShotAgent.create_prompt(tools, prefix=prefix)\\n    llm_chain = LLMChain(\\n        llm=llm,\\n        prompt=prompt,\\n        callback_manager=callback_manager,\\n    )\\n    tool_names = [tool.name for tool in tools]\\n    agent = ZeroShotAgent(llm_chain=llm_chain, allowed_tools=tool_names, **kwargs)\\n    return AgentExecutor.from_agent_and_tools(\\n        agent=agent,\\n        tools=tools,\\n        callback_manager=callback_manager,\\n        verbose=verbose,\\n        **(agent_executor_kwargs or {}),\\n    )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\vectorstore\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# flake8: noqa\\n\\nPREFIX = \"\"\"You are an agent designed to answer questions about sets of documents.\\nYou have access to tools for interacting with the documents, and the inputs to the tools are questions.\\nSometimes, you will be asked to provide sources for your questions, in which case you should use the appropriate tool to do so.\\nIf the question does not seem relevant to any of the tools provided, just return \"I don\\'t know\" as the answer.\\n\"\"\"\\n\\nROUTER_PREFIX = \"\"\"You are an agent designed to answer questions.\\nYou have access to tools for interacting with different sources, and the inputs to the tools are questions.\\nYour main task is to decide which of the tools is relevant for answering question at hand.\\nFor complex questions, you can break the question down into sub questions and use tools to answers the sub questions.\\n\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\vectorstore\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Toolkit for interacting with a vector store.\"\"\"\\nfrom typing import List\\n\\nfrom langchain_community.agent_toolkits.base import BaseToolkit\\nfrom langchain_community.llms.openai import OpenAI\\nfrom langchain_community.tools.vectorstore.tool import (\\n    VectorStoreQATool,\\n    VectorStoreQAWithSourcesTool,\\n)\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.pydantic_v1 import BaseModel, Field\\nfrom langchain_core.vectorstores import VectorStore\\n\\nfrom langchain.tools import BaseTool\\n\\n\\nclass VectorStoreInfo(BaseModel):\\n    \"\"\"Information about a VectorStore.\"\"\"\\n\\n    vectorstore: VectorStore = Field(exclude=True)\\n    name: str\\n    description: str\\n\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n\\n        arbitrary_types_allowed = True\\n\\n\\nclass VectorStoreToolkit(BaseToolkit):\\n    \"\"\"Toolkit for interacting with a Vector Store.\"\"\"\\n\\n    vectorstore_info: VectorStoreInfo = Field(exclude=True)\\n    llm: BaseLanguageModel = Field(default_factory=lambda: OpenAI(temperature=0))\\n\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n\\n        arbitrary_types_allowed = True\\n\\n    def get_tools(self) -> List[BaseTool]:\\n        \"\"\"Get the tools in the toolkit.\"\"\"\\n        description = VectorStoreQATool.get_description(\\n            self.vectorstore_info.name, self.vectorstore_info.description\\n        )\\n        qa_tool = VectorStoreQATool(\\n            name=self.vectorstore_info.name,\\n            description=description,\\n            vectorstore=self.vectorstore_info.vectorstore,\\n            llm=self.llm,\\n        )\\n        description = VectorStoreQAWithSourcesTool.get_description(\\n            self.vectorstore_info.name, self.vectorstore_info.description\\n        )\\n        qa_with_sources_tool = VectorStoreQAWithSourcesTool(\\n            name=f\"{self.vectorstore_info.name}_with_sources\",\\n            description=description,\\n            vectorstore=self.vectorstore_info.vectorstore,\\n            llm=self.llm,\\n        )\\n        return [qa_tool, qa_with_sources_tool]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\vectorstore\\\\toolkit.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='class VectorStoreRouterToolkit(BaseToolkit):\\n    \"\"\"Toolkit for routing between Vector Stores.\"\"\"\\n\\n    vectorstores: List[VectorStoreInfo] = Field(exclude=True)\\n    llm: BaseLanguageModel = Field(default_factory=lambda: OpenAI(temperature=0))\\n\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n\\n        arbitrary_types_allowed = True\\n\\n    def get_tools(self) -> List[BaseTool]:\\n        \"\"\"Get the tools in the toolkit.\"\"\"\\n        tools: List[BaseTool] = []\\n        for vectorstore_info in self.vectorstores:\\n            description = VectorStoreQATool.get_description(\\n                vectorstore_info.name, vectorstore_info.description\\n            )\\n            qa_tool = VectorStoreQATool(\\n                name=vectorstore_info.name,\\n                description=description,\\n                vectorstore=vectorstore_info.vectorstore,\\n                llm=self.llm,\\n            )\\n            tools.append(qa_tool)\\n        return tools' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\vectorstore\\\\toolkit.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Agent toolkit for interacting with vector stores.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\vectorstore\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from pathlib import Path\\nfrom typing import Any\\n\\nfrom langchain_core._api.path import as_import_path\\n\\n\\ndef __getattr__(name: str) -> Any:\\n    \"\"\"Get attr name.\"\"\"\\n\\n    if name == \"create_xorbits_agent\":\\n        # Get directory of langchain package\\n        HERE = Path(__file__).parents[3]\\n        here = as_import_path(Path(__file__).parent, relative_to=HERE)\\n\\n        old_path = \"langchain.\" + here + \".\" + name\\n        new_path = \"langchain_experimental.\" + here + \".\" + name\\n        raise ImportError(\\n            \"This agent has been moved to langchain experiment. \"\\n            \"This agent relies on python REPL tool under the hood, so to use it \"\\n            \"safely please sandbox the python REPL. \"\\n            \"Read https://github.com/langchain-ai/langchain/blob/master/SECURITY.md \"\\n            \"and https://github.com/langchain-ai/langchain/discussions/11680\"\\n            \"To keep using this code as is, install langchain experimental and \"\\n            f\"update your import statement from:\\\\n `{old_path}` to `{new_path}`.\"\\n        )\\n    raise AttributeError(f\"{name} does not exist\")' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\xorbits\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.agent_toolkits.zapier.toolkit import ZapierToolkit\\n\\n__all__ = [\"ZapierToolkit\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\zapier\\\\toolkit.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Zapier Toolkit.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\zapier\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from typing import Any, List, Optional, Sequence, Tuple\\n\\nfrom langchain_core._api import deprecated\\nfrom langchain_core.agents import AgentAction\\nfrom langchain_core.callbacks import BaseCallbackManager\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.prompts import BasePromptTemplate\\nfrom langchain_core.prompts.chat import (\\n    ChatPromptTemplate,\\n    HumanMessagePromptTemplate,\\n    SystemMessagePromptTemplate,\\n)\\nfrom langchain_core.pydantic_v1 import Field\\nfrom langchain_core.tools import BaseTool\\n\\nfrom langchain.agents.agent import Agent, AgentOutputParser\\nfrom langchain.agents.chat.output_parser import ChatOutputParser\\nfrom langchain.agents.chat.prompt import (\\n    FORMAT_INSTRUCTIONS,\\n    HUMAN_MESSAGE,\\n    SYSTEM_MESSAGE_PREFIX,\\n    SYSTEM_MESSAGE_SUFFIX,\\n)\\nfrom langchain.agents.utils import validate_tools_single_input\\nfrom langchain.chains.llm import LLMChain\\n\\n\\n@deprecated(\"0.1.0\", alternative=\"create_react_agent\", removal=\"0.2.0\")\\nclass ChatAgent(Agent):\\n    \"\"\"Chat Agent.\"\"\"\\n\\n    output_parser: AgentOutputParser = Field(default_factory=ChatOutputParser)\\n    \"\"\"Output parser for the agent.\"\"\"\\n\\n    @property\\n    def observation_prefix(self) -> str:\\n        \"\"\"Prefix to append the observation with.\"\"\"\\n        return \"Observation: \"\\n\\n    @property\\n    def llm_prefix(self) -> str:\\n        \"\"\"Prefix to append the llm call with.\"\"\"\\n        return \"Thought:\"\\n\\n    def _construct_scratchpad(\\n        self, intermediate_steps: List[Tuple[AgentAction, str]]\\n    ) -> str:\\n        agent_scratchpad = super()._construct_scratchpad(intermediate_steps)\\n        if not isinstance(agent_scratchpad, str):\\n            raise ValueError(\"agent_scratchpad should be of type string.\")\\n        if agent_scratchpad:\\n            return (\\n                f\"This was your previous work \"\\n                f\"(but I haven\\'t seen any of it! I only see what \"\\n                f\"you return as final answer):\\\\n{agent_scratchpad}\"\\n            )\\n        else:\\n            return agent_scratchpad' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\chat\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@classmethod\\n    def _get_default_output_parser(cls, **kwargs: Any) -> AgentOutputParser:\\n        return ChatOutputParser()\\n\\n    @classmethod\\n    def _validate_tools(cls, tools: Sequence[BaseTool]) -> None:\\n        super()._validate_tools(tools)\\n        validate_tools_single_input(class_name=cls.__name__, tools=tools)\\n\\n    @property\\n    def _stop(self) -> List[str]:\\n        return [\"Observation:\"]\\n\\n    @classmethod\\n    def create_prompt(\\n        cls,\\n        tools: Sequence[BaseTool],\\n        system_message_prefix: str = SYSTEM_MESSAGE_PREFIX,\\n        system_message_suffix: str = SYSTEM_MESSAGE_SUFFIX,\\n        human_message: str = HUMAN_MESSAGE,\\n        format_instructions: str = FORMAT_INSTRUCTIONS,\\n        input_variables: Optional[List[str]] = None,\\n    ) -> BasePromptTemplate:\\n        tool_strings = \"\\\\n\".join([f\"{tool.name}: {tool.description}\" for tool in tools])\\n        tool_names = \", \".join([tool.name for tool in tools])\\n        format_instructions = format_instructions.format(tool_names=tool_names)\\n        template = \"\\\\n\\\\n\".join(\\n            [\\n                system_message_prefix,\\n                tool_strings,\\n                format_instructions,\\n                system_message_suffix,\\n            ]\\n        )\\n        messages = [\\n            SystemMessagePromptTemplate.from_template(template),\\n            HumanMessagePromptTemplate.from_template(human_message),\\n        ]\\n        if input_variables is None:\\n            input_variables = [\"input\", \"agent_scratchpad\"]\\n        return ChatPromptTemplate(input_variables=input_variables, messages=messages)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\chat\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@classmethod\\n    def from_llm_and_tools(\\n        cls,\\n        llm: BaseLanguageModel,\\n        tools: Sequence[BaseTool],\\n        callback_manager: Optional[BaseCallbackManager] = None,\\n        output_parser: Optional[AgentOutputParser] = None,\\n        system_message_prefix: str = SYSTEM_MESSAGE_PREFIX,\\n        system_message_suffix: str = SYSTEM_MESSAGE_SUFFIX,\\n        human_message: str = HUMAN_MESSAGE,\\n        format_instructions: str = FORMAT_INSTRUCTIONS,\\n        input_variables: Optional[List[str]] = None,\\n        **kwargs: Any,\\n    ) -> Agent:\\n        \"\"\"Construct an agent from an LLM and tools.\"\"\"\\n        cls._validate_tools(tools)\\n        prompt = cls.create_prompt(\\n            tools,\\n            system_message_prefix=system_message_prefix,\\n            system_message_suffix=system_message_suffix,\\n            human_message=human_message,\\n            format_instructions=format_instructions,\\n            input_variables=input_variables,\\n        )\\n        llm_chain = LLMChain(\\n            llm=llm,\\n            prompt=prompt,\\n            callback_manager=callback_manager,\\n        )\\n        tool_names = [tool.name for tool in tools]\\n        _output_parser = output_parser or cls._get_default_output_parser()\\n        return cls(\\n            llm_chain=llm_chain,\\n            allowed_tools=tool_names,\\n            output_parser=_output_parser,\\n            **kwargs,\\n        )\\n\\n    @property\\n    def _agent_type(self) -> str:\\n        raise ValueError' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\chat\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='import json\\nimport re\\nfrom typing import Union\\n\\nfrom langchain_core.agents import AgentAction, AgentFinish\\nfrom langchain_core.exceptions import OutputParserException\\n\\nfrom langchain.agents.agent import AgentOutputParser\\nfrom langchain.agents.chat.prompt import FORMAT_INSTRUCTIONS\\n\\nFINAL_ANSWER_ACTION = \"Final Answer:\"\\n\\n\\nclass ChatOutputParser(AgentOutputParser):\\n    \"\"\"Output parser for the chat agent.\"\"\"\\n\\n    pattern = re.compile(r\"^.*?`{3}(?:json)?\\\\n(.*?)`{3}.*?$\", re.DOTALL)\\n    \"\"\"Regex pattern to parse the output.\"\"\"\\n\\n    def get_format_instructions(self) -> str:\\n        return FORMAT_INSTRUCTIONS\\n\\n    def parse(self, text: str) -> Union[AgentAction, AgentFinish]:\\n        includes_answer = FINAL_ANSWER_ACTION in text\\n        try:\\n            found = self.pattern.search(text)\\n            if not found:\\n                # Fast fail to parse Final Answer.\\n                raise ValueError(\"action not found\")\\n            action = found.group(1)\\n            response = json.loads(action.strip())\\n            includes_action = \"action\" in response\\n            if includes_answer and includes_action:\\n                raise OutputParserException(\\n                    \"Parsing LLM output produced a final answer \"\\n                    f\"and a parse-able action: {text}\"\\n                )\\n            return AgentAction(\\n                response[\"action\"], response.get(\"action_input\", {}), text\\n            )\\n\\n        except Exception as exc:\\n            if not includes_answer:\\n                raise OutputParserException(\\n                    f\"Could not parse LLM output: {text}\"\\n                ) from exc\\n            output = text.split(FINAL_ANSWER_ACTION)[-1].strip()\\n            return AgentFinish({\"output\": output}, text)\\n\\n    @property\\n    def _type(self) -> str:\\n        return \"chat\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\chat\\\\output_parser.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# flake8: noqa\\nSYSTEM_MESSAGE_PREFIX = \"\"\"Answer the following questions as best you can. You have access to the following tools:\"\"\"\\nFORMAT_INSTRUCTIONS = \"\"\"The way you use the tools is by specifying a json blob.\\nSpecifically, this json should have a `action` key (with the name of the tool to use) and a `action_input` key (with the input to the tool going here).\\n\\nThe only values that should be in the \"action\" field are: {tool_names}\\n\\nThe $JSON_BLOB should only contain a SINGLE action, do NOT return a list of multiple actions. Here is an example of a valid $JSON_BLOB:\\n\\n```\\n{{{{\\n  \"action\": $TOOL_NAME,\\n  \"action_input\": $INPUT\\n}}}}\\n```\\n\\nALWAYS use the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction:\\n```\\n$JSON_BLOB\\n```\\nObservation: the result of the action\\n... (this Thought/Action/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\"\"\"\\nSYSTEM_MESSAGE_SUFFIX = \"\"\"Begin! Reminder to always use the exact characters `Final Answer` when responding.\"\"\"\\nHUMAN_MESSAGE = \"{input}\\\\n\\\\n{agent_scratchpad}\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\chat\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"An agent designed to hold a conversation in addition to using tools.\"\"\"\\nfrom __future__ import annotations\\n\\nfrom typing import Any, List, Optional, Sequence\\n\\nfrom langchain_core._api import deprecated\\nfrom langchain_core.callbacks import BaseCallbackManager\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.prompts import PromptTemplate\\nfrom langchain_core.pydantic_v1 import Field\\nfrom langchain_core.tools import BaseTool\\n\\nfrom langchain.agents.agent import Agent, AgentOutputParser\\nfrom langchain.agents.agent_types import AgentType\\nfrom langchain.agents.conversational.output_parser import ConvoOutputParser\\nfrom langchain.agents.conversational.prompt import FORMAT_INSTRUCTIONS, PREFIX, SUFFIX\\nfrom langchain.agents.utils import validate_tools_single_input\\nfrom langchain.chains import LLMChain\\n\\n\\n@deprecated(\"0.1.0\", alternative=\"create_react_agent\", removal=\"0.2.0\")\\nclass ConversationalAgent(Agent):\\n    \"\"\"An agent that holds a conversation in addition to using tools.\"\"\"\\n\\n    ai_prefix: str = \"AI\"\\n    \"\"\"Prefix to use before AI output.\"\"\"\\n    output_parser: AgentOutputParser = Field(default_factory=ConvoOutputParser)\\n    \"\"\"Output parser for the agent.\"\"\"\\n\\n    @classmethod\\n    def _get_default_output_parser(\\n        cls, ai_prefix: str = \"AI\", **kwargs: Any\\n    ) -> AgentOutputParser:\\n        return ConvoOutputParser(ai_prefix=ai_prefix)\\n\\n    @property\\n    def _agent_type(self) -> str:\\n        \"\"\"Return Identifier of agent type.\"\"\"\\n        return AgentType.CONVERSATIONAL_REACT_DESCRIPTION\\n\\n    @property\\n    def observation_prefix(self) -> str:\\n        \"\"\"Prefix to append the observation with.\"\"\"\\n        return \"Observation: \"\\n\\n    @property\\n    def llm_prefix(self) -> str:\\n        \"\"\"Prefix to append the llm call with.\"\"\"\\n        return \"Thought:\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\conversational\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@classmethod\\n    def create_prompt(\\n        cls,\\n        tools: Sequence[BaseTool],\\n        prefix: str = PREFIX,\\n        suffix: str = SUFFIX,\\n        format_instructions: str = FORMAT_INSTRUCTIONS,\\n        ai_prefix: str = \"AI\",\\n        human_prefix: str = \"Human\",\\n        input_variables: Optional[List[str]] = None,\\n    ) -> PromptTemplate:\\n        \"\"\"Create prompt in the style of the zero-shot agent.\\n\\n        Args:\\n            tools: List of tools the agent will have access to, used to format the\\n                prompt.\\n            prefix: String to put before the list of tools.\\n            suffix: String to put after the list of tools.\\n            ai_prefix: String to use before AI output.\\n            human_prefix: String to use before human output.\\n            input_variables: List of input variables the final prompt will expect.\\n\\n        Returns:\\n            A PromptTemplate with the template assembled from the pieces here.\\n        \"\"\"\\n        tool_strings = \"\\\\n\".join(\\n            [f\"> {tool.name}: {tool.description}\" for tool in tools]\\n        )\\n        tool_names = \", \".join([tool.name for tool in tools])\\n        format_instructions = format_instructions.format(\\n            tool_names=tool_names, ai_prefix=ai_prefix, human_prefix=human_prefix\\n        )\\n        template = \"\\\\n\\\\n\".join([prefix, tool_strings, format_instructions, suffix])\\n        if input_variables is None:\\n            input_variables = [\"input\", \"chat_history\", \"agent_scratchpad\"]\\n        return PromptTemplate(template=template, input_variables=input_variables)\\n\\n    @classmethod\\n    def _validate_tools(cls, tools: Sequence[BaseTool]) -> None:\\n        super()._validate_tools(tools)\\n        validate_tools_single_input(cls.__name__, tools)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\conversational\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@classmethod\\n    def from_llm_and_tools(\\n        cls,\\n        llm: BaseLanguageModel,\\n        tools: Sequence[BaseTool],\\n        callback_manager: Optional[BaseCallbackManager] = None,\\n        output_parser: Optional[AgentOutputParser] = None,\\n        prefix: str = PREFIX,\\n        suffix: str = SUFFIX,\\n        format_instructions: str = FORMAT_INSTRUCTIONS,\\n        ai_prefix: str = \"AI\",\\n        human_prefix: str = \"Human\",\\n        input_variables: Optional[List[str]] = None,\\n        **kwargs: Any,\\n    ) -> Agent:\\n        \"\"\"Construct an agent from an LLM and tools.\"\"\"\\n        cls._validate_tools(tools)\\n        prompt = cls.create_prompt(\\n            tools,\\n            ai_prefix=ai_prefix,\\n            human_prefix=human_prefix,\\n            prefix=prefix,\\n            suffix=suffix,\\n            format_instructions=format_instructions,\\n            input_variables=input_variables,\\n        )\\n        llm_chain = LLMChain(\\n            llm=llm,\\n            prompt=prompt,\\n            callback_manager=callback_manager,\\n        )\\n        tool_names = [tool.name for tool in tools]\\n        _output_parser = output_parser or cls._get_default_output_parser(\\n            ai_prefix=ai_prefix\\n        )\\n        return cls(\\n            llm_chain=llm_chain,\\n            allowed_tools=tool_names,\\n            ai_prefix=ai_prefix,\\n            output_parser=_output_parser,\\n            **kwargs,\\n        )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\conversational\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='import re\\nfrom typing import Union\\n\\nfrom langchain_core.agents import AgentAction, AgentFinish\\nfrom langchain_core.exceptions import OutputParserException\\n\\nfrom langchain.agents.agent import AgentOutputParser\\nfrom langchain.agents.conversational.prompt import FORMAT_INSTRUCTIONS\\n\\n\\nclass ConvoOutputParser(AgentOutputParser):\\n    \"\"\"Output parser for the conversational agent.\"\"\"\\n\\n    ai_prefix: str = \"AI\"\\n    \"\"\"Prefix to use before AI output.\"\"\"\\n\\n    def get_format_instructions(self) -> str:\\n        return FORMAT_INSTRUCTIONS\\n\\n    def parse(self, text: str) -> Union[AgentAction, AgentFinish]:\\n        if f\"{self.ai_prefix}:\" in text:\\n            return AgentFinish(\\n                {\"output\": text.split(f\"{self.ai_prefix}:\")[-1].strip()}, text\\n            )\\n        regex = r\"Action: (.*?)[\\\\n]*Action Input: ([\\\\s\\\\S]*)\"\\n        match = re.search(regex, text, re.DOTALL)\\n        if not match:\\n            raise OutputParserException(f\"Could not parse LLM output: `{text}`\")\\n        action = match.group(1)\\n        action_input = match.group(2)\\n        return AgentAction(action.strip(), action_input.strip(\" \").strip(\\'\"\\'), text)\\n\\n    @property\\n    def _type(self) -> str:\\n        return \"conversational\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\conversational\\\\output_parser.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# flake8: noqa\\nPREFIX = \"\"\"Assistant is a large language model trained by OpenAI.\\n\\nAssistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\\n\\nAssistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\\n\\nOverall, Assistant is a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\\n\\nTOOLS:\\n------\\n\\nAssistant has access to the following tools:\"\"\"\\nFORMAT_INSTRUCTIONS = \"\"\"To use a tool, please use the following format:\\n\\n```\\nThought: Do I need to use a tool? Yes\\nAction: the action to take, should be one of [{tool_names}]\\nAction Input: the input to the action\\nObservation: the result of the action\\n```\\n\\nWhen you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:\\n\\n```\\nThought: Do I need to use a tool? No\\n{ai_prefix}: [your response here]\\n```\"\"\"\\n\\nSUFFIX = \"\"\"Begin!\\n\\nPrevious conversation history:\\n{chat_history}\\n\\nNew input: {input}\\n{agent_scratchpad}\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\conversational\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"An agent designed to hold a conversation in addition to using tools.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\conversational\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"An agent designed to hold a conversation in addition to using tools.\"\"\"\\nfrom __future__ import annotations\\n\\nfrom typing import Any, List, Optional, Sequence, Tuple\\n\\nfrom langchain_core._api import deprecated\\nfrom langchain_core.agents import AgentAction\\nfrom langchain_core.callbacks import BaseCallbackManager\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.messages import AIMessage, BaseMessage, HumanMessage\\nfrom langchain_core.output_parsers import BaseOutputParser\\nfrom langchain_core.prompts import BasePromptTemplate\\nfrom langchain_core.prompts.chat import (\\n    ChatPromptTemplate,\\n    HumanMessagePromptTemplate,\\n    MessagesPlaceholder,\\n    SystemMessagePromptTemplate,\\n)\\nfrom langchain_core.pydantic_v1 import Field\\nfrom langchain_core.tools import BaseTool\\n\\nfrom langchain.agents.agent import Agent, AgentOutputParser\\nfrom langchain.agents.conversational_chat.output_parser import ConvoOutputParser\\nfrom langchain.agents.conversational_chat.prompt import (\\n    PREFIX,\\n    SUFFIX,\\n    TEMPLATE_TOOL_RESPONSE,\\n)\\nfrom langchain.agents.utils import validate_tools_single_input\\nfrom langchain.chains import LLMChain\\n\\n\\n@deprecated(\"0.1.0\", alternative=\"create_json_chat_agent\", removal=\"0.2.0\")\\nclass ConversationalChatAgent(Agent):\\n    \"\"\"An agent designed to hold a conversation in addition to using tools.\"\"\"\\n\\n    output_parser: AgentOutputParser = Field(default_factory=ConvoOutputParser)\\n    template_tool_response: str = TEMPLATE_TOOL_RESPONSE\\n\\n    @classmethod\\n    def _get_default_output_parser(cls, **kwargs: Any) -> AgentOutputParser:\\n        return ConvoOutputParser()\\n\\n    @property\\n    def _agent_type(self) -> str:\\n        raise NotImplementedError\\n\\n    @property\\n    def observation_prefix(self) -> str:\\n        \"\"\"Prefix to append the observation with.\"\"\"\\n        return \"Observation: \"\\n\\n    @property\\n    def llm_prefix(self) -> str:\\n        \"\"\"Prefix to append the llm call with.\"\"\"\\n        return \"Thought:\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\conversational_chat\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@classmethod\\n    def _validate_tools(cls, tools: Sequence[BaseTool]) -> None:\\n        super()._validate_tools(tools)\\n        validate_tools_single_input(cls.__name__, tools)\\n\\n    @classmethod\\n    def create_prompt(\\n        cls,\\n        tools: Sequence[BaseTool],\\n        system_message: str = PREFIX,\\n        human_message: str = SUFFIX,\\n        input_variables: Optional[List[str]] = None,\\n        output_parser: Optional[BaseOutputParser] = None,\\n    ) -> BasePromptTemplate:\\n        tool_strings = \"\\\\n\".join(\\n            [f\"> {tool.name}: {tool.description}\" for tool in tools]\\n        )\\n        tool_names = \", \".join([tool.name for tool in tools])\\n        _output_parser = output_parser or cls._get_default_output_parser()\\n        format_instructions = human_message.format(\\n            format_instructions=_output_parser.get_format_instructions()\\n        )\\n        final_prompt = format_instructions.format(\\n            tool_names=tool_names, tools=tool_strings\\n        )\\n        if input_variables is None:\\n            input_variables = [\"input\", \"chat_history\", \"agent_scratchpad\"]\\n        messages = [\\n            SystemMessagePromptTemplate.from_template(system_message),\\n            MessagesPlaceholder(variable_name=\"chat_history\"),\\n            HumanMessagePromptTemplate.from_template(final_prompt),\\n            MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\\n        ]\\n        return ChatPromptTemplate(input_variables=input_variables, messages=messages)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\conversational_chat\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _construct_scratchpad(\\n        self, intermediate_steps: List[Tuple[AgentAction, str]]\\n    ) -> List[BaseMessage]:\\n        \"\"\"Construct the scratchpad that lets the agent continue its thought process.\"\"\"\\n        thoughts: List[BaseMessage] = []\\n        for action, observation in intermediate_steps:\\n            thoughts.append(AIMessage(content=action.log))\\n            human_message = HumanMessage(\\n                content=self.template_tool_response.format(observation=observation)\\n            )\\n            thoughts.append(human_message)\\n        return thoughts\\n\\n    @classmethod\\n    def from_llm_and_tools(\\n        cls,\\n        llm: BaseLanguageModel,\\n        tools: Sequence[BaseTool],\\n        callback_manager: Optional[BaseCallbackManager] = None,\\n        output_parser: Optional[AgentOutputParser] = None,\\n        system_message: str = PREFIX,\\n        human_message: str = SUFFIX,\\n        input_variables: Optional[List[str]] = None,\\n        **kwargs: Any,\\n    ) -> Agent:\\n        \"\"\"Construct an agent from an LLM and tools.\"\"\"\\n        cls._validate_tools(tools)\\n        _output_parser = output_parser or cls._get_default_output_parser()\\n        prompt = cls.create_prompt(\\n            tools,\\n            system_message=system_message,\\n            human_message=human_message,\\n            input_variables=input_variables,\\n            output_parser=_output_parser,\\n        )\\n        llm_chain = LLMChain(\\n            llm=llm,\\n            prompt=prompt,\\n            callback_manager=callback_manager,\\n        )\\n        tool_names = [tool.name for tool in tools]\\n        return cls(\\n            llm_chain=llm_chain,\\n            allowed_tools=tool_names,\\n            output_parser=_output_parser,\\n            **kwargs,\\n        )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\conversational_chat\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from __future__ import annotations\\n\\nfrom typing import Union\\n\\nfrom langchain_core.agents import AgentAction, AgentFinish\\nfrom langchain_core.exceptions import OutputParserException\\nfrom langchain_core.output_parsers.json import parse_json_markdown\\n\\nfrom langchain.agents import AgentOutputParser\\nfrom langchain.agents.conversational_chat.prompt import FORMAT_INSTRUCTIONS\\n\\n\\n# Define a class that parses output for conversational agents\\nclass ConvoOutputParser(AgentOutputParser):\\n    \"\"\"Output parser for the conversational agent.\"\"\"\\n\\n    def get_format_instructions(self) -> str:\\n        \"\"\"Returns formatting instructions for the given output parser.\"\"\"\\n        return FORMAT_INSTRUCTIONS\\n\\n    def parse(self, text: str) -> Union[AgentAction, AgentFinish]:\\n        \"\"\"Attempts to parse the given text into an AgentAction or AgentFinish.\\n\\n        Raises:\\n             OutputParserException if parsing fails.\\n        \"\"\"\\n        try:\\n            # Attempt to parse the text into a structured format (assumed to be JSON\\n            # stored as markdown)\\n            response = parse_json_markdown(text)\\n\\n            # If the response contains an \\'action\\' and \\'action_input\\'\\n            if \"action\" in response and \"action_input\" in response:\\n                action, action_input = response[\"action\"], response[\"action_input\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\conversational_chat\\\\output_parser.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# If the action indicates a final answer, return an AgentFinish\\n                if action == \"Final Answer\":\\n                    return AgentFinish({\"output\": action_input}, text)\\n                else:\\n                    # Otherwise, return an AgentAction with the specified action and\\n                    # input\\n                    return AgentAction(action, action_input, text)\\n            else:\\n                # If the necessary keys aren\\'t present in the response, raise an\\n                # exception\\n                raise OutputParserException(\\n                    f\"Missing \\'action\\' or \\'action_input\\' in LLM output: {text}\"\\n                )\\n        except Exception as e:\\n            # If any other exception is raised during parsing, also raise an\\n            # OutputParserException\\n            raise OutputParserException(f\"Could not parse LLM output: {text}\") from e\\n\\n    @property\\n    def _type(self) -> str:\\n        return \"conversational_chat\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\conversational_chat\\\\output_parser.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# flake8: noqa\\nPREFIX = \"\"\"Assistant is a large language model trained by OpenAI.\\n\\nAssistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\\n\\nAssistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\\n\\nOverall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\"\"\"\\n\\nFORMAT_INSTRUCTIONS = \"\"\"RESPONSE FORMAT INSTRUCTIONS\\n----------------------------\\n\\nWhen responding to me, please output a response in one of two formats:\\n\\n**Option 1:**\\nUse this if you want the human to use a tool.\\nMarkdown code snippet formatted in the following schema:\\n\\n```json\\n{{{{\\n    \"action\": string, \\\\\\\\\\\\\\\\ The action to take. Must be one of {tool_names}\\n    \"action_input\": string \\\\\\\\\\\\\\\\ The input to the action\\n}}}}\\n```\\n\\n**Option #2:**\\nUse this if you want to respond directly to the human. Markdown code snippet formatted in the following schema:\\n\\n```json\\n{{{{\\n    \"action\": \"Final Answer\",\\n    \"action_input\": string \\\\\\\\\\\\\\\\ You should put what you want to return to use here\\n}}}}\\n```\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\conversational_chat\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='SUFFIX = \"\"\"TOOLS\\n------\\nAssistant can ask the user to use tools to look up information that may be helpful in answering the users original question. The tools the human can use are:\\n\\n{{tools}}\\n\\n{format_instructions}\\n\\nUSER\\'S INPUT\\n--------------------\\nHere is the user\\'s input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\\n\\n{{{{input}}}}\"\"\"\\n\\nTEMPLATE_TOOL_RESPONSE = \"\"\"TOOL RESPONSE: \\n---------------------\\n{observation}\\n\\nUSER\\'S INPUT\\n--------------------\\n\\nOkay, so what is the response to my last comment? If using information obtained from the tools you must mention it explicitly without mentioning the tool names - I have forgotten all TOOL RESPONSES! Remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\conversational_chat\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"An agent designed to hold a conversation in addition to using tools.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\conversational_chat\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from typing import List, Tuple\\n\\nfrom langchain_core.agents import AgentAction\\n\\n\\ndef format_log_to_str(\\n    intermediate_steps: List[Tuple[AgentAction, str]],\\n    observation_prefix: str = \"Observation: \",\\n    llm_prefix: str = \"Thought: \",\\n) -> str:\\n    \"\"\"Construct the scratchpad that lets the agent continue its thought process.\"\"\"\\n    thoughts = \"\"\\n    for action, observation in intermediate_steps:\\n        thoughts += action.log\\n        thoughts += f\"\\\\n{observation_prefix}{observation}\\\\n{llm_prefix}\"\\n    return thoughts' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\format_scratchpad\\\\log.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from typing import List, Tuple\\n\\nfrom langchain_core.agents import AgentAction\\nfrom langchain_core.messages import AIMessage, BaseMessage, HumanMessage\\n\\n\\ndef format_log_to_messages(\\n    intermediate_steps: List[Tuple[AgentAction, str]],\\n    template_tool_response: str = \"{observation}\",\\n) -> List[BaseMessage]:\\n    \"\"\"Construct the scratchpad that lets the agent continue its thought process.\"\"\"\\n    thoughts: List[BaseMessage] = []\\n    for action, observation in intermediate_steps:\\n        thoughts.append(AIMessage(content=action.log))\\n        human_message = HumanMessage(\\n            content=template_tool_response.format(observation=observation)\\n        )\\n        thoughts.append(human_message)\\n    return thoughts' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\format_scratchpad\\\\log_to_messages.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='import json\\nfrom typing import List, Sequence, Tuple\\n\\nfrom langchain_core.agents import AgentAction, AgentActionMessageLog\\nfrom langchain_core.messages import AIMessage, BaseMessage, FunctionMessage\\n\\n\\ndef _convert_agent_action_to_messages(\\n    agent_action: AgentAction, observation: str\\n) -> List[BaseMessage]:\\n    \"\"\"Convert an agent action to a message.\\n\\n    This code is used to reconstruct the original AI message from the agent action.\\n\\n    Args:\\n        agent_action: Agent action to convert.\\n\\n    Returns:\\n        AIMessage that corresponds to the original tool invocation.\\n    \"\"\"\\n    if isinstance(agent_action, AgentActionMessageLog):\\n        return list(agent_action.message_log) + [\\n            _create_function_message(agent_action, observation)\\n        ]\\n    else:\\n        return [AIMessage(content=agent_action.log)]\\n\\n\\ndef _create_function_message(\\n    agent_action: AgentAction, observation: str\\n) -> FunctionMessage:\\n    \"\"\"Convert agent action and observation into a function message.\\n    Args:\\n        agent_action: the tool invocation request from the agent\\n        observation: the result of the tool invocation\\n    Returns:\\n        FunctionMessage that corresponds to the original tool invocation\\n    \"\"\"\\n    if not isinstance(observation, str):\\n        try:\\n            content = json.dumps(observation, ensure_ascii=False)\\n        except Exception:\\n            content = str(observation)\\n    else:\\n        content = observation\\n    return FunctionMessage(\\n        name=agent_action.tool,\\n        content=content,\\n    )\\n\\n\\ndef format_to_openai_function_messages(\\n    intermediate_steps: Sequence[Tuple[AgentAction, str]],\\n) -> List[BaseMessage]:\\n    \"\"\"Convert (AgentAction, tool output) tuples into FunctionMessages.\\n\\n    Args:\\n        intermediate_steps: Steps the LLM has taken to date, along with observations\\n\\n    Returns:\\n        list of messages to send to the LLM for the next prediction\\n\\n    \"\"\"\\n    messages = []' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\format_scratchpad\\\\openai_functions.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='for agent_action, observation in intermediate_steps:\\n        messages.extend(_convert_agent_action_to_messages(agent_action, observation))\\n\\n    return messages\\n\\n\\n# Backwards compatibility\\nformat_to_openai_functions = format_to_openai_function_messages' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\format_scratchpad\\\\openai_functions.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='import json\\nfrom typing import List, Sequence, Tuple\\n\\nfrom langchain_core.agents import AgentAction\\nfrom langchain_core.messages import (\\n    AIMessage,\\n    BaseMessage,\\n    ToolMessage,\\n)\\n\\nfrom langchain.agents.output_parsers.openai_tools import OpenAIToolAgentAction\\n\\n\\ndef _create_tool_message(\\n    agent_action: OpenAIToolAgentAction, observation: str\\n) -> ToolMessage:\\n    \"\"\"Convert agent action and observation into a function message.\\n    Args:\\n        agent_action: the tool invocation request from the agent\\n        observation: the result of the tool invocation\\n    Returns:\\n        FunctionMessage that corresponds to the original tool invocation\\n    \"\"\"\\n    if not isinstance(observation, str):\\n        try:\\n            content = json.dumps(observation, ensure_ascii=False)\\n        except Exception:\\n            content = str(observation)\\n    else:\\n        content = observation\\n    return ToolMessage(\\n        tool_call_id=agent_action.tool_call_id,\\n        content=content,\\n        additional_kwargs={\"name\": agent_action.tool},\\n    )\\n\\n\\ndef format_to_openai_tool_messages(\\n    intermediate_steps: Sequence[Tuple[AgentAction, str]],\\n) -> List[BaseMessage]:\\n    \"\"\"Convert (AgentAction, tool output) tuples into FunctionMessages.\\n\\n    Args:\\n        intermediate_steps: Steps the LLM has taken to date, along with observations\\n\\n    Returns:\\n        list of messages to send to the LLM for the next prediction\\n\\n    \"\"\"\\n    messages = []\\n    for agent_action, observation in intermediate_steps:\\n        if isinstance(agent_action, OpenAIToolAgentAction):\\n            new_messages = list(agent_action.message_log) + [\\n                _create_tool_message(agent_action, observation)\\n            ]\\n            messages.extend([new for new in new_messages if new not in messages])\\n        else:\\n            messages.append(AIMessage(content=agent_action.log))\\n    return messages' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\format_scratchpad\\\\openai_tools.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from typing import List, Tuple\\n\\nfrom langchain_core.agents import AgentAction\\n\\n\\ndef format_xml(\\n    intermediate_steps: List[Tuple[AgentAction, str]],\\n) -> str:\\n    \"\"\"Format the intermediate steps as XML.\\n\\n    Args:\\n        intermediate_steps: The intermediate steps.\\n\\n    Returns:\\n        The intermediate steps as XML.\\n    \"\"\"\\n    log = \"\"\\n    for action, observation in intermediate_steps:\\n        log += (\\n            f\"<tool>{action.tool}</tool><tool_input>{action.tool_input}\"\\n            f\"</tool_input><observation>{observation}</observation>\"\\n        )\\n    return log' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\format_scratchpad\\\\xml.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Logic for formatting intermediate steps into an agent scratchpad.\\n\\nIntermediate steps refers to the list of (AgentAction, observation) tuples\\nthat result from previous iterations of the agent.\\nDepending on the prompting strategy you are using, you may want to format these\\ndifferently before passing them into the LLM.\\n\"\"\"\\nfrom langchain.agents.format_scratchpad.log import format_log_to_str\\nfrom langchain.agents.format_scratchpad.log_to_messages import format_log_to_messages\\nfrom langchain.agents.format_scratchpad.openai_functions import (\\n    format_to_openai_function_messages,\\n    format_to_openai_functions,\\n)\\nfrom langchain.agents.format_scratchpad.xml import format_xml\\n\\n__all__ = [\\n    \"format_xml\",\\n    \"format_to_openai_function_messages\",\\n    \"format_to_openai_functions\",\\n    \"format_log_to_str\",\\n    \"format_log_to_messages\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\format_scratchpad\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from typing import Sequence\\n\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.prompts.chat import ChatPromptTemplate\\nfrom langchain_core.runnables import Runnable, RunnablePassthrough\\nfrom langchain_core.tools import BaseTool\\n\\nfrom langchain.agents.format_scratchpad import format_log_to_messages\\nfrom langchain.agents.json_chat.prompt import TEMPLATE_TOOL_RESPONSE\\nfrom langchain.agents.output_parsers import JSONAgentOutputParser\\nfrom langchain.tools.render import render_text_description\\n\\n\\ndef create_json_chat_agent(\\n    llm: BaseLanguageModel, tools: Sequence[BaseTool], prompt: ChatPromptTemplate\\n) -> Runnable:\\n    \"\"\"Create an agent that uses JSON to format its logic, build for Chat Models.\\n\\n    Args:\\n        llm: LLM to use as the agent.\\n        tools: Tools this agent has access to.\\n        prompt: The prompt to use. See Prompt section below for more.\\n\\n    Returns:\\n        A Runnable sequence representing an agent. It takes as input all the same input\\n        variables as the prompt passed in does. It returns as output either an\\n        AgentAction or AgentFinish.\\n\\n    Example:\\n\\n        .. code-block:: python\\n\\n            from langchain import hub\\n            from langchain_community.chat_models import ChatOpenAI\\n            from langchain.agents import AgentExecutor, create_json_chat_agent\\n\\n            prompt = hub.pull(\"hwchase17/react-chat-json\")\\n            model = ChatOpenAI()\\n            tools = ...\\n\\n            agent = create_json_chat_agent(model, tools, prompt)\\n            agent_executor = AgentExecutor(agent=agent, tools=tools)\\n\\n            agent_executor.invoke({\"input\": \"hi\"})' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\json_chat\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# Using with chat history\\n            from langchain_core.messages import AIMessage, HumanMessage\\n            agent_executor.invoke(\\n                {\\n                    \"input\": \"what\\'s my name?\",\\n                    \"chat_history\": [\\n                        HumanMessage(content=\"hi! my name is bob\"),\\n                        AIMessage(content=\"Hello Bob! How can I assist you today?\"),\\n                    ],\\n                }\\n            )\\n\\n    Prompt:\\n    \\n        The prompt must have input keys:\\n            * `tools`: contains descriptions and arguments for each tool.\\n            * `tool_names`: contains all tool names.\\n            * `agent_scratchpad`: must be a MessagesPlaceholder. Contains previous agent actions and tool outputs as messages.\\n        \\n        Here\\'s an example:\\n\\n        .. code-block:: python\\n\\n            from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\\n            \\n            system = \\'\\'\\'Assistant is a large language model trained by OpenAI.\\n\\n            Assistant is designed to be able to assist with a wide range of tasks, from answering \\\\\\n            simple questions to providing in-depth explanations and discussions on a wide range of \\\\\\n            topics. As a language model, Assistant is able to generate human-like text based on \\\\\\n            the input it receives, allowing it to engage in natural-sounding conversations and \\\\\\n            provide responses that are coherent and relevant to the topic at hand.\\n\\n            Assistant is constantly learning and improving, and its capabilities are constantly \\\\\\n            evolving. It is able to process and understand large amounts of text, and can use this \\\\\\n            knowledge to provide accurate and informative responses to a wide range of questions. \\\\\\n            Additionally, Assistant is able to generate its own text based on the input it \\\\\\n            receives, allowing it to engage in discussions and provide explanations and \\\\\\n            descriptions on a wide range of topics.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\json_chat\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Overall, Assistant is a powerful system that can help with a wide range of tasks \\\\\\n            and provide valuable insights and information on a wide range of topics. Whether \\\\\\n            you need help with a specific question or just want to have a conversation about \\\\\\n            a particular topic, Assistant is here to assist.\\'\\'\\'\\n            \\n            human = \\'\\'\\'TOOLS\\n            ------\\n            Assistant can ask the user to use tools to look up information that may be helpful in \\\\\\n            answering the users original question. The tools the human can use are:\\n\\n            {tools}\\n\\n            RESPONSE FORMAT INSTRUCTIONS\\n            ----------------------------\\n\\n            When responding to me, please output a response in one of two formats:\\n\\n            **Option 1:**\\n            Use this if you want the human to use a tool.\\n            Markdown code snippet formatted in the following schema:\\n\\n            ```json\\n            {{\\n                \"action\": string, \\\\ The action to take. Must be one of {tool_names}\\n                \"action_input\": string \\\\ The input to the action\\n            }}\\n            ```\\n\\n            **Option #2:**\\n            Use this if you want to respond directly to the human. Markdown code snippet formatted \\\\\\n            in the following schema:\\n\\n            ```json\\n            {{\\n                \"action\": \"Final Answer\",\\n                \"action_input\": string \\\\ You should put what you want to return to use here\\n            }}\\n            ```\\n\\n            USER\\'S INPUT\\n            --------------------\\n            Here is the user\\'s input (remember to respond with a markdown code snippet of a json \\\\\\n            blob with a single action, and NOTHING else):' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\json_chat\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='{input}\\'\\'\\'\\n            \\n            prompt = ChatPromptTemplate.from_messages(\\n                [\\n                    (\"system\", system),\\n                    MessagesPlaceholder(\"chat_history\", optional=True),\\n                    (\"human\", human),\\n                    MessagesPlaceholder(\"agent_scratchpad\"),\\n                ]\\n            )\\n    \"\"\"  # noqa: E501\\n    missing_vars = {\"tools\", \"tool_names\", \"agent_scratchpad\"}.difference(\\n        prompt.input_variables\\n    )\\n    if missing_vars:\\n        raise ValueError(f\"Prompt missing required variables: {missing_vars}\")\\n\\n    prompt = prompt.partial(\\n        tools=render_text_description(list(tools)),\\n        tool_names=\", \".join([t.name for t in tools]),\\n    )\\n    llm_with_stop = llm.bind(stop=[\"\\\\nObservation\"])\\n\\n    agent = (\\n        RunnablePassthrough.assign(\\n            agent_scratchpad=lambda x: format_log_to_messages(\\n                x[\"intermediate_steps\"], template_tool_response=TEMPLATE_TOOL_RESPONSE\\n            )\\n        )\\n        | prompt\\n        | llm_with_stop\\n        | JSONAgentOutputParser()\\n    )\\n    return agent' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\json_chat\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# flake8: noqa\\nTEMPLATE_TOOL_RESPONSE = \"\"\"TOOL RESPONSE: \\n---------------------\\n{observation}\\n\\nUSER\\'S INPUT\\n--------------------\\n\\nOkay, so what is the response to my last comment? If using information obtained from the tools you must mention it explicitly without mentioning the tool names - I have forgotten all TOOL RESPONSES! Remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else - even if you just want to respond to the user. Do NOT respond with anything except a JSON snippet no matter what!\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\json_chat\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Attempt to implement MRKL systems as described in arxiv.org/pdf/2205.00445.pdf.\"\"\"\\nfrom __future__ import annotations\\n\\nfrom typing import Any, Callable, List, NamedTuple, Optional, Sequence\\n\\nfrom langchain_core._api import deprecated\\nfrom langchain_core.callbacks import BaseCallbackManager\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.prompts import PromptTemplate\\nfrom langchain_core.pydantic_v1 import Field\\nfrom langchain_core.tools import BaseTool\\n\\nfrom langchain.agents.agent import Agent, AgentExecutor, AgentOutputParser\\nfrom langchain.agents.agent_types import AgentType\\nfrom langchain.agents.mrkl.output_parser import MRKLOutputParser\\nfrom langchain.agents.mrkl.prompt import FORMAT_INSTRUCTIONS, PREFIX, SUFFIX\\nfrom langchain.agents.tools import Tool\\nfrom langchain.agents.utils import validate_tools_single_input\\nfrom langchain.chains import LLMChain\\n\\n\\nclass ChainConfig(NamedTuple):\\n    \"\"\"Configuration for chain to use in MRKL system.\\n\\n    Args:\\n        action_name: Name of the action.\\n        action: Action function to call.\\n        action_description: Description of the action.\\n    \"\"\"\\n\\n    action_name: str\\n    action: Callable\\n    action_description: str\\n\\n\\n@deprecated(\"0.1.0\", alternative=\"create_react_agent\", removal=\"0.2.0\")\\nclass ZeroShotAgent(Agent):\\n    \"\"\"Agent for the MRKL chain.\"\"\"\\n\\n    output_parser: AgentOutputParser = Field(default_factory=MRKLOutputParser)\\n\\n    @classmethod\\n    def _get_default_output_parser(cls, **kwargs: Any) -> AgentOutputParser:\\n        return MRKLOutputParser()\\n\\n    @property\\n    def _agent_type(self) -> str:\\n        \"\"\"Return Identifier of agent type.\"\"\"\\n        return AgentType.ZERO_SHOT_REACT_DESCRIPTION\\n\\n    @property\\n    def observation_prefix(self) -> str:\\n        \"\"\"Prefix to append the observation with.\"\"\"\\n        return \"Observation: \"\\n\\n    @property\\n    def llm_prefix(self) -> str:\\n        \"\"\"Prefix to append the llm call with.\"\"\"\\n        return \"Thought:\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\mrkl\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@classmethod\\n    def create_prompt(\\n        cls,\\n        tools: Sequence[BaseTool],\\n        prefix: str = PREFIX,\\n        suffix: str = SUFFIX,\\n        format_instructions: str = FORMAT_INSTRUCTIONS,\\n        input_variables: Optional[List[str]] = None,\\n    ) -> PromptTemplate:\\n        \"\"\"Create prompt in the style of the zero shot agent.\\n\\n        Args:\\n            tools: List of tools the agent will have access to, used to format the\\n                prompt.\\n            prefix: String to put before the list of tools.\\n            suffix: String to put after the list of tools.\\n            input_variables: List of input variables the final prompt will expect.\\n\\n        Returns:\\n            A PromptTemplate with the template assembled from the pieces here.\\n        \"\"\"\\n        tool_strings = \"\\\\n\".join([f\"{tool.name}: {tool.description}\" for tool in tools])\\n        tool_names = \", \".join([tool.name for tool in tools])\\n        format_instructions = format_instructions.format(tool_names=tool_names)\\n        template = \"\\\\n\\\\n\".join([prefix, tool_strings, format_instructions, suffix])\\n        if input_variables:\\n            return PromptTemplate(template=template, input_variables=input_variables)\\n        return PromptTemplate.from_template(template)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\mrkl\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@classmethod\\n    def from_llm_and_tools(\\n        cls,\\n        llm: BaseLanguageModel,\\n        tools: Sequence[BaseTool],\\n        callback_manager: Optional[BaseCallbackManager] = None,\\n        output_parser: Optional[AgentOutputParser] = None,\\n        prefix: str = PREFIX,\\n        suffix: str = SUFFIX,\\n        format_instructions: str = FORMAT_INSTRUCTIONS,\\n        input_variables: Optional[List[str]] = None,\\n        **kwargs: Any,\\n    ) -> Agent:\\n        \"\"\"Construct an agent from an LLM and tools.\"\"\"\\n        cls._validate_tools(tools)\\n        prompt = cls.create_prompt(\\n            tools,\\n            prefix=prefix,\\n            suffix=suffix,\\n            format_instructions=format_instructions,\\n            input_variables=input_variables,\\n        )\\n        llm_chain = LLMChain(\\n            llm=llm,\\n            prompt=prompt,\\n            callback_manager=callback_manager,\\n        )\\n        tool_names = [tool.name for tool in tools]\\n        _output_parser = output_parser or cls._get_default_output_parser()\\n        return cls(\\n            llm_chain=llm_chain,\\n            allowed_tools=tool_names,\\n            output_parser=_output_parser,\\n            **kwargs,\\n        )\\n\\n    @classmethod\\n    def _validate_tools(cls, tools: Sequence[BaseTool]) -> None:\\n        validate_tools_single_input(cls.__name__, tools)\\n        if len(tools) == 0:\\n            raise ValueError(\\n                f\"Got no tools for {cls.__name__}. At least one tool must be provided.\"\\n            )\\n        for tool in tools:\\n            if tool.description is None:\\n                raise ValueError(\\n                    f\"Got a tool {tool.name} without a description. For this agent, \"\\n                    f\"a description must always be provided.\"\\n                )\\n        super()._validate_tools(tools)\\n\\n\\n@deprecated(\"0.1.0\", removal=\"0.2.0\")\\nclass MRKLChain(AgentExecutor):\\n    \"\"\"[Deprecated] Chain that implements the MRKL system.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\mrkl\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@classmethod\\n    def from_chains(\\n        cls, llm: BaseLanguageModel, chains: List[ChainConfig], **kwargs: Any\\n    ) -> AgentExecutor:\\n        \"\"\"User friendly way to initialize the MRKL chain.\\n\\n        This is intended to be an easy way to get up and running with the\\n        MRKL chain.\\n\\n        Args:\\n            llm: The LLM to use as the agent LLM.\\n            chains: The chains the MRKL system has access to.\\n            **kwargs: parameters to be passed to initialization.\\n\\n        Returns:\\n            An initialized MRKL chain.\\n        \"\"\"\\n        tools = [\\n            Tool(\\n                name=c.action_name,\\n                func=c.action,\\n                description=c.action_description,\\n            )\\n            for c in chains\\n        ]\\n        agent = ZeroShotAgent.from_llm_and_tools(llm, tools)\\n        return cls(agent=agent, tools=tools, **kwargs)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\mrkl\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='import re\\nfrom typing import Union\\n\\nfrom langchain_core.agents import AgentAction, AgentFinish\\nfrom langchain_core.exceptions import OutputParserException\\n\\nfrom langchain.agents.agent import AgentOutputParser\\nfrom langchain.agents.mrkl.prompt import FORMAT_INSTRUCTIONS\\n\\nFINAL_ANSWER_ACTION = \"Final Answer:\"\\nMISSING_ACTION_AFTER_THOUGHT_ERROR_MESSAGE = (\\n    \"Invalid Format: Missing \\'Action:\\' after \\'Thought:\"\\n)\\nMISSING_ACTION_INPUT_AFTER_ACTION_ERROR_MESSAGE = (\\n    \"Invalid Format: Missing \\'Action Input:\\' after \\'Action:\\'\"\\n)\\nFINAL_ANSWER_AND_PARSABLE_ACTION_ERROR_MESSAGE = (\\n    \"Parsing LLM output produced both a final answer and a parse-able action:\"\\n)\\n\\n\\nclass MRKLOutputParser(AgentOutputParser):\\n    \"\"\"MRKL Output parser for the chat agent.\"\"\"\\n\\n    def get_format_instructions(self) -> str:\\n        return FORMAT_INSTRUCTIONS\\n\\n    def parse(self, text: str) -> Union[AgentAction, AgentFinish]:\\n        includes_answer = FINAL_ANSWER_ACTION in text\\n        regex = (\\n            r\"Action\\\\s*\\\\d*\\\\s*:[\\\\s]*(.*?)[\\\\s]*Action\\\\s*\\\\d*\\\\s*Input\\\\s*\\\\d*\\\\s*:[\\\\s]*(.*)\"\\n        )\\n        action_match = re.search(regex, text, re.DOTALL)\\n        if action_match and includes_answer:\\n            if text.find(FINAL_ANSWER_ACTION) < text.find(action_match.group(0)):\\n                # if final answer is before the hallucination, return final answer\\n                start_index = text.find(FINAL_ANSWER_ACTION) + len(FINAL_ANSWER_ACTION)\\n                end_index = text.find(\"\\\\n\\\\n\", start_index)\\n                return AgentFinish(\\n                    {\"output\": text[start_index:end_index].strip()}, text[:end_index]\\n                )\\n            else:\\n                raise OutputParserException(\\n                    f\"{FINAL_ANSWER_AND_PARSABLE_ACTION_ERROR_MESSAGE}: {text}\"\\n                )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\mrkl\\\\output_parser.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='if action_match:\\n            action = action_match.group(1).strip()\\n            action_input = action_match.group(2)\\n            tool_input = action_input.strip(\" \")\\n            # ensure if its a well formed SQL query we don\\'t remove any trailing \" chars\\n            if tool_input.startswith(\"SELECT \") is False:\\n                tool_input = tool_input.strip(\\'\"\\')\\n\\n            return AgentAction(action, tool_input, text)\\n\\n        elif includes_answer:\\n            return AgentFinish(\\n                {\"output\": text.split(FINAL_ANSWER_ACTION)[-1].strip()}, text\\n            )\\n\\n        if not re.search(r\"Action\\\\s*\\\\d*\\\\s*:[\\\\s]*(.*?)\", text, re.DOTALL):\\n            raise OutputParserException(\\n                f\"Could not parse LLM output: `{text}`\",\\n                observation=MISSING_ACTION_AFTER_THOUGHT_ERROR_MESSAGE,\\n                llm_output=text,\\n                send_to_llm=True,\\n            )\\n        elif not re.search(\\n            r\"[\\\\s]*Action\\\\s*\\\\d*\\\\s*Input\\\\s*\\\\d*\\\\s*:[\\\\s]*(.*)\", text, re.DOTALL\\n        ):\\n            raise OutputParserException(\\n                f\"Could not parse LLM output: `{text}`\",\\n                observation=MISSING_ACTION_INPUT_AFTER_ACTION_ERROR_MESSAGE,\\n                llm_output=text,\\n                send_to_llm=True,\\n            )\\n        else:\\n            raise OutputParserException(f\"Could not parse LLM output: `{text}`\")\\n\\n    @property\\n    def _type(self) -> str:\\n        return \"mrkl\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\mrkl\\\\output_parser.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# flake8: noqa\\nPREFIX = \"\"\"Answer the following questions as best you can. You have access to the following tools:\"\"\"\\nFORMAT_INSTRUCTIONS = \"\"\"Use the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [{tool_names}]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\"\"\"\\nSUFFIX = \"\"\"Begin!\\n\\nQuestion: {input}\\nThought:{agent_scratchpad}\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\mrkl\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Attempt to implement MRKL systems as described in arxiv.org/pdf/2205.00445.pdf.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\mrkl\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from __future__ import annotations\\n\\nimport json\\nfrom json import JSONDecodeError\\nfrom time import sleep\\nfrom typing import TYPE_CHECKING, Any, Dict, List, Optional, Sequence, Tuple, Union\\n\\nfrom langchain_community.tools.convert_to_openai import format_tool_to_openai_tool\\nfrom langchain_core.agents import AgentAction, AgentFinish\\nfrom langchain_core.callbacks import CallbackManager\\nfrom langchain_core.load import dumpd\\nfrom langchain_core.pydantic_v1 import Field\\nfrom langchain_core.runnables import RunnableConfig, RunnableSerializable, ensure_config\\nfrom langchain_core.tools import BaseTool\\n\\nif TYPE_CHECKING:\\n    import openai\\n    from openai.types.beta.threads import ThreadMessage\\n    from openai.types.beta.threads.required_action_function_tool_call import (\\n        RequiredActionFunctionToolCall,\\n    )\\n\\n\\nclass OpenAIAssistantFinish(AgentFinish):\\n    \"\"\"AgentFinish with run and thread metadata.\"\"\"\\n\\n    run_id: str\\n    thread_id: str\\n\\n    @classmethod\\n    def is_lc_serializable(cls) -> bool:\\n        return False\\n\\n\\nclass OpenAIAssistantAction(AgentAction):\\n    \"\"\"AgentAction with info needed to submit custom tool output to existing run.\"\"\"\\n\\n    tool_call_id: str\\n    run_id: str\\n    thread_id: str\\n\\n    @classmethod\\n    def is_lc_serializable(cls) -> bool:\\n        return False\\n\\n\\ndef _get_openai_client() -> openai.OpenAI:\\n    try:\\n        import openai\\n\\n        return openai.OpenAI()\\n    except ImportError as e:\\n        raise ImportError(\\n            \"Unable to import openai, please install with `pip install openai`.\"\\n        ) from e\\n    except AttributeError as e:\\n        raise AttributeError(\\n            \"Please make sure you are using a v1.1-compatible version of openai. You \"\\n            \\'can install with `pip install \"openai>=1.1\"`.\\'\\n        ) from e\\n\\n\\nOutputType = Union[\\n    List[OpenAIAssistantAction],\\n    OpenAIAssistantFinish,\\n    List[\"ThreadMessage\"],\\n    List[\"RequiredActionFunctionToolCall\"],\\n]\\n\\n\\nclass OpenAIAssistantRunnable(RunnableSerializable[Dict, OutputType]):\\n    \"\"\"Run an OpenAI Assistant.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\openai_assistant\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Example using OpenAI tools:\\n        .. code-block:: python\\n\\n            from langchain_experimental.openai_assistant import OpenAIAssistantRunnable\\n\\n            interpreter_assistant = OpenAIAssistantRunnable.create_assistant(\\n                name=\"langchain assistant\",\\n                instructions=\"You are a personal math tutor. Write and run code to answer math questions.\",\\n                tools=[{\"type\": \"code_interpreter\"}],\\n                model=\"gpt-4-1106-preview\"\\n            )\\n            output = interpreter_assistant.invoke({\"content\": \"What\\'s 10 - 4 raised to the 2.7\"})\\n\\n    Example using custom tools and AgentExecutor:\\n        .. code-block:: python\\n\\n            from langchain_experimental.openai_assistant import OpenAIAssistantRunnable\\n            from langchain.agents import AgentExecutor\\n            from langchain.tools import E2BDataAnalysisTool\\n\\n\\n            tools = [E2BDataAnalysisTool(api_key=\"...\")]\\n            agent = OpenAIAssistantRunnable.create_assistant(\\n                name=\"langchain assistant e2b tool\",\\n                instructions=\"You are a personal math tutor. Write and run code to answer math questions.\",\\n                tools=tools,\\n                model=\"gpt-4-1106-preview\",\\n                as_agent=True\\n            )\\n\\n            agent_executor = AgentExecutor(agent=agent, tools=tools)\\n            agent_executor.invoke({\"content\": \"What\\'s 10 - 4 raised to the 2.7\"})\\n\\n\\n    Example using custom tools and custom execution:\\n        .. code-block:: python\\n\\n            from langchain_experimental.openai_assistant import OpenAIAssistantRunnable\\n            from langchain.agents import AgentExecutor\\n            from langchain_core.agents import AgentFinish\\n            from langchain.tools import E2BDataAnalysisTool' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\openai_assistant\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='tools = [E2BDataAnalysisTool(api_key=\"...\")]\\n            agent = OpenAIAssistantRunnable.create_assistant(\\n                name=\"langchain assistant e2b tool\",\\n                instructions=\"You are a personal math tutor. Write and run code to answer math questions.\",\\n                tools=tools,\\n                model=\"gpt-4-1106-preview\",\\n                as_agent=True\\n            )\\n\\n            def execute_agent(agent, tools, input):\\n                tool_map = {tool.name: tool for tool in tools}\\n                response = agent.invoke(input)\\n                while not isinstance(response, AgentFinish):\\n                    tool_outputs = []\\n                    for action in response:\\n                        tool_output = tool_map[action.tool].invoke(action.tool_input)\\n                        tool_outputs.append({\"output\": tool_output, \"tool_call_id\": action.tool_call_id})\\n                    response = agent.invoke(\\n                        {\\n                            \"tool_outputs\": tool_outputs,\\n                            \"run_id\": action.run_id,\\n                            \"thread_id\": action.thread_id\\n                        }\\n                    )\\n\\n                return response\\n\\n            response = execute_agent(agent, tools, {\"content\": \"What\\'s 10 - 4 raised to the 2.7\"})\\n            next_response = execute_agent(agent, tools, {\"content\": \"now add 17.241\", \"thread_id\": response.thread_id})\\n\\n    \"\"\"  # noqa: E501\\n\\n    client: openai.OpenAI = Field(default_factory=_get_openai_client)\\n    \"\"\"OpenAI client.\"\"\"\\n    assistant_id: str\\n    \"\"\"OpenAI assistant id.\"\"\"\\n    check_every_ms: float = 1_000.0\\n    \"\"\"Frequency with which to check run progress in ms.\"\"\"\\n    as_agent: bool = False\\n    \"\"\"Use as a LangChain agent, compatible with the AgentExecutor.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\openai_assistant\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@classmethod\\n    def create_assistant(\\n        cls,\\n        name: str,\\n        instructions: str,\\n        tools: Sequence[Union[BaseTool, dict]],\\n        model: str,\\n        *,\\n        client: Optional[openai.OpenAI] = None,\\n        **kwargs: Any,\\n    ) -> OpenAIAssistantRunnable:\\n        \"\"\"Create an OpenAI Assistant and instantiate the Runnable.\\n\\n        Args:\\n            name: Assistant name.\\n            instructions: Assistant instructions.\\n            tools: Assistant tools. Can be passed in OpenAI format or as BaseTools.\\n            model: Assistant model to use.\\n            client: OpenAI client. Will create default client if not specified.\\n\\n        Returns:\\n            OpenAIAssistantRunnable configured to run using the created assistant.\\n        \"\"\"\\n        client = client or _get_openai_client()\\n        openai_tools: List = []\\n        for tool in tools:\\n            oai_tool = (\\n                tool if isinstance(tool, dict) else format_tool_to_openai_tool(tool)\\n            )\\n            openai_tools.append(oai_tool)\\n        assistant = client.beta.assistants.create(\\n            name=name,\\n            instructions=instructions,\\n            tools=openai_tools,\\n            model=model,\\n        )\\n        return cls(assistant_id=assistant.id, **kwargs)\\n\\n    def invoke(\\n        self, input: dict, config: Optional[RunnableConfig] = None\\n    ) -> OutputType:\\n        \"\"\"Invoke assistant.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\openai_assistant\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Args:\\n            input: Runnable input dict that can have:\\n                content: User message when starting a new run.\\n                thread_id: Existing thread to use.\\n                run_id: Existing run to use. Should only be supplied when providing\\n                    the tool output for a required action after an initial invocation.\\n                file_ids: File ids to include in new run. Used for retrieval.\\n                message_metadata: Metadata to associate with new message.\\n                thread_metadata: Metadata to associate with new thread. Only relevant\\n                    when new thread being created.\\n                instructions: Additional run instructions.\\n                model: Override Assistant model for this run.\\n                tools: Override Assistant tools for this run.\\n                run_metadata: Metadata to associate with new run.\\n            config: Runnable config:\\n\\n        Return:\\n            If self.as_agent, will return\\n                Union[List[OpenAIAssistantAction], OpenAIAssistantFinish]. Otherwise,\\n                will return OpenAI types\\n                Union[List[ThreadMessage], List[RequiredActionFunctionToolCall]].\\n        \"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\openai_assistant\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='config = ensure_config(config)\\n        callback_manager = CallbackManager.configure(\\n            inheritable_callbacks=config.get(\"callbacks\"),\\n            inheritable_tags=config.get(\"tags\"),\\n            inheritable_metadata=config.get(\"metadata\"),\\n        )\\n        run_manager = callback_manager.on_chain_start(\\n            dumpd(self), input, name=config.get(\"run_name\")\\n        )\\n        try:\\n            # Being run within AgentExecutor and there are tool outputs to submit.\\n            if self.as_agent and input.get(\"intermediate_steps\"):\\n                tool_outputs = self._parse_intermediate_steps(\\n                    input[\"intermediate_steps\"]\\n                )\\n                run = self.client.beta.threads.runs.submit_tool_outputs(**tool_outputs)\\n            # Starting a new thread and a new run.\\n            elif \"thread_id\" not in input:\\n                thread = {\\n                    \"messages\": [\\n                        {\\n                            \"role\": \"user\",\\n                            \"content\": input[\"content\"],\\n                            \"file_ids\": input.get(\"file_ids\", []),\\n                            \"metadata\": input.get(\"message_metadata\"),\\n                        }\\n                    ],\\n                    \"metadata\": input.get(\"thread_metadata\"),\\n                }\\n                run = self._create_thread_and_run(input, thread)\\n            # Starting a new run in an existing thread.\\n            elif \"run_id\" not in input:\\n                _ = self.client.beta.threads.messages.create(\\n                    input[\"thread_id\"],\\n                    content=input[\"content\"],\\n                    role=\"user\",\\n                    file_ids=input.get(\"file_ids\", []),\\n                    metadata=input.get(\"message_metadata\"),\\n                )\\n                run = self._create_run(input)\\n            # Submitting tool outputs to an existing run, outside the AgentExecutor\\n            # framework.\\n            else:' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\openai_assistant\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='run = self.client.beta.threads.runs.submit_tool_outputs(**input)\\n            run = self._wait_for_run(run.id, run.thread_id)\\n        except BaseException as e:\\n            run_manager.on_chain_error(e)\\n            raise e\\n        try:\\n            response = self._get_response(run)\\n        except BaseException as e:\\n            run_manager.on_chain_error(e, metadata=run.dict())\\n            raise e\\n        else:\\n            run_manager.on_chain_end(response)\\n            return response' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\openai_assistant\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _parse_intermediate_steps(\\n        self, intermediate_steps: List[Tuple[OpenAIAssistantAction, str]]\\n    ) -> dict:\\n        last_action, last_output = intermediate_steps[-1]\\n        run = self._wait_for_run(last_action.run_id, last_action.thread_id)\\n        required_tool_call_ids = {\\n            tc.id for tc in run.required_action.submit_tool_outputs.tool_calls\\n        }\\n        tool_outputs = [\\n            {\"output\": str(output), \"tool_call_id\": action.tool_call_id}\\n            for action, output in intermediate_steps\\n            if action.tool_call_id in required_tool_call_ids\\n        ]\\n        submit_tool_outputs = {\\n            \"tool_outputs\": tool_outputs,\\n            \"run_id\": last_action.run_id,\\n            \"thread_id\": last_action.thread_id,\\n        }\\n        return submit_tool_outputs\\n\\n    def _create_run(self, input: dict) -> Any:\\n        params = {\\n            k: v\\n            for k, v in input.items()\\n            if k in (\"instructions\", \"model\", \"tools\", \"run_metadata\")\\n        }\\n        return self.client.beta.threads.runs.create(\\n            input[\"thread_id\"],\\n            assistant_id=self.assistant_id,\\n            **params,\\n        )\\n\\n    def _create_thread_and_run(self, input: dict, thread: dict) -> Any:\\n        params = {\\n            k: v\\n            for k, v in input.items()\\n            if k in (\"instructions\", \"model\", \"tools\", \"run_metadata\")\\n        }\\n        run = self.client.beta.threads.create_and_run(\\n            assistant_id=self.assistant_id,\\n            thread=thread,\\n            **params,\\n        )\\n        return run\\n\\n    def _get_response(self, run: Any) -> Any:\\n        # TODO: Pagination\\n\\n        if run.status == \"completed\":\\n            import openai' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\openai_assistant\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='messages = self.client.beta.threads.messages.list(\\n                run.thread_id, order=\"asc\"\\n            )\\n            new_messages = [msg for msg in messages if msg.run_id == run.id]\\n            if not self.as_agent:\\n                return new_messages\\n            answer: Any = [\\n                msg_content for msg in new_messages for msg_content in msg.content\\n            ]\\n            if all(\\n                isinstance(content, openai.types.beta.threads.MessageContentText)\\n                for content in answer\\n            ):\\n                answer = \"\\\\n\".join(content.text.value for content in answer)\\n            return OpenAIAssistantFinish(\\n                return_values={\\n                    \"output\": answer,\\n                    \"thread_id\": run.thread_id,\\n                    \"run_id\": run.id,\\n                },\\n                log=\"\",\\n                run_id=run.id,\\n                thread_id=run.thread_id,\\n            )\\n        elif run.status == \"requires_action\":\\n            if not self.as_agent:\\n                return run.required_action.submit_tool_outputs.tool_calls\\n            actions = []\\n            for tool_call in run.required_action.submit_tool_outputs.tool_calls:\\n                function = tool_call.function\\n                try:\\n                    args = json.loads(function.arguments, strict=False)\\n                except JSONDecodeError as e:\\n                    raise ValueError(\\n                        f\"Received invalid JSON function arguments: \"\\n                        f\"{function.arguments} for function {function.name}\"\\n                    ) from e\\n                if len(args) == 1 and \"__arg1\" in args:\\n                    args = args[\"__arg1\"]\\n                actions.append(\\n                    OpenAIAssistantAction(\\n                        tool=function.name,\\n                        tool_input=args,\\n                        tool_call_id=tool_call.id,\\n                        log=\"\",\\n                        run_id=run.id,\\n                        thread_id=run.thread_id,' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\openai_assistant\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content=')\\n                )\\n            return actions\\n        else:\\n            run_info = json.dumps(run.dict(), indent=2)\\n            raise ValueError(\\n                f\"Unexpected run status: {run.status}. Full run info:\\\\n\\\\n{run_info})\"\\n            )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\openai_assistant\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _wait_for_run(self, run_id: str, thread_id: str) -> Any:\\n        in_progress = True\\n        while in_progress:\\n            run = self.client.beta.threads.runs.retrieve(run_id, thread_id=thread_id)\\n            in_progress = run.status in (\"in_progress\", \"queued\")\\n            if in_progress:\\n                sleep(self.check_every_ms / 1000)\\n        return run' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\openai_assistant\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain.agents.openai_assistant.base import OpenAIAssistantRunnable\\n\\n__all__ = [\"OpenAIAssistantRunnable\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\openai_assistant\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Memory used to save agent output AND intermediate steps.\"\"\"\\nfrom typing import Any, Dict, List\\n\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.messages import BaseMessage, get_buffer_string\\n\\nfrom langchain.agents.format_scratchpad.openai_functions import (\\n    format_to_openai_function_messages,\\n)\\nfrom langchain.memory.chat_memory import BaseChatMemory\\n\\n\\nclass AgentTokenBufferMemory(BaseChatMemory):\\n    \"\"\"Memory used to save agent output AND intermediate steps.\"\"\"\\n\\n    human_prefix: str = \"Human\"\\n    ai_prefix: str = \"AI\"\\n    llm: BaseLanguageModel\\n    memory_key: str = \"history\"\\n    max_token_limit: int = 12000\\n    \"\"\"The max number of tokens to keep in the buffer. \\n    Once the buffer exceeds this many tokens, the oldest messages will be pruned.\"\"\"\\n    return_messages: bool = True\\n    output_key: str = \"output\"\\n    intermediate_steps_key: str = \"intermediate_steps\"\\n\\n    @property\\n    def buffer(self) -> List[BaseMessage]:\\n        \"\"\"String buffer of memory.\"\"\"\\n        return self.chat_memory.messages\\n\\n    @property\\n    def memory_variables(self) -> List[str]:\\n        \"\"\"Will always return list of memory variables.\\n\\n        :meta private:\\n        \"\"\"\\n        return [self.memory_key]\\n\\n    def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, Any]:\\n        \"\"\"Return history buffer.\"\"\"\\n        if self.return_messages:\\n            final_buffer: Any = self.buffer\\n        else:\\n            final_buffer = get_buffer_string(\\n                self.buffer,\\n                human_prefix=self.human_prefix,\\n                ai_prefix=self.ai_prefix,\\n            )\\n        return {self.memory_key: final_buffer}' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\openai_functions_agent\\\\agent_token_buffer_memory.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, Any]) -> None:\\n        \"\"\"Save context from this conversation to buffer. Pruned.\"\"\"\\n        input_str, output_str = self._get_input_output(inputs, outputs)\\n        self.chat_memory.add_user_message(input_str)\\n        steps = format_to_openai_function_messages(outputs[self.intermediate_steps_key])\\n        for msg in steps:\\n            self.chat_memory.add_message(msg)\\n        self.chat_memory.add_ai_message(output_str)\\n        # Prune buffer if it exceeds max token limit\\n        buffer = self.chat_memory.messages\\n        curr_buffer_length = self.llm.get_num_tokens_from_messages(buffer)\\n        if curr_buffer_length > self.max_token_limit:\\n            while curr_buffer_length > self.max_token_limit:\\n                buffer.pop(0)\\n                curr_buffer_length = self.llm.get_num_tokens_from_messages(buffer)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\openai_functions_agent\\\\agent_token_buffer_memory.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Module implements an agent that uses OpenAI\\'s APIs function enabled API.\"\"\"\\nfrom typing import Any, List, Optional, Sequence, Tuple, Type, Union\\n\\nfrom langchain_community.tools.convert_to_openai import format_tool_to_openai_function\\nfrom langchain_core._api import deprecated\\nfrom langchain_core.agents import AgentAction, AgentFinish\\nfrom langchain_core.callbacks import BaseCallbackManager, Callbacks\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.messages import (\\n    BaseMessage,\\n    SystemMessage,\\n)\\nfrom langchain_core.prompts import BasePromptTemplate\\nfrom langchain_core.prompts.chat import (\\n    BaseMessagePromptTemplate,\\n    ChatPromptTemplate,\\n    HumanMessagePromptTemplate,\\n    MessagesPlaceholder,\\n)\\nfrom langchain_core.pydantic_v1 import root_validator\\nfrom langchain_core.runnables import Runnable, RunnablePassthrough\\nfrom langchain_core.tools import BaseTool\\n\\nfrom langchain.agents import BaseSingleActionAgent\\nfrom langchain.agents.format_scratchpad.openai_functions import (\\n    format_to_openai_function_messages,\\n)\\nfrom langchain.agents.output_parsers.openai_functions import (\\n    OpenAIFunctionsAgentOutputParser,\\n)\\n\\n\\n@deprecated(\"0.1.0\", alternative=\"create_openai_functions_agent\", removal=\"0.2.0\")\\nclass OpenAIFunctionsAgent(BaseSingleActionAgent):\\n    \"\"\"An Agent driven by OpenAIs function powered API.\\n\\n    Args:\\n        llm: This should be an instance of ChatOpenAI, specifically a model\\n            that supports using `functions`.\\n        tools: The tools this agent has access to.\\n        prompt: The prompt for this agent, should support agent_scratchpad as one\\n            of the variables. For an easy way to construct this prompt, use\\n            `OpenAIFunctionsAgent.create_prompt(...)`\\n    \"\"\"\\n\\n    llm: BaseLanguageModel\\n    tools: Sequence[BaseTool]\\n    prompt: BasePromptTemplate\\n    output_parser: Type[\\n        OpenAIFunctionsAgentOutputParser\\n    ] = OpenAIFunctionsAgentOutputParser' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\openai_functions_agent\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def get_allowed_tools(self) -> List[str]:\\n        \"\"\"Get allowed tools.\"\"\"\\n        return [t.name for t in self.tools]\\n\\n    @root_validator\\n    def validate_prompt(cls, values: dict) -> dict:\\n        prompt: BasePromptTemplate = values[\"prompt\"]\\n        if \"agent_scratchpad\" not in prompt.input_variables:\\n            raise ValueError(\\n                \"`agent_scratchpad` should be one of the variables in the prompt, \"\\n                f\"got {prompt.input_variables}\"\\n            )\\n        return values\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Get input keys. Input refers to user input here.\"\"\"\\n        return [\"input\"]\\n\\n    @property\\n    def functions(self) -> List[dict]:\\n        return [dict(format_tool_to_openai_function(t)) for t in self.tools]\\n\\n    def plan(\\n        self,\\n        intermediate_steps: List[Tuple[AgentAction, str]],\\n        callbacks: Callbacks = None,\\n        with_functions: bool = True,\\n        **kwargs: Any,\\n    ) -> Union[AgentAction, AgentFinish]:\\n        \"\"\"Given input, decided what to do.\\n\\n        Args:\\n            intermediate_steps: Steps the LLM has taken to date, along with observations\\n            **kwargs: User inputs.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\openai_functions_agent\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Returns:\\n            Action specifying what tool to use.\\n        \"\"\"\\n        agent_scratchpad = format_to_openai_function_messages(intermediate_steps)\\n        selected_inputs = {\\n            k: kwargs[k] for k in self.prompt.input_variables if k != \"agent_scratchpad\"\\n        }\\n        full_inputs = dict(**selected_inputs, agent_scratchpad=agent_scratchpad)\\n        prompt = self.prompt.format_prompt(**full_inputs)\\n        messages = prompt.to_messages()\\n        if with_functions:\\n            predicted_message = self.llm.predict_messages(\\n                messages,\\n                functions=self.functions,\\n                callbacks=callbacks,\\n            )\\n        else:\\n            predicted_message = self.llm.predict_messages(\\n                messages,\\n                callbacks=callbacks,\\n            )\\n        agent_decision = self.output_parser._parse_ai_message(predicted_message)\\n        return agent_decision\\n\\n    async def aplan(\\n        self,\\n        intermediate_steps: List[Tuple[AgentAction, str]],\\n        callbacks: Callbacks = None,\\n        **kwargs: Any,\\n    ) -> Union[AgentAction, AgentFinish]:\\n        \"\"\"Given input, decided what to do.\\n\\n        Args:\\n            intermediate_steps: Steps the LLM has taken to date,\\n                along with observations\\n            **kwargs: User inputs.\\n\\n        Returns:\\n            Action specifying what tool to use.\\n        \"\"\"\\n        agent_scratchpad = format_to_openai_function_messages(intermediate_steps)\\n        selected_inputs = {\\n            k: kwargs[k] for k in self.prompt.input_variables if k != \"agent_scratchpad\"\\n        }\\n        full_inputs = dict(**selected_inputs, agent_scratchpad=agent_scratchpad)\\n        prompt = self.prompt.format_prompt(**full_inputs)\\n        messages = prompt.to_messages()\\n        predicted_message = await self.llm.apredict_messages(\\n            messages, functions=self.functions, callbacks=callbacks\\n        )\\n        agent_decision = self.output_parser._parse_ai_message(predicted_message)\\n        return agent_decision' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\openai_functions_agent\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def return_stopped_response(\\n        self,\\n        early_stopping_method: str,\\n        intermediate_steps: List[Tuple[AgentAction, str]],\\n        **kwargs: Any,\\n    ) -> AgentFinish:\\n        \"\"\"Return response when agent has been stopped due to max iterations.\"\"\"\\n        if early_stopping_method == \"force\":\\n            # `force` just returns a constant string\\n            return AgentFinish(\\n                {\"output\": \"Agent stopped due to iteration limit or time limit.\"}, \"\"\\n            )\\n        elif early_stopping_method == \"generate\":\\n            # Generate does one final forward pass\\n            agent_decision = self.plan(\\n                intermediate_steps, with_functions=False, **kwargs\\n            )\\n            if isinstance(agent_decision, AgentFinish):\\n                return agent_decision\\n            else:\\n                raise ValueError(\\n                    f\"got AgentAction with no functions provided: {agent_decision}\"\\n                )\\n        else:\\n            raise ValueError(\\n                \"early_stopping_method should be one of `force` or `generate`, \"\\n                f\"got {early_stopping_method}\"\\n            )\\n\\n    @classmethod\\n    def create_prompt(\\n        cls,\\n        system_message: Optional[SystemMessage] = SystemMessage(\\n            content=\"You are a helpful AI assistant.\"\\n        ),\\n        extra_prompt_messages: Optional[List[BaseMessagePromptTemplate]] = None,\\n    ) -> BasePromptTemplate:\\n        \"\"\"Create prompt for this agent.\\n\\n        Args:\\n            system_message: Message to use as the system message that will be the\\n                first in the prompt.\\n            extra_prompt_messages: Prompt messages that will be placed between the\\n                system message and the new human input.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\openai_functions_agent\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Returns:\\n            A prompt template to pass into this agent.\\n        \"\"\"\\n        _prompts = extra_prompt_messages or []\\n        messages: List[Union[BaseMessagePromptTemplate, BaseMessage]]\\n        if system_message:\\n            messages = [system_message]\\n        else:\\n            messages = []\\n\\n        messages.extend(\\n            [\\n                *_prompts,\\n                HumanMessagePromptTemplate.from_template(\"{input}\"),\\n                MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\\n            ]\\n        )\\n        return ChatPromptTemplate(messages=messages)\\n\\n    @classmethod\\n    def from_llm_and_tools(\\n        cls,\\n        llm: BaseLanguageModel,\\n        tools: Sequence[BaseTool],\\n        callback_manager: Optional[BaseCallbackManager] = None,\\n        extra_prompt_messages: Optional[List[BaseMessagePromptTemplate]] = None,\\n        system_message: Optional[SystemMessage] = SystemMessage(\\n            content=\"You are a helpful AI assistant.\"\\n        ),\\n        **kwargs: Any,\\n    ) -> BaseSingleActionAgent:\\n        \"\"\"Construct an agent from an LLM and tools.\"\"\"\\n        prompt = cls.create_prompt(\\n            extra_prompt_messages=extra_prompt_messages,\\n            system_message=system_message,\\n        )\\n        return cls(\\n            llm=llm,\\n            prompt=prompt,\\n            tools=tools,\\n            callback_manager=callback_manager,\\n            **kwargs,\\n        )\\n\\n\\ndef create_openai_functions_agent(\\n    llm: BaseLanguageModel, tools: Sequence[BaseTool], prompt: ChatPromptTemplate\\n) -> Runnable:\\n    \"\"\"Create an agent that uses OpenAI function calling.\\n\\n    Args:\\n        llm: LLM to use as the agent. Should work with OpenAI function calling,\\n            so either be an OpenAI model that supports that or a wrapper of\\n            a different model that adds in equivalent support.\\n        tools: Tools this agent has access to.\\n        prompt: The prompt to use. See Prompt section below for more.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\openai_functions_agent\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Returns:\\n        A Runnable sequence representing an agent. It takes as input all the same input\\n        variables as the prompt passed in does. It returns as output either an\\n        AgentAction or AgentFinish.\\n\\n    Example:\\n\\n        Creating an agent with no memory\\n\\n        .. code-block:: python\\n\\n            from langchain_community.chat_models import ChatOpenAI\\n            from langchain.agents import AgentExecutor, create_openai_functions_agent\\n            from langchain import hub\\n\\n            prompt = hub.pull(\"hwchase17/openai-functions-agent\")\\n            model = ChatOpenAI()\\n            tools = ...\\n\\n            agent = create_openai_functions_agent(model, tools, prompt)\\n            agent_executor = AgentExecutor(agent=agent, tools=tools)\\n\\n            agent_executor.invoke({\"input\": \"hi\"})\\n\\n            # Using with chat history\\n            from langchain_core.messages import AIMessage, HumanMessage\\n            agent_executor.invoke(\\n                {\\n                    \"input\": \"what\\'s my name?\",\\n                    \"chat_history\": [\\n                        HumanMessage(content=\"hi! my name is bob\"),\\n                        AIMessage(content=\"Hello Bob! How can I assist you today?\"),\\n                    ],\\n                }\\n            )\\n\\n    Prompt:\\n\\n        The agent prompt must have an `agent_scratchpad` key that is a\\n            ``MessagesPlaceholder``. Intermediate agent actions and tool output\\n            messages will be passed in here.\\n\\n        Here\\'s an example:\\n\\n        .. code-block:: python\\n\\n            from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\openai_functions_agent\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='prompt = ChatPromptTemplate.from_messages(\\n                [\\n                    (\"system\", \"You are a helpful assistant\"),\\n                    MessagesPlaceholder(\"chat_history\", optional=True),\\n                    (\"human\", \"{input}\"),\\n                    MessagesPlaceholder(\"agent_scratchpad\"),\\n                ]\\n            )\\n    \"\"\"\\n    if \"agent_scratchpad\" not in prompt.input_variables:\\n        raise ValueError(\\n            \"Prompt must have input variable `agent_scratchpad`, but wasn\\'t found. \"\\n            f\"Found {prompt.input_variables} instead.\"\\n        )\\n    llm_with_tools = llm.bind(\\n        functions=[format_tool_to_openai_function(t) for t in tools]\\n    )\\n    agent = (\\n        RunnablePassthrough.assign(\\n            agent_scratchpad=lambda x: format_to_openai_function_messages(\\n                x[\"intermediate_steps\"]\\n            )\\n        )\\n        | prompt\\n        | llm_with_tools\\n        | OpenAIFunctionsAgentOutputParser()\\n    )\\n    return agent' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\openai_functions_agent\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Module implements an agent that uses OpenAI\\'s APIs function enabled API.\"\"\"\\nimport json\\nfrom json import JSONDecodeError\\nfrom typing import Any, List, Optional, Sequence, Tuple, Union\\n\\nfrom langchain_core._api import deprecated\\nfrom langchain_core.agents import AgentAction, AgentActionMessageLog, AgentFinish\\nfrom langchain_core.callbacks import BaseCallbackManager, Callbacks\\nfrom langchain_core.exceptions import OutputParserException\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.messages import (\\n    AIMessage,\\n    BaseMessage,\\n    SystemMessage,\\n)\\nfrom langchain_core.prompts import BasePromptTemplate\\nfrom langchain_core.prompts.chat import (\\n    BaseMessagePromptTemplate,\\n    ChatPromptTemplate,\\n    HumanMessagePromptTemplate,\\n    MessagesPlaceholder,\\n)\\nfrom langchain_core.pydantic_v1 import root_validator\\nfrom langchain_core.tools import BaseTool\\n\\nfrom langchain.agents import BaseMultiActionAgent\\nfrom langchain.agents.format_scratchpad.openai_functions import (\\n    format_to_openai_function_messages,\\n)\\n\\n# For backwards compatibility\\n_FunctionsAgentAction = AgentActionMessageLog\\n\\n\\ndef _parse_ai_message(message: BaseMessage) -> Union[List[AgentAction], AgentFinish]:\\n    \"\"\"Parse an AI message.\"\"\"\\n    if not isinstance(message, AIMessage):\\n        raise TypeError(f\"Expected an AI message got {type(message)}\")\\n\\n    function_call = message.additional_kwargs.get(\"function_call\", {})\\n\\n    if function_call:\\n        try:\\n            arguments = json.loads(function_call[\"arguments\"], strict=False)\\n        except JSONDecodeError:\\n            raise OutputParserException(\\n                f\"Could not parse tool input: {function_call} because \"\\n                f\"the `arguments` is not valid JSON.\"\\n            )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\openai_functions_multi_agent\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='try:\\n            tools = arguments[\"actions\"]\\n        except (TypeError, KeyError):\\n            raise OutputParserException(\\n                f\"Could not parse tool input: {function_call} because \"\\n                f\"the `arguments` JSON does not contain `actions` key.\"\\n            )\\n\\n        final_tools: List[AgentAction] = []\\n        for tool_schema in tools:\\n            if \"action\" in tool_schema:\\n                _tool_input = tool_schema[\"action\"]\\n            else:\\n                # drop action_name from schema\\n                _tool_input = tool_schema.copy()\\n                del _tool_input[\"action_name\"]\\n            function_name = tool_schema[\"action_name\"]\\n\\n            # HACK HACK HACK:\\n            # The code that encodes tool input into Open AI uses a special variable\\n            # name called `__arg1` to handle old style tools that do not expose a\\n            # schema and expect a single string argument as an input.\\n            # We unpack the argument here if it exists.\\n            # Open AI does not support passing in a JSON array as an argument.\\n            if \"__arg1\" in _tool_input:\\n                tool_input = _tool_input[\"__arg1\"]\\n            else:\\n                tool_input = _tool_input\\n\\n            content_msg = f\"responded: {message.content}\\\\n\" if message.content else \"\\\\n\"\\n            log = f\"\\\\nInvoking: `{function_name}` with `{tool_input}`\\\\n{content_msg}\\\\n\"\\n            _tool = _FunctionsAgentAction(\\n                tool=function_name,\\n                tool_input=tool_input,\\n                log=log,\\n                message_log=[message],\\n            )\\n            final_tools.append(_tool)\\n        return final_tools\\n\\n    return AgentFinish(\\n        return_values={\"output\": message.content}, log=str(message.content)\\n    )\\n\\n\\n@deprecated(\"0.1.0\", alternative=\"create_openai_tools_agent\", removal=\"0.2.0\")\\nclass OpenAIMultiFunctionsAgent(BaseMultiActionAgent):\\n    \"\"\"An Agent driven by OpenAIs function powered API.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\openai_functions_multi_agent\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Args:\\n        llm: This should be an instance of ChatOpenAI, specifically a model\\n            that supports using `functions`.\\n        tools: The tools this agent has access to.\\n        prompt: The prompt for this agent, should support agent_scratchpad as one\\n            of the variables. For an easy way to construct this prompt, use\\n            `OpenAIMultiFunctionsAgent.create_prompt(...)`\\n    \"\"\"\\n\\n    llm: BaseLanguageModel\\n    tools: Sequence[BaseTool]\\n    prompt: BasePromptTemplate\\n\\n    def get_allowed_tools(self) -> List[str]:\\n        \"\"\"Get allowed tools.\"\"\"\\n        return [t.name for t in self.tools]\\n\\n    @root_validator\\n    def validate_prompt(cls, values: dict) -> dict:\\n        prompt: BasePromptTemplate = values[\"prompt\"]\\n        if \"agent_scratchpad\" not in prompt.input_variables:\\n            raise ValueError(\\n                \"`agent_scratchpad` should be one of the variables in the prompt, \"\\n                f\"got {prompt.input_variables}\"\\n            )\\n        return values\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Get input keys. Input refers to user input here.\"\"\"\\n        return [\"input\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\openai_functions_multi_agent\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@property\\n    def functions(self) -> List[dict]:\\n        enum_vals = [t.name for t in self.tools]\\n        tool_selection = {\\n            # OpenAI functions returns a single tool invocation\\n            # Here we force the single tool invocation it returns to\\n            # itself be a list of tool invocations. We do this by constructing\\n            # a new tool that has one argument which is a list of tools\\n            # to use.\\n            \"name\": \"tool_selection\",\\n            \"description\": \"A list of actions to take.\",\\n            \"parameters\": {\\n                \"title\": \"tool_selection\",\\n                \"description\": \"A list of actions to take.\",\\n                \"type\": \"object\",\\n                \"properties\": {\\n                    \"actions\": {\\n                        \"title\": \"actions\",\\n                        \"type\": \"array\",\\n                        \"items\": {\\n                            # This is a custom item which bundles the action_name\\n                            # and the action. We do this because some actions\\n                            # could have the same schema, and without this there\\n                            # is no way to differentiate them.\\n                            \"title\": \"tool_call\",\\n                            \"type\": \"object\",\\n                            \"properties\": {\\n                                # This is the name of the action to take\\n                                \"action_name\": {\\n                                    \"title\": \"action_name\",\\n                                    \"enum\": enum_vals,\\n                                    \"type\": \"string\",\\n                                    \"description\": (\\n                                        \"Name of the action to take. The name \"\\n                                        \"provided here should match up with the \"\\n                                        \"parameters for the action below.\"\\n                                    ),\\n                                },\\n                                # This is the action to take.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\openai_functions_multi_agent\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"action\": {\\n                                    \"title\": \"Action\",\\n                                    \"anyOf\": [\\n                                        {\\n                                            \"title\": t.name,\\n                                            \"type\": \"object\",\\n                                            \"properties\": t.args,\\n                                        }\\n                                        for t in self.tools\\n                                    ],\\n                                },\\n                            },\\n                            \"required\": [\"action_name\", \"action\"],\\n                        },\\n                    }\\n                },\\n                \"required\": [\"actions\"],\\n            },\\n        }\\n        return [tool_selection]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\openai_functions_multi_agent\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def plan(\\n        self,\\n        intermediate_steps: List[Tuple[AgentAction, str]],\\n        callbacks: Callbacks = None,\\n        **kwargs: Any,\\n    ) -> Union[List[AgentAction], AgentFinish]:\\n        \"\"\"Given input, decided what to do.\\n\\n        Args:\\n            intermediate_steps: Steps the LLM has taken to date, along with observations\\n            **kwargs: User inputs.\\n\\n        Returns:\\n            Action specifying what tool to use.\\n        \"\"\"\\n        agent_scratchpad = format_to_openai_function_messages(intermediate_steps)\\n        selected_inputs = {\\n            k: kwargs[k] for k in self.prompt.input_variables if k != \"agent_scratchpad\"\\n        }\\n        full_inputs = dict(**selected_inputs, agent_scratchpad=agent_scratchpad)\\n        prompt = self.prompt.format_prompt(**full_inputs)\\n        messages = prompt.to_messages()\\n        predicted_message = self.llm.predict_messages(\\n            messages, functions=self.functions, callbacks=callbacks\\n        )\\n        agent_decision = _parse_ai_message(predicted_message)\\n        return agent_decision\\n\\n    async def aplan(\\n        self,\\n        intermediate_steps: List[Tuple[AgentAction, str]],\\n        callbacks: Callbacks = None,\\n        **kwargs: Any,\\n    ) -> Union[List[AgentAction], AgentFinish]:\\n        \"\"\"Given input, decided what to do.\\n\\n        Args:\\n            intermediate_steps: Steps the LLM has taken to date,\\n                along with observations\\n            **kwargs: User inputs.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\openai_functions_multi_agent\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Returns:\\n            Action specifying what tool to use.\\n        \"\"\"\\n        agent_scratchpad = format_to_openai_function_messages(intermediate_steps)\\n        selected_inputs = {\\n            k: kwargs[k] for k in self.prompt.input_variables if k != \"agent_scratchpad\"\\n        }\\n        full_inputs = dict(**selected_inputs, agent_scratchpad=agent_scratchpad)\\n        prompt = self.prompt.format_prompt(**full_inputs)\\n        messages = prompt.to_messages()\\n        predicted_message = await self.llm.apredict_messages(\\n            messages, functions=self.functions, callbacks=callbacks\\n        )\\n        agent_decision = _parse_ai_message(predicted_message)\\n        return agent_decision\\n\\n    @classmethod\\n    def create_prompt(\\n        cls,\\n        system_message: Optional[SystemMessage] = SystemMessage(\\n            content=\"You are a helpful AI assistant.\"\\n        ),\\n        extra_prompt_messages: Optional[List[BaseMessagePromptTemplate]] = None,\\n    ) -> BasePromptTemplate:\\n        \"\"\"Create prompt for this agent.\\n\\n        Args:\\n            system_message: Message to use as the system message that will be the\\n                first in the prompt.\\n            extra_prompt_messages: Prompt messages that will be placed between the\\n                system message and the new human input.\\n\\n        Returns:\\n            A prompt template to pass into this agent.\\n        \"\"\"\\n        _prompts = extra_prompt_messages or []\\n        messages: List[Union[BaseMessagePromptTemplate, BaseMessage]]\\n        if system_message:\\n            messages = [system_message]\\n        else:\\n            messages = []\\n\\n        messages.extend(\\n            [\\n                *_prompts,\\n                HumanMessagePromptTemplate.from_template(\"{input}\"),\\n                MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\\n            ]\\n        )\\n        return ChatPromptTemplate(messages=messages)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\openai_functions_multi_agent\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@classmethod\\n    def from_llm_and_tools(\\n        cls,\\n        llm: BaseLanguageModel,\\n        tools: Sequence[BaseTool],\\n        callback_manager: Optional[BaseCallbackManager] = None,\\n        extra_prompt_messages: Optional[List[BaseMessagePromptTemplate]] = None,\\n        system_message: Optional[SystemMessage] = SystemMessage(\\n            content=\"You are a helpful AI assistant.\"\\n        ),\\n        **kwargs: Any,\\n    ) -> BaseMultiActionAgent:\\n        \"\"\"Construct an agent from an LLM and tools.\"\"\"\\n        prompt = cls.create_prompt(\\n            extra_prompt_messages=extra_prompt_messages,\\n            system_message=system_message,\\n        )\\n        return cls(\\n            llm=llm,\\n            prompt=prompt,\\n            tools=tools,\\n            callback_manager=callback_manager,\\n            **kwargs,\\n        )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\openai_functions_multi_agent\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from typing import Sequence\\n\\nfrom langchain_community.tools.convert_to_openai import format_tool_to_openai_tool\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.prompts.chat import ChatPromptTemplate\\nfrom langchain_core.runnables import Runnable, RunnablePassthrough\\nfrom langchain_core.tools import BaseTool\\n\\nfrom langchain.agents.format_scratchpad.openai_tools import (\\n    format_to_openai_tool_messages,\\n)\\nfrom langchain.agents.output_parsers.openai_tools import OpenAIToolsAgentOutputParser\\n\\n\\ndef create_openai_tools_agent(\\n    llm: BaseLanguageModel, tools: Sequence[BaseTool], prompt: ChatPromptTemplate\\n) -> Runnable:\\n    \"\"\"Create an agent that uses OpenAI tools.\\n\\n    Args:\\n        llm: LLM to use as the agent.\\n        tools: Tools this agent has access to.\\n        prompt: The prompt to use. See Prompt section below for more on the expected\\n            input variables.\\n\\n    Returns:\\n        A Runnable sequence representing an agent. It takes as input all the same input\\n        variables as the prompt passed in does. It returns as output either an\\n        AgentAction or AgentFinish.\\n\\n    Example:\\n\\n        .. code-block:: python\\n\\n            from langchain import hub\\n            from langchain_community.chat_models import ChatOpenAI\\n            from langchain.agents import AgentExecutor, create_openai_tools_agent\\n\\n            prompt = hub.pull(\"hwchase17/openai-tools-agent\")\\n            model = ChatOpenAI()\\n            tools = ...\\n\\n            agent = create_openai_tools_agent(model, tools, prompt)\\n            agent_executor = AgentExecutor(agent=agent, tools=tools)\\n\\n            agent_executor.invoke({\"input\": \"hi\"})' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\openai_tools\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# Using with chat history\\n            from langchain_core.messages import AIMessage, HumanMessage\\n            agent_executor.invoke(\\n                {\\n                    \"input\": \"what\\'s my name?\",\\n                    \"chat_history\": [\\n                        HumanMessage(content=\"hi! my name is bob\"),\\n                        AIMessage(content=\"Hello Bob! How can I assist you today?\"),\\n                    ],\\n                }\\n            )\\n\\n    Prompt:\\n\\n        The agent prompt must have an `agent_scratchpad` key that is a\\n            ``MessagesPlaceholder``. Intermediate agent actions and tool output\\n            messages will be passed in here.\\n\\n        Here\\'s an example:\\n\\n        .. code-block:: python\\n\\n            from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\\n\\n            prompt = ChatPromptTemplate.from_messages(\\n                [\\n                    (\"system\", \"You are a helpful assistant\"),\\n                    MessagesPlaceholder(\"chat_history\", optional=True),\\n                    (\"human\", \"{input}\"),\\n                    MessagesPlaceholder(\"agent_scratchpad\"),\\n                ]\\n            )\\n    \"\"\"\\n    missing_vars = {\"agent_scratchpad\"}.difference(prompt.input_variables)\\n    if missing_vars:\\n        raise ValueError(f\"Prompt missing required variables: {missing_vars}\")\\n\\n    llm_with_tools = llm.bind(\\n        tools=[format_tool_to_openai_tool(tool) for tool in tools]\\n    )\\n\\n    agent = (\\n        RunnablePassthrough.assign(\\n            agent_scratchpad=lambda x: format_to_openai_tool_messages(\\n                x[\"intermediate_steps\"]\\n            )\\n        )\\n        | prompt\\n        | llm_with_tools\\n        | OpenAIToolsAgentOutputParser()\\n    )\\n    return agent' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\openai_tools\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from __future__ import annotations\\n\\nimport logging\\nfrom typing import Union\\n\\nfrom langchain_core.agents import AgentAction, AgentFinish\\nfrom langchain_core.exceptions import OutputParserException\\nfrom langchain_core.output_parsers.json import parse_json_markdown\\n\\nfrom langchain.agents.agent import AgentOutputParser\\n\\nlogger = logging.getLogger(__name__)\\n\\n\\nclass JSONAgentOutputParser(AgentOutputParser):\\n    \"\"\"Parses tool invocations and final answers in JSON format.\\n\\n    Expects output to be in one of two formats.\\n\\n    If the output signals that an action should be taken,\\n    should be in the below format. This will result in an AgentAction\\n    being returned.\\n\\n    ```\\n    {\\n      \"action\": \"search\",\\n      \"action_input\": \"2+2\"\\n    }\\n    ```\\n\\n    If the output signals that a final answer should be given,\\n    should be in the below format. This will result in an AgentFinish\\n    being returned.\\n\\n    ```\\n    {\\n      \"action\": \"Final Answer\",\\n      \"action_input\": \"4\"\\n    }\\n    ```\\n    \"\"\"\\n\\n    def parse(self, text: str) -> Union[AgentAction, AgentFinish]:\\n        try:\\n            response = parse_json_markdown(text)\\n            if isinstance(response, list):\\n                # gpt turbo frequently ignores the directive to emit a single action\\n                logger.warning(\"Got multiple action responses: %s\", response)\\n                response = response[0]\\n            if response[\"action\"] == \"Final Answer\":\\n                return AgentFinish({\"output\": response[\"action_input\"]}, text)\\n            else:\\n                return AgentAction(\\n                    response[\"action\"], response.get(\"action_input\", {}), text\\n                )\\n        except Exception as e:\\n            raise OutputParserException(f\"Could not parse LLM output: {text}\") from e\\n\\n    @property\\n    def _type(self) -> str:\\n        return \"json-agent\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\output_parsers\\\\json.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='import json\\nfrom json import JSONDecodeError\\nfrom typing import List, Union\\n\\nfrom langchain_core.agents import AgentAction, AgentActionMessageLog, AgentFinish\\nfrom langchain_core.exceptions import OutputParserException\\nfrom langchain_core.messages import (\\n    AIMessage,\\n    BaseMessage,\\n)\\nfrom langchain_core.outputs import ChatGeneration, Generation\\n\\nfrom langchain.agents.agent import AgentOutputParser\\n\\n\\nclass OpenAIFunctionsAgentOutputParser(AgentOutputParser):\\n    \"\"\"Parses a message into agent action/finish.\\n\\n    Is meant to be used with OpenAI models, as it relies on the specific\\n    function_call parameter from OpenAI to convey what tools to use.\\n\\n    If a function_call parameter is passed, then that is used to get\\n    the tool and tool input.\\n\\n    If one is not passed, then the AIMessage is assumed to be the final output.\\n    \"\"\"\\n\\n    @property\\n    def _type(self) -> str:\\n        return \"openai-functions-agent\"\\n\\n    @staticmethod\\n    def _parse_ai_message(message: BaseMessage) -> Union[AgentAction, AgentFinish]:\\n        \"\"\"Parse an AI message.\"\"\"\\n        if not isinstance(message, AIMessage):\\n            raise TypeError(f\"Expected an AI message got {type(message)}\")\\n\\n        function_call = message.additional_kwargs.get(\"function_call\", {})\\n\\n        if function_call:\\n            function_name = function_call[\"name\"]\\n            try:\\n                if len(function_call[\"arguments\"].strip()) == 0:\\n                    # OpenAI returns an empty string for functions containing no args\\n                    _tool_input = {}\\n                else:\\n                    # otherwise it returns a json object\\n                    _tool_input = json.loads(function_call[\"arguments\"], strict=False)\\n            except JSONDecodeError:\\n                raise OutputParserException(\\n                    f\"Could not parse tool input: {function_call} because \"\\n                    f\"the `arguments` is not valid JSON.\"\\n                )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\output_parsers\\\\openai_functions.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# HACK HACK HACK:\\n            # The code that encodes tool input into Open AI uses a special variable\\n            # name called `__arg1` to handle old style tools that do not expose a\\n            # schema and expect a single string argument as an input.\\n            # We unpack the argument here if it exists.\\n            # Open AI does not support passing in a JSON array as an argument.\\n            if \"__arg1\" in _tool_input:\\n                tool_input = _tool_input[\"__arg1\"]\\n            else:\\n                tool_input = _tool_input\\n\\n            content_msg = f\"responded: {message.content}\\\\n\" if message.content else \"\\\\n\"\\n            log = f\"\\\\nInvoking: `{function_name}` with `{tool_input}`\\\\n{content_msg}\\\\n\"\\n            return AgentActionMessageLog(\\n                tool=function_name,\\n                tool_input=tool_input,\\n                log=log,\\n                message_log=[message],\\n            )\\n\\n        return AgentFinish(\\n            return_values={\"output\": message.content}, log=str(message.content)\\n        )\\n\\n    def parse_result(\\n        self, result: List[Generation], *, partial: bool = False\\n    ) -> Union[AgentAction, AgentFinish]:\\n        if not isinstance(result[0], ChatGeneration):\\n            raise ValueError(\"This output parser only works on ChatGeneration output\")\\n        message = result[0].message\\n        return self._parse_ai_message(message)\\n\\n    def parse(self, text: str) -> Union[AgentAction, AgentFinish]:\\n        raise ValueError(\"Can only parse messages\")' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\output_parsers\\\\openai_functions.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='import json\\nfrom json import JSONDecodeError\\nfrom typing import List, Union\\n\\nfrom langchain_core.agents import AgentAction, AgentActionMessageLog, AgentFinish\\nfrom langchain_core.exceptions import OutputParserException\\nfrom langchain_core.messages import (\\n    AIMessage,\\n    BaseMessage,\\n)\\nfrom langchain_core.outputs import ChatGeneration, Generation\\n\\nfrom langchain.agents.agent import MultiActionAgentOutputParser\\n\\n\\nclass OpenAIToolAgentAction(AgentActionMessageLog):\\n    tool_call_id: str\\n    \"\"\"Tool call that this message is responding to.\"\"\"\\n\\n\\ndef parse_ai_message_to_openai_tool_action(\\n    message: BaseMessage,\\n) -> Union[List[AgentAction], AgentFinish]:\\n    \"\"\"Parse an AI message potentially containing tool_calls.\"\"\"\\n    if not isinstance(message, AIMessage):\\n        raise TypeError(f\"Expected an AI message got {type(message)}\")\\n\\n    if not message.additional_kwargs.get(\"tool_calls\"):\\n        return AgentFinish(\\n            return_values={\"output\": message.content}, log=str(message.content)\\n        )\\n\\n    actions: List = []\\n    for tool_call in message.additional_kwargs[\"tool_calls\"]:\\n        function = tool_call[\"function\"]\\n        function_name = function[\"name\"]\\n        try:\\n            _tool_input = json.loads(function[\"arguments\"] or \"{}\")\\n        except JSONDecodeError:\\n            raise OutputParserException(\\n                f\"Could not parse tool input: {function} because \"\\n                f\"the `arguments` is not valid JSON.\"\\n            )\\n\\n        # HACK HACK HACK:\\n        # The code that encodes tool input into Open AI uses a special variable\\n        # name called `__arg1` to handle old style tools that do not expose a\\n        # schema and expect a single string argument as an input.\\n        # We unpack the argument here if it exists.\\n        # Open AI does not support passing in a JSON array as an argument.\\n        if \"__arg1\" in _tool_input:\\n            tool_input = _tool_input[\"__arg1\"]\\n        else:\\n            tool_input = _tool_input' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\output_parsers\\\\openai_tools.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='content_msg = f\"responded: {message.content}\\\\n\" if message.content else \"\\\\n\"\\n        log = f\"\\\\nInvoking: `{function_name}` with `{tool_input}`\\\\n{content_msg}\\\\n\"\\n        actions.append(\\n            OpenAIToolAgentAction(\\n                tool=function_name,\\n                tool_input=tool_input,\\n                log=log,\\n                message_log=[message],\\n                tool_call_id=tool_call[\"id\"],\\n            )\\n        )\\n    return actions\\n\\n\\nclass OpenAIToolsAgentOutputParser(MultiActionAgentOutputParser):\\n    \"\"\"Parses a message into agent actions/finish.\\n\\n    Is meant to be used with OpenAI models, as it relies on the specific\\n    tool_calls parameter from OpenAI to convey what tools to use.\\n\\n    If a tool_calls parameter is passed, then that is used to get\\n    the tool names and tool inputs.\\n\\n    If one is not passed, then the AIMessage is assumed to be the final output.\\n    \"\"\"\\n\\n    @property\\n    def _type(self) -> str:\\n        return \"openai-tools-agent-output-parser\"\\n\\n    def parse_result(\\n        self, result: List[Generation], *, partial: bool = False\\n    ) -> Union[List[AgentAction], AgentFinish]:\\n        if not isinstance(result[0], ChatGeneration):\\n            raise ValueError(\"This output parser only works on ChatGeneration output\")\\n        message = result[0].message\\n        return parse_ai_message_to_openai_tool_action(message)\\n\\n    def parse(self, text: str) -> Union[List[AgentAction], AgentFinish]:\\n        raise ValueError(\"Can only parse messages\")' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\output_parsers\\\\openai_tools.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='import json\\nimport re\\nfrom typing import Union\\n\\nfrom langchain_core.agents import AgentAction, AgentFinish\\nfrom langchain_core.exceptions import OutputParserException\\n\\nfrom langchain.agents.agent import AgentOutputParser\\nfrom langchain.agents.chat.prompt import FORMAT_INSTRUCTIONS\\n\\nFINAL_ANSWER_ACTION = \"Final Answer:\"\\n\\n\\nclass ReActJsonSingleInputOutputParser(AgentOutputParser):\\n    \"\"\"Parses ReAct-style LLM calls that have a single tool input in json format.\\n\\n    Expects output to be in one of two formats.\\n\\n    If the output signals that an action should be taken,\\n    should be in the below format. This will result in an AgentAction\\n    being returned.\\n\\n    ```\\n    Thought: agent thought here\\n    Action:\\n    ```\\n    {\\n        \"action\": \"search\",\\n        \"action_input\": \"what is the temperature in SF\"\\n    }\\n    ```\\n    ```\\n\\n    If the output signals that a final answer should be given,\\n    should be in the below format. This will result in an AgentFinish\\n    being returned.\\n\\n    ```\\n    Thought: agent thought here\\n    Final Answer: The temperature is 100 degrees\\n    ```\\n\\n    \"\"\"\\n\\n    pattern = re.compile(r\"^.*?`{3}(?:json)?\\\\n(.*?)`{3}.*?$\", re.DOTALL)\\n    \"\"\"Regex pattern to parse the output.\"\"\"\\n\\n    def get_format_instructions(self) -> str:\\n        return FORMAT_INSTRUCTIONS' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\output_parsers\\\\react_json_single_input.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def parse(self, text: str) -> Union[AgentAction, AgentFinish]:\\n        includes_answer = FINAL_ANSWER_ACTION in text\\n        try:\\n            found = self.pattern.search(text)\\n            if not found:\\n                # Fast fail to parse Final Answer.\\n                raise ValueError(\"action not found\")\\n            action = found.group(1)\\n            response = json.loads(action.strip())\\n            includes_action = \"action\" in response\\n            if includes_answer and includes_action:\\n                raise OutputParserException(\\n                    \"Parsing LLM output produced a final answer \"\\n                    f\"and a parse-able action: {text}\"\\n                )\\n            return AgentAction(\\n                response[\"action\"], response.get(\"action_input\", {}), text\\n            )\\n\\n        except Exception:\\n            if not includes_answer:\\n                raise OutputParserException(f\"Could not parse LLM output: {text}\")\\n            output = text.split(FINAL_ANSWER_ACTION)[-1].strip()\\n            return AgentFinish({\"output\": output}, text)\\n\\n    @property\\n    def _type(self) -> str:\\n        return \"react-json-single-input\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\output_parsers\\\\react_json_single_input.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='import re\\nfrom typing import Union\\n\\nfrom langchain_core.agents import AgentAction, AgentFinish\\nfrom langchain_core.exceptions import OutputParserException\\n\\nfrom langchain.agents.agent import AgentOutputParser\\nfrom langchain.agents.mrkl.prompt import FORMAT_INSTRUCTIONS\\n\\nFINAL_ANSWER_ACTION = \"Final Answer:\"\\nMISSING_ACTION_AFTER_THOUGHT_ERROR_MESSAGE = (\\n    \"Invalid Format: Missing \\'Action:\\' after \\'Thought:\"\\n)\\nMISSING_ACTION_INPUT_AFTER_ACTION_ERROR_MESSAGE = (\\n    \"Invalid Format: Missing \\'Action Input:\\' after \\'Action:\\'\"\\n)\\nFINAL_ANSWER_AND_PARSABLE_ACTION_ERROR_MESSAGE = (\\n    \"Parsing LLM output produced both a final answer and a parse-able action:\"\\n)\\n\\n\\nclass ReActSingleInputOutputParser(AgentOutputParser):\\n    \"\"\"Parses ReAct-style LLM calls that have a single tool input.\\n\\n    Expects output to be in one of two formats.\\n\\n    If the output signals that an action should be taken,\\n    should be in the below format. This will result in an AgentAction\\n    being returned.\\n\\n    ```\\n    Thought: agent thought here\\n    Action: search\\n    Action Input: what is the temperature in SF?\\n    ```\\n\\n    If the output signals that a final answer should be given,\\n    should be in the below format. This will result in an AgentFinish\\n    being returned.\\n\\n    ```\\n    Thought: agent thought here\\n    Final Answer: The temperature is 100 degrees\\n    ```\\n\\n    \"\"\"\\n\\n    def get_format_instructions(self) -> str:\\n        return FORMAT_INSTRUCTIONS' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\output_parsers\\\\react_single_input.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def parse(self, text: str) -> Union[AgentAction, AgentFinish]:\\n        includes_answer = FINAL_ANSWER_ACTION in text\\n        regex = (\\n            r\"Action\\\\s*\\\\d*\\\\s*:[\\\\s]*(.*?)[\\\\s]*Action\\\\s*\\\\d*\\\\s*Input\\\\s*\\\\d*\\\\s*:[\\\\s]*(.*)\"\\n        )\\n        action_match = re.search(regex, text, re.DOTALL)\\n        if action_match:\\n            if includes_answer:\\n                raise OutputParserException(\\n                    f\"{FINAL_ANSWER_AND_PARSABLE_ACTION_ERROR_MESSAGE}: {text}\"\\n                )\\n            action = action_match.group(1).strip()\\n            action_input = action_match.group(2)\\n            tool_input = action_input.strip(\" \")\\n            tool_input = tool_input.strip(\\'\"\\')\\n\\n            return AgentAction(action, tool_input, text)\\n\\n        elif includes_answer:\\n            return AgentFinish(\\n                {\"output\": text.split(FINAL_ANSWER_ACTION)[-1].strip()}, text\\n            )\\n\\n        if not re.search(r\"Action\\\\s*\\\\d*\\\\s*:[\\\\s]*(.*?)\", text, re.DOTALL):\\n            raise OutputParserException(\\n                f\"Could not parse LLM output: `{text}`\",\\n                observation=MISSING_ACTION_AFTER_THOUGHT_ERROR_MESSAGE,\\n                llm_output=text,\\n                send_to_llm=True,\\n            )\\n        elif not re.search(\\n            r\"[\\\\s]*Action\\\\s*\\\\d*\\\\s*Input\\\\s*\\\\d*\\\\s*:[\\\\s]*(.*)\", text, re.DOTALL\\n        ):\\n            raise OutputParserException(\\n                f\"Could not parse LLM output: `{text}`\",\\n                observation=MISSING_ACTION_INPUT_AFTER_ACTION_ERROR_MESSAGE,\\n                llm_output=text,\\n                send_to_llm=True,\\n            )\\n        else:\\n            raise OutputParserException(f\"Could not parse LLM output: `{text}`\")\\n\\n    @property\\n    def _type(self) -> str:\\n        return \"react-single-input\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\output_parsers\\\\react_single_input.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from typing import Sequence, Union\\n\\nfrom langchain_core.agents import AgentAction, AgentFinish\\nfrom langchain_core.exceptions import OutputParserException\\n\\nfrom langchain.agents.agent import AgentOutputParser\\n\\n\\nclass SelfAskOutputParser(AgentOutputParser):\\n    \"\"\"Parses self-ask style LLM calls.\\n\\n    Expects output to be in one of two formats.\\n\\n    If the output signals that an action should be taken,\\n    should be in the below format. This will result in an AgentAction\\n    being returned.\\n\\n    ```\\n    Thoughts go here...\\n    Follow up: what is the temperature in SF?\\n    ```\\n\\n    If the output signals that a final answer should be given,\\n    should be in the below format. This will result in an AgentFinish\\n    being returned.\\n\\n    ```\\n    Thoughts go here...\\n    So the final answer is: The temperature is 100 degrees\\n    ```\\n\\n    \"\"\"\\n\\n    followups: Sequence[str] = (\"Follow up:\", \"Followup:\")\\n    finish_string: str = \"So the final answer is: \"\\n\\n    def parse(self, text: str) -> Union[AgentAction, AgentFinish]:\\n        last_line = text.split(\"\\\\n\")[-1]\\n        if not any([follow in last_line for follow in self.followups]):\\n            if self.finish_string not in last_line:\\n                raise OutputParserException(f\"Could not parse output: {text}\")\\n            return AgentFinish({\"output\": last_line[len(self.finish_string) :]}, text)\\n\\n        after_colon = text.split(\":\")[-1].strip()\\n        return AgentAction(\"Intermediate Answer\", after_colon, text)\\n\\n    @property\\n    def _type(self) -> str:\\n        return \"self_ask\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\output_parsers\\\\self_ask.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from typing import Union\\n\\nfrom langchain_core.agents import AgentAction, AgentFinish\\n\\nfrom langchain.agents import AgentOutputParser\\n\\n\\nclass XMLAgentOutputParser(AgentOutputParser):\\n    \"\"\"Parses tool invocations and final answers in XML format.\\n\\n    Expects output to be in one of two formats.\\n\\n    If the output signals that an action should be taken,\\n    should be in the below format. This will result in an AgentAction\\n    being returned.\\n\\n    ```\\n    <tool>search</tool>\\n    <tool_input>what is 2 + 2</tool_input>\\n    ```\\n\\n    If the output signals that a final answer should be given,\\n    should be in the below format. This will result in an AgentFinish\\n    being returned.\\n\\n    ```\\n    <final_answer>Foo</final_answer>\\n    ```\\n    \"\"\"\\n\\n    def parse(self, text: str) -> Union[AgentAction, AgentFinish]:\\n        if \"</tool>\" in text:\\n            tool, tool_input = text.split(\"</tool>\")\\n            _tool = tool.split(\"<tool>\")[1]\\n            _tool_input = tool_input.split(\"<tool_input>\")[1]\\n            if \"</tool_input>\" in _tool_input:\\n                _tool_input = _tool_input.split(\"</tool_input>\")[0]\\n            return AgentAction(tool=_tool, tool_input=_tool_input, log=text)\\n        elif \"<final_answer>\" in text:\\n            _, answer = text.split(\"<final_answer>\")\\n            if \"</final_answer>\" in answer:\\n                answer = answer.split(\"</final_answer>\")[0]\\n            return AgentFinish(return_values={\"output\": answer}, log=text)\\n        else:\\n            raise ValueError\\n\\n    def get_format_instructions(self) -> str:\\n        raise NotImplementedError\\n\\n    @property\\n    def _type(self) -> str:\\n        return \"xml-agent\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\output_parsers\\\\xml.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Parsing utils to go from string to AgentAction or Agent Finish.\\n\\nAgentAction means that an action should be taken.\\nThis contains the name of the tool to use, the input to pass to that tool,\\nand a `log` variable (which contains a log of the agent\\'s thinking).\\n\\nAgentFinish means that a response should be given.\\nThis contains a `return_values` dictionary. This usually contains a\\nsingle `output` key, but can be extended to contain more.\\nThis also contains a `log` variable (which contains a log of the agent\\'s thinking).\\n\"\"\"\\nfrom langchain.agents.output_parsers.json import JSONAgentOutputParser\\nfrom langchain.agents.output_parsers.openai_functions import (\\n    OpenAIFunctionsAgentOutputParser,\\n)\\nfrom langchain.agents.output_parsers.react_json_single_input import (\\n    ReActJsonSingleInputOutputParser,\\n)\\nfrom langchain.agents.output_parsers.react_single_input import (\\n    ReActSingleInputOutputParser,\\n)\\nfrom langchain.agents.output_parsers.self_ask import SelfAskOutputParser\\nfrom langchain.agents.output_parsers.xml import XMLAgentOutputParser\\n\\n__all__ = [\\n    \"ReActSingleInputOutputParser\",\\n    \"SelfAskOutputParser\",\\n    \"ReActJsonSingleInputOutputParser\",\\n    \"OpenAIFunctionsAgentOutputParser\",\\n    \"XMLAgentOutputParser\",\\n    \"JSONAgentOutputParser\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\output_parsers\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from __future__ import annotations\\n\\nfrom typing import Sequence\\n\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.prompts import BasePromptTemplate\\nfrom langchain_core.runnables import Runnable, RunnablePassthrough\\nfrom langchain_core.tools import BaseTool\\n\\nfrom langchain.agents.format_scratchpad import format_log_to_str\\nfrom langchain.agents.output_parsers import ReActSingleInputOutputParser\\nfrom langchain.tools.render import render_text_description\\n\\n\\ndef create_react_agent(\\n    llm: BaseLanguageModel, tools: Sequence[BaseTool], prompt: BasePromptTemplate\\n) -> Runnable:\\n    \"\"\"Create an agent that uses ReAct prompting.\\n\\n    Args:\\n        llm: LLM to use as the agent.\\n        tools: Tools this agent has access to.\\n        prompt: The prompt to use. See Prompt section below for more.\\n\\n    Returns:\\n        A Runnable sequence representing an agent. It takes as input all the same input\\n        variables as the prompt passed in does. It returns as output either an\\n        AgentAction or AgentFinish.\\n\\n    Examples:\\n\\n        .. code-block:: python\\n\\n            from langchain import hub\\n            from langchain_community.llms import OpenAI\\n            from langchain.agents import AgentExecutor, create_react_agent\\n\\n            prompt = hub.pull(\"hwchase17/react\")\\n            model = OpenAI()\\n            tools = ...\\n\\n            agent = create_react_agent(model, tools, prompt)\\n            agent_executor = AgentExecutor(agent=agent, tools=tools)\\n\\n            agent_executor.invoke({\"input\": \"hi\"})\\n\\n            # Use with chat history\\n            from langchain_core.messages import AIMessage, HumanMessage\\n            agent_executor.invoke(\\n                {\\n                    \"input\": \"what\\'s my name?\",\\n                    # Notice that chat_history is a string\\n                    # since this prompt is aimed at LLMs, not chat models\\n                    \"chat_history\": \"Human: My name is Bob\\\\nAI: Hello Bob!\",\\n                }\\n            )\\n\\n    Prompt:' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\react\\\\agent.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='The prompt must have input keys:\\n            * `tools`: contains descriptions and arguments for each tool.\\n            * `tool_names`: contains all tool names.\\n            * `agent_scratchpad`: contains previous agent actions and tool outputs as a string.\\n\\n        Here\\'s an example:\\n\\n        .. code-block:: python\\n\\n            from langchain_core.prompts import PromptTemplate\\n\\n            template = \\'\\'\\'Answer the following questions as best you can. You have access to the following tools:\\n\\n            {tools}\\n\\n            Use the following format:\\n\\n            Question: the input question you must answer\\n            Thought: you should always think about what to do\\n            Action: the action to take, should be one of [{tool_names}]\\n            Action Input: the input to the action\\n            Observation: the result of the action\\n            ... (this Thought/Action/Action Input/Observation can repeat N times)\\n            Thought: I now know the final answer\\n            Final Answer: the final answer to the original input question\\n\\n            Begin!\\n\\n            Question: {input}\\n            Thought:{agent_scratchpad}\\'\\'\\'\\n\\n            prompt = PromptTemplate.from_template(template)\\n    \"\"\"  # noqa: E501\\n    missing_vars = {\"tools\", \"tool_names\", \"agent_scratchpad\"}.difference(\\n        prompt.input_variables\\n    )\\n    if missing_vars:\\n        raise ValueError(f\"Prompt missing required variables: {missing_vars}\")\\n\\n    prompt = prompt.partial(\\n        tools=render_text_description(list(tools)),\\n        tool_names=\", \".join([t.name for t in tools]),\\n    )\\n    llm_with_stop = llm.bind(stop=[\"\\\\nObservation\"])\\n    agent = (\\n        RunnablePassthrough.assign(\\n            agent_scratchpad=lambda x: format_log_to_str(x[\"intermediate_steps\"]),\\n        )\\n        | prompt\\n        | llm_with_stop\\n        | ReActSingleInputOutputParser()\\n    )\\n    return agent' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\react\\\\agent.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Chain that implements the ReAct paper from https://arxiv.org/pdf/2210.03629.pdf.\"\"\"\\nfrom typing import Any, List, Optional, Sequence\\n\\nfrom langchain_core._api import deprecated\\nfrom langchain_core.documents import Document\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.prompts import BasePromptTemplate\\nfrom langchain_core.pydantic_v1 import Field\\nfrom langchain_core.tools import BaseTool, Tool\\n\\nfrom langchain.agents.agent import Agent, AgentExecutor, AgentOutputParser\\nfrom langchain.agents.agent_types import AgentType\\nfrom langchain.agents.react.output_parser import ReActOutputParser\\nfrom langchain.agents.react.textworld_prompt import TEXTWORLD_PROMPT\\nfrom langchain.agents.react.wiki_prompt import WIKI_PROMPT\\nfrom langchain.agents.utils import validate_tools_single_input\\nfrom langchain.docstore.base import Docstore\\n\\n\\n@deprecated(\"0.1.0\", removal=\"0.2.0\")\\nclass ReActDocstoreAgent(Agent):\\n    \"\"\"Agent for the ReAct chain.\"\"\"\\n\\n    output_parser: AgentOutputParser = Field(default_factory=ReActOutputParser)\\n\\n    @classmethod\\n    def _get_default_output_parser(cls, **kwargs: Any) -> AgentOutputParser:\\n        return ReActOutputParser()\\n\\n    @property\\n    def _agent_type(self) -> str:\\n        \"\"\"Return Identifier of an agent type.\"\"\"\\n        return AgentType.REACT_DOCSTORE\\n\\n    @classmethod\\n    def create_prompt(cls, tools: Sequence[BaseTool]) -> BasePromptTemplate:\\n        \"\"\"Return default prompt.\"\"\"\\n        return WIKI_PROMPT\\n\\n    @classmethod\\n    def _validate_tools(cls, tools: Sequence[BaseTool]) -> None:\\n        validate_tools_single_input(cls.__name__, tools)\\n        super()._validate_tools(tools)\\n        if len(tools) != 2:\\n            raise ValueError(f\"Exactly two tools must be specified, but got {tools}\")\\n        tool_names = {tool.name for tool in tools}\\n        if tool_names != {\"Lookup\", \"Search\"}:\\n            raise ValueError(\\n                f\"Tool names should be Lookup and Search, got {tool_names}\"\\n            )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\react\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@property\\n    def observation_prefix(self) -> str:\\n        \"\"\"Prefix to append the observation with.\"\"\"\\n        return \"Observation: \"\\n\\n    @property\\n    def _stop(self) -> List[str]:\\n        return [\"\\\\nObservation:\"]\\n\\n    @property\\n    def llm_prefix(self) -> str:\\n        \"\"\"Prefix to append the LLM call with.\"\"\"\\n        return \"Thought:\"\\n\\n\\n@deprecated(\"0.1.0\", removal=\"0.2.0\")\\nclass DocstoreExplorer:\\n    \"\"\"Class to assist with exploration of a document store.\"\"\"\\n\\n    def __init__(self, docstore: Docstore):\\n        \"\"\"Initialize with a docstore, and set initial document to None.\"\"\"\\n        self.docstore = docstore\\n        self.document: Optional[Document] = None\\n        self.lookup_str = \"\"\\n        self.lookup_index = 0\\n\\n    def search(self, term: str) -> str:\\n        \"\"\"Search for a term in the docstore, and if found save.\"\"\"\\n        result = self.docstore.search(term)\\n        if isinstance(result, Document):\\n            self.document = result\\n            return self._summary\\n        else:\\n            self.document = None\\n            return result\\n\\n    def lookup(self, term: str) -> str:\\n        \"\"\"Lookup a term in document (if saved).\"\"\"\\n        if self.document is None:\\n            raise ValueError(\"Cannot lookup without a successful search first\")\\n        if term.lower() != self.lookup_str:\\n            self.lookup_str = term.lower()\\n            self.lookup_index = 0\\n        else:\\n            self.lookup_index += 1\\n        lookups = [p for p in self._paragraphs if self.lookup_str in p.lower()]\\n        if len(lookups) == 0:\\n            return \"No Results\"\\n        elif self.lookup_index >= len(lookups):\\n            return \"No More Results\"\\n        else:\\n            result_prefix = f\"(Result {self.lookup_index + 1}/{len(lookups)})\"\\n            return f\"{result_prefix} {lookups[self.lookup_index]}\"\\n\\n    @property\\n    def _summary(self) -> str:\\n        return self._paragraphs[0]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\react\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@property\\n    def _paragraphs(self) -> List[str]:\\n        if self.document is None:\\n            raise ValueError(\"Cannot get paragraphs without a document\")\\n        return self.document.page_content.split(\"\\\\n\\\\n\")\\n\\n\\n@deprecated(\"0.1.0\", removal=\"0.2.0\")\\nclass ReActTextWorldAgent(ReActDocstoreAgent):\\n    \"\"\"Agent for the ReAct TextWorld chain.\"\"\"\\n\\n    @classmethod\\n    def create_prompt(cls, tools: Sequence[BaseTool]) -> BasePromptTemplate:\\n        \"\"\"Return default prompt.\"\"\"\\n        return TEXTWORLD_PROMPT\\n\\n    @classmethod\\n    def _validate_tools(cls, tools: Sequence[BaseTool]) -> None:\\n        validate_tools_single_input(cls.__name__, tools)\\n        super()._validate_tools(tools)\\n        if len(tools) != 1:\\n            raise ValueError(f\"Exactly one tool must be specified, but got {tools}\")\\n        tool_names = {tool.name for tool in tools}\\n        if tool_names != {\"Play\"}:\\n            raise ValueError(f\"Tool name should be Play, got {tool_names}\")\\n\\n\\n@deprecated(\"0.1.0\", removal=\"0.2.0\")\\nclass ReActChain(AgentExecutor):\\n    \"\"\"[Deprecated] Chain that implements the ReAct paper.\"\"\"\\n\\n    def __init__(self, llm: BaseLanguageModel, docstore: Docstore, **kwargs: Any):\\n        \"\"\"Initialize with the LLM and a docstore.\"\"\"\\n        docstore_explorer = DocstoreExplorer(docstore)\\n        tools = [\\n            Tool(\\n                name=\"Search\",\\n                func=docstore_explorer.search,\\n                description=\"Search for a term in the docstore.\",\\n            ),\\n            Tool(\\n                name=\"Lookup\",\\n                func=docstore_explorer.lookup,\\n                description=\"Lookup a term in the docstore.\",\\n            ),\\n        ]\\n        agent = ReActDocstoreAgent.from_llm_and_tools(llm, tools)\\n        super().__init__(agent=agent, tools=tools, **kwargs)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\react\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='import re\\nfrom typing import Union\\n\\nfrom langchain_core.agents import AgentAction, AgentFinish\\nfrom langchain_core.exceptions import OutputParserException\\n\\nfrom langchain.agents.agent import AgentOutputParser\\n\\n\\nclass ReActOutputParser(AgentOutputParser):\\n    \"\"\"Output parser for the ReAct agent.\"\"\"\\n\\n    def parse(self, text: str) -> Union[AgentAction, AgentFinish]:\\n        action_prefix = \"Action: \"\\n        if not text.strip().split(\"\\\\n\")[-1].startswith(action_prefix):\\n            raise OutputParserException(f\"Could not parse LLM Output: {text}\")\\n        action_block = text.strip().split(\"\\\\n\")[-1]\\n\\n        action_str = action_block[len(action_prefix) :]\\n        # Parse out the action and the directive.\\n        re_matches = re.search(r\"(.*?)\\\\[(.*?)\\\\]\", action_str)\\n        if re_matches is None:\\n            raise OutputParserException(\\n                f\"Could not parse action directive: {action_str}\"\\n            )\\n        action, action_input = re_matches.group(1), re_matches.group(2)\\n        if action == \"Finish\":\\n            return AgentFinish({\"output\": action_input}, text)\\n        else:\\n            return AgentAction(action, action_input, text)\\n\\n    @property\\n    def _type(self) -> str:\\n        return \"react\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\react\\\\output_parser.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# flake8: noqa\\nfrom langchain_core.prompts.prompt import PromptTemplate\\n\\nEXAMPLES = [\\n    \"\"\"Setup: You are now playing a fast paced round of TextWorld! Here is your task for\\ntoday. First of all, you could, like, try to travel east. After that, take the\\nbinder from the locker. With the binder, place the binder on the mantelpiece.\\nAlright, thanks!\\n\\n-= Vault =-\\nYou\\'ve just walked into a vault. You begin to take stock of what\\'s here.\\n\\nAn open safe is here. What a letdown! The safe is empty! You make out a shelf.\\nBut the thing hasn\\'t got anything on it. What, you think everything in TextWorld\\nshould have stuff on it?\\n\\nYou don\\'t like doors? Why not try going east, that entranceway is unguarded.\\n\\nThought: I need to travel east\\nAction: Play[go east]\\nObservation: -= Office =-\\nYou arrive in an office. An ordinary one.\\n\\nYou can make out a locker. The locker contains a binder. You see a case. The\\ncase is empty, what a horrible day! You lean against the wall, inadvertently\\npressing a secret button. The wall opens up to reveal a mantelpiece. You wonder\\nidly who left that here. The mantelpiece is standard. The mantelpiece appears to\\nbe empty. If you haven\\'t noticed it already, there seems to be something there\\nby the wall, it\\'s a table. Unfortunately, there isn\\'t a thing on it. Hm. Oh well\\nThere is an exit to the west. Don\\'t worry, it is unguarded.\\n\\nThought: I need to take the binder from the locker\\nAction: Play[take binder]\\nObservation: You take the binder from the locker.\\n\\nThought: I need to place the binder on the mantelpiece\\nAction: Play[put binder on mantelpiece]\\n\\nObservation: You put the binder on the mantelpiece.\\nYour score has just gone up by one point.\\n*** The End ***\\nThought: The End has occurred\\nAction: Finish[yes]\\n\\n\"\"\"\\n]\\nSUFFIX = \"\"\"\\\\n\\\\nSetup: {input}\\n{agent_scratchpad}\"\"\"\\n\\nTEXTWORLD_PROMPT = PromptTemplate.from_examples(\\n    EXAMPLES, SUFFIX, [\"input\", \"agent_scratchpad\"]\\n)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\react\\\\textworld_prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# flake8: noqa\\nfrom langchain_core.prompts.prompt import PromptTemplate' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\react\\\\wiki_prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='EXAMPLES = [\\n    \"\"\"Question: What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\\nThought: I need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.\\nAction: Search[Colorado orogeny]\\nObservation: The Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\\nThought: It does not mention the eastern sector. So I need to look up eastern sector.\\nAction: Lookup[eastern sector]\\nObservation: (Result 1 / 1) The eastern sector extends into the High Plains and is called the Central Plains orogeny.\\nThought: The eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range.\\nAction: Search[High Plains]\\nObservation: High Plains refers to one of two distinct land regions\\nThought: I need to instead search High Plains (United States).\\nAction: Search[High Plains (United States)]\\nObservation: The High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130 m).[3]\\nThought: High Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.\\nAction: Finish[1,800 to 7,000 ft]\"\"\",\\n    \"\"\"Question: Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\\nThought: The question simplifies to \"The Simpsons\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.\\nAction: Search[Milhouse]\\nObservation: Milhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\\nThought: The paragraph does not tell who Milhouse is named after, maybe I can look up \"named after\".\\nAction: Lookup[named after]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\react\\\\wiki_prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Observation: (Result 1 / 1) Milhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\\nThought: Milhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\\nAction: Finish[Richard Nixon]\"\"\",\\n    \"\"\"Question: Which documentary is about Finnish rock groups, Adam Clayton Powell or The Saimaa Gesture?\\nThought: I need to search Adam Clayton Powell and The Saimaa Gesture, and find which documentary is about Finnish rock groups.\\nAction: Search[Adam Clayton Powell]\\nObservation: Could not find [Adam Clayton Powell]. Similar: [’Adam Clayton Powell III’, ’Seventh Avenue (Manhattan)’, ’Adam Clayton Powell Jr. State Office Building’, ’Isabel Washington Powell’, ’Adam Powell’, ’Adam Clayton Powell (film)’, ’Giancarlo Esposito’].\\nThought: To find the documentary, I can search Adam Clayton Powell (film).\\nAction: Search[Adam Clayton Powell (film)]\\nObservation: Adam Clayton Powell is a 1989 American documentary film directed by Richard Kilberg. The film is about the rise and fall of influential African-American politician Adam Clayton Powell Jr.[3][4] It was later aired as part of the PBS series The American Experience.\\nThought: Adam Clayton Powell (film) is a documentary about an African-American politician, not Finnish rock groups. So the documentary about Finnish rock groups must instead be The Saimaa Gesture.\\nAction: Finish[The Saimaa Gesture]\"\"\",\\n    \"\"\"Question: What profession does Nicholas Ray and Elia Kazan have in common?\\nThought: I need to search Nicholas Ray and Elia Kazan, find their professions, then find the profession they have in common.\\nAction: Search[Nicholas Ray]\\nObservation: Nicholas Ray (born Raymond Nicholas Kienzle Jr., August 7, 1911 - June 16, 1979) was an American film director, screenwriter, and actor best known for the 1955 film Rebel Without a Cause.\\nThought: Professions of Nicholas Ray are director, screenwriter, and actor. I need to search Elia Kazan next and find his professions.\\nAction: Search[Elia Kazan]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\react\\\\wiki_prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Observation: Elia Kazan was an American film and theatre director, producer, screenwriter and actor.\\nThought: Professions of Elia Kazan are director, producer, screenwriter, and actor. So profession Nicholas Ray and Elia Kazan have in common is director, screenwriter, and actor.\\nAction: Finish[director, screenwriter, actor]\"\"\",\\n    \"\"\"Question: Which magazine was started first Arthur’s Magazine or First for Women?\\nThought: I need to search Arthur’s Magazine and First for Women, and find which was started first.\\nAction: Search[Arthur’s Magazine]\\nObservation: Arthur’s Magazine (1844-1846) was an American literary periodical published in Philadelphia in the 19th century.\\nThought: Arthur’s Magazine was started in 1844. I need to search First for Women next.\\nAction: Search[First for Women]\\nObservation: First for Women is a woman’s magazine published by Bauer Media Group in the USA.[1] The magazine was started in 1989.\\nThought: First for Women was started in 1989. 1844 (Arthur’s Magazine) < 1989 (First for Women), so Arthur’s Magazine was started first.\\nAction: Finish[Arthur’s Magazine]\"\"\",\\n    \"\"\"Question: Were Pavel Urysohn and Leonid Levin known for the same type of work?\\nThought: I need to search Pavel Urysohn and Leonid Levin, find their types of work, then find if they are the same.\\nAction: Search[Pavel Urysohn]\\nObservation: Pavel Samuilovich Urysohn (February 3, 1898 - August 17, 1924) was a Soviet mathematician who is best known for his contributions in dimension theory.\\nThought: Pavel Urysohn is a mathematician. I need to search Leonid Levin next and find its type of work.\\nAction: Search[Leonid Levin]\\nObservation: Leonid Anatolievich Levin is a Soviet-American mathematician and computer scientist.\\nThought: Leonid Levin is a mathematician and computer scientist. So Pavel Urysohn and Leonid Levin have the same type of work.\\nAction: Finish[yes]\"\"\",\\n]\\nSUFFIX = \"\"\"\\\\nQuestion: {input}\\n{agent_scratchpad}\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\react\\\\wiki_prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='WIKI_PROMPT = PromptTemplate.from_examples(\\n    EXAMPLES, SUFFIX, [\"input\", \"agent_scratchpad\"]\\n)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\react\\\\wiki_prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Implements the ReAct paper from https://arxiv.org/pdf/2210.03629.pdf.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\react\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Chain that does self-ask with search.\"\"\"\\nfrom typing import Any, Sequence, Union\\n\\nfrom langchain_community.utilities.google_serper import GoogleSerperAPIWrapper\\nfrom langchain_community.utilities.searchapi import SearchApiAPIWrapper\\nfrom langchain_community.utilities.serpapi import SerpAPIWrapper\\nfrom langchain_core._api import deprecated\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.prompts import BasePromptTemplate\\nfrom langchain_core.pydantic_v1 import Field\\nfrom langchain_core.runnables import Runnable, RunnablePassthrough\\nfrom langchain_core.tools import BaseTool, Tool\\n\\nfrom langchain.agents.agent import Agent, AgentExecutor, AgentOutputParser\\nfrom langchain.agents.agent_types import AgentType\\nfrom langchain.agents.format_scratchpad import format_log_to_str\\nfrom langchain.agents.self_ask_with_search.output_parser import SelfAskOutputParser\\nfrom langchain.agents.self_ask_with_search.prompt import PROMPT\\nfrom langchain.agents.utils import validate_tools_single_input\\n\\n\\n@deprecated(\"0.1.0\", alternative=\"create_self_ask_with_search\", removal=\"0.2.0\")\\nclass SelfAskWithSearchAgent(Agent):\\n    \"\"\"Agent for the self-ask-with-search paper.\"\"\"\\n\\n    output_parser: AgentOutputParser = Field(default_factory=SelfAskOutputParser)\\n\\n    @classmethod\\n    def _get_default_output_parser(cls, **kwargs: Any) -> AgentOutputParser:\\n        return SelfAskOutputParser()\\n\\n    @property\\n    def _agent_type(self) -> str:\\n        \"\"\"Return Identifier of an agent type.\"\"\"\\n        return AgentType.SELF_ASK_WITH_SEARCH\\n\\n    @classmethod\\n    def create_prompt(cls, tools: Sequence[BaseTool]) -> BasePromptTemplate:\\n        \"\"\"Prompt does not depend on tools.\"\"\"\\n        return PROMPT' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\self_ask_with_search\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@classmethod\\n    def _validate_tools(cls, tools: Sequence[BaseTool]) -> None:\\n        validate_tools_single_input(cls.__name__, tools)\\n        super()._validate_tools(tools)\\n        if len(tools) != 1:\\n            raise ValueError(f\"Exactly one tool must be specified, but got {tools}\")\\n        tool_names = {tool.name for tool in tools}\\n        if tool_names != {\"Intermediate Answer\"}:\\n            raise ValueError(\\n                f\"Tool name should be Intermediate Answer, got {tool_names}\"\\n            )\\n\\n    @property\\n    def observation_prefix(self) -> str:\\n        \"\"\"Prefix to append the observation with.\"\"\"\\n        return \"Intermediate answer: \"\\n\\n    @property\\n    def llm_prefix(self) -> str:\\n        \"\"\"Prefix to append the LLM call with.\"\"\"\\n        return \"\"\\n\\n\\n@deprecated(\"0.1.0\", removal=\"0.2.0\")\\nclass SelfAskWithSearchChain(AgentExecutor):\\n    \"\"\"[Deprecated] Chain that does self-ask with search.\"\"\"\\n\\n    def __init__(\\n        self,\\n        llm: BaseLanguageModel,\\n        search_chain: Union[\\n            GoogleSerperAPIWrapper, SearchApiAPIWrapper, SerpAPIWrapper\\n        ],\\n        **kwargs: Any,\\n    ):\\n        \"\"\"Initialize only with an LLM and a search chain.\"\"\"\\n        search_tool = Tool(\\n            name=\"Intermediate Answer\",\\n            func=search_chain.run,\\n            coroutine=search_chain.arun,\\n            description=\"Search\",\\n        )\\n        agent = SelfAskWithSearchAgent.from_llm_and_tools(llm, [search_tool])\\n        super().__init__(agent=agent, tools=[search_tool], **kwargs)\\n\\n\\ndef create_self_ask_with_search_agent(\\n    llm: BaseLanguageModel, tools: Sequence[BaseTool], prompt: BasePromptTemplate\\n) -> Runnable:\\n    \"\"\"Create an agent that uses self-ask with search prompting.\\n\\n    Args:\\n        llm: LLM to use as the agent.\\n        tools: List of tools. Should just be of length 1, with that tool having\\n            name `Intermediate Answer`\\n        prompt: The prompt to use, must have input key `agent_scratchpad` which will\\n            contain agent actions and tool outputs.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\self_ask_with_search\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Returns:\\n        A Runnable sequence representing an agent. It takes as input all the same input\\n        variables as the prompt passed in does. It returns as output either an\\n        AgentAction or AgentFinish.\\n\\n    Examples:\\n\\n        .. code-block:: python\\n\\n            from langchain import hub\\n            from langchain_community.chat_models import ChatAnthropic\\n            from langchain.agents import (\\n                AgentExecutor, create_self_ask_with_search_agent\\n            )\\n\\n            prompt = hub.pull(\"hwchase17/self-ask-with-search\")\\n            model = ChatAnthropic()\\n            tools = [...]  # Should just be one tool with name `Intermediate Answer`\\n\\n            agent = create_self_ask_with_search_agent(model, tools, prompt)\\n            agent_executor = AgentExecutor(agent=agent, tools=tools)\\n\\n            agent_executor.invoke({\"input\": \"hi\"})\\n\\n    Prompt:\\n\\n        The prompt must have input key `agent_scratchpad` which will\\n            contain agent actions and tool outputs as a string.\\n\\n        Here\\'s an example:\\n\\n        .. code-block:: python\\n\\n            from langchain_core.prompts import PromptTemplate\\n\\n            template = \\'\\'\\'Question: Who lived longer, Muhammad Ali or Alan Turing?\\n            Are follow up questions needed here: Yes.\\n            Follow up: How old was Muhammad Ali when he died?\\n            Intermediate answer: Muhammad Ali was 74 years old when he died.\\n            Follow up: How old was Alan Turing when he died?\\n            Intermediate answer: Alan Turing was 41 years old when he died.\\n            So the final answer is: Muhammad Ali\\n\\n            Question: When was the founder of craigslist born?\\n            Are follow up questions needed here: Yes.\\n            Follow up: Who was the founder of craigslist?\\n            Intermediate answer: Craigslist was founded by Craig Newmark.\\n            Follow up: When was Craig Newmark born?\\n            Intermediate answer: Craig Newmark was born on December 6, 1952.\\n            So the final answer is: December 6, 1952' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\self_ask_with_search\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Question: Who was the maternal grandfather of George Washington?\\n            Are follow up questions needed here: Yes.\\n            Follow up: Who was the mother of George Washington?\\n            Intermediate answer: The mother of George Washington was Mary Ball Washington.\\n            Follow up: Who was the father of Mary Ball Washington?\\n            Intermediate answer: The father of Mary Ball Washington was Joseph Ball.\\n            So the final answer is: Joseph Ball\\n\\n            Question: Are both the directors of Jaws and Casino Royale from the same country?\\n            Are follow up questions needed here: Yes.\\n            Follow up: Who is the director of Jaws?\\n            Intermediate answer: The director of Jaws is Steven Spielberg.\\n            Follow up: Where is Steven Spielberg from?\\n            Intermediate answer: The United States.\\n            Follow up: Who is the director of Casino Royale?\\n            Intermediate answer: The director of Casino Royale is Martin Campbell.\\n            Follow up: Where is Martin Campbell from?\\n            Intermediate answer: New Zealand.\\n            So the final answer is: No\\n\\n            Question: {input}\\n            Are followup questions needed here:{agent_scratchpad}\\'\\'\\'\\n\\n            prompt = PromptTemplate.from_template(template)\\n    \"\"\"  # noqa: E501\\n    missing_vars = {\"agent_scratchpad\"}.difference(prompt.input_variables)\\n    if missing_vars:\\n        raise ValueError(f\"Prompt missing required variables: {missing_vars}\")\\n\\n    if len(tools) != 1:\\n        raise ValueError(\"This agent expects exactly one tool\")\\n    tool = list(tools)[0]\\n    if tool.name != \"Intermediate Answer\":\\n        raise ValueError(\\n            \"This agent expects the tool to be named `Intermediate Answer`\"\\n        )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\self_ask_with_search\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='llm_with_stop = llm.bind(stop=[\"\\\\nIntermediate answer:\"])\\n    agent = (\\n        RunnablePassthrough.assign(\\n            agent_scratchpad=lambda x: format_log_to_str(\\n                x[\"intermediate_steps\"],\\n                observation_prefix=\"\\\\nIntermediate answer: \",\\n                llm_prefix=\"\",\\n            ),\\n            # Give it a default\\n            chat_history=lambda x: x.get(\"chat_history\", \"\"),\\n        )\\n        | prompt\\n        | llm_with_stop\\n        | SelfAskOutputParser()\\n    )\\n    return agent' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\self_ask_with_search\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain.agents.output_parsers.self_ask import SelfAskOutputParser\\n\\n# For backwards compatibility\\n__all__ = [\"SelfAskOutputParser\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\self_ask_with_search\\\\output_parser.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# flake8: noqa\\nfrom langchain_core.prompts.prompt import PromptTemplate\\n\\n_DEFAULT_TEMPLATE = \"\"\"Question: Who lived longer, Muhammad Ali or Alan Turing?\\nAre follow up questions needed here: Yes.\\nFollow up: How old was Muhammad Ali when he died?\\nIntermediate answer: Muhammad Ali was 74 years old when he died.\\nFollow up: How old was Alan Turing when he died?\\nIntermediate answer: Alan Turing was 41 years old when he died.\\nSo the final answer is: Muhammad Ali\\n\\nQuestion: When was the founder of craigslist born?\\nAre follow up questions needed here: Yes.\\nFollow up: Who was the founder of craigslist?\\nIntermediate answer: Craigslist was founded by Craig Newmark.\\nFollow up: When was Craig Newmark born?\\nIntermediate answer: Craig Newmark was born on December 6, 1952.\\nSo the final answer is: December 6, 1952\\n\\nQuestion: Who was the maternal grandfather of George Washington?\\nAre follow up questions needed here: Yes.\\nFollow up: Who was the mother of George Washington?\\nIntermediate answer: The mother of George Washington was Mary Ball Washington.\\nFollow up: Who was the father of Mary Ball Washington?\\nIntermediate answer: The father of Mary Ball Washington was Joseph Ball.\\nSo the final answer is: Joseph Ball\\n\\nQuestion: Are both the directors of Jaws and Casino Royale from the same country?\\nAre follow up questions needed here: Yes.\\nFollow up: Who is the director of Jaws?\\nIntermediate answer: The director of Jaws is Steven Spielberg.\\nFollow up: Where is Steven Spielberg from?\\nIntermediate answer: The United States.\\nFollow up: Who is the director of Casino Royale?\\nIntermediate answer: The director of Casino Royale is Martin Campbell.\\nFollow up: Where is Martin Campbell from?\\nIntermediate answer: New Zealand.\\nSo the final answer is: No\\n\\nQuestion: {input}\\nAre followup questions needed here:{agent_scratchpad}\"\"\"\\nPROMPT = PromptTemplate(\\n    input_variables=[\"input\", \"agent_scratchpad\"], template=_DEFAULT_TEMPLATE\\n)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\self_ask_with_search\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Chain that does self ask with search.\\n\\nHeavily borrowed from https://github.com/ofirpress/self-ask\\n\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\self_ask_with_search\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='import re\\nfrom typing import Any, List, Optional, Sequence, Tuple\\n\\nfrom langchain_core._api import deprecated\\nfrom langchain_core.agents import AgentAction\\nfrom langchain_core.callbacks import BaseCallbackManager\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.prompts import BasePromptTemplate\\nfrom langchain_core.prompts.chat import (\\n    ChatPromptTemplate,\\n    HumanMessagePromptTemplate,\\n    SystemMessagePromptTemplate,\\n)\\nfrom langchain_core.pydantic_v1 import Field\\nfrom langchain_core.runnables import Runnable, RunnablePassthrough\\nfrom langchain_core.tools import BaseTool\\n\\nfrom langchain.agents.agent import Agent, AgentOutputParser\\nfrom langchain.agents.format_scratchpad import format_log_to_str\\nfrom langchain.agents.output_parsers import JSONAgentOutputParser\\nfrom langchain.agents.structured_chat.output_parser import (\\n    StructuredChatOutputParserWithRetries,\\n)\\nfrom langchain.agents.structured_chat.prompt import FORMAT_INSTRUCTIONS, PREFIX, SUFFIX\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.tools.render import render_text_description_and_args\\n\\nHUMAN_MESSAGE_TEMPLATE = \"{input}\\\\n\\\\n{agent_scratchpad}\"\\n\\n\\n@deprecated(\"0.1.0\", alternative=\"create_structured_chat_agent\", removal=\"0.2.0\")\\nclass StructuredChatAgent(Agent):\\n    \"\"\"Structured Chat Agent.\"\"\"\\n\\n    output_parser: AgentOutputParser = Field(\\n        default_factory=StructuredChatOutputParserWithRetries\\n    )\\n    \"\"\"Output parser for the agent.\"\"\"\\n\\n    @property\\n    def observation_prefix(self) -> str:\\n        \"\"\"Prefix to append the observation with.\"\"\"\\n        return \"Observation: \"\\n\\n    @property\\n    def llm_prefix(self) -> str:\\n        \"\"\"Prefix to append the llm call with.\"\"\"\\n        return \"Thought:\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\structured_chat\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _construct_scratchpad(\\n        self, intermediate_steps: List[Tuple[AgentAction, str]]\\n    ) -> str:\\n        agent_scratchpad = super()._construct_scratchpad(intermediate_steps)\\n        if not isinstance(agent_scratchpad, str):\\n            raise ValueError(\"agent_scratchpad should be of type string.\")\\n        if agent_scratchpad:\\n            return (\\n                f\"This was your previous work \"\\n                f\"(but I haven\\'t seen any of it! I only see what \"\\n                f\"you return as final answer):\\\\n{agent_scratchpad}\"\\n            )\\n        else:\\n            return agent_scratchpad\\n\\n    @classmethod\\n    def _validate_tools(cls, tools: Sequence[BaseTool]) -> None:\\n        pass\\n\\n    @classmethod\\n    def _get_default_output_parser(\\n        cls, llm: Optional[BaseLanguageModel] = None, **kwargs: Any\\n    ) -> AgentOutputParser:\\n        return StructuredChatOutputParserWithRetries.from_llm(llm=llm)\\n\\n    @property\\n    def _stop(self) -> List[str]:\\n        return [\"Observation:\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\structured_chat\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@classmethod\\n    def create_prompt(\\n        cls,\\n        tools: Sequence[BaseTool],\\n        prefix: str = PREFIX,\\n        suffix: str = SUFFIX,\\n        human_message_template: str = HUMAN_MESSAGE_TEMPLATE,\\n        format_instructions: str = FORMAT_INSTRUCTIONS,\\n        input_variables: Optional[List[str]] = None,\\n        memory_prompts: Optional[List[BasePromptTemplate]] = None,\\n    ) -> BasePromptTemplate:\\n        tool_strings = []\\n        for tool in tools:\\n            args_schema = re.sub(\"}\", \"}}\", re.sub(\"{\", \"{{\", str(tool.args)))\\n            tool_strings.append(f\"{tool.name}: {tool.description}, args: {args_schema}\")\\n        formatted_tools = \"\\\\n\".join(tool_strings)\\n        tool_names = \", \".join([tool.name for tool in tools])\\n        format_instructions = format_instructions.format(tool_names=tool_names)\\n        template = \"\\\\n\\\\n\".join([prefix, formatted_tools, format_instructions, suffix])\\n        if input_variables is None:\\n            input_variables = [\"input\", \"agent_scratchpad\"]\\n        _memory_prompts = memory_prompts or []\\n        messages = [\\n            SystemMessagePromptTemplate.from_template(template),\\n            *_memory_prompts,\\n            HumanMessagePromptTemplate.from_template(human_message_template),\\n        ]\\n        return ChatPromptTemplate(input_variables=input_variables, messages=messages)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\structured_chat\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@classmethod\\n    def from_llm_and_tools(\\n        cls,\\n        llm: BaseLanguageModel,\\n        tools: Sequence[BaseTool],\\n        callback_manager: Optional[BaseCallbackManager] = None,\\n        output_parser: Optional[AgentOutputParser] = None,\\n        prefix: str = PREFIX,\\n        suffix: str = SUFFIX,\\n        human_message_template: str = HUMAN_MESSAGE_TEMPLATE,\\n        format_instructions: str = FORMAT_INSTRUCTIONS,\\n        input_variables: Optional[List[str]] = None,\\n        memory_prompts: Optional[List[BasePromptTemplate]] = None,\\n        **kwargs: Any,\\n    ) -> Agent:\\n        \"\"\"Construct an agent from an LLM and tools.\"\"\"\\n        cls._validate_tools(tools)\\n        prompt = cls.create_prompt(\\n            tools,\\n            prefix=prefix,\\n            suffix=suffix,\\n            human_message_template=human_message_template,\\n            format_instructions=format_instructions,\\n            input_variables=input_variables,\\n            memory_prompts=memory_prompts,\\n        )\\n        llm_chain = LLMChain(\\n            llm=llm,\\n            prompt=prompt,\\n            callback_manager=callback_manager,\\n        )\\n        tool_names = [tool.name for tool in tools]\\n        _output_parser = output_parser or cls._get_default_output_parser(llm=llm)\\n        return cls(\\n            llm_chain=llm_chain,\\n            allowed_tools=tool_names,\\n            output_parser=_output_parser,\\n            **kwargs,\\n        )\\n\\n    @property\\n    def _agent_type(self) -> str:\\n        raise ValueError\\n\\n\\ndef create_structured_chat_agent(\\n    llm: BaseLanguageModel, tools: Sequence[BaseTool], prompt: ChatPromptTemplate\\n) -> Runnable:\\n    \"\"\"Create an agent aimed at supporting tools with multiple inputs.\\n\\n    Args:\\n        llm: LLM to use as the agent.\\n        tools: Tools this agent has access to.\\n        prompt: The prompt to use. See Prompt section below for more.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\structured_chat\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Returns:\\n        A Runnable sequence representing an agent. It takes as input all the same input\\n        variables as the prompt passed in does. It returns as output either an\\n        AgentAction or AgentFinish.\\n\\n    Examples:\\n\\n        .. code-block:: python\\n\\n            from langchain import hub\\n            from langchain_community.chat_models import ChatOpenAI\\n            from langchain.agents import AgentExecutor, create_structured_chat_agent\\n\\n            prompt = hub.pull(\"hwchase17/structured-chat-agent\")\\n            model = ChatOpenAI()\\n            tools = ...\\n\\n            agent = create_structured_chat_agent(model, tools, prompt)\\n            agent_executor = AgentExecutor(agent=agent, tools=tools)\\n\\n            agent_executor.invoke({\"input\": \"hi\"})\\n\\n            # Using with chat history\\n            from langchain_core.messages import AIMessage, HumanMessage\\n            agent_executor.invoke(\\n                {\\n                    \"input\": \"what\\'s my name?\",\\n                    \"chat_history\": [\\n                        HumanMessage(content=\"hi! my name is bob\"),\\n                        AIMessage(content=\"Hello Bob! How can I assist you today?\"),\\n                    ],\\n                }\\n            )\\n\\n    Prompt:\\n\\n        The prompt must have input keys:\\n            * `tools`: contains descriptions and arguments for each tool.\\n            * `tool_names`: contains all tool names.\\n            * `agent_scratchpad`: contains previous agent actions and tool outputs as a string.\\n\\n        Here\\'s an example:\\n\\n        .. code-block:: python\\n\\n            from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\\n\\n            system = \\'\\'\\'Respond to the human as helpfully and accurately as possible. You have access to the following tools:\\n\\n            {tools}\\n\\n            Use a json blob to specify a tool by providing an action key (tool name) and an action_input key (tool input).\\n\\n            Valid \"action\" values: \"Final Answer\" or {tool_names}' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\structured_chat\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Provide only ONE action per $JSON_BLOB, as shown:\\n\\n            ```\\n            {{\\n              \"action\": $TOOL_NAME,\\n              \"action_input\": $INPUT\\n            }}\\n            ```\\n\\n            Follow this format:\\n\\n            Question: input question to answer\\n            Thought: consider previous and subsequent steps\\n            Action:\\n            ```\\n            $JSON_BLOB\\n            ```\\n            Observation: action result\\n            ... (repeat Thought/Action/Observation N times)\\n            Thought: I know what to respond\\n            Action:\\n            ```\\n            {{\\n              \"action\": \"Final Answer\",\\n              \"action_input\": \"Final response to human\"\\n            }}\\n\\n            Begin! Reminder to ALWAYS respond with a valid json blob of a single action. Use tools if necessary. Respond directly if appropriate. Format is Action:```$JSON_BLOB```then Observation\\'\\'\\'\\n\\n            human = \\'\\'\\'{input}\\n\\n            {agent_scratchpad}\\n\\n            (reminder to respond in a JSON blob no matter what)\\'\\'\\'\\n\\n            prompt = ChatPromptTemplate.from_messages(\\n                [\\n                    (\"system\", system),\\n                    MessagesPlaceholder(\"chat_history\", optional=True),\\n                    (\"human\", human),\\n                ]\\n            )\\n    \"\"\"  # noqa: E501\\n    missing_vars = {\"tools\", \"tool_names\", \"agent_scratchpad\"}.difference(\\n        prompt.input_variables\\n    )\\n    if missing_vars:\\n        raise ValueError(f\"Prompt missing required variables: {missing_vars}\")\\n\\n    prompt = prompt.partial(\\n        tools=render_text_description_and_args(list(tools)),\\n        tool_names=\", \".join([t.name for t in tools]),\\n    )\\n    llm_with_stop = llm.bind(stop=[\"Observation\"])\\n\\n    agent = (\\n        RunnablePassthrough.assign(\\n            agent_scratchpad=lambda x: format_log_to_str(x[\"intermediate_steps\"]),\\n        )\\n        | prompt\\n        | llm_with_stop\\n        | JSONAgentOutputParser()\\n    )\\n    return agent' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\structured_chat\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from __future__ import annotations\\n\\nimport json\\nimport logging\\nimport re\\nfrom typing import Optional, Union\\n\\nfrom langchain_core.agents import AgentAction, AgentFinish\\nfrom langchain_core.exceptions import OutputParserException\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.pydantic_v1 import Field\\n\\nfrom langchain.agents.agent import AgentOutputParser\\nfrom langchain.agents.structured_chat.prompt import FORMAT_INSTRUCTIONS\\nfrom langchain.output_parsers import OutputFixingParser\\n\\nlogger = logging.getLogger(__name__)\\n\\n\\nclass StructuredChatOutputParser(AgentOutputParser):\\n    \"\"\"Output parser for the structured chat agent.\"\"\"\\n\\n    pattern = re.compile(r\"```(?:json\\\\s+)?(\\\\W.*?)```\", re.DOTALL)\\n\\n    def get_format_instructions(self) -> str:\\n        return FORMAT_INSTRUCTIONS\\n\\n    def parse(self, text: str) -> Union[AgentAction, AgentFinish]:\\n        try:\\n            action_match = self.pattern.search(text)\\n            if action_match is not None:\\n                response = json.loads(action_match.group(1).strip(), strict=False)\\n                if isinstance(response, list):\\n                    # gpt turbo frequently ignores the directive to emit a single action\\n                    logger.warning(\"Got multiple action responses: %s\", response)\\n                    response = response[0]\\n                if response[\"action\"] == \"Final Answer\":\\n                    return AgentFinish({\"output\": response[\"action_input\"]}, text)\\n                else:\\n                    return AgentAction(\\n                        response[\"action\"], response.get(\"action_input\", {}), text\\n                    )\\n            else:\\n                return AgentFinish({\"output\": text}, text)\\n        except Exception as e:\\n            raise OutputParserException(f\"Could not parse LLM output: {text}\") from e\\n\\n    @property\\n    def _type(self) -> str:\\n        return \"structured_chat\"\\n\\n\\nclass StructuredChatOutputParserWithRetries(AgentOutputParser):\\n    \"\"\"Output parser with retries for the structured chat agent.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\structured_chat\\\\output_parser.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='base_parser: AgentOutputParser = Field(default_factory=StructuredChatOutputParser)\\n    \"\"\"The base parser to use.\"\"\"\\n    output_fixing_parser: Optional[OutputFixingParser] = None\\n    \"\"\"The output fixing parser to use.\"\"\"\\n\\n    def get_format_instructions(self) -> str:\\n        return FORMAT_INSTRUCTIONS\\n\\n    def parse(self, text: str) -> Union[AgentAction, AgentFinish]:\\n        try:\\n            if self.output_fixing_parser is not None:\\n                parsed_obj: Union[\\n                    AgentAction, AgentFinish\\n                ] = self.output_fixing_parser.parse(text)\\n            else:\\n                parsed_obj = self.base_parser.parse(text)\\n            return parsed_obj\\n        except Exception as e:\\n            raise OutputParserException(f\"Could not parse LLM output: {text}\") from e\\n\\n    @classmethod\\n    def from_llm(\\n        cls,\\n        llm: Optional[BaseLanguageModel] = None,\\n        base_parser: Optional[StructuredChatOutputParser] = None,\\n    ) -> StructuredChatOutputParserWithRetries:\\n        if llm is not None:\\n            base_parser = base_parser or StructuredChatOutputParser()\\n            output_fixing_parser: OutputFixingParser = OutputFixingParser.from_llm(\\n                llm=llm, parser=base_parser\\n            )\\n            return cls(output_fixing_parser=output_fixing_parser)\\n        elif base_parser is not None:\\n            return cls(base_parser=base_parser)\\n        else:\\n            return cls()\\n\\n    @property\\n    def _type(self) -> str:\\n        return \"structured_chat_with_retries\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\structured_chat\\\\output_parser.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# flake8: noqa\\nPREFIX = \"\"\"Respond to the human as helpfully and accurately as possible. You have access to the following tools:\"\"\"\\nFORMAT_INSTRUCTIONS = \"\"\"Use a json blob to specify a tool by providing an action key (tool name) and an action_input key (tool input).\\n\\nValid \"action\" values: \"Final Answer\" or {tool_names}\\n\\nProvide only ONE action per $JSON_BLOB, as shown:\\n\\n```\\n{{{{\\n  \"action\": $TOOL_NAME,\\n  \"action_input\": $INPUT\\n}}}}\\n```\\n\\nFollow this format:\\n\\nQuestion: input question to answer\\nThought: consider previous and subsequent steps\\nAction:\\n```\\n$JSON_BLOB\\n```\\nObservation: action result\\n... (repeat Thought/Action/Observation N times)\\nThought: I know what to respond\\nAction:\\n```\\n{{{{\\n  \"action\": \"Final Answer\",\\n  \"action_input\": \"Final response to human\"\\n}}}}\\n```\"\"\"\\nSUFFIX = \"\"\"Begin! Reminder to ALWAYS respond with a valid json blob of a single action. Use tools if necessary. Respond directly if appropriate. Format is Action:```$JSON_BLOB```then Observation:.\\nThought:\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\structured_chat\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from typing import Any, List, Sequence, Tuple, Union\\n\\nfrom langchain_core._api import deprecated\\nfrom langchain_core.agents import AgentAction, AgentFinish\\nfrom langchain_core.callbacks import Callbacks\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.prompts.base import BasePromptTemplate\\nfrom langchain_core.prompts.chat import AIMessagePromptTemplate, ChatPromptTemplate\\nfrom langchain_core.runnables import Runnable, RunnablePassthrough\\nfrom langchain_core.tools import BaseTool\\n\\nfrom langchain.agents.agent import BaseSingleActionAgent\\nfrom langchain.agents.format_scratchpad import format_xml\\nfrom langchain.agents.output_parsers import XMLAgentOutputParser\\nfrom langchain.agents.xml.prompt import agent_instructions\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.tools.render import render_text_description\\n\\n\\n@deprecated(\"0.1.0\", alternative=\"create_xml_agent\", removal=\"0.2.0\")\\nclass XMLAgent(BaseSingleActionAgent):\\n    \"\"\"Agent that uses XML tags.\\n\\n    Args:\\n        tools: list of tools the agent can choose from\\n        llm_chain: The LLMChain to call to predict the next action\\n\\n    Examples:\\n\\n        .. code-block:: python\\n\\n            from langchain.agents import XMLAgent\\n            from langchain\\n\\n            tools = ...\\n            model =\\n\\n\\n    \"\"\"\\n\\n    tools: List[BaseTool]\\n    \"\"\"List of tools this agent has access to.\"\"\"\\n    llm_chain: LLMChain\\n    \"\"\"Chain to use to predict action.\"\"\"\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        return [\"input\"]\\n\\n    @staticmethod\\n    def get_default_prompt() -> ChatPromptTemplate:\\n        base_prompt = ChatPromptTemplate.from_template(agent_instructions)\\n        return base_prompt + AIMessagePromptTemplate.from_template(\\n            \"{intermediate_steps}\"\\n        )\\n\\n    @staticmethod\\n    def get_default_output_parser() -> XMLAgentOutputParser:\\n        return XMLAgentOutputParser()' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\xml\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def plan(\\n        self,\\n        intermediate_steps: List[Tuple[AgentAction, str]],\\n        callbacks: Callbacks = None,\\n        **kwargs: Any,\\n    ) -> Union[AgentAction, AgentFinish]:\\n        log = \"\"\\n        for action, observation in intermediate_steps:\\n            log += (\\n                f\"<tool>{action.tool}</tool><tool_input>{action.tool_input}\"\\n                f\"</tool_input><observation>{observation}</observation>\"\\n            )\\n        tools = \"\"\\n        for tool in self.tools:\\n            tools += f\"{tool.name}: {tool.description}\\\\n\"\\n        inputs = {\\n            \"intermediate_steps\": log,\\n            \"tools\": tools,\\n            \"question\": kwargs[\"input\"],\\n            \"stop\": [\"</tool_input>\", \"</final_answer>\"],\\n        }\\n        response = self.llm_chain(inputs, callbacks=callbacks)\\n        return response[self.llm_chain.output_key]\\n\\n    async def aplan(\\n        self,\\n        intermediate_steps: List[Tuple[AgentAction, str]],\\n        callbacks: Callbacks = None,\\n        **kwargs: Any,\\n    ) -> Union[AgentAction, AgentFinish]:\\n        log = \"\"\\n        for action, observation in intermediate_steps:\\n            log += (\\n                f\"<tool>{action.tool}</tool><tool_input>{action.tool_input}\"\\n                f\"</tool_input><observation>{observation}</observation>\"\\n            )\\n        tools = \"\"\\n        for tool in self.tools:\\n            tools += f\"{tool.name}: {tool.description}\\\\n\"\\n        inputs = {\\n            \"intermediate_steps\": log,\\n            \"tools\": tools,\\n            \"question\": kwargs[\"input\"],\\n            \"stop\": [\"</tool_input>\", \"</final_answer>\"],\\n        }\\n        response = await self.llm_chain.acall(inputs, callbacks=callbacks)\\n        return response[self.llm_chain.output_key]\\n\\n\\ndef create_xml_agent(\\n    llm: BaseLanguageModel, tools: Sequence[BaseTool], prompt: BasePromptTemplate\\n) -> Runnable:\\n    \"\"\"Create an agent that uses XML to format its logic.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\xml\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Args:\\n        llm: LLM to use as the agent.\\n        tools: Tools this agent has access to.\\n        prompt: The prompt to use, must have input keys\\n            `tools`: contains descriptions for each tool.\\n            `agent_scratchpad`: contains previous agent actions and tool outputs.\\n\\n    Returns:\\n        A Runnable sequence representing an agent. It takes as input all the same input\\n        variables as the prompt passed in does. It returns as output either an\\n        AgentAction or AgentFinish.\\n\\n    Example:\\n\\n        .. code-block:: python\\n\\n            from langchain import hub\\n            from langchain_community.chat_models import ChatAnthropic\\n            from langchain.agents import AgentExecutor, create_xml_agent\\n\\n            prompt = hub.pull(\"hwchase17/xml-agent-convo\")\\n            model = ChatAnthropic()\\n            tools = ...\\n\\n            agent = create_xml_agent(model, tools, prompt)\\n            agent_executor = AgentExecutor(agent=agent, tools=tools)\\n\\n            agent_executor.invoke({\"input\": \"hi\"})\\n\\n            # Use with chat history\\n            from langchain_core.messages import AIMessage, HumanMessage\\n            agent_executor.invoke(\\n                {\\n                    \"input\": \"what\\'s my name?\",\\n                    # Notice that chat_history is a string\\n                    # since this prompt is aimed at LLMs, not chat models\\n                    \"chat_history\": \"Human: My name is Bob\\\\\\\\nAI: Hello Bob!\",\\n                }\\n            )\\n\\n    Prompt:\\n\\n        The prompt must have input keys:\\n            * `tools`: contains descriptions for each tool.\\n            * `agent_scratchpad`: contains previous agent actions and tool outputs as an XML string.\\n\\n        Here\\'s an example:\\n\\n        .. code-block:: python\\n\\n            from langchain_core.prompts import PromptTemplate\\n\\n            template = \\'\\'\\'You are a helpful assistant. Help the user answer any questions.\\n\\n            You have access to the following tools:\\n\\n            {tools}' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\xml\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='In order to use a tool, you can use <tool></tool> and <tool_input></tool_input> tags. You will then get back a response in the form <observation></observation>\\n            For example, if you have a tool called \\'search\\' that could run a google search, in order to search for the weather in SF you would respond:\\n\\n            <tool>search</tool><tool_input>weather in SF</tool_input>\\n            <observation>64 degrees</observation>\\n\\n            When you are done, respond with a final answer between <final_answer></final_answer>. For example:\\n\\n            <final_answer>The weather in SF is 64 degrees</final_answer>\\n\\n            Begin!\\n\\n            Previous Conversation:\\n            {chat_history}\\n\\n            Question: {input}\\n            {agent_scratchpad}\\'\\'\\'\\n            prompt = PromptTemplate.from_template(template)\\n    \"\"\"  # noqa: E501\\n    missing_vars = {\"tools\", \"agent_scratchpad\"}.difference(prompt.input_variables)\\n    if missing_vars:\\n        raise ValueError(f\"Prompt missing required variables: {missing_vars}\")\\n\\n    prompt = prompt.partial(\\n        tools=render_text_description(list(tools)),\\n    )\\n    llm_with_stop = llm.bind(stop=[\"</tool_input>\"])\\n\\n    agent = (\\n        RunnablePassthrough.assign(\\n            agent_scratchpad=lambda x: format_xml(x[\"intermediate_steps\"]),\\n        )\\n        | prompt\\n        | llm_with_stop\\n        | XMLAgentOutputParser()\\n    )\\n    return agent' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\xml\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# flake8: noqa\\n# TODO: deprecate\\nagent_instructions = \"\"\"You are a helpful assistant. Help the user answer any questions.\\n\\nYou have access to the following tools:\\n\\n{tools}\\n\\nIn order to use a tool, you can use <tool></tool> and <tool_input></tool_input> tags. \\\\\\nYou will then get back a response in the form <observation></observation>\\nFor example, if you have a tool called \\'search\\' that could run a google search, in order to search for the weather in SF you would respond:\\n\\n<tool>search</tool><tool_input>weather in SF</tool_input>\\n<observation>64 degrees</observation>\\n\\nWhen you are done, respond with a final answer between <final_answer></final_answer>. For example:\\n\\n<final_answer>The weather in SF is 64 degrees</final_answer>\\n\\nBegin!\\n\\nQuestion: {question}\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\xml\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.callbacks.aim_callback import (\\n    AimCallbackHandler,\\n    BaseMetadataCallbackHandler,\\n    import_aim,\\n)\\n\\n__all__ = [\"import_aim\", \"BaseMetadataCallbackHandler\", \"AimCallbackHandler\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\aim_callback.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.callbacks.argilla_callback import ArgillaCallbackHandler\\n\\n__all__ = [\"ArgillaCallbackHandler\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\argilla_callback.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.callbacks.arize_callback import ArizeCallbackHandler\\n\\n__all__ = [\"ArizeCallbackHandler\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\arize_callback.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.callbacks.arthur_callback import (\\n    ArthurCallbackHandler,\\n)\\n\\n__all__ = [\\n    \"ArthurCallbackHandler\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\arthur_callback.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Base callback handler that can be used to handle callbacks in langchain.\"\"\"\\nfrom __future__ import annotations\\n\\nfrom langchain_core.callbacks import (\\n    AsyncCallbackHandler,\\n    BaseCallbackHandler,\\n    BaseCallbackManager,\\n    CallbackManagerMixin,\\n    Callbacks,\\n    ChainManagerMixin,\\n    LLMManagerMixin,\\n    RetrieverManagerMixin,\\n    RunManagerMixin,\\n    ToolManagerMixin,\\n)\\n\\n__all__ = [\\n    \"RetrieverManagerMixin\",\\n    \"LLMManagerMixin\",\\n    \"ChainManagerMixin\",\\n    \"ToolManagerMixin\",\\n    \"CallbackManagerMixin\",\\n    \"RunManagerMixin\",\\n    \"BaseCallbackHandler\",\\n    \"AsyncCallbackHandler\",\\n    \"BaseCallbackManager\",\\n    \"Callbacks\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.callbacks.clearml_callback import (\\n    ClearMLCallbackHandler,\\n)\\n\\n__all__ = [\"ClearMLCallbackHandler\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\clearml_callback.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.callbacks.comet_ml_callback import (\\n    CometCallbackHandler,\\n)\\n\\n__all__ = [\\n    \"CometCallbackHandler\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\comet_ml_callback.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.callbacks.confident_callback import DeepEvalCallbackHandler\\n\\n__all__ = [\"DeepEvalCallbackHandler\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\confident_callback.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.callbacks.context_callback import (\\n    ContextCallbackHandler,\\n)\\n\\n__all__ = [\"ContextCallbackHandler\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\context_callback.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Callback Handler that writes to a file.\"\"\"\\nfrom typing import Any, Dict, Optional, TextIO, cast\\n\\nfrom langchain_core.agents import AgentAction, AgentFinish\\nfrom langchain_core.callbacks import BaseCallbackHandler\\nfrom langchain_core.utils.input import print_text\\n\\n\\nclass FileCallbackHandler(BaseCallbackHandler):\\n    \"\"\"Callback Handler that writes to a file.\"\"\"\\n\\n    def __init__(\\n        self, filename: str, mode: str = \"a\", color: Optional[str] = None\\n    ) -> None:\\n        \"\"\"Initialize callback handler.\"\"\"\\n        self.file = cast(TextIO, open(filename, mode, encoding=\"utf-8\"))\\n        self.color = color\\n\\n    def __del__(self) -> None:\\n        \"\"\"Destructor to cleanup when done.\"\"\"\\n        self.file.close()\\n\\n    def on_chain_start(\\n        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any\\n    ) -> None:\\n        \"\"\"Print out that we are entering a chain.\"\"\"\\n        class_name = serialized.get(\"name\", serialized.get(\"id\", [\"<unknown>\"])[-1])\\n        print_text(\\n            f\"\\\\n\\\\n\\\\033[1m> Entering new {class_name} chain...\\\\033[0m\",\\n            end=\"\\\\n\",\\n            file=self.file,\\n        )\\n\\n    def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> None:\\n        \"\"\"Print out that we finished a chain.\"\"\"\\n        print_text(\"\\\\n\\\\033[1m> Finished chain.\\\\033[0m\", end=\"\\\\n\", file=self.file)\\n\\n    def on_agent_action(\\n        self, action: AgentAction, color: Optional[str] = None, **kwargs: Any\\n    ) -> Any:\\n        \"\"\"Run on agent action.\"\"\"\\n        print_text(action.log, color=color or self.color, file=self.file)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\file.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def on_tool_end(\\n        self,\\n        output: str,\\n        color: Optional[str] = None,\\n        observation_prefix: Optional[str] = None,\\n        llm_prefix: Optional[str] = None,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"If not the final action, print out observation.\"\"\"\\n        if observation_prefix is not None:\\n            print_text(f\"\\\\n{observation_prefix}\", file=self.file)\\n        print_text(output, color=color or self.color, file=self.file)\\n        if llm_prefix is not None:\\n            print_text(f\"\\\\n{llm_prefix}\", file=self.file)\\n\\n    def on_text(\\n        self, text: str, color: Optional[str] = None, end: str = \"\", **kwargs: Any\\n    ) -> None:\\n        \"\"\"Run when agent ends.\"\"\"\\n        print_text(text, color=color or self.color, end=end, file=self.file)\\n\\n    def on_agent_finish(\\n        self, finish: AgentFinish, color: Optional[str] = None, **kwargs: Any\\n    ) -> None:\\n        \"\"\"Run on agent end.\"\"\"\\n        print_text(finish.log, color=color or self.color, end=\"\\\\n\", file=self.file)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\file.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.callbacks.flyte_callback import (\\n    FlyteCallbackHandler,\\n)\\n\\n__all__ = [\"FlyteCallbackHandler\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\flyte_callback.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.callbacks.human import (\\n    AsyncHumanApprovalCallbackHandler,\\n    HumanApprovalCallbackHandler,\\n    HumanRejectedException,\\n)\\n\\n__all__ = [\\n    \"HumanRejectedException\",\\n    \"HumanApprovalCallbackHandler\",\\n    \"AsyncHumanApprovalCallbackHandler\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\human.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.callbacks.infino_callback import (\\n    InfinoCallbackHandler,\\n)\\n\\n__all__ = [\\n    \"InfinoCallbackHandler\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\infino_callback.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.callbacks.labelstudio_callback import (\\n    LabelStudioCallbackHandler,\\n    LabelStudioMode,\\n    get_default_label_configs,\\n)\\n\\n__all__ = [\"LabelStudioMode\", \"get_default_label_configs\", \"LabelStudioCallbackHandler\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\labelstudio_callback.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.callbacks.llmonitor_callback import (\\n    LLMonitorCallbackHandler,\\n)\\n\\n__all__ = [\\n    \"LLMonitorCallbackHandler\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\llmonitor_callback.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from __future__ import annotations\\n\\nfrom langchain_community.callbacks.manager import (\\n    get_openai_callback,\\n    wandb_tracing_enabled,\\n)\\nfrom langchain_core.callbacks.manager import (\\n    AsyncCallbackManager,\\n    AsyncCallbackManagerForChainGroup,\\n    AsyncCallbackManagerForChainRun,\\n    AsyncCallbackManagerForLLMRun,\\n    AsyncCallbackManagerForRetrieverRun,\\n    AsyncCallbackManagerForToolRun,\\n    AsyncParentRunManager,\\n    AsyncRunManager,\\n    BaseRunManager,\\n    CallbackManager,\\n    CallbackManagerForChainGroup,\\n    CallbackManagerForChainRun,\\n    CallbackManagerForLLMRun,\\n    CallbackManagerForRetrieverRun,\\n    CallbackManagerForToolRun,\\n    Callbacks,\\n    ParentRunManager,\\n    RunManager,\\n    ahandle_event,\\n    atrace_as_chain_group,\\n    handle_event,\\n    trace_as_chain_group,\\n)\\nfrom langchain_core.tracers.context import (\\n    collect_runs,\\n    tracing_enabled,\\n    tracing_v2_enabled,\\n)\\nfrom langchain_core.utils.env import env_var_is_set\\n\\n__all__ = [\\n    \"BaseRunManager\",\\n    \"RunManager\",\\n    \"ParentRunManager\",\\n    \"AsyncRunManager\",\\n    \"AsyncParentRunManager\",\\n    \"CallbackManagerForLLMRun\",\\n    \"AsyncCallbackManagerForLLMRun\",\\n    \"CallbackManagerForChainRun\",\\n    \"AsyncCallbackManagerForChainRun\",\\n    \"CallbackManagerForToolRun\",\\n    \"AsyncCallbackManagerForToolRun\",\\n    \"CallbackManagerForRetrieverRun\",\\n    \"AsyncCallbackManagerForRetrieverRun\",\\n    \"CallbackManager\",\\n    \"CallbackManagerForChainGroup\",\\n    \"AsyncCallbackManager\",\\n    \"AsyncCallbackManagerForChainGroup\",\\n    \"tracing_enabled\",\\n    \"tracing_v2_enabled\",\\n    \"collect_runs\",\\n    \"atrace_as_chain_group\",\\n    \"trace_as_chain_group\",\\n    \"handle_event\",\\n    \"ahandle_event\",\\n    \"Callbacks\",\\n    \"env_var_is_set\",\\n    \"get_openai_callback\",\\n    \"wandb_tracing_enabled\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\manager.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.callbacks.mlflow_callback import (\\n    MlflowCallbackHandler,\\n    MlflowLogger,\\n    analyze_text,\\n    construct_html_from_prompt_and_generation,\\n)\\n\\n__all__ = [\\n    \"analyze_text\",\\n    \"construct_html_from_prompt_and_generation\",\\n    \"MlflowLogger\",\\n    \"MlflowCallbackHandler\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\mlflow_callback.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.callbacks.openai_info import OpenAICallbackHandler\\n\\n__all__ = [\"OpenAICallbackHandler\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\openai_info.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.callbacks.promptlayer_callback import (\\n    PromptLayerCallbackHandler,\\n)\\n\\n__all__ = [\"PromptLayerCallbackHandler\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\promptlayer_callback.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.callbacks.sagemaker_callback import (\\n    SageMakerCallbackHandler,\\n)\\n\\n__all__ = [\"SageMakerCallbackHandler\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\sagemaker_callback.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.callbacks.stdout import StdOutCallbackHandler\\n\\n__all__ = [\"StdOutCallbackHandler\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\stdout.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from __future__ import annotations\\n\\nimport asyncio\\nfrom typing import Any, AsyncIterator, Dict, List, Literal, Union, cast\\n\\nfrom langchain_core.callbacks import AsyncCallbackHandler\\nfrom langchain_core.outputs import LLMResult\\n\\n# TODO If used by two LLM runs in parallel this won\\'t work as expected\\n\\n\\nclass AsyncIteratorCallbackHandler(AsyncCallbackHandler):\\n    \"\"\"Callback handler that returns an async iterator.\"\"\"\\n\\n    queue: asyncio.Queue[str]\\n\\n    done: asyncio.Event\\n\\n    @property\\n    def always_verbose(self) -> bool:\\n        return True\\n\\n    def __init__(self) -> None:\\n        self.queue = asyncio.Queue()\\n        self.done = asyncio.Event()\\n\\n    async def on_llm_start(\\n        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\\n    ) -> None:\\n        # If two calls are made in a row, this resets the state\\n        self.done.clear()\\n\\n    async def on_llm_new_token(self, token: str, **kwargs: Any) -> None:\\n        if token is not None and token != \"\":\\n            self.queue.put_nowait(token)\\n\\n    async def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:\\n        self.done.set()\\n\\n    async def on_llm_error(self, error: BaseException, **kwargs: Any) -> None:\\n        self.done.set()\\n\\n    # TODO implement the other methods\\n\\n    async def aiter(self) -> AsyncIterator[str]:\\n        while not self.queue.empty() or not self.done.is_set():\\n            # Wait for the next token in the queue,\\n            # but stop waiting if the done event is set\\n            done, other = await asyncio.wait(\\n                [\\n                    # NOTE: If you add other tasks here, update the code below,\\n                    # which assumes each set has exactly one task each\\n                    asyncio.ensure_future(self.queue.get()),\\n                    asyncio.ensure_future(self.done.wait()),\\n                ],\\n                return_when=asyncio.FIRST_COMPLETED,\\n            )\\n\\n            # Cancel the other task\\n            if other:\\n                other.pop().cancel()' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\streaming_aiter.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# Extract the value of the first completed task\\n            token_or_done = cast(Union[str, Literal[True]], done.pop().result())\\n\\n            # If the extracted value is the boolean True, the done event was set\\n            if token_or_done is True:\\n                break\\n\\n            # Otherwise, the extracted value is a token, which we yield\\n            yield token_or_done' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\streaming_aiter.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from __future__ import annotations\\n\\nfrom typing import Any, Dict, List, Optional\\n\\nfrom langchain_core.outputs import LLMResult\\n\\nfrom langchain.callbacks.streaming_aiter import AsyncIteratorCallbackHandler\\n\\nDEFAULT_ANSWER_PREFIX_TOKENS = [\"Final\", \"Answer\", \":\"]\\n\\n\\nclass AsyncFinalIteratorCallbackHandler(AsyncIteratorCallbackHandler):\\n    \"\"\"Callback handler that returns an async iterator.\\n    Only the final output of the agent will be iterated.\\n    \"\"\"\\n\\n    def append_to_last_tokens(self, token: str) -> None:\\n        self.last_tokens.append(token)\\n        self.last_tokens_stripped.append(token.strip())\\n        if len(self.last_tokens) > len(self.answer_prefix_tokens):\\n            self.last_tokens.pop(0)\\n            self.last_tokens_stripped.pop(0)\\n\\n    def check_if_answer_reached(self) -> bool:\\n        if self.strip_tokens:\\n            return self.last_tokens_stripped == self.answer_prefix_tokens_stripped\\n        else:\\n            return self.last_tokens == self.answer_prefix_tokens\\n\\n    def __init__(\\n        self,\\n        *,\\n        answer_prefix_tokens: Optional[List[str]] = None,\\n        strip_tokens: bool = True,\\n        stream_prefix: bool = False,\\n    ) -> None:\\n        \"\"\"Instantiate AsyncFinalIteratorCallbackHandler.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\streaming_aiter_final_only.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Args:\\n            answer_prefix_tokens: Token sequence that prefixes the answer.\\n                Default is [\"Final\", \"Answer\", \":\"]\\n            strip_tokens: Ignore white spaces and new lines when comparing\\n                answer_prefix_tokens to last tokens? (to determine if answer has been\\n                reached)\\n            stream_prefix: Should answer prefix itself also be streamed?\\n        \"\"\"\\n        super().__init__()\\n        if answer_prefix_tokens is None:\\n            self.answer_prefix_tokens = DEFAULT_ANSWER_PREFIX_TOKENS\\n        else:\\n            self.answer_prefix_tokens = answer_prefix_tokens\\n        if strip_tokens:\\n            self.answer_prefix_tokens_stripped = [\\n                token.strip() for token in self.answer_prefix_tokens\\n            ]\\n        else:\\n            self.answer_prefix_tokens_stripped = self.answer_prefix_tokens\\n        self.last_tokens = [\"\"] * len(self.answer_prefix_tokens)\\n        self.last_tokens_stripped = [\"\"] * len(self.answer_prefix_tokens)\\n        self.strip_tokens = strip_tokens\\n        self.stream_prefix = stream_prefix\\n        self.answer_reached = False\\n\\n    async def on_llm_start(\\n        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\\n    ) -> None:\\n        # If two calls are made in a row, this resets the state\\n        self.done.clear()\\n        self.answer_reached = False\\n\\n    async def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:\\n        if self.answer_reached:\\n            self.done.set()\\n\\n    async def on_llm_new_token(self, token: str, **kwargs: Any) -> None:\\n        # Remember the last n tokens, where n = len(answer_prefix_tokens)\\n        self.append_to_last_tokens(token)\\n\\n        # Check if the last n tokens match the answer_prefix_tokens list ...\\n        if self.check_if_answer_reached():\\n            self.answer_reached = True\\n            if self.stream_prefix:\\n                for t in self.last_tokens:\\n                    self.queue.put_nowait(t)\\n            return' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\streaming_aiter_final_only.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# If yes, then put tokens from now on\\n        if self.answer_reached:\\n            self.queue.put_nowait(token)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\streaming_aiter_final_only.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Callback Handler streams to stdout on new llm token.\"\"\"\\nfrom langchain_core.callbacks import StreamingStdOutCallbackHandler\\n\\n__all__ = [\"StreamingStdOutCallbackHandler\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\streaming_stdout.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Callback Handler streams to stdout on new llm token.\"\"\"\\nimport sys\\nfrom typing import Any, Dict, List, Optional\\n\\nfrom langchain_core.callbacks import StreamingStdOutCallbackHandler\\n\\nDEFAULT_ANSWER_PREFIX_TOKENS = [\"Final\", \"Answer\", \":\"]\\n\\n\\nclass FinalStreamingStdOutCallbackHandler(StreamingStdOutCallbackHandler):\\n    \"\"\"Callback handler for streaming in agents.\\n    Only works with agents using LLMs that support streaming.\\n\\n    Only the final output of the agent will be streamed.\\n    \"\"\"\\n\\n    def append_to_last_tokens(self, token: str) -> None:\\n        self.last_tokens.append(token)\\n        self.last_tokens_stripped.append(token.strip())\\n        if len(self.last_tokens) > len(self.answer_prefix_tokens):\\n            self.last_tokens.pop(0)\\n            self.last_tokens_stripped.pop(0)\\n\\n    def check_if_answer_reached(self) -> bool:\\n        if self.strip_tokens:\\n            return self.last_tokens_stripped == self.answer_prefix_tokens_stripped\\n        else:\\n            return self.last_tokens == self.answer_prefix_tokens\\n\\n    def __init__(\\n        self,\\n        *,\\n        answer_prefix_tokens: Optional[List[str]] = None,\\n        strip_tokens: bool = True,\\n        stream_prefix: bool = False,\\n    ) -> None:\\n        \"\"\"Instantiate FinalStreamingStdOutCallbackHandler.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\streaming_stdout_final_only.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Args:\\n            answer_prefix_tokens: Token sequence that prefixes the answer.\\n                Default is [\"Final\", \"Answer\", \":\"]\\n            strip_tokens: Ignore white spaces and new lines when comparing\\n                answer_prefix_tokens to last tokens? (to determine if answer has been\\n                reached)\\n            stream_prefix: Should answer prefix itself also be streamed?\\n        \"\"\"\\n        super().__init__()\\n        if answer_prefix_tokens is None:\\n            self.answer_prefix_tokens = DEFAULT_ANSWER_PREFIX_TOKENS\\n        else:\\n            self.answer_prefix_tokens = answer_prefix_tokens\\n        if strip_tokens:\\n            self.answer_prefix_tokens_stripped = [\\n                token.strip() for token in self.answer_prefix_tokens\\n            ]\\n        else:\\n            self.answer_prefix_tokens_stripped = self.answer_prefix_tokens\\n        self.last_tokens = [\"\"] * len(self.answer_prefix_tokens)\\n        self.last_tokens_stripped = [\"\"] * len(self.answer_prefix_tokens)\\n        self.strip_tokens = strip_tokens\\n        self.stream_prefix = stream_prefix\\n        self.answer_reached = False\\n\\n    def on_llm_start(\\n        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\\n    ) -> None:\\n        \"\"\"Run when LLM starts running.\"\"\"\\n        self.answer_reached = False\\n\\n    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:\\n        \"\"\"Run on new LLM token. Only available when streaming is enabled.\"\"\"\\n\\n        # Remember the last n tokens, where n = len(answer_prefix_tokens)\\n        self.append_to_last_tokens(token)\\n\\n        # Check if the last n tokens match the answer_prefix_tokens list ...\\n        if self.check_if_answer_reached():\\n            self.answer_reached = True\\n            if self.stream_prefix:\\n                for t in self.last_tokens:\\n                    sys.stdout.write(t)\\n                sys.stdout.flush()\\n            return' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\streaming_stdout_final_only.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# ... if yes, then print tokens from now on\\n        if self.answer_reached:\\n            sys.stdout.write(token)\\n            sys.stdout.flush()' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\streaming_stdout_final_only.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.callbacks.trubrics_callback import (\\n    TrubricsCallbackHandler,\\n)\\n\\n__all__ = [\"TrubricsCallbackHandler\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\trubrics_callback.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.callbacks.utils import (\\n    BaseMetadataCallbackHandler,\\n    _flatten_dict,\\n    flatten_dict,\\n    hash_string,\\n    import_pandas,\\n    import_spacy,\\n    import_textstat,\\n    load_json,\\n)\\n\\n__all__ = [\\n    \"import_spacy\",\\n    \"import_pandas\",\\n    \"import_textstat\",\\n    \"_flatten_dict\",\\n    \"flatten_dict\",\\n    \"hash_string\",\\n    \"load_json\",\\n    \"BaseMetadataCallbackHandler\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\utils.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.callbacks.wandb_callback import (\\n    WandbCallbackHandler,\\n)\\n\\n__all__ = [\\n    \"WandbCallbackHandler\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\wandb_callback.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.callbacks.whylabs_callback import (\\n    WhyLabsCallbackHandler,\\n)\\n\\n__all__ = [\"WhyLabsCallbackHandler\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\whylabs_callback.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"**Callback handlers** allow listening to events in LangChain.\\n\\n**Class hierarchy:**\\n\\n.. code-block::\\n\\n    BaseCallbackHandler --> <name>CallbackHandler  # Example: AimCallbackHandler\\n\"\"\"\\nimport warnings\\nfrom typing import Any\\n\\nfrom langchain_core._api import LangChainDeprecationWarning\\nfrom langchain_core.callbacks import (\\n    StdOutCallbackHandler,\\n    StreamingStdOutCallbackHandler,\\n)\\nfrom langchain_core.tracers.context import (\\n    collect_runs,\\n    tracing_enabled,\\n    tracing_v2_enabled,\\n)\\nfrom langchain_core.tracers.langchain import LangChainTracer\\n\\nfrom langchain.callbacks.file import FileCallbackHandler\\nfrom langchain.callbacks.streaming_aiter import AsyncIteratorCallbackHandler\\nfrom langchain.callbacks.streaming_stdout_final_only import (\\n    FinalStreamingStdOutCallbackHandler,\\n)\\nfrom langchain.utils.interactive_env import is_interactive_env\\n\\n\\ndef __getattr__(name: str) -> Any:\\n    from langchain_community import callbacks\\n\\n    # If not in interactive env, raise warning.\\n    if not is_interactive_env():\\n        warnings.warn(\\n            \"Importing this callback from langchain is deprecated. Importing it from \"\\n            \"langchain will no longer be supported as of langchain==0.2.0. \"\\n            \"Please import from langchain-community instead:\\\\n\\\\n\"\\n            f\"`from langchain_community.callbacks import {name}`.\\\\n\\\\n\"\\n            \"To install langchain-community run `pip install -U langchain-community`.\",\\n            category=LangChainDeprecationWarning,\\n        )\\n\\n    return getattr(callbacks, name)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='__all__ = [\\n    \"AimCallbackHandler\",\\n    \"ArgillaCallbackHandler\",\\n    \"ArizeCallbackHandler\",\\n    \"PromptLayerCallbackHandler\",\\n    \"ArthurCallbackHandler\",\\n    \"ClearMLCallbackHandler\",\\n    \"CometCallbackHandler\",\\n    \"ContextCallbackHandler\",\\n    \"FileCallbackHandler\",\\n    \"HumanApprovalCallbackHandler\",\\n    \"InfinoCallbackHandler\",\\n    \"MlflowCallbackHandler\",\\n    \"LLMonitorCallbackHandler\",\\n    \"OpenAICallbackHandler\",\\n    \"StdOutCallbackHandler\",\\n    \"AsyncIteratorCallbackHandler\",\\n    \"StreamingStdOutCallbackHandler\",\\n    \"FinalStreamingStdOutCallbackHandler\",\\n    \"LLMThoughtLabeler\",\\n    \"LangChainTracer\",\\n    \"StreamlitCallbackHandler\",\\n    \"WandbCallbackHandler\",\\n    \"WhyLabsCallbackHandler\",\\n    \"get_openai_callback\",\\n    \"tracing_enabled\",\\n    \"tracing_v2_enabled\",\\n    \"collect_runs\",\\n    \"wandb_tracing_enabled\",\\n    \"FlyteCallbackHandler\",\\n    \"SageMakerCallbackHandler\",\\n    \"LabelStudioCallbackHandler\",\\n    \"TrubricsCallbackHandler\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.callbacks.streamlit.mutable_expander import (\\n    ChildRecord,\\n    ChildType,\\n    MutableExpander,\\n)\\n\\n__all__ = [\"ChildType\", \"ChildRecord\", \"MutableExpander\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\streamlit\\\\mutable_expander.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.callbacks.streamlit.streamlit_callback_handler import (\\n    CHECKMARK_EMOJI,\\n    EXCEPTION_EMOJI,\\n    HISTORY_EMOJI,\\n    THINKING_EMOJI,\\n    LLMThought,\\n    LLMThoughtLabeler,\\n    LLMThoughtState,\\n    StreamlitCallbackHandler,\\n    ToolRecord,\\n)\\n\\n__all__ = [\\n    \"CHECKMARK_EMOJI\",\\n    \"THINKING_EMOJI\",\\n    \"HISTORY_EMOJI\",\\n    \"EXCEPTION_EMOJI\",\\n    \"LLMThoughtState\",\\n    \"ToolRecord\",\\n    \"LLMThoughtLabeler\",\\n    \"LLMThought\",\\n    \"StreamlitCallbackHandler\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\streamlit\\\\streamlit_callback_handler.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from __future__ import annotations\\n\\nfrom typing import TYPE_CHECKING, Optional\\n\\nfrom langchain.callbacks.base import BaseCallbackHandler\\nfrom langchain.callbacks.streamlit.streamlit_callback_handler import (\\n    LLMThoughtLabeler as LLMThoughtLabeler,\\n)\\nfrom langchain.callbacks.streamlit.streamlit_callback_handler import (\\n    StreamlitCallbackHandler as _InternalStreamlitCallbackHandler,\\n)\\n\\nif TYPE_CHECKING:\\n    from streamlit.delta_generator import DeltaGenerator\\n\\n\\ndef StreamlitCallbackHandler(\\n    parent_container: DeltaGenerator,\\n    *,\\n    max_thought_containers: int = 4,\\n    expand_new_thoughts: bool = True,\\n    collapse_completed_thoughts: bool = True,\\n    thought_labeler: Optional[LLMThoughtLabeler] = None,\\n) -> BaseCallbackHandler:\\n    \"\"\"Callback Handler that writes to a Streamlit app.\\n\\n    This CallbackHandler is geared towards\\n    use with a LangChain Agent; it displays the Agent\\'s LLM and tool-usage \"thoughts\"\\n    inside a series of Streamlit expanders.\\n\\n    Parameters\\n    ----------\\n    parent_container\\n        The `st.container` that will contain all the Streamlit elements that the\\n        Handler creates.\\n    max_thought_containers\\n        The max number of completed LLM thought containers to show at once. When this\\n        threshold is reached, a new thought will cause the oldest thoughts to be\\n        collapsed into a \"History\" expander. Defaults to 4.\\n    expand_new_thoughts\\n        Each LLM \"thought\" gets its own `st.expander`. This param controls whether that\\n        expander is expanded by default. Defaults to True.\\n    collapse_completed_thoughts\\n        If True, LLM thought expanders will be collapsed when completed.\\n        Defaults to True.\\n    thought_labeler\\n        An optional custom LLMThoughtLabeler instance. If unspecified, the handler\\n        will use the default thought labeling logic. Defaults to None.\\n\\n    Returns\\n    -------\\n    A new StreamlitCallbackHandler instance.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\streamlit\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Note that this is an \"auto-updating\" API: if the installed version of Streamlit\\n    has a more recent StreamlitCallbackHandler implementation, an instance of that class\\n    will be used.\\n\\n    \"\"\"\\n    # If we\\'re using a version of Streamlit that implements StreamlitCallbackHandler,\\n    # delegate to it instead of using our built-in handler. The official handler is\\n    # guaranteed to support the same set of kwargs.\\n    try:\\n        from streamlit.external.langchain import (\\n            StreamlitCallbackHandler as OfficialStreamlitCallbackHandler,  # type: ignore # noqa: 501\\n        )\\n\\n        return OfficialStreamlitCallbackHandler(\\n            parent_container,\\n            max_thought_containers=max_thought_containers,\\n            expand_new_thoughts=expand_new_thoughts,\\n            collapse_completed_thoughts=collapse_completed_thoughts,\\n            thought_labeler=thought_labeler,\\n        )\\n    except ImportError:\\n        return _InternalStreamlitCallbackHandler(\\n            parent_container,\\n            max_thought_containers=max_thought_containers,\\n            expand_new_thoughts=expand_new_thoughts,\\n            collapse_completed_thoughts=collapse_completed_thoughts,\\n            thought_labeler=thought_labeler,\\n        )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\streamlit\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Base interfaces for tracing runs.\"\"\"\\n\\nfrom langchain_core.tracers.base import BaseTracer, TracerException\\n\\n__all__ = [\"BaseTracer\", \"TracerException\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\tracers\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.callbacks.tracers.comet import (\\n    CometTracer,\\n    import_comet_llm_api,\\n)\\n\\n__all__ = [\"import_comet_llm_api\", \"CometTracer\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\tracers\\\\comet.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"A tracer that runs evaluators over completed runs.\"\"\"\\nfrom langchain_core.tracers.evaluation import (\\n    EvaluatorCallbackHandler,\\n    wait_for_all_evaluators,\\n)\\n\\n__all__ = [\"wait_for_all_evaluators\", \"EvaluatorCallbackHandler\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\tracers\\\\evaluation.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"A Tracer implementation that records to LangChain endpoint.\"\"\"\\n\\nfrom langchain_core.tracers.langchain import (\\n    LangChainTracer,\\n    wait_for_all_tracers,\\n)\\n\\n__all__ = [\"LangChainTracer\", \"wait_for_all_tracers\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\tracers\\\\langchain.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.tracers.langchain_v1 import LangChainTracerV1\\n\\n__all__ = [\"LangChainTracerV1\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\tracers\\\\langchain_v1.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='__all__ = [\"LoggingCallbackHandler\"]\\n\\nimport logging\\nfrom typing import Any, Optional\\nfrom uuid import UUID\\n\\nfrom langchain_core.exceptions import TracerException\\nfrom langchain_core.tracers.stdout import FunctionCallbackHandler\\nfrom langchain_core.utils.input import get_bolded_text, get_colored_text\\n\\n\\nclass LoggingCallbackHandler(FunctionCallbackHandler):\\n    \"\"\"Tracer that logs via the input Logger.\"\"\"\\n\\n    name: str = \"logging_callback_handler\"\\n\\n    def __init__(\\n        self,\\n        logger: logging.Logger,\\n        log_level: int = logging.INFO,\\n        extra: Optional[dict] = None,\\n        **kwargs: Any,\\n    ) -> None:\\n        log_method = getattr(logger, logging.getLevelName(level=log_level).lower())\\n\\n        def callback(text: str) -> None:\\n            log_method(text, extra=extra)\\n\\n        super().__init__(function=callback, **kwargs)\\n\\n    def on_text(\\n        self,\\n        text: str,\\n        *,\\n        run_id: UUID,\\n        parent_run_id: Optional[UUID] = None,  # noqa: ARG002\\n        **kwargs: Any,  # noqa: ARG002\\n    ) -> None:\\n        try:\\n            crumbs_str = f\"[{self.get_breadcrumbs(run=self._get_run(run_id=run_id))}] \"\\n        except TracerException:\\n            crumbs_str = \"\"\\n        self.function_callback(\\n            f\\'{get_colored_text(\"[text]\", color=\"blue\")}\\'\\n            f\\' {get_bolded_text(f\"{crumbs_str}New text:\")}\\\\n{text}\\'\\n        )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\tracers\\\\logging.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.tracers.log_stream import (\\n    LogEntry,\\n    LogStreamCallbackHandler,\\n    RunLog,\\n    RunLogPatch,\\n    RunState,\\n)\\n\\n__all__ = [\"LogEntry\", \"RunState\", \"RunLog\", \"RunLogPatch\", \"LogStreamCallbackHandler\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\tracers\\\\log_stream.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.tracers.root_listeners import RootListenersTracer\\n\\n__all__ = [\"RootListenersTracer\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\tracers\\\\root_listeners.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.tracers.run_collector import RunCollectorCallbackHandler\\n\\n__all__ = [\"RunCollectorCallbackHandler\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\tracers\\\\run_collector.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.tracers.schemas import (\\n    BaseRun,\\n    ChainRun,\\n    LLMRun,\\n    Run,\\n    RunTypeEnum,\\n    ToolRun,\\n    TracerSession,\\n    TracerSessionBase,\\n    TracerSessionV1,\\n    TracerSessionV1Base,\\n    TracerSessionV1Create,\\n)\\n\\n__all__ = [\\n    \"BaseRun\",\\n    \"ChainRun\",\\n    \"LLMRun\",\\n    \"Run\",\\n    \"RunTypeEnum\",\\n    \"ToolRun\",\\n    \"TracerSession\",\\n    \"TracerSessionBase\",\\n    \"TracerSessionV1\",\\n    \"TracerSessionV1Base\",\\n    \"TracerSessionV1Create\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\tracers\\\\schemas.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.tracers.stdout import (\\n    ConsoleCallbackHandler,\\n    FunctionCallbackHandler,\\n)\\n\\n__all__ = [\"FunctionCallbackHandler\", \"ConsoleCallbackHandler\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\tracers\\\\stdout.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.callbacks.tracers.wandb import (\\n    PRINT_WARNINGS,\\n    RunProcessor,\\n    WandbRunArgs,\\n    WandbTracer,\\n)\\n\\n__all__ = [\\n    \"PRINT_WARNINGS\",\\n    \"RunProcessor\",\\n    \"WandbRunArgs\",\\n    \"WandbTracer\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\tracers\\\\wandb.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Tracers that record execution of LangChain runs.\"\"\"\\n\\nfrom langchain_core.tracers.langchain import LangChainTracer\\nfrom langchain_core.tracers.langchain_v1 import LangChainTracerV1\\nfrom langchain_core.tracers.stdout import (\\n    ConsoleCallbackHandler,\\n    FunctionCallbackHandler,\\n)\\n\\nfrom langchain.callbacks.tracers.logging import LoggingCallbackHandler\\nfrom langchain.callbacks.tracers.wandb import WandbTracer\\n\\n__all__ = [\\n    \"ConsoleCallbackHandler\",\\n    \"FunctionCallbackHandler\",\\n    \"LoggingCallbackHandler\",\\n    \"LangChainTracer\",\\n    \"LangChainTracerV1\",\\n    \"WandbTracer\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\tracers\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_verbosity() -> bool:\\n    from langchain.globals import get_verbose\\n\\n    return get_verbose()' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\base.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='class Chain(RunnableSerializable[Dict[str, Any], Dict[str, Any]], ABC):\\n    \"\"\"Abstract base class for creating structured sequences of calls to components.\\n\\n    Chains should be used to encode a sequence of calls to components like\\n    models, document retrievers, other chains, etc., and provide a simple interface\\n    to this sequence.\\n\\n    The Chain interface makes it easy to create apps that are:\\n        - Stateful: add Memory to any Chain to give it state,\\n        - Observable: pass Callbacks to a Chain to execute additional functionality,\\n            like logging, outside the main sequence of component calls,\\n        - Composable: the Chain API is flexible enough that it is easy to combine\\n            Chains with other components, including other Chains.\\n\\n    The main methods exposed by chains are:\\n        - `__call__`: Chains are callable. The `__call__` method is the primary way to\\n            execute a Chain. This takes inputs as a dictionary and returns a\\n            dictionary output.\\n        - `run`: A convenience method that takes inputs as args/kwargs and returns the\\n            output as a string or object. This method can only be used for a subset of\\n            chains and cannot return as rich of an output as `__call__`.\\n    \"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\base.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='memory: Optional[BaseMemory] = None\\n    \"\"\"Optional memory object. Defaults to None.\\n    Memory is a class that gets called at the start\\n    and at the end of every chain. At the start, memory loads variables and passes\\n    them along in the chain. At the end, it saves any returned variables.\\n    There are many different types of memory - please see memory docs\\n    for the full catalog.\"\"\"\\n    callbacks: Callbacks = Field(default=None, exclude=True)\\n    \"\"\"Optional list of callback handlers (or callback manager). Defaults to None.\\n    Callback handlers are called throughout the lifecycle of a call to a chain,\\n    starting with on_chain_start, ending with on_chain_end or on_chain_error.\\n    Each custom chain can optionally call additional callback methods, see Callback docs\\n    for full details.\"\"\"\\n    verbose: bool = Field(default_factory=_get_verbosity)\\n    \"\"\"Whether or not run in verbose mode. In verbose mode, some intermediate logs\\n    will be printed to the console. Defaults to the global `verbose` value,\\n    accessible via `langchain.globals.get_verbose()`.\"\"\"\\n    tags: Optional[List[str]] = None\\n    \"\"\"Optional list of tags associated with the chain. Defaults to None.\\n    These tags will be associated with each call to this chain,\\n    and passed as arguments to the handlers defined in `callbacks`.\\n    You can use these to eg identify a specific instance of a chain with its use case.\\n    \"\"\"\\n    metadata: Optional[Dict[str, Any]] = None\\n    \"\"\"Optional metadata associated with the chain. Defaults to None.\\n    This metadata will be associated with each call to this chain,\\n    and passed as arguments to the handlers defined in `callbacks`.\\n    You can use these to eg identify a specific instance of a chain with its use case.\\n    \"\"\"\\n    callback_manager: Optional[BaseCallbackManager] = Field(default=None, exclude=True)\\n    \"\"\"[DEPRECATED] Use `callbacks` instead.\"\"\"\\n\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n\\n        arbitrary_types_allowed = True' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\base.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def get_input_schema(\\n        self, config: Optional[RunnableConfig] = None\\n    ) -> Type[BaseModel]:\\n        # This is correct, but pydantic typings/mypy don\\'t think so.\\n        return create_model(  # type: ignore[call-overload]\\n            \"ChainInput\", **{k: (Any, None) for k in self.input_keys}\\n        )\\n\\n    def get_output_schema(\\n        self, config: Optional[RunnableConfig] = None\\n    ) -> Type[BaseModel]:\\n        # This is correct, but pydantic typings/mypy don\\'t think so.\\n        return create_model(  # type: ignore[call-overload]\\n            \"ChainOutput\", **{k: (Any, None) for k in self.output_keys}\\n        )\\n\\n    def invoke(\\n        self,\\n        input: Dict[str, Any],\\n        config: Optional[RunnableConfig] = None,\\n        **kwargs: Any,\\n    ) -> Dict[str, Any]:\\n        config = ensure_config(config)\\n        callbacks = config.get(\"callbacks\")\\n        tags = config.get(\"tags\")\\n        metadata = config.get(\"metadata\")\\n        run_name = config.get(\"run_name\") or self.get_name()\\n        include_run_info = kwargs.get(\"include_run_info\", False)\\n        return_only_outputs = kwargs.get(\"return_only_outputs\", False)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\base.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='inputs = self.prep_inputs(input)\\n        callback_manager = CallbackManager.configure(\\n            callbacks,\\n            self.callbacks,\\n            self.verbose,\\n            tags,\\n            self.tags,\\n            metadata,\\n            self.metadata,\\n        )\\n        new_arg_supported = inspect.signature(self._call).parameters.get(\"run_manager\")\\n        run_manager = callback_manager.on_chain_start(\\n            dumpd(self),\\n            inputs,\\n            name=run_name,\\n        )\\n        try:\\n            outputs = (\\n                self._call(inputs, run_manager=run_manager)\\n                if new_arg_supported\\n                else self._call(inputs)\\n            )\\n        except BaseException as e:\\n            run_manager.on_chain_error(e)\\n            raise e\\n        run_manager.on_chain_end(outputs)\\n        final_outputs: Dict[str, Any] = self.prep_outputs(\\n            inputs, outputs, return_only_outputs\\n        )\\n        if include_run_info:\\n            final_outputs[RUN_KEY] = RunInfo(run_id=run_manager.run_id)\\n        return final_outputs\\n\\n    async def ainvoke(\\n        self,\\n        input: Dict[str, Any],\\n        config: Optional[RunnableConfig] = None,\\n        **kwargs: Any,\\n    ) -> Dict[str, Any]:\\n        config = ensure_config(config)\\n        callbacks = config.get(\"callbacks\")\\n        tags = config.get(\"tags\")\\n        metadata = config.get(\"metadata\")\\n        run_name = config.get(\"run_name\") or self.get_name()\\n        include_run_info = kwargs.get(\"include_run_info\", False)\\n        return_only_outputs = kwargs.get(\"return_only_outputs\", False)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\base.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='inputs = self.prep_inputs(input)\\n        callback_manager = AsyncCallbackManager.configure(\\n            callbacks,\\n            self.callbacks,\\n            self.verbose,\\n            tags,\\n            self.tags,\\n            metadata,\\n            self.metadata,\\n        )\\n        new_arg_supported = inspect.signature(self._acall).parameters.get(\"run_manager\")\\n        run_manager = await callback_manager.on_chain_start(\\n            dumpd(self),\\n            inputs,\\n            name=run_name,\\n        )\\n        try:\\n            outputs = (\\n                await self._acall(inputs, run_manager=run_manager)\\n                if new_arg_supported\\n                else await self._acall(inputs)\\n            )\\n        except BaseException as e:\\n            await run_manager.on_chain_error(e)\\n            raise e\\n        await run_manager.on_chain_end(outputs)\\n        final_outputs: Dict[str, Any] = self.prep_outputs(\\n            inputs, outputs, return_only_outputs\\n        )\\n        if include_run_info:\\n            final_outputs[RUN_KEY] = RunInfo(run_id=run_manager.run_id)\\n        return final_outputs\\n\\n    @property\\n    def _chain_type(self) -> str:\\n        raise NotImplementedError(\"Saving not supported for this chain type.\")\\n\\n    @root_validator()\\n    def raise_callback_manager_deprecation(cls, values: Dict) -> Dict:\\n        \"\"\"Raise deprecation warning if callback_manager is used.\"\"\"\\n        if values.get(\"callback_manager\") is not None:\\n            if values.get(\"callbacks\") is not None:\\n                raise ValueError(\\n                    \"Cannot specify both callback_manager and callbacks. \"\\n                    \"callback_manager is deprecated, callbacks is the preferred \"\\n                    \"parameter to pass in.\"\\n                )\\n            warnings.warn(\\n                \"callback_manager is deprecated. Please use callbacks instead.\",\\n                DeprecationWarning,\\n            )\\n            values[\"callbacks\"] = values.pop(\"callback_manager\", None)\\n        return values' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\base.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@validator(\"verbose\", pre=True, always=True)\\n    def set_verbose(cls, verbose: Optional[bool]) -> bool:\\n        \"\"\"Set the chain verbosity.\\n\\n        Defaults to the global setting if not specified by the user.\\n        \"\"\"\\n        if verbose is None:\\n            return _get_verbosity()\\n        else:\\n            return verbose\\n\\n    @property\\n    @abstractmethod\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Keys expected to be in the chain input.\"\"\"\\n\\n    @property\\n    @abstractmethod\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Keys expected to be in the chain output.\"\"\"\\n\\n    def _validate_inputs(self, inputs: Dict[str, Any]) -> None:\\n        \"\"\"Check that all inputs are present.\"\"\"\\n        missing_keys = set(self.input_keys).difference(inputs)\\n        if missing_keys:\\n            raise ValueError(f\"Missing some input keys: {missing_keys}\")\\n\\n    def _validate_outputs(self, outputs: Dict[str, Any]) -> None:\\n        missing_keys = set(self.output_keys).difference(outputs)\\n        if missing_keys:\\n            raise ValueError(f\"Missing some output keys: {missing_keys}\")\\n\\n    @abstractmethod\\n    def _call(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        \"\"\"Execute the chain.\\n\\n        This is a private method that is not user-facing. It is only called within\\n            `Chain.__call__`, which is the user-facing wrapper method that handles\\n            callbacks configuration and some input/output processing.\\n\\n        Args:\\n            inputs: A dict of named inputs to the chain. Assumed to contain all inputs\\n                specified in `Chain.input_keys`, including any inputs added by memory.\\n            run_manager: The callbacks manager that contains the callback handlers for\\n                this run of the chain.\\n\\n        Returns:\\n            A dict of named outputs. Should contain all outputs specified in\\n                `Chain.output_keys`.\\n        \"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\base.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='async def _acall(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        \"\"\"Asynchronously execute the chain.\\n\\n        This is a private method that is not user-facing. It is only called within\\n            `Chain.acall`, which is the user-facing wrapper method that handles\\n            callbacks configuration and some input/output processing.\\n\\n        Args:\\n            inputs: A dict of named inputs to the chain. Assumed to contain all inputs\\n                specified in `Chain.input_keys`, including any inputs added by memory.\\n            run_manager: The callbacks manager that contains the callback handlers for\\n                this run of the chain.\\n\\n        Returns:\\n            A dict of named outputs. Should contain all outputs specified in\\n                `Chain.output_keys`.\\n        \"\"\"\\n        return await run_in_executor(\\n            None, self._call, inputs, run_manager.get_sync() if run_manager else None\\n        )\\n\\n    @deprecated(\"0.1.0\", alternative=\"invoke\", removal=\"0.2.0\")\\n    def __call__(\\n        self,\\n        inputs: Union[Dict[str, Any], Any],\\n        return_only_outputs: bool = False,\\n        callbacks: Callbacks = None,\\n        *,\\n        tags: Optional[List[str]] = None,\\n        metadata: Optional[Dict[str, Any]] = None,\\n        run_name: Optional[str] = None,\\n        include_run_info: bool = False,\\n    ) -> Dict[str, Any]:\\n        \"\"\"Execute the chain.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\base.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Args:\\n            inputs: Dictionary of inputs, or single input if chain expects\\n                only one param. Should contain all inputs specified in\\n                `Chain.input_keys` except for inputs that will be set by the chain\\'s\\n                memory.\\n            return_only_outputs: Whether to return only outputs in the\\n                response. If True, only new keys generated by this chain will be\\n                returned. If False, both input keys and new keys generated by this\\n                chain will be returned. Defaults to False.\\n            callbacks: Callbacks to use for this chain run. These will be called in\\n                addition to callbacks passed to the chain during construction, but only\\n                these runtime callbacks will propagate to calls to other objects.\\n            tags: List of string tags to pass to all callbacks. These will be passed in\\n                addition to tags passed to the chain during construction, but only\\n                these runtime tags will propagate to calls to other objects.\\n            metadata: Optional metadata associated with the chain. Defaults to None\\n            include_run_info: Whether to include run info in the response. Defaults\\n                to False.\\n\\n        Returns:\\n            A dict of named outputs. Should contain all outputs specified in\\n                `Chain.output_keys`.\\n        \"\"\"\\n        config = {\\n            \"callbacks\": callbacks,\\n            \"tags\": tags,\\n            \"metadata\": metadata,\\n            \"run_name\": run_name,\\n        }\\n\\n        return self.invoke(\\n            inputs,\\n            cast(RunnableConfig, {k: v for k, v in config.items() if v is not None}),\\n            return_only_outputs=return_only_outputs,\\n            include_run_info=include_run_info,\\n        )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\base.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@deprecated(\"0.1.0\", alternative=\"ainvoke\", removal=\"0.2.0\")\\n    async def acall(\\n        self,\\n        inputs: Union[Dict[str, Any], Any],\\n        return_only_outputs: bool = False,\\n        callbacks: Callbacks = None,\\n        *,\\n        tags: Optional[List[str]] = None,\\n        metadata: Optional[Dict[str, Any]] = None,\\n        run_name: Optional[str] = None,\\n        include_run_info: bool = False,\\n    ) -> Dict[str, Any]:\\n        \"\"\"Asynchronously execute the chain.\\n\\n        Args:\\n            inputs: Dictionary of inputs, or single input if chain expects\\n                only one param. Should contain all inputs specified in\\n                `Chain.input_keys` except for inputs that will be set by the chain\\'s\\n                memory.\\n            return_only_outputs: Whether to return only outputs in the\\n                response. If True, only new keys generated by this chain will be\\n                returned. If False, both input keys and new keys generated by this\\n                chain will be returned. Defaults to False.\\n            callbacks: Callbacks to use for this chain run. These will be called in\\n                addition to callbacks passed to the chain during construction, but only\\n                these runtime callbacks will propagate to calls to other objects.\\n            tags: List of string tags to pass to all callbacks. These will be passed in\\n                addition to tags passed to the chain during construction, but only\\n                these runtime tags will propagate to calls to other objects.\\n            metadata: Optional metadata associated with the chain. Defaults to None\\n            include_run_info: Whether to include run info in the response. Defaults\\n                to False.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\base.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Returns:\\n            A dict of named outputs. Should contain all outputs specified in\\n                `Chain.output_keys`.\\n        \"\"\"\\n        config = {\\n            \"callbacks\": callbacks,\\n            \"tags\": tags,\\n            \"metadata\": metadata,\\n            \"run_name\": run_name,\\n        }\\n        return await self.ainvoke(\\n            inputs,\\n            cast(RunnableConfig, {k: v for k, v in config.items() if k is not None}),\\n            return_only_outputs=return_only_outputs,\\n            include_run_info=include_run_info,\\n        )\\n\\n    def prep_outputs(\\n        self,\\n        inputs: Dict[str, str],\\n        outputs: Dict[str, str],\\n        return_only_outputs: bool = False,\\n    ) -> Dict[str, str]:\\n        \"\"\"Validate and prepare chain outputs, and save info about this run to memory.\\n\\n        Args:\\n            inputs: Dictionary of chain inputs, including any inputs added by chain\\n                memory.\\n            outputs: Dictionary of initial chain outputs.\\n            return_only_outputs: Whether to only return the chain outputs. If False,\\n                inputs are also added to the final outputs.\\n\\n        Returns:\\n            A dict of the final chain outputs.\\n        \"\"\"\\n        self._validate_outputs(outputs)\\n        if self.memory is not None:\\n            self.memory.save_context(inputs, outputs)\\n        if return_only_outputs:\\n            return outputs\\n        else:\\n            return {**inputs, **outputs}\\n\\n    def prep_inputs(self, inputs: Union[Dict[str, Any], Any]) -> Dict[str, str]:\\n        \"\"\"Validate and prepare chain inputs, including adding inputs from memory.\\n\\n        Args:\\n            inputs: Dictionary of raw inputs, or single input if chain expects\\n                only one param. Should contain all inputs specified in\\n                `Chain.input_keys` except for inputs that will be set by the chain\\'s\\n                memory.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\base.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Returns:\\n            A dictionary of all inputs, including those added by the chain\\'s memory.\\n        \"\"\"\\n        if not isinstance(inputs, dict):\\n            _input_keys = set(self.input_keys)\\n            if self.memory is not None:\\n                # If there are multiple input keys, but some get set by memory so that\\n                # only one is not set, we can still figure out which key it is.\\n                _input_keys = _input_keys.difference(self.memory.memory_variables)\\n            if len(_input_keys) != 1:\\n                raise ValueError(\\n                    f\"A single string input was passed in, but this chain expects \"\\n                    f\"multiple inputs ({_input_keys}). When a chain expects \"\\n                    f\"multiple inputs, please call it by passing in a dictionary, \"\\n                    \"eg `chain({\\'foo\\': 1, \\'bar\\': 2})`\"\\n                )\\n            inputs = {list(_input_keys)[0]: inputs}\\n        if self.memory is not None:\\n            external_context = self.memory.load_memory_variables(inputs)\\n            inputs = dict(inputs, **external_context)\\n        self._validate_inputs(inputs)\\n        return inputs\\n\\n    @property\\n    def _run_output_key(self) -> str:\\n        if len(self.output_keys) != 1:\\n            raise ValueError(\\n                f\"`run` not supported when there is not exactly \"\\n                f\"one output key. Got {self.output_keys}.\"\\n            )\\n        return self.output_keys[0]\\n\\n    @deprecated(\"0.1.0\", alternative=\"invoke\", removal=\"0.2.0\")\\n    def run(\\n        self,\\n        *args: Any,\\n        callbacks: Callbacks = None,\\n        tags: Optional[List[str]] = None,\\n        metadata: Optional[Dict[str, Any]] = None,\\n        **kwargs: Any,\\n    ) -> Any:\\n        \"\"\"Convenience method for executing chain.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\base.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='The main difference between this method and `Chain.__call__` is that this\\n        method expects inputs to be passed directly in as positional arguments or\\n        keyword arguments, whereas `Chain.__call__` expects a single input dictionary\\n        with all the inputs\\n\\n        Args:\\n            *args: If the chain expects a single input, it can be passed in as the\\n                sole positional argument.\\n            callbacks: Callbacks to use for this chain run. These will be called in\\n                addition to callbacks passed to the chain during construction, but only\\n                these runtime callbacks will propagate to calls to other objects.\\n            tags: List of string tags to pass to all callbacks. These will be passed in\\n                addition to tags passed to the chain during construction, but only\\n                these runtime tags will propagate to calls to other objects.\\n            **kwargs: If the chain expects multiple inputs, they can be passed in\\n                directly as keyword arguments.\\n\\n        Returns:\\n            The chain output.\\n\\n        Example:\\n            .. code-block:: python\\n\\n                # Suppose we have a single-input chain that takes a \\'question\\' string:\\n                chain.run(\"What\\'s the temperature in Boise, Idaho?\")\\n                # -> \"The temperature in Boise is...\"\\n\\n                # Suppose we have a multi-input chain that takes a \\'question\\' string\\n                # and \\'context\\' string:\\n                question = \"What\\'s the temperature in Boise, Idaho?\"\\n                context = \"Weather report for Boise, Idaho on 07/03/23...\"\\n                chain.run(question=question, context=context)\\n                # -> \"The temperature in Boise is...\"\\n        \"\"\"\\n        # Run at start to make sure this is possible/defined\\n        _output_key = self._run_output_key' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\base.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='if args and not kwargs:\\n            if len(args) != 1:\\n                raise ValueError(\"`run` supports only one positional argument.\")\\n            return self(args[0], callbacks=callbacks, tags=tags, metadata=metadata)[\\n                _output_key\\n            ]\\n\\n        if kwargs and not args:\\n            return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[\\n                _output_key\\n            ]\\n\\n        if not kwargs and not args:\\n            raise ValueError(\\n                \"`run` supported with either positional arguments or keyword arguments,\"\\n                \" but none were provided.\"\\n            )\\n        else:\\n            raise ValueError(\\n                f\"`run` supported with either positional arguments or keyword arguments\"\\n                f\" but not both. Got args: {args} and kwargs: {kwargs}.\"\\n            )\\n\\n    @deprecated(\"0.1.0\", alternative=\"ainvoke\", removal=\"0.2.0\")\\n    async def arun(\\n        self,\\n        *args: Any,\\n        callbacks: Callbacks = None,\\n        tags: Optional[List[str]] = None,\\n        metadata: Optional[Dict[str, Any]] = None,\\n        **kwargs: Any,\\n    ) -> Any:\\n        \"\"\"Convenience method for executing chain.\\n\\n        The main difference between this method and `Chain.__call__` is that this\\n        method expects inputs to be passed directly in as positional arguments or\\n        keyword arguments, whereas `Chain.__call__` expects a single input dictionary\\n        with all the inputs' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\base.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Args:\\n            *args: If the chain expects a single input, it can be passed in as the\\n                sole positional argument.\\n            callbacks: Callbacks to use for this chain run. These will be called in\\n                addition to callbacks passed to the chain during construction, but only\\n                these runtime callbacks will propagate to calls to other objects.\\n            tags: List of string tags to pass to all callbacks. These will be passed in\\n                addition to tags passed to the chain during construction, but only\\n                these runtime tags will propagate to calls to other objects.\\n            **kwargs: If the chain expects multiple inputs, they can be passed in\\n                directly as keyword arguments.\\n\\n        Returns:\\n            The chain output.\\n\\n        Example:\\n            .. code-block:: python\\n\\n                # Suppose we have a single-input chain that takes a \\'question\\' string:\\n                await chain.arun(\"What\\'s the temperature in Boise, Idaho?\")\\n                # -> \"The temperature in Boise is...\"\\n\\n                # Suppose we have a multi-input chain that takes a \\'question\\' string\\n                # and \\'context\\' string:\\n                question = \"What\\'s the temperature in Boise, Idaho?\"\\n                context = \"Weather report for Boise, Idaho on 07/03/23...\"\\n                await chain.arun(question=question, context=context)\\n                # -> \"The temperature in Boise is...\"\\n        \"\"\"\\n        if len(self.output_keys) != 1:\\n            raise ValueError(\\n                f\"`run` not supported when there is not exactly \"\\n                f\"one output key. Got {self.output_keys}.\"\\n            )\\n        elif args and not kwargs:\\n            if len(args) != 1:\\n                raise ValueError(\"`run` supports only one positional argument.\")\\n            return (\\n                await self.acall(\\n                    args[0], callbacks=callbacks, tags=tags, metadata=metadata\\n                )\\n            )[self.output_keys[0]]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\base.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='if kwargs and not args:\\n            return (\\n                await self.acall(\\n                    kwargs, callbacks=callbacks, tags=tags, metadata=metadata\\n                )\\n            )[self.output_keys[0]]\\n\\n        raise ValueError(\\n            f\"`run` supported with either positional arguments or keyword arguments\"\\n            f\" but not both. Got args: {args} and kwargs: {kwargs}.\"\\n        )\\n\\n    def dict(self, **kwargs: Any) -> Dict:\\n        \"\"\"Dictionary representation of chain.\\n\\n        Expects `Chain._chain_type` property to be implemented and for memory to be\\n            null.\\n\\n        Args:\\n            **kwargs: Keyword arguments passed to default `pydantic.BaseModel.dict`\\n                method.\\n\\n        Returns:\\n            A dictionary representation of the chain.\\n\\n        Example:\\n            .. code-block:: python\\n\\n                chain.dict(exclude_unset=True)\\n                # -> {\"_type\": \"foo\", \"verbose\": False, ...}\\n        \"\"\"\\n        _dict = super().dict(**kwargs)\\n        try:\\n            _dict[\"_type\"] = self._chain_type\\n        except NotImplementedError:\\n            pass\\n        return _dict\\n\\n    def save(self, file_path: Union[Path, str]) -> None:\\n        \"\"\"Save the chain.\\n\\n        Expects `Chain._chain_type` property to be implemented and for memory to be\\n            null.\\n\\n        Args:\\n            file_path: Path to file to save the chain to.\\n\\n        Example:\\n            .. code-block:: python\\n\\n                chain.save(file_path=\"path/chain.yaml\")\\n        \"\"\"\\n        if self.memory is not None:\\n            raise ValueError(\"Saving of memory is not yet supported.\")\\n\\n        # Fetch dictionary to save\\n        chain_dict = self.dict()\\n        if \"_type\" not in chain_dict:\\n            raise NotImplementedError(f\"Chain {self} does not support saving.\")\\n\\n        # Convert file to Path object.\\n        if isinstance(file_path, str):\\n            save_path = Path(file_path)\\n        else:\\n            save_path = file_path' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\base.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='directory_path = save_path.parent\\n        directory_path.mkdir(parents=True, exist_ok=True)\\n\\n        if save_path.suffix == \".json\":\\n            with open(file_path, \"w\") as f:\\n                json.dump(chain_dict, f, indent=4)\\n        elif save_path.suffix == \".yaml\":\\n            with open(file_path, \"w\") as f:\\n                yaml.dump(chain_dict, f, default_flow_style=False)\\n        else:\\n            raise ValueError(f\"{save_path} must be json or yaml\")\\n\\n    @deprecated(\"0.1.0\", alternative=\"batch\", removal=\"0.2.0\")\\n    def apply(\\n        self, input_list: List[Dict[str, Any]], callbacks: Callbacks = None\\n    ) -> List[Dict[str, str]]:\\n        \"\"\"Call the chain on all inputs in the list.\"\"\"\\n        return [self(inputs, callbacks=callbacks) for inputs in input_list]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\base.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Base interface that all chains should implement.\"\"\"\\nimport inspect\\nimport json\\nimport logging\\nimport warnings\\nfrom abc import ABC, abstractmethod\\nfrom pathlib import Path\\nfrom typing import Any, Dict, List, Optional, Type, Union, cast\\n\\nimport yaml\\nfrom langchain_core._api import deprecated\\nfrom langchain_core.callbacks import (\\n    AsyncCallbackManager,\\n    AsyncCallbackManagerForChainRun,\\n    BaseCallbackManager,\\n    CallbackManager,\\n    CallbackManagerForChainRun,\\n    Callbacks,\\n)\\nfrom langchain_core.load.dump import dumpd\\nfrom langchain_core.memory import BaseMemory\\nfrom langchain_core.outputs import RunInfo\\nfrom langchain_core.pydantic_v1 import (\\n    BaseModel,\\n    Field,\\n    create_model,\\n    root_validator,\\n    validator,\\n)\\nfrom langchain_core.runnables import (\\n    RunnableConfig,\\n    RunnableSerializable,\\n    ensure_config,\\n    run_in_executor,\\n)\\n\\nfrom langchain.schema import RUN_KEY\\n\\nlogger = logging.getLogger(__name__)\\n\\n\\n# Code for: def _get_verbosity() -> bool:\\n\\n\\n# Code for: class Chain(RunnableSerializable[Dict[str, Any], Dict[str, Any]], ABC):' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\base.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from typing import List\\n\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.prompts.few_shot import FewShotPromptTemplate\\nfrom langchain_core.prompts.prompt import PromptTemplate\\n\\nfrom langchain.chains.llm import LLMChain\\n\\nTEST_GEN_TEMPLATE_SUFFIX = \"Add another example.\"\\n\\n\\ndef generate_example(\\n    examples: List[dict], llm: BaseLanguageModel, prompt_template: PromptTemplate\\n) -> str:\\n    \"\"\"Return another example given a list of examples for a prompt.\"\"\"\\n    prompt = FewShotPromptTemplate(\\n        examples=examples,\\n        suffix=TEST_GEN_TEMPLATE_SUFFIX,\\n        input_variables=[],\\n        example_prompt=prompt_template,\\n    )\\n    chain = LLMChain(llm=llm, prompt=prompt)\\n    return chain.predict()' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\example_generator.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from __future__ import annotations\\n\\nfrom langchain_core.language_models import LanguageModelLike\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.prompts import BasePromptTemplate\\nfrom langchain_core.retrievers import RetrieverLike, RetrieverOutputLike\\nfrom langchain_core.runnables import RunnableBranch\\n\\n\\ndef create_history_aware_retriever(\\n    llm: LanguageModelLike,\\n    retriever: RetrieverLike,\\n    prompt: BasePromptTemplate,\\n) -> RetrieverOutputLike:\\n    \"\"\"Create a chain that takes conversation history and returns documents.\\n\\n    If there is no `chat_history`, then the `input` is just passed directly to the\\n    retriever. If there is `chat_history`, then the prompt and LLM will be used\\n    to generate a search query. That search query is then passed to the retriever.\\n\\n    Args:\\n        llm: Language model to use for generating a search term given chat history\\n        retriever: RetrieverLike object that takes a string as input and outputs\\n            a list of Documents.\\n        prompt: The prompt used to generate the search query for the retriever.\\n\\n    Returns:\\n        An LCEL Runnable. The runnable input must take in `input`, and if there\\n        is chat history should take it in the form of `chat_history`.\\n        The Runnable output is a list of Documents\\n\\n    Example:\\n        .. code-block:: python\\n\\n            # pip install -U langchain langchain-community\\n\\n            from langchain_community.chat_models import ChatOpenAI\\n            from langchain.chains import create_history_aware_retriever\\n            from langchain import hub\\n\\n            rephrase_prompt = hub.pull(\"langchain-ai/chat-langchain-rephrase\")\\n            llm = ChatOpenAI()\\n            retriever = ...\\n            chat_retriever_chain = create_history_aware_retriever(\\n                llm, retriever, rephrase_prompt\\n            )\\n\\n            chain.invoke({\"input\": \"...\", \"chat_history\": })' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\history_aware_retriever.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"\\n    if \"input\" not in prompt.input_variables:\\n        raise ValueError(\\n            \"Expected `input` to be a prompt variable, \"\\n            f\"but got {prompt.input_variables}\"\\n        )\\n\\n    retrieve_documents: RetrieverOutputLike = RunnableBranch(\\n        (\\n            # Both empty string and empty list evaluate to False\\n            lambda x: not x.get(\"chat_history\", False),\\n            # If no chat history, then we just pass input to retriever\\n            (lambda x: x[\"input\"]) | retriever,\\n        ),\\n        # If chat history, then we pass inputs to LLM chain, then to retriever\\n        prompt | llm | StrOutputParser() | retriever,\\n    ).with_config(run_name=\"chat_retriever_chain\")\\n    return retrieve_documents' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\history_aware_retriever.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Chain that just formats a prompt and calls an LLM.\"\"\"\\nfrom __future__ import annotations\\n\\nimport warnings\\nfrom typing import Any, Dict, List, Optional, Sequence, Tuple, Union, cast\\n\\nfrom langchain_core.callbacks import (\\n    AsyncCallbackManager,\\n    AsyncCallbackManagerForChainRun,\\n    CallbackManager,\\n    CallbackManagerForChainRun,\\n    Callbacks,\\n)\\nfrom langchain_core.language_models import (\\n    BaseLanguageModel,\\n    LanguageModelInput,\\n)\\nfrom langchain_core.load.dump import dumpd\\nfrom langchain_core.messages import BaseMessage\\nfrom langchain_core.output_parsers import BaseLLMOutputParser, StrOutputParser\\nfrom langchain_core.outputs import ChatGeneration, Generation, LLMResult\\nfrom langchain_core.prompt_values import PromptValue\\nfrom langchain_core.prompts import BasePromptTemplate, PromptTemplate\\nfrom langchain_core.pydantic_v1 import Extra, Field\\nfrom langchain_core.runnables import (\\n    Runnable,\\n    RunnableBinding,\\n    RunnableBranch,\\n    RunnableWithFallbacks,\\n)\\nfrom langchain_core.runnables.configurable import DynamicRunnable\\nfrom langchain_core.utils.input import get_colored_text\\n\\nfrom langchain.chains.base import Chain\\n\\n\\nclass LLMChain(Chain):\\n    \"\"\"Chain to run queries against LLMs.\\n\\n    Example:\\n        .. code-block:: python\\n\\n            from langchain.chains import LLMChain\\n            from langchain_community.llms import OpenAI\\n            from langchain_core.prompts import PromptTemplate\\n            prompt_template = \"Tell me a {adjective} joke\"\\n            prompt = PromptTemplate(\\n                input_variables=[\"adjective\"], template=prompt_template\\n            )\\n            llm = LLMChain(llm=OpenAI(), prompt=prompt)\\n    \"\"\"\\n\\n    @classmethod\\n    def is_lc_serializable(self) -> bool:\\n        return True' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\llm.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='prompt: BasePromptTemplate\\n    \"\"\"Prompt object to use.\"\"\"\\n    llm: Union[\\n        Runnable[LanguageModelInput, str], Runnable[LanguageModelInput, BaseMessage]\\n    ]\\n    \"\"\"Language model to call.\"\"\"\\n    output_key: str = \"text\"  #: :meta private:\\n    output_parser: BaseLLMOutputParser = Field(default_factory=StrOutputParser)\\n    \"\"\"Output parser to use.\\n    Defaults to one that takes the most likely string but does not change it \\n    otherwise.\"\"\"\\n    return_final_only: bool = True\\n    \"\"\"Whether to return only the final parsed result. Defaults to True.\\n    If false, will return a bunch of extra information about the generation.\"\"\"\\n    llm_kwargs: dict = Field(default_factory=dict)\\n\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n\\n        extra = Extra.forbid\\n        arbitrary_types_allowed = True\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Will be whatever keys the prompt expects.\\n\\n        :meta private:\\n        \"\"\"\\n        return self.prompt.input_variables\\n\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Will always return text key.\\n\\n        :meta private:\\n        \"\"\"\\n        if self.return_final_only:\\n            return [self.output_key]\\n        else:\\n            return [self.output_key, \"full_generation\"]\\n\\n    def _call(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, str]:\\n        response = self.generate([inputs], run_manager=run_manager)\\n        return self.create_outputs(response)[0]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\llm.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def generate(\\n        self,\\n        input_list: List[Dict[str, Any]],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> LLMResult:\\n        \"\"\"Generate LLM result from inputs.\"\"\"\\n        prompts, stop = self.prep_prompts(input_list, run_manager=run_manager)\\n        callbacks = run_manager.get_child() if run_manager else None\\n        if isinstance(self.llm, BaseLanguageModel):\\n            return self.llm.generate_prompt(\\n                prompts,\\n                stop,\\n                callbacks=callbacks,\\n                **self.llm_kwargs,\\n            )\\n        else:\\n            results = self.llm.bind(stop=stop, **self.llm_kwargs).batch(\\n                cast(List, prompts), {\"callbacks\": callbacks}\\n            )\\n            generations: List[List[Generation]] = []\\n            for res in results:\\n                if isinstance(res, BaseMessage):\\n                    generations.append([ChatGeneration(message=res)])\\n                else:\\n                    generations.append([Generation(text=res)])\\n            return LLMResult(generations=generations)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\llm.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='async def agenerate(\\n        self,\\n        input_list: List[Dict[str, Any]],\\n        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\\n    ) -> LLMResult:\\n        \"\"\"Generate LLM result from inputs.\"\"\"\\n        prompts, stop = await self.aprep_prompts(input_list, run_manager=run_manager)\\n        callbacks = run_manager.get_child() if run_manager else None\\n        if isinstance(self.llm, BaseLanguageModel):\\n            return await self.llm.agenerate_prompt(\\n                prompts,\\n                stop,\\n                callbacks=callbacks,\\n                **self.llm_kwargs,\\n            )\\n        else:\\n            results = await self.llm.bind(stop=stop, **self.llm_kwargs).abatch(\\n                cast(List, prompts), {\"callbacks\": callbacks}\\n            )\\n            generations: List[List[Generation]] = []\\n            for res in results:\\n                if isinstance(res, BaseMessage):\\n                    generations.append([ChatGeneration(message=res)])\\n                else:\\n                    generations.append([Generation(text=res)])\\n            return LLMResult(generations=generations)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\llm.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def prep_prompts(\\n        self,\\n        input_list: List[Dict[str, Any]],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Tuple[List[PromptValue], Optional[List[str]]]:\\n        \"\"\"Prepare prompts from inputs.\"\"\"\\n        stop = None\\n        if len(input_list) == 0:\\n            return [], stop\\n        if \"stop\" in input_list[0]:\\n            stop = input_list[0][\"stop\"]\\n        prompts = []\\n        for inputs in input_list:\\n            selected_inputs = {k: inputs[k] for k in self.prompt.input_variables}\\n            prompt = self.prompt.format_prompt(**selected_inputs)\\n            _colored_text = get_colored_text(prompt.to_string(), \"green\")\\n            _text = \"Prompt after formatting:\\\\n\" + _colored_text\\n            if run_manager:\\n                run_manager.on_text(_text, end=\"\\\\n\", verbose=self.verbose)\\n            if \"stop\" in inputs and inputs[\"stop\"] != stop:\\n                raise ValueError(\\n                    \"If `stop` is present in any inputs, should be present in all.\"\\n                )\\n            prompts.append(prompt)\\n        return prompts, stop' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\llm.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='async def aprep_prompts(\\n        self,\\n        input_list: List[Dict[str, Any]],\\n        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\\n    ) -> Tuple[List[PromptValue], Optional[List[str]]]:\\n        \"\"\"Prepare prompts from inputs.\"\"\"\\n        stop = None\\n        if len(input_list) == 0:\\n            return [], stop\\n        if \"stop\" in input_list[0]:\\n            stop = input_list[0][\"stop\"]\\n        prompts = []\\n        for inputs in input_list:\\n            selected_inputs = {k: inputs[k] for k in self.prompt.input_variables}\\n            prompt = self.prompt.format_prompt(**selected_inputs)\\n            _colored_text = get_colored_text(prompt.to_string(), \"green\")\\n            _text = \"Prompt after formatting:\\\\n\" + _colored_text\\n            if run_manager:\\n                await run_manager.on_text(_text, end=\"\\\\n\", verbose=self.verbose)\\n            if \"stop\" in inputs and inputs[\"stop\"] != stop:\\n                raise ValueError(\\n                    \"If `stop` is present in any inputs, should be present in all.\"\\n                )\\n            prompts.append(prompt)\\n        return prompts, stop\\n\\n    def apply(\\n        self, input_list: List[Dict[str, Any]], callbacks: Callbacks = None\\n    ) -> List[Dict[str, str]]:\\n        \"\"\"Utilize the LLM generate method for speed gains.\"\"\"\\n        callback_manager = CallbackManager.configure(\\n            callbacks, self.callbacks, self.verbose\\n        )\\n        run_manager = callback_manager.on_chain_start(\\n            dumpd(self),\\n            {\"input_list\": input_list},\\n        )\\n        try:\\n            response = self.generate(input_list, run_manager=run_manager)\\n        except BaseException as e:\\n            run_manager.on_chain_error(e)\\n            raise e\\n        outputs = self.create_outputs(response)\\n        run_manager.on_chain_end({\"outputs\": outputs})\\n        return outputs' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\llm.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='async def aapply(\\n        self, input_list: List[Dict[str, Any]], callbacks: Callbacks = None\\n    ) -> List[Dict[str, str]]:\\n        \"\"\"Utilize the LLM generate method for speed gains.\"\"\"\\n        callback_manager = AsyncCallbackManager.configure(\\n            callbacks, self.callbacks, self.verbose\\n        )\\n        run_manager = await callback_manager.on_chain_start(\\n            dumpd(self),\\n            {\"input_list\": input_list},\\n        )\\n        try:\\n            response = await self.agenerate(input_list, run_manager=run_manager)\\n        except BaseException as e:\\n            await run_manager.on_chain_error(e)\\n            raise e\\n        outputs = self.create_outputs(response)\\n        await run_manager.on_chain_end({\"outputs\": outputs})\\n        return outputs\\n\\n    @property\\n    def _run_output_key(self) -> str:\\n        return self.output_key\\n\\n    def create_outputs(self, llm_result: LLMResult) -> List[Dict[str, Any]]:\\n        \"\"\"Create outputs from response.\"\"\"\\n        result = [\\n            # Get the text of the top generated string.\\n            {\\n                self.output_key: self.output_parser.parse_result(generation),\\n                \"full_generation\": generation,\\n            }\\n            for generation in llm_result.generations\\n        ]\\n        if self.return_final_only:\\n            result = [{self.output_key: r[self.output_key]} for r in result]\\n        return result\\n\\n    async def _acall(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\\n    ) -> Dict[str, str]:\\n        response = await self.agenerate([inputs], run_manager=run_manager)\\n        return self.create_outputs(response)[0]\\n\\n    def predict(self, callbacks: Callbacks = None, **kwargs: Any) -> str:\\n        \"\"\"Format prompt with kwargs and pass to LLM.\\n\\n        Args:\\n            callbacks: Callbacks to pass to LLMChain\\n            **kwargs: Keys to pass to prompt template.\\n\\n        Returns:\\n            Completion from LLM.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\llm.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Example:\\n            .. code-block:: python\\n\\n                completion = llm.predict(adjective=\"funny\")\\n        \"\"\"\\n        return self(kwargs, callbacks=callbacks)[self.output_key]\\n\\n    async def apredict(self, callbacks: Callbacks = None, **kwargs: Any) -> str:\\n        \"\"\"Format prompt with kwargs and pass to LLM.\\n\\n        Args:\\n            callbacks: Callbacks to pass to LLMChain\\n            **kwargs: Keys to pass to prompt template.\\n\\n        Returns:\\n            Completion from LLM.\\n\\n        Example:\\n            .. code-block:: python\\n\\n                completion = llm.predict(adjective=\"funny\")\\n        \"\"\"\\n        return (await self.acall(kwargs, callbacks=callbacks))[self.output_key]\\n\\n    def predict_and_parse(\\n        self, callbacks: Callbacks = None, **kwargs: Any\\n    ) -> Union[str, List[str], Dict[str, Any]]:\\n        \"\"\"Call predict and then parse the results.\"\"\"\\n        warnings.warn(\\n            \"The predict_and_parse method is deprecated, \"\\n            \"instead pass an output parser directly to LLMChain.\"\\n        )\\n        result = self.predict(callbacks=callbacks, **kwargs)\\n        if self.prompt.output_parser is not None:\\n            return self.prompt.output_parser.parse(result)\\n        else:\\n            return result\\n\\n    async def apredict_and_parse(\\n        self, callbacks: Callbacks = None, **kwargs: Any\\n    ) -> Union[str, List[str], Dict[str, str]]:\\n        \"\"\"Call apredict and then parse the results.\"\"\"\\n        warnings.warn(\\n            \"The apredict_and_parse method is deprecated, \"\\n            \"instead pass an output parser directly to LLMChain.\"\\n        )\\n        result = await self.apredict(callbacks=callbacks, **kwargs)\\n        if self.prompt.output_parser is not None:\\n            return self.prompt.output_parser.parse(result)\\n        else:\\n            return result' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\llm.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def apply_and_parse(\\n        self, input_list: List[Dict[str, Any]], callbacks: Callbacks = None\\n    ) -> Sequence[Union[str, List[str], Dict[str, str]]]:\\n        \"\"\"Call apply and then parse the results.\"\"\"\\n        warnings.warn(\\n            \"The apply_and_parse method is deprecated, \"\\n            \"instead pass an output parser directly to LLMChain.\"\\n        )\\n        result = self.apply(input_list, callbacks=callbacks)\\n        return self._parse_generation(result)\\n\\n    def _parse_generation(\\n        self, generation: List[Dict[str, str]]\\n    ) -> Sequence[Union[str, List[str], Dict[str, str]]]:\\n        if self.prompt.output_parser is not None:\\n            return [\\n                self.prompt.output_parser.parse(res[self.output_key])\\n                for res in generation\\n            ]\\n        else:\\n            return generation\\n\\n    async def aapply_and_parse(\\n        self, input_list: List[Dict[str, Any]], callbacks: Callbacks = None\\n    ) -> Sequence[Union[str, List[str], Dict[str, str]]]:\\n        \"\"\"Call apply and then parse the results.\"\"\"\\n        warnings.warn(\\n            \"The aapply_and_parse method is deprecated, \"\\n            \"instead pass an output parser directly to LLMChain.\"\\n        )\\n        result = await self.aapply(input_list, callbacks=callbacks)\\n        return self._parse_generation(result)\\n\\n    @property\\n    def _chain_type(self) -> str:\\n        return \"llm_chain\"\\n\\n    @classmethod\\n    def from_string(cls, llm: BaseLanguageModel, template: str) -> LLMChain:\\n        \"\"\"Create LLMChain from LLM and template.\"\"\"\\n        prompt_template = PromptTemplate.from_template(template)\\n        return cls(llm=llm, prompt=prompt_template)\\n\\n    def _get_num_tokens(self, text: str) -> int:\\n        return _get_language_model(self.llm).get_num_tokens(text)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\llm.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_language_model(llm_like: Runnable) -> BaseLanguageModel:\\n    if isinstance(llm_like, BaseLanguageModel):\\n        return llm_like\\n    elif isinstance(llm_like, RunnableBinding):\\n        return _get_language_model(llm_like.bound)\\n    elif isinstance(llm_like, RunnableWithFallbacks):\\n        return _get_language_model(llm_like.runnable)\\n    elif isinstance(llm_like, (RunnableBranch, DynamicRunnable)):\\n        return _get_language_model(llm_like.default)\\n    else:\\n        raise ValueError(\\n            f\"Unable to extract BaseLanguageModel from llm_like object of type \"\\n            f\"{type(llm_like)}\"\\n        )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\llm.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Chain that hits a URL and then uses an LLM to parse results.\"\"\"\\nfrom __future__ import annotations\\n\\nfrom typing import Any, Dict, List, Optional\\n\\nfrom langchain_community.utilities.requests import TextRequestsWrapper\\nfrom langchain_core.callbacks import CallbackManagerForChainRun\\nfrom langchain_core.pydantic_v1 import Extra, Field, root_validator\\n\\nfrom langchain.chains import LLMChain\\nfrom langchain.chains.base import Chain\\n\\nDEFAULT_HEADERS = {\\n    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36\"  # noqa: E501\\n}\\n\\n\\nclass LLMRequestsChain(Chain):\\n    \"\"\"Chain that requests a URL and then uses an LLM to parse results.\\n\\n    **Security Note**: This chain can make GET requests to arbitrary URLs,\\n        including internal URLs.\\n\\n        Control access to who can run this chain and what network access\\n        this chain has.\\n\\n        See https://python.langchain.com/docs/security for more information.\\n    \"\"\"\\n\\n    llm_chain: LLMChain\\n    requests_wrapper: TextRequestsWrapper = Field(\\n        default_factory=lambda: TextRequestsWrapper(headers=DEFAULT_HEADERS),\\n        exclude=True,\\n    )\\n    text_length: int = 8000\\n    requests_key: str = \"requests_result\"  #: :meta private:\\n    input_key: str = \"url\"  #: :meta private:\\n    output_key: str = \"output\"  #: :meta private:\\n\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n\\n        extra = Extra.forbid\\n        arbitrary_types_allowed = True\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Will be whatever keys the prompt expects.\\n\\n        :meta private:\\n        \"\"\"\\n        return [self.input_key]\\n\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Will always return text key.\\n\\n        :meta private:\\n        \"\"\"\\n        return [self.output_key]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\llm_requests.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@root_validator()\\n    def validate_environment(cls, values: Dict) -> Dict:\\n        \"\"\"Validate that api key and python package exists in environment.\"\"\"\\n        try:\\n            from bs4 import BeautifulSoup  # noqa: F401\\n\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import bs4 python package. \"\\n                \"Please install it with `pip install bs4`.\"\\n            )\\n        return values\\n\\n    def _call(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        from bs4 import BeautifulSoup\\n\\n        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\\n        # Other keys are assumed to be needed for LLM prediction\\n        other_keys = {k: v for k, v in inputs.items() if k != self.input_key}\\n        url = inputs[self.input_key]\\n        res = self.requests_wrapper.get(url)\\n        # extract the text from the html\\n        soup = BeautifulSoup(res, \"html.parser\")\\n        other_keys[self.requests_key] = soup.get_text()[: self.text_length]\\n        result = self.llm_chain.predict(\\n            callbacks=_run_manager.get_child(), **other_keys\\n        )\\n        return {self.output_key: result}\\n\\n    @property\\n    def _chain_type(self) -> str:\\n        return \"llm_requests_chain\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\llm_requests.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _load_llm_chain(config: dict, **kwargs: Any) -> LLMChain:\\n    \"\"\"Load LLM chain from config dict.\"\"\"\\n    if \"llm\" in config:\\n        llm_config = config.pop(\"llm\")\\n        llm = load_llm_from_config(llm_config)\\n    elif \"llm_path\" in config:\\n        llm = load_llm(config.pop(\"llm_path\"))\\n    else:\\n        raise ValueError(\"One of `llm` or `llm_path` must be present.\")\\n\\n    if \"prompt\" in config:\\n        prompt_config = config.pop(\"prompt\")\\n        prompt = load_prompt_from_config(prompt_config)\\n    elif \"prompt_path\" in config:\\n        prompt = load_prompt(config.pop(\"prompt_path\"))\\n    else:\\n        raise ValueError(\"One of `prompt` or `prompt_path` must be present.\")\\n    _load_output_parser(config)\\n\\n    return LLMChain(llm=llm, prompt=prompt, **config)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\loading.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _load_hyde_chain(config: dict, **kwargs: Any) -> HypotheticalDocumentEmbedder:\\n    \"\"\"Load hypothetical document embedder chain from config dict.\"\"\"\\n    if \"llm_chain\" in config:\\n        llm_chain_config = config.pop(\"llm_chain\")\\n        llm_chain = load_chain_from_config(llm_chain_config)\\n    elif \"llm_chain_path\" in config:\\n        llm_chain = load_chain(config.pop(\"llm_chain_path\"))\\n    else:\\n        raise ValueError(\"One of `llm_chain` or `llm_chain_path` must be present.\")\\n    if \"embeddings\" in kwargs:\\n        embeddings = kwargs.pop(\"embeddings\")\\n    else:\\n        raise ValueError(\"`embeddings` must be present.\")\\n    return HypotheticalDocumentEmbedder(\\n        llm_chain=llm_chain, base_embeddings=embeddings, **config\\n    )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\loading.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _load_stuff_documents_chain(config: dict, **kwargs: Any) -> StuffDocumentsChain:\\n    if \"llm_chain\" in config:\\n        llm_chain_config = config.pop(\"llm_chain\")\\n        llm_chain = load_chain_from_config(llm_chain_config)\\n    elif \"llm_chain_path\" in config:\\n        llm_chain = load_chain(config.pop(\"llm_chain_path\"))\\n    else:\\n        raise ValueError(\"One of `llm_chain` or `llm_chain_path` must be present.\")\\n\\n    if not isinstance(llm_chain, LLMChain):\\n        raise ValueError(f\"Expected LLMChain, got {llm_chain}\")\\n\\n    if \"document_prompt\" in config:\\n        prompt_config = config.pop(\"document_prompt\")\\n        document_prompt = load_prompt_from_config(prompt_config)\\n    elif \"document_prompt_path\" in config:\\n        document_prompt = load_prompt(config.pop(\"document_prompt_path\"))\\n    else:\\n        raise ValueError(\\n            \"One of `document_prompt` or `document_prompt_path` must be present.\"\\n        )\\n\\n    return StuffDocumentsChain(\\n        llm_chain=llm_chain, document_prompt=document_prompt, **config\\n    )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\loading.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _load_map_reduce_documents_chain(\\n    config: dict, **kwargs: Any\\n) -> MapReduceDocumentsChain:\\n    if \"llm_chain\" in config:\\n        llm_chain_config = config.pop(\"llm_chain\")\\n        llm_chain = load_chain_from_config(llm_chain_config)\\n    elif \"llm_chain_path\" in config:\\n        llm_chain = load_chain(config.pop(\"llm_chain_path\"))\\n    else:\\n        raise ValueError(\"One of `llm_chain` or `llm_chain_path` must be present.\")\\n\\n    if not isinstance(llm_chain, LLMChain):\\n        raise ValueError(f\"Expected LLMChain, got {llm_chain}\")\\n\\n    if \"reduce_documents_chain\" in config:\\n        reduce_documents_chain = load_chain_from_config(\\n            config.pop(\"reduce_documents_chain\")\\n        )\\n    elif \"reduce_documents_chain_path\" in config:\\n        reduce_documents_chain = load_chain(config.pop(\"reduce_documents_chain_path\"))\\n    else:\\n        reduce_documents_chain = _load_reduce_documents_chain(config)\\n\\n    return MapReduceDocumentsChain(\\n        llm_chain=llm_chain,\\n        reduce_documents_chain=reduce_documents_chain,\\n        **config,\\n    )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\loading.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _load_reduce_documents_chain(config: dict, **kwargs: Any) -> ReduceDocumentsChain:\\n    combine_documents_chain = None\\n    collapse_documents_chain = None\\n\\n    if \"combine_documents_chain\" in config:\\n        combine_document_chain_config = config.pop(\"combine_documents_chain\")\\n        combine_documents_chain = load_chain_from_config(combine_document_chain_config)\\n    elif \"combine_document_chain\" in config:\\n        combine_document_chain_config = config.pop(\"combine_document_chain\")\\n        combine_documents_chain = load_chain_from_config(combine_document_chain_config)\\n    elif \"combine_documents_chain_path\" in config:\\n        combine_documents_chain = load_chain(config.pop(\"combine_documents_chain_path\"))\\n    elif \"combine_document_chain_path\" in config:\\n        combine_documents_chain = load_chain(config.pop(\"combine_document_chain_path\"))\\n    else:\\n        raise ValueError(\\n            \"One of `combine_documents_chain` or \"\\n            \"`combine_documents_chain_path` must be present.\"\\n        )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\loading.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='if \"collapse_documents_chain\" in config:\\n        collapse_document_chain_config = config.pop(\"collapse_documents_chain\")\\n        if collapse_document_chain_config is None:\\n            collapse_documents_chain = None\\n        else:\\n            collapse_documents_chain = load_chain_from_config(\\n                collapse_document_chain_config\\n            )\\n    elif \"collapse_documents_chain_path\" in config:\\n        collapse_documents_chain = load_chain(\\n            config.pop(\"collapse_documents_chain_path\")\\n        )\\n    elif \"collapse_document_chain\" in config:\\n        collapse_document_chain_config = config.pop(\"collapse_document_chain\")\\n        if collapse_document_chain_config is None:\\n            collapse_documents_chain = None\\n        else:\\n            collapse_documents_chain = load_chain_from_config(\\n                collapse_document_chain_config\\n            )\\n    elif \"collapse_document_chain_path\" in config:\\n        collapse_documents_chain = load_chain(\\n            config.pop(\"collapse_document_chain_path\")\\n        )\\n\\n    return ReduceDocumentsChain(\\n        combine_documents_chain=combine_documents_chain,\\n        collapse_documents_chain=collapse_documents_chain,\\n        **config,\\n    )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\loading.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _load_llm_bash_chain(config: dict, **kwargs: Any) -> Any:\\n    from langchain_experimental.llm_bash.base import LLMBashChain\\n\\n    llm_chain = None\\n    if \"llm_chain\" in config:\\n        llm_chain_config = config.pop(\"llm_chain\")\\n        llm_chain = load_chain_from_config(llm_chain_config)\\n    elif \"llm_chain_path\" in config:\\n        llm_chain = load_chain(config.pop(\"llm_chain_path\"))\\n    # llm attribute is deprecated in favor of llm_chain, here to support old configs\\n    elif \"llm\" in config:\\n        llm_config = config.pop(\"llm\")\\n        llm = load_llm_from_config(llm_config)\\n    # llm_path attribute is deprecated in favor of llm_chain_path,\\n    # its to support old configs\\n    elif \"llm_path\" in config:\\n        llm = load_llm(config.pop(\"llm_path\"))\\n    else:\\n        raise ValueError(\"One of `llm_chain` or `llm_chain_path` must be present.\")\\n    if \"prompt\" in config:\\n        prompt_config = config.pop(\"prompt\")\\n        prompt = load_prompt_from_config(prompt_config)\\n    elif \"prompt_path\" in config:\\n        prompt = load_prompt(config.pop(\"prompt_path\"))\\n    if llm_chain:\\n        return LLMBashChain(llm_chain=llm_chain, prompt=prompt, **config)\\n    else:\\n        return LLMBashChain(llm=llm, prompt=prompt, **config)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\loading.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _load_llm_checker_chain(config: dict, **kwargs: Any) -> LLMCheckerChain:\\n    if \"llm\" in config:\\n        llm_config = config.pop(\"llm\")\\n        llm = load_llm_from_config(llm_config)\\n    elif \"llm_path\" in config:\\n        llm = load_llm(config.pop(\"llm_path\"))\\n    else:\\n        raise ValueError(\"One of `llm` or `llm_path` must be present.\")\\n    if \"create_draft_answer_prompt\" in config:\\n        create_draft_answer_prompt_config = config.pop(\"create_draft_answer_prompt\")\\n        create_draft_answer_prompt = load_prompt_from_config(\\n            create_draft_answer_prompt_config\\n        )\\n    elif \"create_draft_answer_prompt_path\" in config:\\n        create_draft_answer_prompt = load_prompt(\\n            config.pop(\"create_draft_answer_prompt_path\")\\n        )\\n    if \"list_assertions_prompt\" in config:\\n        list_assertions_prompt_config = config.pop(\"list_assertions_prompt\")\\n        list_assertions_prompt = load_prompt_from_config(list_assertions_prompt_config)\\n    elif \"list_assertions_prompt_path\" in config:\\n        list_assertions_prompt = load_prompt(config.pop(\"list_assertions_prompt_path\"))\\n    if \"check_assertions_prompt\" in config:\\n        check_assertions_prompt_config = config.pop(\"check_assertions_prompt\")\\n        check_assertions_prompt = load_prompt_from_config(\\n            check_assertions_prompt_config\\n        )\\n    elif \"check_assertions_prompt_path\" in config:\\n        check_assertions_prompt = load_prompt(\\n            config.pop(\"check_assertions_prompt_path\")\\n        )\\n    if \"revised_answer_prompt\" in config:\\n        revised_answer_prompt_config = config.pop(\"revised_answer_prompt\")\\n        revised_answer_prompt = load_prompt_from_config(revised_answer_prompt_config)\\n    elif \"revised_answer_prompt_path\" in config:\\n        revised_answer_prompt = load_prompt(config.pop(\"revised_answer_prompt_path\"))\\n    return LLMCheckerChain(\\n        llm=llm,\\n        create_draft_answer_prompt=create_draft_answer_prompt,\\n        list_assertions_prompt=list_assertions_prompt,' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\loading.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='check_assertions_prompt=check_assertions_prompt,\\n        revised_answer_prompt=revised_answer_prompt,\\n        **config,\\n    )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\loading.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _load_llm_math_chain(config: dict, **kwargs: Any) -> LLMMathChain:\\n    llm_chain = None\\n    if \"llm_chain\" in config:\\n        llm_chain_config = config.pop(\"llm_chain\")\\n        llm_chain = load_chain_from_config(llm_chain_config)\\n    elif \"llm_chain_path\" in config:\\n        llm_chain = load_chain(config.pop(\"llm_chain_path\"))\\n    # llm attribute is deprecated in favor of llm_chain, here to support old configs\\n    elif \"llm\" in config:\\n        llm_config = config.pop(\"llm\")\\n        llm = load_llm_from_config(llm_config)\\n    # llm_path attribute is deprecated in favor of llm_chain_path,\\n    # its to support old configs\\n    elif \"llm_path\" in config:\\n        llm = load_llm(config.pop(\"llm_path\"))\\n    else:\\n        raise ValueError(\"One of `llm_chain` or `llm_chain_path` must be present.\")\\n    if \"prompt\" in config:\\n        prompt_config = config.pop(\"prompt\")\\n        prompt = load_prompt_from_config(prompt_config)\\n    elif \"prompt_path\" in config:\\n        prompt = load_prompt(config.pop(\"prompt_path\"))\\n    if llm_chain:\\n        return LLMMathChain(llm_chain=llm_chain, prompt=prompt, **config)\\n    else:\\n        return LLMMathChain(llm=llm, prompt=prompt, **config)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\loading.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _load_map_rerank_documents_chain(\\n    config: dict, **kwargs: Any\\n) -> MapRerankDocumentsChain:\\n    if \"llm_chain\" in config:\\n        llm_chain_config = config.pop(\"llm_chain\")\\n        llm_chain = load_chain_from_config(llm_chain_config)\\n    elif \"llm_chain_path\" in config:\\n        llm_chain = load_chain(config.pop(\"llm_chain_path\"))\\n    else:\\n        raise ValueError(\"One of `llm_chain` or `llm_chain_path` must be present.\")\\n    return MapRerankDocumentsChain(llm_chain=llm_chain, **config)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\loading.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _load_pal_chain(config: dict, **kwargs: Any) -> Any:\\n    from langchain_experimental.pal_chain import PALChain\\n\\n    if \"llm_chain\" in config:\\n        llm_chain_config = config.pop(\"llm_chain\")\\n        llm_chain = load_chain_from_config(llm_chain_config)\\n    elif \"llm_chain_path\" in config:\\n        llm_chain = load_chain(config.pop(\"llm_chain_path\"))\\n    else:\\n        raise ValueError(\"One of `llm_chain` or `llm_chain_path` must be present.\")\\n    return PALChain(llm_chain=llm_chain, **config)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\loading.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _load_refine_documents_chain(config: dict, **kwargs: Any) -> RefineDocumentsChain:\\n    if \"initial_llm_chain\" in config:\\n        initial_llm_chain_config = config.pop(\"initial_llm_chain\")\\n        initial_llm_chain = load_chain_from_config(initial_llm_chain_config)\\n    elif \"initial_llm_chain_path\" in config:\\n        initial_llm_chain = load_chain(config.pop(\"initial_llm_chain_path\"))\\n    else:\\n        raise ValueError(\\n            \"One of `initial_llm_chain` or `initial_llm_chain_path` must be present.\"\\n        )\\n    if \"refine_llm_chain\" in config:\\n        refine_llm_chain_config = config.pop(\"refine_llm_chain\")\\n        refine_llm_chain = load_chain_from_config(refine_llm_chain_config)\\n    elif \"refine_llm_chain_path\" in config:\\n        refine_llm_chain = load_chain(config.pop(\"refine_llm_chain_path\"))\\n    else:\\n        raise ValueError(\\n            \"One of `refine_llm_chain` or `refine_llm_chain_path` must be present.\"\\n        )\\n    if \"document_prompt\" in config:\\n        prompt_config = config.pop(\"document_prompt\")\\n        document_prompt = load_prompt_from_config(prompt_config)\\n    elif \"document_prompt_path\" in config:\\n        document_prompt = load_prompt(config.pop(\"document_prompt_path\"))\\n    return RefineDocumentsChain(\\n        initial_llm_chain=initial_llm_chain,\\n        refine_llm_chain=refine_llm_chain,\\n        document_prompt=document_prompt,\\n        **config,\\n    )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\loading.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _load_qa_with_sources_chain(config: dict, **kwargs: Any) -> QAWithSourcesChain:\\n    if \"combine_documents_chain\" in config:\\n        combine_documents_chain_config = config.pop(\"combine_documents_chain\")\\n        combine_documents_chain = load_chain_from_config(combine_documents_chain_config)\\n    elif \"combine_documents_chain_path\" in config:\\n        combine_documents_chain = load_chain(config.pop(\"combine_documents_chain_path\"))\\n    else:\\n        raise ValueError(\\n            \"One of `combine_documents_chain` or \"\\n            \"`combine_documents_chain_path` must be present.\"\\n        )\\n    return QAWithSourcesChain(combine_documents_chain=combine_documents_chain, **config)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\loading.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _load_sql_database_chain(config: dict, **kwargs: Any) -> Any:\\n    from langchain_experimental.sql import SQLDatabaseChain\\n\\n    if \"database\" in kwargs:\\n        database = kwargs.pop(\"database\")\\n    else:\\n        raise ValueError(\"`database` must be present.\")\\n    if \"llm_chain\" in config:\\n        llm_chain_config = config.pop(\"llm_chain\")\\n        chain = load_chain_from_config(llm_chain_config)\\n        return SQLDatabaseChain(llm_chain=chain, database=database, **config)\\n    if \"llm\" in config:\\n        llm_config = config.pop(\"llm\")\\n        llm = load_llm_from_config(llm_config)\\n    elif \"llm_path\" in config:\\n        llm = load_llm(config.pop(\"llm_path\"))\\n    else:\\n        raise ValueError(\"One of `llm` or `llm_path` must be present.\")\\n    if \"prompt\" in config:\\n        prompt_config = config.pop(\"prompt\")\\n        prompt = load_prompt_from_config(prompt_config)\\n    else:\\n        prompt = None\\n\\n    return SQLDatabaseChain.from_llm(llm, database, prompt=prompt, **config)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\loading.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _load_vector_db_qa_with_sources_chain(\\n    config: dict, **kwargs: Any\\n) -> VectorDBQAWithSourcesChain:\\n    if \"vectorstore\" in kwargs:\\n        vectorstore = kwargs.pop(\"vectorstore\")\\n    else:\\n        raise ValueError(\"`vectorstore` must be present.\")\\n    if \"combine_documents_chain\" in config:\\n        combine_documents_chain_config = config.pop(\"combine_documents_chain\")\\n        combine_documents_chain = load_chain_from_config(combine_documents_chain_config)\\n    elif \"combine_documents_chain_path\" in config:\\n        combine_documents_chain = load_chain(config.pop(\"combine_documents_chain_path\"))\\n    else:\\n        raise ValueError(\\n            \"One of `combine_documents_chain` or \"\\n            \"`combine_documents_chain_path` must be present.\"\\n        )\\n    return VectorDBQAWithSourcesChain(\\n        combine_documents_chain=combine_documents_chain,\\n        vectorstore=vectorstore,\\n        **config,\\n    )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\loading.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _load_retrieval_qa(config: dict, **kwargs: Any) -> RetrievalQA:\\n    if \"retriever\" in kwargs:\\n        retriever = kwargs.pop(\"retriever\")\\n    else:\\n        raise ValueError(\"`retriever` must be present.\")\\n    if \"combine_documents_chain\" in config:\\n        combine_documents_chain_config = config.pop(\"combine_documents_chain\")\\n        combine_documents_chain = load_chain_from_config(combine_documents_chain_config)\\n    elif \"combine_documents_chain_path\" in config:\\n        combine_documents_chain = load_chain(config.pop(\"combine_documents_chain_path\"))\\n    else:\\n        raise ValueError(\\n            \"One of `combine_documents_chain` or \"\\n            \"`combine_documents_chain_path` must be present.\"\\n        )\\n    return RetrievalQA(\\n        combine_documents_chain=combine_documents_chain,\\n        retriever=retriever,\\n        **config,\\n    )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\loading.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _load_retrieval_qa_with_sources_chain(\\n    config: dict, **kwargs: Any\\n) -> RetrievalQAWithSourcesChain:\\n    if \"retriever\" in kwargs:\\n        retriever = kwargs.pop(\"retriever\")\\n    else:\\n        raise ValueError(\"`retriever` must be present.\")\\n    if \"combine_documents_chain\" in config:\\n        combine_documents_chain_config = config.pop(\"combine_documents_chain\")\\n        combine_documents_chain = load_chain_from_config(combine_documents_chain_config)\\n    elif \"combine_documents_chain_path\" in config:\\n        combine_documents_chain = load_chain(config.pop(\"combine_documents_chain_path\"))\\n    else:\\n        raise ValueError(\\n            \"One of `combine_documents_chain` or \"\\n            \"`combine_documents_chain_path` must be present.\"\\n        )\\n    return RetrievalQAWithSourcesChain(\\n        combine_documents_chain=combine_documents_chain,\\n        retriever=retriever,\\n        **config,\\n    )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\loading.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _load_vector_db_qa(config: dict, **kwargs: Any) -> VectorDBQA:\\n    if \"vectorstore\" in kwargs:\\n        vectorstore = kwargs.pop(\"vectorstore\")\\n    else:\\n        raise ValueError(\"`vectorstore` must be present.\")\\n    if \"combine_documents_chain\" in config:\\n        combine_documents_chain_config = config.pop(\"combine_documents_chain\")\\n        combine_documents_chain = load_chain_from_config(combine_documents_chain_config)\\n    elif \"combine_documents_chain_path\" in config:\\n        combine_documents_chain = load_chain(config.pop(\"combine_documents_chain_path\"))\\n    else:\\n        raise ValueError(\\n            \"One of `combine_documents_chain` or \"\\n            \"`combine_documents_chain_path` must be present.\"\\n        )\\n    return VectorDBQA(\\n        combine_documents_chain=combine_documents_chain,\\n        vectorstore=vectorstore,\\n        **config,\\n    )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\loading.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _load_graph_cypher_chain(config: dict, **kwargs: Any) -> GraphCypherQAChain:\\n    if \"graph\" in kwargs:\\n        graph = kwargs.pop(\"graph\")\\n    else:\\n        raise ValueError(\"`graph` must be present.\")\\n    if \"cypher_generation_chain\" in config:\\n        cypher_generation_chain_config = config.pop(\"cypher_generation_chain\")\\n        cypher_generation_chain = load_chain_from_config(cypher_generation_chain_config)\\n    else:\\n        raise ValueError(\"`cypher_generation_chain` must be present.\")\\n    if \"qa_chain\" in config:\\n        qa_chain_config = config.pop(\"qa_chain\")\\n        qa_chain = load_chain_from_config(qa_chain_config)\\n    else:\\n        raise ValueError(\"`qa_chain` must be present.\")\\n\\n    return GraphCypherQAChain(\\n        graph=graph,\\n        cypher_generation_chain=cypher_generation_chain,\\n        qa_chain=qa_chain,\\n        **config,\\n    )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\loading.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _load_api_chain(config: dict, **kwargs: Any) -> APIChain:\\n    if \"api_request_chain\" in config:\\n        api_request_chain_config = config.pop(\"api_request_chain\")\\n        api_request_chain = load_chain_from_config(api_request_chain_config)\\n    elif \"api_request_chain_path\" in config:\\n        api_request_chain = load_chain(config.pop(\"api_request_chain_path\"))\\n    else:\\n        raise ValueError(\\n            \"One of `api_request_chain` or `api_request_chain_path` must be present.\"\\n        )\\n    if \"api_answer_chain\" in config:\\n        api_answer_chain_config = config.pop(\"api_answer_chain\")\\n        api_answer_chain = load_chain_from_config(api_answer_chain_config)\\n    elif \"api_answer_chain_path\" in config:\\n        api_answer_chain = load_chain(config.pop(\"api_answer_chain_path\"))\\n    else:\\n        raise ValueError(\\n            \"One of `api_answer_chain` or `api_answer_chain_path` must be present.\"\\n        )\\n    if \"requests_wrapper\" in kwargs:\\n        requests_wrapper = kwargs.pop(\"requests_wrapper\")\\n    else:\\n        raise ValueError(\"`requests_wrapper` must be present.\")\\n    return APIChain(\\n        api_request_chain=api_request_chain,\\n        api_answer_chain=api_answer_chain,\\n        requests_wrapper=requests_wrapper,\\n        **config,\\n    )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\loading.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _load_llm_requests_chain(config: dict, **kwargs: Any) -> LLMRequestsChain:\\n    if \"llm_chain\" in config:\\n        llm_chain_config = config.pop(\"llm_chain\")\\n        llm_chain = load_chain_from_config(llm_chain_config)\\n    elif \"llm_chain_path\" in config:\\n        llm_chain = load_chain(config.pop(\"llm_chain_path\"))\\n    else:\\n        raise ValueError(\"One of `llm_chain` or `llm_chain_path` must be present.\")\\n    if \"requests_wrapper\" in kwargs:\\n        requests_wrapper = kwargs.pop(\"requests_wrapper\")\\n        return LLMRequestsChain(\\n            llm_chain=llm_chain, requests_wrapper=requests_wrapper, **config\\n        )\\n    else:\\n        return LLMRequestsChain(llm_chain=llm_chain, **config)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\loading.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def load_chain_from_config(config: dict, **kwargs: Any) -> Chain:\\n    \"\"\"Load chain from Config Dict.\"\"\"\\n    if \"_type\" not in config:\\n        raise ValueError(\"Must specify a chain Type in config\")\\n    config_type = config.pop(\"_type\")\\n\\n    if config_type not in type_to_loader_dict:\\n        raise ValueError(f\"Loading {config_type} chain not supported\")\\n\\n    chain_loader = type_to_loader_dict[config_type]\\n    return chain_loader(config, **kwargs)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\loading.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def load_chain(path: Union[str, Path], **kwargs: Any) -> Chain:\\n    \"\"\"Unified method for loading a chain from LangChainHub or local fs.\"\"\"\\n    if hub_result := try_load_from_hub(\\n        path, _load_chain_from_file, \"chains\", {\"json\", \"yaml\"}, **kwargs\\n    ):\\n        return hub_result\\n    else:\\n        return _load_chain_from_file(path, **kwargs)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\loading.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _load_chain_from_file(file: Union[str, Path], **kwargs: Any) -> Chain:\\n    \"\"\"Load chain from file.\"\"\"\\n    # Convert file to Path object.\\n    if isinstance(file, str):\\n        file_path = Path(file)\\n    else:\\n        file_path = file\\n    # Load from either json or yaml.\\n    if file_path.suffix == \".json\":\\n        with open(file_path) as f:\\n            config = json.load(f)\\n    elif file_path.suffix == \".yaml\":\\n        with open(file_path, \"r\") as f:\\n            config = yaml.safe_load(f)\\n    else:\\n        raise ValueError(\"File type must be json or yaml\")\\n\\n    # Override default \\'verbose\\' and \\'memory\\' for the chain\\n    if \"verbose\" in kwargs:\\n        config[\"verbose\"] = kwargs.pop(\"verbose\")\\n    if \"memory\" in kwargs:\\n        config[\"memory\"] = kwargs.pop(\"memory\")\\n\\n    # Load the chain from the config now.\\n    return load_chain_from_config(config, **kwargs)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\loading.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Functionality for loading chains.\"\"\"\\nimport json\\nfrom pathlib import Path\\nfrom typing import Any, Union\\n\\nimport yaml\\nfrom langchain_community.llms.loading import load_llm, load_llm_from_config\\nfrom langchain_core.prompts.loading import (\\n    _load_output_parser,\\n    load_prompt,\\n    load_prompt_from_config,\\n)\\nfrom langchain_core.utils.loading import try_load_from_hub\\n\\nfrom langchain.chains import ReduceDocumentsChain\\nfrom langchain.chains.api.base import APIChain\\nfrom langchain.chains.base import Chain\\nfrom langchain.chains.combine_documents.map_reduce import MapReduceDocumentsChain\\nfrom langchain.chains.combine_documents.map_rerank import MapRerankDocumentsChain\\nfrom langchain.chains.combine_documents.refine import RefineDocumentsChain\\nfrom langchain.chains.combine_documents.stuff import StuffDocumentsChain\\nfrom langchain.chains.graph_qa.cypher import GraphCypherQAChain\\nfrom langchain.chains.hyde.base import HypotheticalDocumentEmbedder\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.chains.llm_checker.base import LLMCheckerChain\\nfrom langchain.chains.llm_math.base import LLMMathChain\\nfrom langchain.chains.llm_requests import LLMRequestsChain\\nfrom langchain.chains.qa_with_sources.base import QAWithSourcesChain\\nfrom langchain.chains.qa_with_sources.retrieval import RetrievalQAWithSourcesChain\\nfrom langchain.chains.qa_with_sources.vector_db import VectorDBQAWithSourcesChain\\nfrom langchain.chains.retrieval_qa.base import RetrievalQA, VectorDBQA\\n\\nURL_BASE = \"https://raw.githubusercontent.com/hwchase17/langchain-hub/master/chains/\"\\n\\n\\n# Code for: def _load_llm_chain(config: dict, **kwargs: Any) -> LLMChain:\\n\\n\\n# Code for: def _load_hyde_chain(config: dict, **kwargs: Any) -> HypotheticalDocumentEmbedder:\\n\\n\\n# Code for: def _load_stuff_documents_chain(config: dict, **kwargs: Any) -> StuffDocumentsChain:\\n\\n\\n# Code for: def _load_map_reduce_documents_chain(\\n\\n\\n# Code for: def _load_reduce_documents_chain(config: dict, **kwargs: Any) -> ReduceDocumentsChain:' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\loading.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# Code for: def _load_llm_bash_chain(config: dict, **kwargs: Any) -> Any:\\n\\n\\n# Code for: def _load_llm_checker_chain(config: dict, **kwargs: Any) -> LLMCheckerChain:\\n\\n\\n# Code for: def _load_llm_math_chain(config: dict, **kwargs: Any) -> LLMMathChain:\\n\\n\\n# Code for: def _load_map_rerank_documents_chain(\\n\\n\\n# Code for: def _load_pal_chain(config: dict, **kwargs: Any) -> Any:\\n\\n\\n# Code for: def _load_refine_documents_chain(config: dict, **kwargs: Any) -> RefineDocumentsChain:\\n\\n\\n# Code for: def _load_qa_with_sources_chain(config: dict, **kwargs: Any) -> QAWithSourcesChain:\\n\\n\\n# Code for: def _load_sql_database_chain(config: dict, **kwargs: Any) -> Any:\\n\\n\\n# Code for: def _load_vector_db_qa_with_sources_chain(\\n\\n\\n# Code for: def _load_retrieval_qa(config: dict, **kwargs: Any) -> RetrievalQA:\\n\\n\\n# Code for: def _load_retrieval_qa_with_sources_chain(\\n\\n\\n# Code for: def _load_vector_db_qa(config: dict, **kwargs: Any) -> VectorDBQA:\\n\\n\\n# Code for: def _load_graph_cypher_chain(config: dict, **kwargs: Any) -> GraphCypherQAChain:\\n\\n\\n# Code for: def _load_api_chain(config: dict, **kwargs: Any) -> APIChain:\\n\\n\\n# Code for: def _load_llm_requests_chain(config: dict, **kwargs: Any) -> LLMRequestsChain:' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\loading.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='type_to_loader_dict = {\\n    \"api_chain\": _load_api_chain,\\n    \"hyde_chain\": _load_hyde_chain,\\n    \"llm_chain\": _load_llm_chain,\\n    \"llm_bash_chain\": _load_llm_bash_chain,\\n    \"llm_checker_chain\": _load_llm_checker_chain,\\n    \"llm_math_chain\": _load_llm_math_chain,\\n    \"llm_requests_chain\": _load_llm_requests_chain,\\n    \"pal_chain\": _load_pal_chain,\\n    \"qa_with_sources_chain\": _load_qa_with_sources_chain,\\n    \"stuff_documents_chain\": _load_stuff_documents_chain,\\n    \"map_reduce_documents_chain\": _load_map_reduce_documents_chain,\\n    \"reduce_documents_chain\": _load_reduce_documents_chain,\\n    \"map_rerank_documents_chain\": _load_map_rerank_documents_chain,\\n    \"refine_documents_chain\": _load_refine_documents_chain,\\n    \"sql_database_chain\": _load_sql_database_chain,\\n    \"vector_db_qa_with_sources_chain\": _load_vector_db_qa_with_sources_chain,\\n    \"vector_db_qa\": _load_vector_db_qa,\\n    \"retrieval_qa\": _load_retrieval_qa,\\n    \"retrieval_qa_with_sources_chain\": _load_retrieval_qa_with_sources_chain,\\n    \"graph_cypher_chain\": _load_graph_cypher_chain,\\n}\\n\\n\\n# Code for: def load_chain_from_config(config: dict, **kwargs: Any) -> Chain:\\n\\n\\n# Code for: def load_chain(path: Union[str, Path], **kwargs: Any) -> Chain:\\n\\n\\n# Code for: def _load_chain_from_file(file: Union[str, Path], **kwargs: Any) -> Chain:' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\loading.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Map-reduce chain.\\n\\nSplits up a document, sends the smaller parts to the LLM with one prompt,\\nthen combines the results with another one.\\n\"\"\"\\nfrom __future__ import annotations\\n\\nfrom typing import Any, Dict, List, Mapping, Optional\\n\\nfrom langchain_core.callbacks import CallbackManagerForChainRun, Callbacks\\nfrom langchain_core.documents import Document\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.prompts import BasePromptTemplate\\nfrom langchain_core.pydantic_v1 import Extra\\n\\nfrom langchain.chains import ReduceDocumentsChain\\nfrom langchain.chains.base import Chain\\nfrom langchain.chains.combine_documents.base import BaseCombineDocumentsChain\\nfrom langchain.chains.combine_documents.map_reduce import MapReduceDocumentsChain\\nfrom langchain.chains.combine_documents.stuff import StuffDocumentsChain\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.text_splitter import TextSplitter\\n\\n\\nclass MapReduceChain(Chain):\\n    \"\"\"Map-reduce chain.\"\"\"\\n\\n    combine_documents_chain: BaseCombineDocumentsChain\\n    \"\"\"Chain to use to combine documents.\"\"\"\\n    text_splitter: TextSplitter\\n    \"\"\"Text splitter to use.\"\"\"\\n    input_key: str = \"input_text\"  #: :meta private:\\n    output_key: str = \"output_text\"  #: :meta private:' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\mapreduce.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@classmethod\\n    def from_params(\\n        cls,\\n        llm: BaseLanguageModel,\\n        prompt: BasePromptTemplate,\\n        text_splitter: TextSplitter,\\n        callbacks: Callbacks = None,\\n        combine_chain_kwargs: Optional[Mapping[str, Any]] = None,\\n        reduce_chain_kwargs: Optional[Mapping[str, Any]] = None,\\n        **kwargs: Any,\\n    ) -> MapReduceChain:\\n        \"\"\"Construct a map-reduce chain that uses the chain for map and reduce.\"\"\"\\n        llm_chain = LLMChain(llm=llm, prompt=prompt, callbacks=callbacks)\\n        stuff_chain = StuffDocumentsChain(\\n            llm_chain=llm_chain,\\n            callbacks=callbacks,\\n            **(reduce_chain_kwargs if reduce_chain_kwargs else {}),\\n        )\\n        reduce_documents_chain = ReduceDocumentsChain(\\n            combine_documents_chain=stuff_chain\\n        )\\n        combine_documents_chain = MapReduceDocumentsChain(\\n            llm_chain=llm_chain,\\n            reduce_documents_chain=reduce_documents_chain,\\n            callbacks=callbacks,\\n            **(combine_chain_kwargs if combine_chain_kwargs else {}),\\n        )\\n        return cls(\\n            combine_documents_chain=combine_documents_chain,\\n            text_splitter=text_splitter,\\n            callbacks=callbacks,\\n            **kwargs,\\n        )\\n\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n\\n        extra = Extra.forbid\\n        arbitrary_types_allowed = True\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Expect input key.\\n\\n        :meta private:\\n        \"\"\"\\n        return [self.input_key]\\n\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Return output key.\\n\\n        :meta private:\\n        \"\"\"\\n        return [self.output_key]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\mapreduce.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _call(\\n        self,\\n        inputs: Dict[str, str],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, str]:\\n        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\\n        # Split the larger text into smaller chunks.\\n        doc_text = inputs.pop(self.input_key)\\n        texts = self.text_splitter.split_text(doc_text)\\n        docs = [Document(page_content=text) for text in texts]\\n        _inputs: Dict[str, Any] = {\\n            **inputs,\\n            self.combine_documents_chain.input_key: docs,\\n        }\\n        outputs = self.combine_documents_chain.run(\\n            _inputs, callbacks=_run_manager.get_child()\\n        )\\n        return {self.output_key: outputs}' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\mapreduce.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Pass input through a moderation endpoint.\"\"\"\\nfrom typing import Any, Dict, List, Optional\\n\\nfrom langchain_core.callbacks import CallbackManagerForChainRun\\nfrom langchain_core.pydantic_v1 import root_validator\\nfrom langchain_core.utils import get_from_dict_or_env\\n\\nfrom langchain.chains.base import Chain\\n\\n\\nclass OpenAIModerationChain(Chain):\\n    \"\"\"Pass input through a moderation endpoint.\\n\\n    To use, you should have the ``openai`` python package installed, and the\\n    environment variable ``OPENAI_API_KEY`` set with your API key.\\n\\n    Any parameters that are valid to be passed to the openai.create call can be passed\\n    in, even if not explicitly saved on this class.\\n\\n    Example:\\n        .. code-block:: python\\n\\n            from langchain.chains import OpenAIModerationChain\\n            moderation = OpenAIModerationChain()\\n    \"\"\"\\n\\n    client: Any  #: :meta private:\\n    model_name: Optional[str] = None\\n    \"\"\"Moderation model name to use.\"\"\"\\n    error: bool = False\\n    \"\"\"Whether or not to error if bad content was found.\"\"\"\\n    input_key: str = \"input\"  #: :meta private:\\n    output_key: str = \"output\"  #: :meta private:\\n    openai_api_key: Optional[str] = None\\n    openai_organization: Optional[str] = None\\n\\n    @root_validator()\\n    def validate_environment(cls, values: Dict) -> Dict:\\n        \"\"\"Validate that api key and python package exists in environment.\"\"\"\\n        openai_api_key = get_from_dict_or_env(\\n            values, \"openai_api_key\", \"OPENAI_API_KEY\"\\n        )\\n        openai_organization = get_from_dict_or_env(\\n            values,\\n            \"openai_organization\",\\n            \"OPENAI_ORGANIZATION\",\\n            default=\"\",\\n        )\\n        try:\\n            import openai' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\moderation.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='openai.api_key = openai_api_key\\n            if openai_organization:\\n                openai.organization = openai_organization\\n            values[\"client\"] = openai.Moderation\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import openai python package. \"\\n                \"Please install it with `pip install openai`.\"\\n            )\\n        return values\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Expect input key.\\n\\n        :meta private:\\n        \"\"\"\\n        return [self.input_key]\\n\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Return output key.\\n\\n        :meta private:\\n        \"\"\"\\n        return [self.output_key]\\n\\n    def _moderate(self, text: str, results: dict) -> str:\\n        if results[\"flagged\"]:\\n            error_str = \"Text was found that violates OpenAI\\'s content policy.\"\\n            if self.error:\\n                raise ValueError(error_str)\\n            else:\\n                return error_str\\n        return text\\n\\n    def _call(\\n        self,\\n        inputs: Dict[str, str],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, str]:\\n        text = inputs[self.input_key]\\n        results = self.client.create(text)\\n        output = self._moderate(text, results[\"results\"][0])\\n        return {self.output_key: output}' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\moderation.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from abc import ABC, abstractmethod\\nfrom typing import Callable, List, Tuple\\n\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.language_models.chat_models import BaseChatModel\\nfrom langchain_core.language_models.llms import BaseLLM\\nfrom langchain_core.prompts import BasePromptTemplate\\nfrom langchain_core.pydantic_v1 import BaseModel, Field\\n\\n\\nclass BasePromptSelector(BaseModel, ABC):\\n    \"\"\"Base class for prompt selectors.\"\"\"\\n\\n    @abstractmethod\\n    def get_prompt(self, llm: BaseLanguageModel) -> BasePromptTemplate:\\n        \"\"\"Get default prompt for a language model.\"\"\"\\n\\n\\nclass ConditionalPromptSelector(BasePromptSelector):\\n    \"\"\"Prompt collection that goes through conditionals.\"\"\"\\n\\n    default_prompt: BasePromptTemplate\\n    \"\"\"Default prompt to use if no conditionals match.\"\"\"\\n    conditionals: List[\\n        Tuple[Callable[[BaseLanguageModel], bool], BasePromptTemplate]\\n    ] = Field(default_factory=list)\\n    \"\"\"List of conditionals and prompts to use if the conditionals match.\"\"\"\\n\\n    def get_prompt(self, llm: BaseLanguageModel) -> BasePromptTemplate:\\n        \"\"\"Get default prompt for a language model.\\n\\n        Args:\\n            llm: Language model to get prompt for.\\n\\n        Returns:\\n            Prompt to use for the language model.\\n        \"\"\"\\n        for condition, prompt in self.conditionals:\\n            if condition(llm):\\n                return prompt\\n        return self.default_prompt\\n\\n\\ndef is_llm(llm: BaseLanguageModel) -> bool:\\n    \"\"\"Check if the language model is a LLM.\\n\\n    Args:\\n        llm: Language model to check.\\n\\n    Returns:\\n        True if the language model is a BaseLLM model, False otherwise.\\n    \"\"\"\\n    return isinstance(llm, BaseLLM)\\n\\n\\ndef is_chat_model(llm: BaseLanguageModel) -> bool:\\n    \"\"\"Check if the language model is a chat model.\\n\\n    Args:\\n        llm: Language model to check.\\n\\n    Returns:\\n        True if the language model is a BaseChatModel model, False otherwise.\\n    \"\"\"\\n    return isinstance(llm, BaseChatModel)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\prompt_selector.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from __future__ import annotations\\n\\nfrom typing import Any, Dict, Union\\n\\nfrom langchain_core.retrievers import (\\n    BaseRetriever,\\n    RetrieverOutput,\\n)\\nfrom langchain_core.runnables import Runnable, RunnablePassthrough\\n\\n\\ndef create_retrieval_chain(\\n    retriever: Union[BaseRetriever, Runnable[dict, RetrieverOutput]],\\n    combine_docs_chain: Runnable[Dict[str, Any], str],\\n) -> Runnable:\\n    \"\"\"Create retrieval chain that retrieves documents and then passes them on.\\n\\n    Args:\\n        retriever: Retriever-like object that returns list of documents. Should\\n            either be a subclass of BaseRetriever or a Runnable that returns\\n            a list of documents. If a subclass of BaseRetriever, then it\\n            is expected that an `input` key be passed in - this is what\\n            is will be used to pass into the retriever. If this is NOT a\\n            subclass of BaseRetriever, then all the inputs will be passed\\n            into this runnable, meaning that runnable should take a dictionary\\n            as input.\\n        combine_docs_chain: Runnable that takes inputs and produces a string output.\\n            The inputs to this will be any original inputs to this chain, a new\\n            context key with the retrieved documents, and chat_history (if not present\\n            in the inputs) with a value of `[]` (to easily enable conversational\\n            retrieval.\\n\\n    Returns:\\n        An LCEL Runnable. The Runnable return is a dictionary containing at the very\\n        least a `context` and `answer` key.\\n\\n    Example:\\n        .. code-block:: python\\n\\n            # pip install -U langchain langchain-community\\n\\n            from langchain_community.chat_models import ChatOpenAI\\n            from langchain.chains.combine_documents import create_stuff_documents_chain\\n            from langchain.chains import create_retrieval_chain\\n            from langchain import hub' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\retrieval.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='retrieval_qa_chat_prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")\\n            llm = ChatOpenAI()\\n            retriever = ...\\n            combine_docs_chain = create_stuff_documents_chain(\\n                llm, retrieval_qa_chat_prompt\\n            )\\n            retrieval_chain = create_retrieval_chain(retriever, combine_docs_chain)\\n\\n            chain.invoke({\"input\": \"...\"})\\n\\n    \"\"\"\\n    if not isinstance(retriever, BaseRetriever):\\n        retrieval_docs: Runnable[dict, RetrieverOutput] = retriever\\n    else:\\n        retrieval_docs = (lambda x: x[\"input\"]) | retriever\\n\\n    retrieval_chain = (\\n        RunnablePassthrough.assign(\\n            context=retrieval_docs.with_config(run_name=\"retrieve_documents\"),\\n        ).assign(answer=combine_docs_chain)\\n    ).with_config(run_name=\"retrieval_chain\")\\n\\n    return retrieval_chain' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\retrieval.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Chain pipeline where the outputs of one step feed directly into next.\"\"\"\\nfrom typing import Any, Dict, List, Optional\\n\\nfrom langchain_core.callbacks import (\\n    AsyncCallbackManagerForChainRun,\\n    CallbackManagerForChainRun,\\n)\\nfrom langchain_core.pydantic_v1 import Extra, root_validator\\nfrom langchain_core.utils.input import get_color_mapping\\n\\nfrom langchain.chains.base import Chain\\n\\n\\nclass SequentialChain(Chain):\\n    \"\"\"Chain where the outputs of one chain feed directly into next.\"\"\"\\n\\n    chains: List[Chain]\\n    input_variables: List[str]\\n    output_variables: List[str]  #: :meta private:\\n    return_all: bool = False\\n\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n\\n        extra = Extra.forbid\\n        arbitrary_types_allowed = True\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Return expected input keys to the chain.\\n\\n        :meta private:\\n        \"\"\"\\n        return self.input_variables\\n\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Return output key.\\n\\n        :meta private:\\n        \"\"\"\\n        return self.output_variables\\n\\n    @root_validator(pre=True)\\n    def validate_chains(cls, values: Dict) -> Dict:\\n        \"\"\"Validate that the correct inputs exist for all chains.\"\"\"\\n        chains = values[\"chains\"]\\n        input_variables = values[\"input_variables\"]\\n        memory_keys = list()\\n        if \"memory\" in values and values[\"memory\"] is not None:\\n            \"\"\"Validate that prompt input variables are consistent.\"\"\"\\n            memory_keys = values[\"memory\"].memory_variables\\n            if set(input_variables).intersection(set(memory_keys)):\\n                overlapping_keys = set(input_variables) & set(memory_keys)\\n                raise ValueError(\\n                    f\"The input key(s) {\\'\\'.join(overlapping_keys)} are found \"\\n                    f\"in the Memory keys ({memory_keys}) - please use input and \"\\n                    f\"memory keys that don\\'t overlap.\"\\n                )\\n\\n        known_variables = set(input_variables + memory_keys)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\sequential.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='for chain in chains:\\n            missing_vars = set(chain.input_keys).difference(known_variables)\\n            if chain.memory:\\n                missing_vars = missing_vars.difference(chain.memory.memory_variables)\\n\\n            if missing_vars:\\n                raise ValueError(\\n                    f\"Missing required input keys: {missing_vars}, \"\\n                    f\"only had {known_variables}\"\\n                )\\n            overlapping_keys = known_variables.intersection(chain.output_keys)\\n            if overlapping_keys:\\n                raise ValueError(\\n                    f\"Chain returned keys that already exist: {overlapping_keys}\"\\n                )\\n\\n            known_variables |= set(chain.output_keys)\\n\\n        if \"output_variables\" not in values:\\n            if values.get(\"return_all\", False):\\n                output_keys = known_variables.difference(input_variables)\\n            else:\\n                output_keys = chains[-1].output_keys\\n            values[\"output_variables\"] = output_keys\\n        else:\\n            missing_vars = set(values[\"output_variables\"]).difference(known_variables)\\n            if missing_vars:\\n                raise ValueError(\\n                    f\"Expected output variables that were not found: {missing_vars}.\"\\n                )\\n\\n        return values\\n\\n    def _call(\\n        self,\\n        inputs: Dict[str, str],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, str]:\\n        known_values = inputs.copy()\\n        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\\n        for i, chain in enumerate(self.chains):\\n            callbacks = _run_manager.get_child()\\n            outputs = chain(known_values, return_only_outputs=True, callbacks=callbacks)\\n            known_values.update(outputs)\\n        return {k: known_values[k] for k in self.output_variables}' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\sequential.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='async def _acall(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        known_values = inputs.copy()\\n        _run_manager = run_manager or AsyncCallbackManagerForChainRun.get_noop_manager()\\n        callbacks = _run_manager.get_child()\\n        for i, chain in enumerate(self.chains):\\n            outputs = await chain.acall(\\n                known_values, return_only_outputs=True, callbacks=callbacks\\n            )\\n            known_values.update(outputs)\\n        return {k: known_values[k] for k in self.output_variables}\\n\\n\\nclass SimpleSequentialChain(Chain):\\n    \"\"\"Simple chain where the outputs of one step feed directly into next.\"\"\"\\n\\n    chains: List[Chain]\\n    strip_outputs: bool = False\\n    input_key: str = \"input\"  #: :meta private:\\n    output_key: str = \"output\"  #: :meta private:\\n\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n\\n        extra = Extra.forbid\\n        arbitrary_types_allowed = True\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Expect input key.\\n\\n        :meta private:\\n        \"\"\"\\n        return [self.input_key]\\n\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Return output key.\\n\\n        :meta private:\\n        \"\"\"\\n        return [self.output_key]\\n\\n    @root_validator()\\n    def validate_chains(cls, values: Dict) -> Dict:\\n        \"\"\"Validate that chains are all single input/output.\"\"\"\\n        for chain in values[\"chains\"]:\\n            if len(chain.input_keys) != 1:\\n                raise ValueError(\\n                    \"Chains used in SimplePipeline should all have one input, got \"\\n                    f\"{chain} with {len(chain.input_keys)} inputs.\"\\n                )\\n            if len(chain.output_keys) != 1:\\n                raise ValueError(\\n                    \"Chains used in SimplePipeline should all have one output, got \"\\n                    f\"{chain} with {len(chain.output_keys)} outputs.\"\\n                )\\n        return values' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\sequential.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _call(\\n        self,\\n        inputs: Dict[str, str],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, str]:\\n        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\\n        _input = inputs[self.input_key]\\n        color_mapping = get_color_mapping([str(i) for i in range(len(self.chains))])\\n        for i, chain in enumerate(self.chains):\\n            _input = chain.run(_input, callbacks=_run_manager.get_child(f\"step_{i+1}\"))\\n            if self.strip_outputs:\\n                _input = _input.strip()\\n            _run_manager.on_text(\\n                _input, color=color_mapping[str(i)], end=\"\\\\n\", verbose=self.verbose\\n            )\\n        return {self.output_key: _input}\\n\\n    async def _acall(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        _run_manager = run_manager or AsyncCallbackManagerForChainRun.get_noop_manager()\\n        _input = inputs[self.input_key]\\n        color_mapping = get_color_mapping([str(i) for i in range(len(self.chains))])\\n        for i, chain in enumerate(self.chains):\\n            _input = await chain.arun(\\n                _input, callbacks=_run_manager.get_child(f\"step_{i+1}\")\\n            )\\n            if self.strip_outputs:\\n                _input = _input.strip()\\n            await _run_manager.on_text(\\n                _input, color=color_mapping[str(i)], end=\"\\\\n\", verbose=self.verbose\\n            )\\n        return {self.output_key: _input}' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\sequential.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Chain that runs an arbitrary python function.\"\"\"\\nimport functools\\nimport logging\\nfrom typing import Any, Awaitable, Callable, Dict, List, Optional\\n\\nfrom langchain_core.callbacks import (\\n    AsyncCallbackManagerForChainRun,\\n    CallbackManagerForChainRun,\\n)\\nfrom langchain_core.pydantic_v1 import Field\\n\\nfrom langchain.chains.base import Chain\\n\\nlogger = logging.getLogger(__name__)\\n\\n\\nclass TransformChain(Chain):\\n    \"\"\"Chain that transforms the chain output.\\n\\n    Example:\\n        .. code-block:: python\\n\\n            from langchain.chains import TransformChain\\n            transform_chain = TransformChain(input_variables=[\"text\"],\\n             output_variables[\"entities\"], transform=func())\\n    \"\"\"\\n\\n    input_variables: List[str]\\n    \"\"\"The keys expected by the transform\\'s input dictionary.\"\"\"\\n    output_variables: List[str]\\n    \"\"\"The keys returned by the transform\\'s output dictionary.\"\"\"\\n    transform_cb: Callable[[Dict[str, str]], Dict[str, str]] = Field(alias=\"transform\")\\n    \"\"\"The transform function.\"\"\"\\n    atransform_cb: Optional[\\n        Callable[[Dict[str, Any]], Awaitable[Dict[str, Any]]]\\n    ] = Field(None, alias=\"atransform\")\\n    \"\"\"The async coroutine transform function.\"\"\"\\n\\n    @staticmethod\\n    @functools.lru_cache\\n    def _log_once(msg: str) -> None:\\n        \"\"\"Log a message once.\\n\\n        :meta private:\\n        \"\"\"\\n        logger.warning(msg)\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Expect input keys.\\n\\n        :meta private:\\n        \"\"\"\\n        return self.input_variables\\n\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Return output keys.\\n\\n        :meta private:\\n        \"\"\"\\n        return self.output_variables\\n\\n    def _call(\\n        self,\\n        inputs: Dict[str, str],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, str]:\\n        return self.transform_cb(inputs)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\transform.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='async def _acall(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        if self.atransform_cb is not None:\\n            return await self.atransform_cb(inputs)\\n        else:\\n            self._log_once(\\n                \"TransformChain\\'s atransform is not provided, falling\"\\n                \" back to synchronous transform\"\\n            )\\n            return self.transform_cb(inputs)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\transform.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"**Chains** are easily reusable components linked together.\\n\\nChains encode a sequence of calls to components like models, document retrievers,\\nother Chains, etc., and provide a simple interface to this sequence.\\n\\nThe Chain interface makes it easy to create apps that are:\\n\\n    - **Stateful:** add Memory to any Chain to give it state,\\n    - **Observable:** pass Callbacks to a Chain to execute additional functionality,\\n      like logging, outside the main sequence of component calls,\\n    - **Composable:** combine Chains with other components, including other Chains.\\n\\n**Class hierarchy:**\\n\\n.. code-block::\\n\\n    Chain --> <name>Chain  # Examples: LLMChain, MapReduceChain, RouterChain\\n\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain.chains.api.base import APIChain\\nfrom langchain.chains.api.openapi.chain import OpenAPIEndpointChain\\nfrom langchain.chains.combine_documents.base import AnalyzeDocumentChain\\nfrom langchain.chains.combine_documents.map_reduce import MapReduceDocumentsChain\\nfrom langchain.chains.combine_documents.map_rerank import MapRerankDocumentsChain\\nfrom langchain.chains.combine_documents.reduce import ReduceDocumentsChain\\nfrom langchain.chains.combine_documents.refine import RefineDocumentsChain\\nfrom langchain.chains.combine_documents.stuff import StuffDocumentsChain\\nfrom langchain.chains.constitutional_ai.base import ConstitutionalChain\\nfrom langchain.chains.conversation.base import ConversationChain\\nfrom langchain.chains.conversational_retrieval.base import (\\n    ChatVectorDBChain,\\n    ConversationalRetrievalChain,\\n)\\nfrom langchain.chains.example_generator import generate_example\\nfrom langchain.chains.flare.base import FlareChain\\nfrom langchain.chains.graph_qa.arangodb import ArangoGraphQAChain\\nfrom langchain.chains.graph_qa.base import GraphQAChain\\nfrom langchain.chains.graph_qa.cypher import GraphCypherQAChain\\nfrom langchain.chains.graph_qa.falkordb import FalkorDBQAChain\\nfrom langchain.chains.graph_qa.hugegraph import HugeGraphQAChain\\nfrom langchain.chains.graph_qa.kuzu import KuzuQAChain\\nfrom langchain.chains.graph_qa.nebulagraph import NebulaGraphQAChain\\nfrom langchain.chains.graph_qa.neptune_cypher import NeptuneOpenCypherQAChain\\nfrom langchain.chains.graph_qa.sparql import GraphSparqlQAChain\\nfrom langchain.chains.history_aware_retriever import create_history_aware_retriever\\nfrom langchain.chains.hyde.base import HypotheticalDocumentEmbedder\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.chains.llm_checker.base import LLMCheckerChain\\nfrom langchain.chains.llm_math.base import LLMMathChain\\nfrom langchain.chains.llm_requests import LLMRequestsChain\\nfrom langchain.chains.llm_summarization_checker.base import LLMSummarizationCheckerChain\\nfrom langchain.chains.loading import load_chain' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain.chains.mapreduce import MapReduceChain\\nfrom langchain.chains.moderation import OpenAIModerationChain\\nfrom langchain.chains.natbot.base import NatBotChain\\nfrom langchain.chains.openai_functions import (\\n    create_citation_fuzzy_match_chain,\\n    create_extraction_chain,\\n    create_extraction_chain_pydantic,\\n    create_qa_with_sources_chain,\\n    create_qa_with_structure_chain,\\n    create_tagging_chain,\\n    create_tagging_chain_pydantic,\\n)\\nfrom langchain.chains.qa_generation.base import QAGenerationChain\\nfrom langchain.chains.qa_with_sources.base import QAWithSourcesChain\\nfrom langchain.chains.qa_with_sources.retrieval import RetrievalQAWithSourcesChain\\nfrom langchain.chains.qa_with_sources.vector_db import VectorDBQAWithSourcesChain\\nfrom langchain.chains.retrieval import create_retrieval_chain\\nfrom langchain.chains.retrieval_qa.base import (\\n    RetrievalQA,\\n    VectorDBQA,\\n)\\nfrom langchain.chains.router import (\\n    LLMRouterChain,\\n    MultiPromptChain,\\n    MultiRetrievalQAChain,\\n    MultiRouteChain,\\n    RouterChain,\\n)\\nfrom langchain.chains.sequential import SequentialChain, SimpleSequentialChain\\nfrom langchain.chains.sql_database.query import create_sql_query_chain\\nfrom langchain.chains.summarize import load_summarize_chain\\nfrom langchain.chains.transform import TransformChain' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='__all__ = [\\n    \"APIChain\",\\n    \"AnalyzeDocumentChain\",\\n    \"ArangoGraphQAChain\",\\n    \"ChatVectorDBChain\",\\n    \"ConstitutionalChain\",\\n    \"ConversationChain\",\\n    \"ConversationalRetrievalChain\",\\n    \"FalkorDBQAChain\",\\n    \"FlareChain\",\\n    \"GraphCypherQAChain\",\\n    \"GraphQAChain\",\\n    \"GraphSparqlQAChain\",\\n    \"HugeGraphQAChain\",\\n    \"HypotheticalDocumentEmbedder\",\\n    \"KuzuQAChain\",\\n    \"LLMChain\",\\n    \"LLMCheckerChain\",\\n    \"LLMMathChain\",\\n    \"LLMRequestsChain\",\\n    \"LLMRouterChain\",\\n    \"LLMSummarizationCheckerChain\",\\n    \"MapReduceChain\",\\n    \"MapReduceDocumentsChain\",\\n    \"MapRerankDocumentsChain\",\\n    \"MultiPromptChain\",\\n    \"MultiRetrievalQAChain\",\\n    \"MultiRouteChain\",\\n    \"NatBotChain\",\\n    \"NebulaGraphQAChain\",\\n    \"NeptuneOpenCypherQAChain\",\\n    \"OpenAIModerationChain\",\\n    \"OpenAPIEndpointChain\",\\n    \"QAGenerationChain\",\\n    \"QAWithSourcesChain\",\\n    \"ReduceDocumentsChain\",\\n    \"RefineDocumentsChain\",\\n    \"RetrievalQA\",\\n    \"RetrievalQAWithSourcesChain\",\\n    \"RouterChain\",\\n    \"SequentialChain\",\\n    \"SimpleSequentialChain\",\\n    \"StuffDocumentsChain\",\\n    \"TransformChain\",\\n    \"VectorDBQA\",\\n    \"VectorDBQAWithSourcesChain\",\\n    \"create_citation_fuzzy_match_chain\",\\n    \"create_extraction_chain\",\\n    \"create_extraction_chain_pydantic\",\\n    \"create_qa_with_sources_chain\",\\n    \"create_qa_with_structure_chain\",\\n    \"create_tagging_chain\",\\n    \"create_tagging_chain_pydantic\",\\n    \"generate_example\",\\n    \"load_chain\",\\n    \"create_sql_query_chain\",\\n    \"create_retrieval_chain\",\\n    \"create_history_aware_retriever\",\\n    \"load_summarize_chain\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Chain that makes API calls and summarizes the responses to answer a question.\"\"\"\\nfrom __future__ import annotations\\n\\nfrom typing import Any, Dict, List, Optional, Sequence, Tuple\\nfrom urllib.parse import urlparse\\n\\nfrom langchain_community.utilities.requests import TextRequestsWrapper\\nfrom langchain_core.callbacks import (\\n    AsyncCallbackManagerForChainRun,\\n    CallbackManagerForChainRun,\\n)\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.prompts import BasePromptTemplate\\nfrom langchain_core.pydantic_v1 import Field, root_validator\\n\\nfrom langchain.chains.api.prompt import API_RESPONSE_PROMPT, API_URL_PROMPT\\nfrom langchain.chains.base import Chain\\nfrom langchain.chains.llm import LLMChain\\n\\n\\ndef _extract_scheme_and_domain(url: str) -> Tuple[str, str]:\\n    \"\"\"Extract the scheme + domain from a given URL.\\n\\n    Args:\\n        url (str): The input URL.\\n\\n    Returns:\\n        return a 2-tuple of scheme and domain\\n    \"\"\"\\n    parsed_uri = urlparse(url)\\n    return parsed_uri.scheme, parsed_uri.netloc\\n\\n\\ndef _check_in_allowed_domain(url: str, limit_to_domains: Sequence[str]) -> bool:\\n    \"\"\"Check if a URL is in the allowed domains.\\n\\n    Args:\\n        url (str): The input URL.\\n        limit_to_domains (Sequence[str]): The allowed domains.\\n\\n    Returns:\\n        bool: True if the URL is in the allowed domains, False otherwise.\\n    \"\"\"\\n    scheme, domain = _extract_scheme_and_domain(url)\\n\\n    for allowed_domain in limit_to_domains:\\n        allowed_scheme, allowed_domain = _extract_scheme_and_domain(allowed_domain)\\n        if scheme == allowed_scheme and domain == allowed_domain:\\n            return True\\n    return False\\n\\n\\nclass APIChain(Chain):\\n    \"\"\"Chain that makes API calls and summarizes the responses to answer a question.\\n\\n    *Security Note*: This API chain uses the requests toolkit\\n        to make GET, POST, PATCH, PUT, and DELETE requests to an API.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\api\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Exercise care in who is allowed to use this chain. If exposing\\n        to end users, consider that users will be able to make arbitrary\\n        requests on behalf of the server hosting the code. For example,\\n        users could ask the server to make a request to a private API\\n        that is only accessible from the server.\\n\\n        Control access to who can submit issue requests using this toolkit and\\n        what network access it has.\\n\\n        See https://python.langchain.com/docs/security for more information.\\n    \"\"\"\\n\\n    api_request_chain: LLMChain\\n    api_answer_chain: LLMChain\\n    requests_wrapper: TextRequestsWrapper = Field(exclude=True)\\n    api_docs: str\\n    question_key: str = \"question\"  #: :meta private:\\n    output_key: str = \"output\"  #: :meta private:\\n    limit_to_domains: Optional[Sequence[str]]\\n    \"\"\"Use to limit the domains that can be accessed by the API chain.\\n    \\n    * For example, to limit to just the domain `https://www.example.com`, set\\n        `limit_to_domains=[\"https://www.example.com\"]`.\\n        \\n    * The default value is an empty tuple, which means that no domains are\\n      allowed by default. By design this will raise an error on instantiation.\\n    * Use a None if you want to allow all domains by default -- this is not\\n      recommended for security reasons, as it would allow malicious users to\\n      make requests to arbitrary URLS including internal APIs accessible from\\n      the server.\\n    \"\"\"\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Expect input key.\\n\\n        :meta private:\\n        \"\"\"\\n        return [self.question_key]\\n\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Expect output key.\\n\\n        :meta private:\\n        \"\"\"\\n        return [self.output_key]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\api\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@root_validator(pre=True)\\n    def validate_api_request_prompt(cls, values: Dict) -> Dict:\\n        \"\"\"Check that api request prompt expects the right variables.\"\"\"\\n        input_vars = values[\"api_request_chain\"].prompt.input_variables\\n        expected_vars = {\"question\", \"api_docs\"}\\n        if set(input_vars) != expected_vars:\\n            raise ValueError(\\n                f\"Input variables should be {expected_vars}, got {input_vars}\"\\n            )\\n        return values\\n\\n    @root_validator(pre=True)\\n    def validate_limit_to_domains(cls, values: Dict) -> Dict:\\n        \"\"\"Check that allowed domains are valid.\"\"\"\\n        if \"limit_to_domains\" not in values:\\n            raise ValueError(\\n                \"You must specify a list of domains to limit access using \"\\n                \"`limit_to_domains`\"\\n            )\\n        if not values[\"limit_to_domains\"] and values[\"limit_to_domains\"] is not None:\\n            raise ValueError(\\n                \"Please provide a list of domains to limit access using \"\\n                \"`limit_to_domains`.\"\\n            )\\n        return values\\n\\n    @root_validator(pre=True)\\n    def validate_api_answer_prompt(cls, values: Dict) -> Dict:\\n        \"\"\"Check that api answer prompt expects the right variables.\"\"\"\\n        input_vars = values[\"api_answer_chain\"].prompt.input_variables\\n        expected_vars = {\"question\", \"api_docs\", \"api_url\", \"api_response\"}\\n        if set(input_vars) != expected_vars:\\n            raise ValueError(\\n                f\"Input variables should be {expected_vars}, got {input_vars}\"\\n            )\\n        return values' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\api\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _call(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, str]:\\n        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\\n        question = inputs[self.question_key]\\n        api_url = self.api_request_chain.predict(\\n            question=question,\\n            api_docs=self.api_docs,\\n            callbacks=_run_manager.get_child(),\\n        )\\n        _run_manager.on_text(api_url, color=\"green\", end=\"\\\\n\", verbose=self.verbose)\\n        api_url = api_url.strip()\\n        if self.limit_to_domains and not _check_in_allowed_domain(\\n            api_url, self.limit_to_domains\\n        ):\\n            raise ValueError(\\n                f\"{api_url} is not in the allowed domains: {self.limit_to_domains}\"\\n            )\\n        api_response = self.requests_wrapper.get(api_url)\\n        _run_manager.on_text(\\n            str(api_response), color=\"yellow\", end=\"\\\\n\", verbose=self.verbose\\n        )\\n        answer = self.api_answer_chain.predict(\\n            question=question,\\n            api_docs=self.api_docs,\\n            api_url=api_url,\\n            api_response=api_response,\\n            callbacks=_run_manager.get_child(),\\n        )\\n        return {self.output_key: answer}' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\api\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='async def _acall(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\\n    ) -> Dict[str, str]:\\n        _run_manager = run_manager or AsyncCallbackManagerForChainRun.get_noop_manager()\\n        question = inputs[self.question_key]\\n        api_url = await self.api_request_chain.apredict(\\n            question=question,\\n            api_docs=self.api_docs,\\n            callbacks=_run_manager.get_child(),\\n        )\\n        await _run_manager.on_text(\\n            api_url, color=\"green\", end=\"\\\\n\", verbose=self.verbose\\n        )\\n        api_url = api_url.strip()\\n        if self.limit_to_domains and not _check_in_allowed_domain(\\n            api_url, self.limit_to_domains\\n        ):\\n            raise ValueError(\\n                f\"{api_url} is not in the allowed domains: {self.limit_to_domains}\"\\n            )\\n        api_response = await self.requests_wrapper.aget(api_url)\\n        await _run_manager.on_text(\\n            str(api_response), color=\"yellow\", end=\"\\\\n\", verbose=self.verbose\\n        )\\n        answer = await self.api_answer_chain.apredict(\\n            question=question,\\n            api_docs=self.api_docs,\\n            api_url=api_url,\\n            api_response=api_response,\\n            callbacks=_run_manager.get_child(),\\n        )\\n        return {self.output_key: answer}' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\api\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@classmethod\\n    def from_llm_and_api_docs(\\n        cls,\\n        llm: BaseLanguageModel,\\n        api_docs: str,\\n        headers: Optional[dict] = None,\\n        api_url_prompt: BasePromptTemplate = API_URL_PROMPT,\\n        api_response_prompt: BasePromptTemplate = API_RESPONSE_PROMPT,\\n        limit_to_domains: Optional[Sequence[str]] = tuple(),\\n        **kwargs: Any,\\n    ) -> APIChain:\\n        \"\"\"Load chain from just an LLM and the api docs.\"\"\"\\n        get_request_chain = LLMChain(llm=llm, prompt=api_url_prompt)\\n        requests_wrapper = TextRequestsWrapper(headers=headers)\\n        get_answer_chain = LLMChain(llm=llm, prompt=api_response_prompt)\\n        return cls(\\n            api_request_chain=get_request_chain,\\n            api_answer_chain=get_answer_chain,\\n            requests_wrapper=requests_wrapper,\\n            api_docs=api_docs,\\n            limit_to_domains=limit_to_domains,\\n            **kwargs,\\n        )\\n\\n    @property\\n    def _chain_type(self) -> str:\\n        return \"api_chain\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\api\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# flake8: noqa\\nNEWS_DOCS = \"\"\"API documentation:\\nEndpoint: https://newsapi.org\\nTop headlines /v2/top-headlines\\n\\nThis endpoint provides live top and breaking headlines for a country, specific category in a country, single source, or multiple sources. You can also search with keywords. Articles are sorted by the earliest date published first.\\n\\nThis endpoint is great for retrieving headlines for use with news tickers or similar.\\nRequest parameters\\n\\n    country | The 2-letter ISO 3166-1 code of the country you want to get headlines for. Possible options: ae ar at au be bg br ca ch cn co cu cz de eg fr gb gr hk hu id ie il in it jp kr lt lv ma mx my ng nl no nz ph pl pt ro rs ru sa se sg si sk th tr tw ua us ve za. Note: you can\\'t mix this param with the sources param.\\n    category | The category you want to get headlines for. Possible options: business entertainment general health science sports technology. Note: you can\\'t mix this param with the sources param.\\n    sources | A comma-separated string of identifiers for the news sources or blogs you want headlines from. Use the /top-headlines/sources endpoint to locate these programmatically or look at the sources index. Note: you can\\'t mix this param with the country or category params.\\n    q | Keywords or a phrase to search for.\\n    pageSize | int | The number of results to return per page (request). 20 is the default, 100 is the maximum.\\n    page | int | Use this to page through the results if the total results found is greater than the page size.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\api\\\\news_docs.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Response object\\n    status | string | If the request was successful or not. Options: ok, error. In the case of error a code and message property will be populated.\\n    totalResults | int | The total number of results available for your request.\\n    articles | array[article] | The results of the request.\\n    source | object | The identifier id and a display name name for the source this article came from.\\n    author | string | The author of the article\\n    title | string | The headline or title of the article.\\n    description | string | A description or snippet from the article.\\n    url | string | The direct URL to the article.\\n    urlToImage | string | The URL to a relevant image for the article.\\n    publishedAt | string | The date and time that the article was published, in UTC (+000)\\n    content | string | The unformatted content of the article, where available. This is truncated to 200 chars.\\n\\nUse page size: 2\\n\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\api\\\\news_docs.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# flake8: noqa\\nOPEN_METEO_DOCS = \"\"\"BASE URL: https://api.open-meteo.com/\\n\\nAPI Documentation\\nThe API endpoint /v1/forecast accepts a geographical coordinate, a list of weather variables and responds with a JSON hourly weather forecast for 7 days. Time always starts at 0:00 today and contains 168 hours. All URL parameters are listed below:' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\api\\\\open_meteo_docs.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Parameter\\tFormat\\tRequired\\tDefault\\tDescription\\nlatitude, longitude\\tFloating point\\tYes\\t\\tGeographical WGS84 coordinate of the location\\nhourly\\tString array\\tNo\\t\\tA list of weather variables which should be returned. Values can be comma separated, or multiple &hourly= parameter in the URL can be used.\\ndaily\\tString array\\tNo\\t\\tA list of daily weather variable aggregations which should be returned. Values can be comma separated, or multiple &daily= parameter in the URL can be used. If daily weather variables are specified, parameter timezone is required.\\ncurrent_weather\\tBool\\tNo\\tfalse\\tInclude current weather conditions in the JSON output.\\ntemperature_unit\\tString\\tNo\\tcelsius\\tIf fahrenheit is set, all temperature values are converted to Fahrenheit.\\nwindspeed_unit\\tString\\tNo\\tkmh\\tOther wind speed speed units: ms, mph and kn\\nprecipitation_unit\\tString\\tNo\\tmm\\tOther precipitation amount units: inch\\ntimeformat\\tString\\tNo\\tiso8601\\tIf format unixtime is selected, all time values are returned in UNIX epoch time in seconds. Please note that all timestamp are in GMT+0! For daily values with unix timestamps, please apply utc_offset_seconds again to get the correct date.\\ntimezone\\tString\\tNo\\tGMT\\tIf timezone is set, all timestamps are returned as local-time and data is returned starting at 00:00 local-time. Any time zone name from the time zone database is supported. If auto is set as a time zone, the coordinates will be automatically resolved to the local time zone.\\npast_days\\tInteger (0-2)\\tNo\\t0\\tIf past_days is set, yesterday or the day before yesterday data are also returned.\\nstart_date\\nend_date\\tString (yyyy-mm-dd)\\tNo\\t\\tThe time interval to get weather data. A day must be specified as an ISO8601 date (e.g. 2022-06-30).\\nmodels\\tString array\\tNo\\tauto\\tManually select one or more weather models. Per default, the best suitable weather models will be combined.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\api\\\\open_meteo_docs.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Hourly Parameter Definition\\nThe parameter &hourly= accepts the following values. Most weather variables are given as an instantaneous value for the indicated hour. Some variables like precipitation are calculated from the preceding hour as an average or sum.\\n\\nVariable\\tValid time\\tUnit\\tDescription\\ntemperature_2m\\tInstant\\t°C (°F)\\tAir temperature at 2 meters above ground\\nsnowfall\\tPreceding hour sum\\tcm (inch)\\tSnowfall amount of the preceding hour in centimeters. For the water equivalent in millimeter, divide by 7. E.g. 7 cm snow = 10 mm precipitation water equivalent\\nrain\\tPreceding hour sum\\tmm (inch)\\tRain from large scale weather systems of the preceding hour in millimeter\\nshowers\\tPreceding hour sum\\tmm (inch)\\tShowers from convective precipitation in millimeters from the preceding hour\\nweathercode\\tInstant\\tWMO code\\tWeather condition as a numeric code. Follow WMO weather interpretation codes. See table below for details.\\nsnow_depth\\tInstant\\tmeters\\tSnow depth on the ground\\nfreezinglevel_height\\tInstant\\tmeters\\tAltitude above sea level of the 0°C level\\nvisibility\\tInstant\\tmeters\\tViewing distance in meters. Influenced by low clouds, humidity and aerosols. Maximum visibility is approximately 24 km.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\api\\\\open_meteo_docs.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# flake8: noqa\\nPODCAST_DOCS = \"\"\"API documentation:\\nEndpoint: https://listen-api.listennotes.com/api/v2\\nGET /search\\n\\nThis API is for searching podcasts or episodes.\\n\\nQuery parameters table:\\nq | string | Search term, e.g., person, place, topic... You can use double quotes to do verbatim match, e.g., \"game of thrones\". Otherwise, it\\'s fuzzy search. | required\\ntype | string | What type of contents do you want to search for? Available values: episode, podcast, curated. default: episode | optional\\npage_size | integer | The maximum number of search results per page. A valid value should be an integer between 1 and 10 (inclusive). default: 3 | optional\\nlanguage | string | Limit search results to a specific language, e.g., English, Chinese ... If not specified, it\\'ll be any language. It works only when type is episode or podcast. | optional\\nregion | string | Limit search results to a specific region (e.g., us, gb, in...). If not specified, it\\'ll be any region. It works only when type is episode or podcast. | optional\\nlen_min | integer | Minimum audio length in minutes. Applicable only when type parameter is episode or podcast. If type parameter is episode, it\\'s for audio length of an episode. If type parameter is podcast, it\\'s for average audio length of all episodes in a podcast. | optional\\nlen_max | integer | Maximum audio length in minutes. Applicable only when type parameter is episode or podcast. If type parameter is episode, it\\'s for audio length of an episode. If type parameter is podcast, it\\'s for average audio length of all episodes in a podcast. | optional\\n\\nResponse schema (JSON object):\\nnext_offset | integer | optional\\ntotal | integer | optional\\nresults | array[object] (Episode / Podcast List Result Object)\\n\\nEach object in the \"results\" key has the following schema:\\nlistennotes_url | string | optional\\nid | integer | optional\\ntitle_highlighted | string | optional\\n\\nUse page_size: 3\\n\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\api\\\\podcast_docs.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# flake8: noqa\\nfrom langchain_core.prompts.prompt import PromptTemplate\\n\\nAPI_URL_PROMPT_TEMPLATE = \"\"\"You are given the below API Documentation:\\n{api_docs}\\nUsing this documentation, generate the full API url to call for answering the user question.\\nYou should build the API url in order to get a response that is as short as possible, while still getting the necessary information to answer the question. Pay attention to deliberately exclude any unnecessary pieces of data in the API call.\\n\\nQuestion:{question}\\nAPI url:\"\"\"\\n\\nAPI_URL_PROMPT = PromptTemplate(\\n    input_variables=[\\n        \"api_docs\",\\n        \"question\",\\n    ],\\n    template=API_URL_PROMPT_TEMPLATE,\\n)\\n\\nAPI_RESPONSE_PROMPT_TEMPLATE = (\\n    API_URL_PROMPT_TEMPLATE\\n    + \"\"\" {api_url}\\n\\nHere is the response from the API:\\n\\n{api_response}\\n\\nSummarize this response to answer the original question.\\n\\nSummary:\"\"\"\\n)\\n\\nAPI_RESPONSE_PROMPT = PromptTemplate(\\n    input_variables=[\"api_docs\", \"question\", \"api_url\", \"api_response\"],\\n    template=API_RESPONSE_PROMPT_TEMPLATE,\\n)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\api\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# flake8: noqa\\nTMDB_DOCS = \"\"\"API documentation:\\nEndpoint: https://api.themoviedb.org/3\\nGET /search/movie\\n\\nThis API is for searching movies.\\n\\nQuery parameters table:\\nlanguage | string | Pass a ISO 639-1 value to display translated data for the fields that support it. minLength: 2, pattern: ([a-z]{2})-([A-Z]{2}), default: en-US | optional\\nquery | string | Pass a text query to search. This value should be URI encoded. minLength: 1 | required\\npage | integer | Specify which page to query. minimum: 1, maximum: 1000, default: 1 | optional\\ninclude_adult | boolean | Choose whether to include adult (pornography) content in the results. default | optional\\nregion | string | Specify a ISO 3166-1 code to filter release dates. Must be uppercase. pattern: ^[A-Z]{2}$ | optional\\nyear | integer  | optional\\nprimary_release_year | integer | optional\\n\\nResponse schema (JSON object):\\npage | integer | optional\\ntotal_results | integer | optional\\ntotal_pages | integer | optional\\nresults | array[object] (Movie List Result Object)\\n\\nEach object in the \"results\" key has the following schema:\\nposter_path | string or null | optional\\nadult | boolean | optional\\noverview | string | optional\\nrelease_date | string | optional\\ngenre_ids | array[integer] | optional\\nid | integer | optional\\noriginal_title | string | optional\\noriginal_language | string | optional\\ntitle | string | optional\\nbackdrop_path | string or null | optional\\npopularity | number | optional\\nvote_count | integer | optional\\nvideo | boolean | optional\\nvote_average | number | optional\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\api\\\\tmdb_docs.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Chain that makes API calls and summarizes the responses to answer a question.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\api\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Chain that makes API calls and summarizes the responses to answer a question.\"\"\"\\nfrom __future__ import annotations\\n\\nimport json\\nfrom typing import Any, Dict, List, NamedTuple, Optional, cast\\n\\nfrom langchain_community.tools.openapi.utils.api_models import APIOperation\\nfrom langchain_community.utilities.requests import Requests\\nfrom langchain_core.callbacks import CallbackManagerForChainRun, Callbacks\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.pydantic_v1 import BaseModel, Field\\nfrom requests import Response\\n\\nfrom langchain.chains.api.openapi.requests_chain import APIRequesterChain\\nfrom langchain.chains.api.openapi.response_chain import APIResponderChain\\nfrom langchain.chains.base import Chain\\nfrom langchain.chains.llm import LLMChain\\n\\n\\nclass _ParamMapping(NamedTuple):\\n    \"\"\"Mapping from parameter name to parameter value.\"\"\"\\n\\n    query_params: List[str]\\n    body_params: List[str]\\n    path_params: List[str]\\n\\n\\nclass OpenAPIEndpointChain(Chain, BaseModel):\\n    \"\"\"Chain interacts with an OpenAPI endpoint using natural language.\"\"\"\\n\\n    api_request_chain: LLMChain\\n    api_response_chain: Optional[LLMChain]\\n    api_operation: APIOperation\\n    requests: Requests = Field(exclude=True, default_factory=Requests)\\n    param_mapping: _ParamMapping = Field(alias=\"param_mapping\")\\n    return_intermediate_steps: bool = False\\n    instructions_key: str = \"instructions\"  #: :meta private:\\n    output_key: str = \"output\"  #: :meta private:\\n    max_text_length: Optional[int] = Field(ge=0)  #: :meta private:\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Expect input key.\\n\\n        :meta private:\\n        \"\"\"\\n        return [self.instructions_key]\\n\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Expect output key.\\n\\n        :meta private:\\n        \"\"\"\\n        if not self.return_intermediate_steps:\\n            return [self.output_key]\\n        else:\\n            return [self.output_key, \"intermediate_steps\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\api\\\\openapi\\\\chain.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _construct_path(self, args: Dict[str, str]) -> str:\\n        \"\"\"Construct the path from the deserialized input.\"\"\"\\n        path = self.api_operation.base_url + self.api_operation.path\\n        for param in self.param_mapping.path_params:\\n            path = path.replace(f\"{{{param}}}\", str(args.pop(param, \"\")))\\n        return path\\n\\n    def _extract_query_params(self, args: Dict[str, str]) -> Dict[str, str]:\\n        \"\"\"Extract the query params from the deserialized input.\"\"\"\\n        query_params = {}\\n        for param in self.param_mapping.query_params:\\n            if param in args:\\n                query_params[param] = args.pop(param)\\n        return query_params\\n\\n    def _extract_body_params(self, args: Dict[str, str]) -> Optional[Dict[str, str]]:\\n        \"\"\"Extract the request body params from the deserialized input.\"\"\"\\n        body_params = None\\n        if self.param_mapping.body_params:\\n            body_params = {}\\n            for param in self.param_mapping.body_params:\\n                if param in args:\\n                    body_params[param] = args.pop(param)\\n        return body_params\\n\\n    def deserialize_json_input(self, serialized_args: str) -> dict:\\n        \"\"\"Use the serialized typescript dictionary.\\n\\n        Resolve the path, query params dict, and optional requestBody dict.\\n        \"\"\"\\n        args: dict = json.loads(serialized_args)\\n        path = self._construct_path(args)\\n        body_params = self._extract_body_params(args)\\n        query_params = self._extract_query_params(args)\\n        return {\\n            \"url\": path,\\n            \"data\": body_params,\\n            \"params\": query_params,\\n        }\\n\\n    def _get_output(self, output: str, intermediate_steps: dict) -> dict:\\n        \"\"\"Return the output from the API call.\"\"\"\\n        if self.return_intermediate_steps:\\n            return {\\n                self.output_key: output,\\n                \"intermediate_steps\": intermediate_steps,\\n            }\\n        else:\\n            return {self.output_key: output}' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\api\\\\openapi\\\\chain.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _call(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, str]:\\n        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\\n        intermediate_steps = {}\\n        instructions = inputs[self.instructions_key]\\n        instructions = instructions[: self.max_text_length]\\n        _api_arguments = self.api_request_chain.predict_and_parse(\\n            instructions=instructions, callbacks=_run_manager.get_child()\\n        )\\n        api_arguments = cast(str, _api_arguments)\\n        intermediate_steps[\"request_args\"] = api_arguments\\n        _run_manager.on_text(\\n            api_arguments, color=\"green\", end=\"\\\\n\", verbose=self.verbose\\n        )\\n        if api_arguments.startswith(\"ERROR\"):\\n            return self._get_output(api_arguments, intermediate_steps)\\n        elif api_arguments.startswith(\"MESSAGE:\"):\\n            return self._get_output(\\n                api_arguments[len(\"MESSAGE:\") :], intermediate_steps\\n            )\\n        try:\\n            request_args = self.deserialize_json_input(api_arguments)\\n            method = getattr(self.requests, self.api_operation.method.value)\\n            api_response: Response = method(**request_args)\\n            if api_response.status_code != 200:\\n                method_str = str(self.api_operation.method.value)\\n                response_text = (\\n                    f\"{api_response.status_code}: {api_response.reason}\"\\n                    + f\"\\\\nFor {method_str.upper()}  {request_args[\\'url\\']}\\\\n\"\\n                    + f\"Called with args: {request_args[\\'params\\']}\"\\n                )\\n            else:\\n                response_text = api_response.text\\n        except Exception as e:\\n            response_text = f\"Error with message {str(e)}\"\\n        response_text = response_text[: self.max_text_length]\\n        intermediate_steps[\"response_text\"] = response_text\\n        _run_manager.on_text(\\n            response_text, color=\"blue\", end=\"\\\\n\", verbose=self.verbose\\n        )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\api\\\\openapi\\\\chain.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='if self.api_response_chain is not None:\\n            _answer = self.api_response_chain.predict_and_parse(\\n                response=response_text,\\n                instructions=instructions,\\n                callbacks=_run_manager.get_child(),\\n            )\\n            answer = cast(str, _answer)\\n            _run_manager.on_text(answer, color=\"yellow\", end=\"\\\\n\", verbose=self.verbose)\\n            return self._get_output(answer, intermediate_steps)\\n        else:\\n            return self._get_output(response_text, intermediate_steps)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\api\\\\openapi\\\\chain.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@classmethod\\n    def from_url_and_method(\\n        cls,\\n        spec_url: str,\\n        path: str,\\n        method: str,\\n        llm: BaseLanguageModel,\\n        requests: Optional[Requests] = None,\\n        return_intermediate_steps: bool = False,\\n        **kwargs: Any,\\n        # TODO: Handle async\\n    ) -> \"OpenAPIEndpointChain\":\\n        \"\"\"Create an OpenAPIEndpoint from a spec at the specified url.\"\"\"\\n        operation = APIOperation.from_openapi_url(spec_url, path, method)\\n        return cls.from_api_operation(\\n            operation,\\n            requests=requests,\\n            llm=llm,\\n            return_intermediate_steps=return_intermediate_steps,\\n            **kwargs,\\n        )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\api\\\\openapi\\\\chain.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@classmethod\\n    def from_api_operation(\\n        cls,\\n        operation: APIOperation,\\n        llm: BaseLanguageModel,\\n        requests: Optional[Requests] = None,\\n        verbose: bool = False,\\n        return_intermediate_steps: bool = False,\\n        raw_response: bool = False,\\n        callbacks: Callbacks = None,\\n        **kwargs: Any,\\n        # TODO: Handle async\\n    ) -> \"OpenAPIEndpointChain\":\\n        \"\"\"Create an OpenAPIEndpointChain from an operation and a spec.\"\"\"\\n        param_mapping = _ParamMapping(\\n            query_params=operation.query_params,\\n            body_params=operation.body_params,\\n            path_params=operation.path_params,\\n        )\\n        requests_chain = APIRequesterChain.from_llm_and_typescript(\\n            llm,\\n            typescript_definition=operation.to_typescript(),\\n            verbose=verbose,\\n            callbacks=callbacks,\\n        )\\n        if raw_response:\\n            response_chain = None\\n        else:\\n            response_chain = APIResponderChain.from_llm(\\n                llm, verbose=verbose, callbacks=callbacks\\n            )\\n        _requests = requests or Requests()\\n        return cls(\\n            api_request_chain=requests_chain,\\n            api_response_chain=response_chain,\\n            api_operation=operation,\\n            requests=_requests,\\n            param_mapping=param_mapping,\\n            verbose=verbose,\\n            return_intermediate_steps=return_intermediate_steps,\\n            callbacks=callbacks,\\n            **kwargs,\\n        )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\api\\\\openapi\\\\chain.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# flake8: noqa\\nREQUEST_TEMPLATE = \"\"\"You are a helpful AI Assistant. Please provide JSON arguments to agentFunc() based on the user\\'s instructions.\\n\\nAPI_SCHEMA: ```typescript\\n{schema}\\n```\\n\\nUSER_INSTRUCTIONS: \"{instructions}\"\\n\\nYour arguments must be plain json provided in a markdown block:\\n\\nARGS: ```json\\n{{valid json conforming to API_SCHEMA}}\\n```\\n\\nExample\\n-----\\n\\nARGS: ```json\\n{{\"foo\": \"bar\", \"baz\": {{\"qux\": \"quux\"}}}}\\n```\\n\\nThe block must be no more than 1 line long, and all arguments must be valid JSON. All string arguments must be wrapped in double quotes.\\nYou MUST strictly comply to the types indicated by the provided schema, including all required args.\\n\\nIf you don\\'t have sufficient information to call the function due to things like requiring specific uuid\\'s, you can reply with the following message:\\n\\nMessage: ```text\\nConcise response requesting the additional information that would make calling the function successful.\\n```\\n\\nBegin\\n-----\\nARGS:\\n\"\"\"\\nRESPONSE_TEMPLATE = \"\"\"You are a helpful AI assistant trained to answer user queries from API responses.\\nYou attempted to call an API, which resulted in:\\nAPI_RESPONSE: {response}\\n\\nUSER_COMMENT: \"{instructions}\"\\n\\n\\nIf the API_RESPONSE can answer the USER_COMMENT respond with the following markdown json block:\\nResponse: ```json\\n{{\"response\": \"Human-understandable synthesis of the API_RESPONSE\"}}\\n```\\n\\nOtherwise respond with the following markdown json block:\\nResponse Error: ```json\\n{{\"response\": \"What you did and a concise statement of the resulting error. If it can be easily fixed, provide a suggestion.\"}}\\n```\\n\\nYou MUST respond as a markdown json code block. The person you are responding to CANNOT see the API_RESPONSE, so if there is any relevant information there you must include it in your response.\\n\\nBegin:\\n---\\n\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\api\\\\openapi\\\\prompts.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"request parser.\"\"\"\\n\\nimport json\\nimport re\\nfrom typing import Any\\n\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.output_parsers import BaseOutputParser\\nfrom langchain_core.prompts.prompt import PromptTemplate\\n\\nfrom langchain.chains.api.openapi.prompts import REQUEST_TEMPLATE\\nfrom langchain.chains.llm import LLMChain\\n\\n\\nclass APIRequesterOutputParser(BaseOutputParser):\\n    \"\"\"Parse the request and error tags.\"\"\"\\n\\n    def _load_json_block(self, serialized_block: str) -> str:\\n        try:\\n            return json.dumps(json.loads(serialized_block, strict=False))\\n        except json.JSONDecodeError:\\n            return \"ERROR serializing request.\"\\n\\n    def parse(self, llm_output: str) -> str:\\n        \"\"\"Parse the request and error tags.\"\"\"\\n\\n        json_match = re.search(r\"```json(.*?)```\", llm_output, re.DOTALL)\\n        if json_match:\\n            return self._load_json_block(json_match.group(1).strip())\\n        message_match = re.search(r\"```text(.*?)```\", llm_output, re.DOTALL)\\n        if message_match:\\n            return f\"MESSAGE: {message_match.group(1).strip()}\"\\n        return \"ERROR making request\"\\n\\n    @property\\n    def _type(self) -> str:\\n        return \"api_requester\"\\n\\n\\nclass APIRequesterChain(LLMChain):\\n    \"\"\"Get the request parser.\"\"\"\\n\\n    @classmethod\\n    def is_lc_serializable(cls) -> bool:\\n        return False\\n\\n    @classmethod\\n    def from_llm_and_typescript(\\n        cls,\\n        llm: BaseLanguageModel,\\n        typescript_definition: str,\\n        verbose: bool = True,\\n        **kwargs: Any,\\n    ) -> LLMChain:\\n        \"\"\"Get the request parser.\"\"\"\\n        output_parser = APIRequesterOutputParser()\\n        prompt = PromptTemplate(\\n            template=REQUEST_TEMPLATE,\\n            output_parser=output_parser,\\n            partial_variables={\"schema\": typescript_definition},\\n            input_variables=[\"instructions\"],\\n        )\\n        return cls(prompt=prompt, llm=llm, verbose=verbose, **kwargs)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\api\\\\openapi\\\\requests_chain.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Response parser.\"\"\"\\n\\nimport json\\nimport re\\nfrom typing import Any\\n\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.output_parsers import BaseOutputParser\\nfrom langchain_core.prompts.prompt import PromptTemplate\\n\\nfrom langchain.chains.api.openapi.prompts import RESPONSE_TEMPLATE\\nfrom langchain.chains.llm import LLMChain\\n\\n\\nclass APIResponderOutputParser(BaseOutputParser):\\n    \"\"\"Parse the response and error tags.\"\"\"\\n\\n    def _load_json_block(self, serialized_block: str) -> str:\\n        try:\\n            response_content = json.loads(serialized_block, strict=False)\\n            return response_content.get(\"response\", \"ERROR parsing response.\")\\n        except json.JSONDecodeError:\\n            return \"ERROR parsing response.\"\\n        except:\\n            raise\\n\\n    def parse(self, llm_output: str) -> str:\\n        \"\"\"Parse the response and error tags.\"\"\"\\n        json_match = re.search(r\"```json(.*?)```\", llm_output, re.DOTALL)\\n        if json_match:\\n            return self._load_json_block(json_match.group(1).strip())\\n        else:\\n            raise ValueError(f\"No response found in output: {llm_output}.\")\\n\\n    @property\\n    def _type(self) -> str:\\n        return \"api_responder\"\\n\\n\\nclass APIResponderChain(LLMChain):\\n    \"\"\"Get the response parser.\"\"\"\\n\\n    @classmethod\\n    def is_lc_serializable(cls) -> bool:\\n        return False\\n\\n    @classmethod\\n    def from_llm(\\n        cls, llm: BaseLanguageModel, verbose: bool = True, **kwargs: Any\\n    ) -> LLMChain:\\n        \"\"\"Get the response parser.\"\"\"\\n        output_parser = APIResponderOutputParser()\\n        prompt = PromptTemplate(\\n            template=RESPONSE_TEMPLATE,\\n            output_parser=output_parser,\\n            input_variables=[\"response\", \"instructions\"],\\n        )\\n        return cls(prompt=prompt, llm=llm, verbose=verbose, **kwargs)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\api\\\\openapi\\\\response_chain.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# flake8: noqa\\nfrom langchain_core.prompts.prompt import PromptTemplate\\n\\n_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:\"\"\"\\nCONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\\n\\nprompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\"\"\"\\nQA_PROMPT = PromptTemplate(\\n    template=prompt_template, input_variables=[\"context\", \"question\"]\\n)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\chat_vector_db\\\\prompts.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Base interface for chains combining documents.\"\"\"\\n\\nfrom abc import ABC, abstractmethod\\nfrom typing import Any, Dict, List, Optional, Tuple, Type\\n\\nfrom langchain_core.callbacks import (\\n    AsyncCallbackManagerForChainRun,\\n    CallbackManagerForChainRun,\\n)\\nfrom langchain_core.documents import Document\\nfrom langchain_core.prompts import BasePromptTemplate, PromptTemplate\\nfrom langchain_core.pydantic_v1 import BaseModel, Field, create_model\\nfrom langchain_core.runnables.config import RunnableConfig\\n\\nfrom langchain.chains.base import Chain\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter, TextSplitter\\n\\nDEFAULT_DOCUMENT_SEPARATOR = \"\\\\n\\\\n\"\\nDOCUMENTS_KEY = \"context\"\\nDEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(\"{page_content}\")\\n\\n\\ndef _validate_prompt(prompt: BasePromptTemplate) -> None:\\n    if DOCUMENTS_KEY not in prompt.input_variables:\\n        raise ValueError(\\n            f\"Prompt must accept {DOCUMENTS_KEY} as an input variable. Received prompt \"\\n            f\"with input variables: {prompt.input_variables}\"\\n        )\\n\\n\\nclass BaseCombineDocumentsChain(Chain, ABC):\\n    \"\"\"Base interface for chains combining documents.\\n\\n    Subclasses of this chain deal with combining documents in a variety of\\n    ways. This base class exists to add some uniformity in the interface these types\\n    of chains should expose. Namely, they expect an input key related to the documents\\n    to use (default `input_documents`), and then also expose a method to calculate\\n    the length of a prompt from documents (useful for outside callers to use to\\n    determine whether it\\'s safe to pass a list of documents into this chain or whether\\n    that will longer than the context length).\\n    \"\"\"\\n\\n    input_key: str = \"input_documents\"  #: :meta private:\\n    output_key: str = \"output_text\"  #: :meta private:' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\combine_documents\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def get_input_schema(\\n        self, config: Optional[RunnableConfig] = None\\n    ) -> Type[BaseModel]:\\n        return create_model(\\n            \"CombineDocumentsInput\",\\n            **{self.input_key: (List[Document], None)},  # type: ignore[call-overload]\\n        )\\n\\n    def get_output_schema(\\n        self, config: Optional[RunnableConfig] = None\\n    ) -> Type[BaseModel]:\\n        return create_model(\\n            \"CombineDocumentsOutput\",\\n            **{self.output_key: (str, None)},  # type: ignore[call-overload]\\n        )\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Expect input key.\\n\\n        :meta private:\\n        \"\"\"\\n        return [self.input_key]\\n\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Return output key.\\n\\n        :meta private:\\n        \"\"\"\\n        return [self.output_key]\\n\\n    def prompt_length(self, docs: List[Document], **kwargs: Any) -> Optional[int]:\\n        \"\"\"Return the prompt length given the documents passed in.\\n\\n        This can be used by a caller to determine whether passing in a list\\n        of documents would exceed a certain prompt length. This useful when\\n        trying to ensure that the size of a prompt remains below a certain\\n        context limit.\\n\\n        Args:\\n            docs: List[Document], a list of documents to use to calculate the\\n                total prompt length.\\n\\n        Returns:\\n            Returns None if the method does not depend on the prompt length,\\n            otherwise the length of the prompt in tokens.\\n        \"\"\"\\n        return None\\n\\n    @abstractmethod\\n    def combine_docs(self, docs: List[Document], **kwargs: Any) -> Tuple[str, dict]:\\n        \"\"\"Combine documents into a single string.\\n\\n        Args:\\n            docs: List[Document], the documents to combine\\n            **kwargs: Other parameters to use in combining documents, often\\n                other inputs to the prompt.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\combine_documents\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Returns:\\n            The first element returned is the single string output. The second\\n            element returned is a dictionary of other keys to return.\\n        \"\"\"\\n\\n    @abstractmethod\\n    async def acombine_docs(\\n        self, docs: List[Document], **kwargs: Any\\n    ) -> Tuple[str, dict]:\\n        \"\"\"Combine documents into a single string.\\n\\n        Args:\\n            docs: List[Document], the documents to combine\\n            **kwargs: Other parameters to use in combining documents, often\\n                other inputs to the prompt.\\n\\n        Returns:\\n            The first element returned is the single string output. The second\\n            element returned is a dictionary of other keys to return.\\n        \"\"\"\\n\\n    def _call(\\n        self,\\n        inputs: Dict[str, List[Document]],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, str]:\\n        \"\"\"Prepare inputs, call combine docs, prepare outputs.\"\"\"\\n        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\\n        docs = inputs[self.input_key]\\n        # Other keys are assumed to be needed for LLM prediction\\n        other_keys = {k: v for k, v in inputs.items() if k != self.input_key}\\n        output, extra_return_dict = self.combine_docs(\\n            docs, callbacks=_run_manager.get_child(), **other_keys\\n        )\\n        extra_return_dict[self.output_key] = output\\n        return extra_return_dict' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\combine_documents\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='async def _acall(\\n        self,\\n        inputs: Dict[str, List[Document]],\\n        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\\n    ) -> Dict[str, str]:\\n        \"\"\"Prepare inputs, call combine docs, prepare outputs.\"\"\"\\n        _run_manager = run_manager or AsyncCallbackManagerForChainRun.get_noop_manager()\\n        docs = inputs[self.input_key]\\n        # Other keys are assumed to be needed for LLM prediction\\n        other_keys = {k: v for k, v in inputs.items() if k != self.input_key}\\n        output, extra_return_dict = await self.acombine_docs(\\n            docs, callbacks=_run_manager.get_child(), **other_keys\\n        )\\n        extra_return_dict[self.output_key] = output\\n        return extra_return_dict\\n\\n\\nclass AnalyzeDocumentChain(Chain):\\n    \"\"\"Chain that splits documents, then analyzes it in pieces.\\n\\n    This chain is parameterized by a TextSplitter and a CombineDocumentsChain.\\n    This chain takes a single document as input, and then splits it up into chunks\\n    and then passes those chucks to the CombineDocumentsChain.\\n    \"\"\"\\n\\n    input_key: str = \"input_document\"  #: :meta private:\\n    text_splitter: TextSplitter = Field(default_factory=RecursiveCharacterTextSplitter)\\n    combine_docs_chain: BaseCombineDocumentsChain\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Expect input key.\\n\\n        :meta private:\\n        \"\"\"\\n        return [self.input_key]\\n\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Return output key.\\n\\n        :meta private:\\n        \"\"\"\\n        return self.combine_docs_chain.output_keys\\n\\n    def get_input_schema(\\n        self, config: Optional[RunnableConfig] = None\\n    ) -> Type[BaseModel]:\\n        return create_model(\\n            \"AnalyzeDocumentChain\",\\n            **{self.input_key: (str, None)},  # type: ignore[call-overload]\\n        )\\n\\n    def get_output_schema(\\n        self, config: Optional[RunnableConfig] = None\\n    ) -> Type[BaseModel]:\\n        return self.combine_docs_chain.get_output_schema(config)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\combine_documents\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _call(\\n        self,\\n        inputs: Dict[str, str],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, str]:\\n        \"\"\"Split document into chunks and pass to CombineDocumentsChain.\"\"\"\\n        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\\n        document = inputs[self.input_key]\\n        docs = self.text_splitter.create_documents([document])\\n        # Other keys are assumed to be needed for LLM prediction\\n        other_keys: Dict = {k: v for k, v in inputs.items() if k != self.input_key}\\n        other_keys[self.combine_docs_chain.input_key] = docs\\n        return self.combine_docs_chain(\\n            other_keys, return_only_outputs=True, callbacks=_run_manager.get_child()\\n        )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\combine_documents\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Combining documents by mapping a chain over them first, then combining results.\"\"\"\\n\\nfrom __future__ import annotations\\n\\nfrom typing import Any, Dict, List, Optional, Tuple, Type\\n\\nfrom langchain_core.callbacks import Callbacks\\nfrom langchain_core.documents import Document\\nfrom langchain_core.pydantic_v1 import BaseModel, Extra, create_model, root_validator\\nfrom langchain_core.runnables.config import RunnableConfig\\n\\nfrom langchain.chains.combine_documents.base import BaseCombineDocumentsChain\\nfrom langchain.chains.combine_documents.reduce import ReduceDocumentsChain\\nfrom langchain.chains.llm import LLMChain\\n\\n\\nclass MapReduceDocumentsChain(BaseCombineDocumentsChain):\\n    \"\"\"Combining documents by mapping a chain over them, then combining results.\\n\\n    We first call `llm_chain` on each document individually, passing in the\\n    `page_content` and any other kwargs. This is the `map` step.\\n\\n    We then process the results of that `map` step in a `reduce` step. This should\\n    likely be a ReduceDocumentsChain.\\n\\n    Example:\\n        .. code-block:: python\\n\\n            from langchain.chains import (\\n                StuffDocumentsChain,\\n                LLMChain,\\n                ReduceDocumentsChain,\\n                MapReduceDocumentsChain,\\n            )\\n            from langchain_core.prompts import PromptTemplate\\n            from langchain_community.llms import OpenAI' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\combine_documents\\\\map_reduce.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# This controls how each document will be formatted. Specifically,\\n            # it will be passed to `format_document` - see that function for more\\n            # details.\\n            document_prompt = PromptTemplate(\\n                input_variables=[\"page_content\"],\\n                 template=\"{page_content}\"\\n            )\\n            document_variable_name = \"context\"\\n            llm = OpenAI()\\n            # The prompt here should take as an input variable the\\n            # `document_variable_name`\\n            prompt = PromptTemplate.from_template(\\n                \"Summarize this content: {context}\"\\n            )\\n            llm_chain = LLMChain(llm=llm, prompt=prompt)\\n            # We now define how to combine these summaries\\n            reduce_prompt = PromptTemplate.from_template(\\n                \"Combine these summaries: {context}\"\\n            )\\n            reduce_llm_chain = LLMChain(llm=llm, prompt=reduce_prompt)\\n            combine_documents_chain = StuffDocumentsChain(\\n                llm_chain=reduce_llm_chain,\\n                document_prompt=document_prompt,\\n                document_variable_name=document_variable_name\\n            )\\n            reduce_documents_chain = ReduceDocumentsChain(\\n                combine_documents_chain=combine_documents_chain,\\n            )\\n            chain = MapReduceDocumentsChain(\\n                llm_chain=llm_chain,\\n                reduce_documents_chain=reduce_documents_chain,\\n            )\\n            # If we wanted to, we could also pass in collapse_documents_chain\\n            # which is specifically aimed at collapsing documents BEFORE\\n            # the final call.\\n            prompt = PromptTemplate.from_template(\\n                \"Collapse this content: {context}\"\\n            )\\n            llm_chain = LLMChain(llm=llm, prompt=prompt)\\n            collapse_documents_chain = StuffDocumentsChain(\\n                llm_chain=llm_chain,\\n                document_prompt=document_prompt,\\n                document_variable_name=document_variable_name\\n            )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\combine_documents\\\\map_reduce.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='reduce_documents_chain = ReduceDocumentsChain(\\n                combine_documents_chain=combine_documents_chain,\\n                collapse_documents_chain=collapse_documents_chain,\\n            )\\n            chain = MapReduceDocumentsChain(\\n                llm_chain=llm_chain,\\n                reduce_documents_chain=reduce_documents_chain,\\n            )\\n    \"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\combine_documents\\\\map_reduce.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='llm_chain: LLMChain\\n    \"\"\"Chain to apply to each document individually.\"\"\"\\n    reduce_documents_chain: BaseCombineDocumentsChain\\n    \"\"\"Chain to use to reduce the results of applying `llm_chain` to each doc.\\n    This typically either a ReduceDocumentChain or StuffDocumentChain.\"\"\"\\n    document_variable_name: str\\n    \"\"\"The variable name in the llm_chain to put the documents in.\\n    If only one variable in the llm_chain, this need not be provided.\"\"\"\\n    return_intermediate_steps: bool = False\\n    \"\"\"Return the results of the map steps in the output.\"\"\"\\n\\n    def get_output_schema(\\n        self, config: Optional[RunnableConfig] = None\\n    ) -> Type[BaseModel]:\\n        if self.return_intermediate_steps:\\n            return create_model(\\n                \"MapReduceDocumentsOutput\",\\n                **{\\n                    self.output_key: (str, None),\\n                    \"intermediate_steps\": (List[str], None),\\n                },  # type: ignore[call-overload]\\n            )\\n\\n        return super().get_output_schema(config)\\n\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Expect input key.\\n\\n        :meta private:\\n        \"\"\"\\n        _output_keys = super().output_keys\\n        if self.return_intermediate_steps:\\n            _output_keys = _output_keys + [\"intermediate_steps\"]\\n        return _output_keys\\n\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n\\n        extra = Extra.forbid\\n        arbitrary_types_allowed = True' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\combine_documents\\\\map_reduce.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@root_validator(pre=True)\\n    def get_reduce_chain(cls, values: Dict) -> Dict:\\n        \"\"\"For backwards compatibility.\"\"\"\\n        if \"combine_document_chain\" in values:\\n            if \"reduce_documents_chain\" in values:\\n                raise ValueError(\\n                    \"Both `reduce_documents_chain` and `combine_document_chain` \"\\n                    \"cannot be provided at the same time. `combine_document_chain` \"\\n                    \"is deprecated, please only provide `reduce_documents_chain`\"\\n                )\\n            combine_chain = values[\"combine_document_chain\"]\\n            collapse_chain = values.get(\"collapse_document_chain\")\\n            reduce_chain = ReduceDocumentsChain(\\n                combine_documents_chain=combine_chain,\\n                collapse_documents_chain=collapse_chain,\\n            )\\n            values[\"reduce_documents_chain\"] = reduce_chain\\n            del values[\"combine_document_chain\"]\\n            if \"collapse_document_chain\" in values:\\n                del values[\"collapse_document_chain\"]\\n\\n        return values\\n\\n    @root_validator(pre=True)\\n    def get_return_intermediate_steps(cls, values: Dict) -> Dict:\\n        \"\"\"For backwards compatibility.\"\"\"\\n        if \"return_map_steps\" in values:\\n            values[\"return_intermediate_steps\"] = values[\"return_map_steps\"]\\n            del values[\"return_map_steps\"]\\n        return values' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\combine_documents\\\\map_reduce.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@root_validator(pre=True)\\n    def get_default_document_variable_name(cls, values: Dict) -> Dict:\\n        \"\"\"Get default document variable name, if not provided.\"\"\"\\n        if \"document_variable_name\" not in values:\\n            llm_chain_variables = values[\"llm_chain\"].prompt.input_variables\\n            if len(llm_chain_variables) == 1:\\n                values[\"document_variable_name\"] = llm_chain_variables[0]\\n            else:\\n                raise ValueError(\\n                    \"document_variable_name must be provided if there are \"\\n                    \"multiple llm_chain input_variables\"\\n                )\\n        else:\\n            llm_chain_variables = values[\"llm_chain\"].prompt.input_variables\\n            if values[\"document_variable_name\"] not in llm_chain_variables:\\n                raise ValueError(\\n                    f\"document_variable_name {values[\\'document_variable_name\\']} was \"\\n                    f\"not found in llm_chain input_variables: {llm_chain_variables}\"\\n                )\\n        return values\\n\\n    @property\\n    def collapse_document_chain(self) -> BaseCombineDocumentsChain:\\n        \"\"\"Kept for backward compatibility.\"\"\"\\n        if isinstance(self.reduce_documents_chain, ReduceDocumentsChain):\\n            if self.reduce_documents_chain.collapse_documents_chain:\\n                return self.reduce_documents_chain.collapse_documents_chain\\n            else:\\n                return self.reduce_documents_chain.combine_documents_chain\\n        else:\\n            raise ValueError(\\n                f\"`reduce_documents_chain` is of type \"\\n                f\"{type(self.reduce_documents_chain)} so it does not have \"\\n                f\"this attribute.\"\\n            )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\combine_documents\\\\map_reduce.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@property\\n    def combine_document_chain(self) -> BaseCombineDocumentsChain:\\n        \"\"\"Kept for backward compatibility.\"\"\"\\n        if isinstance(self.reduce_documents_chain, ReduceDocumentsChain):\\n            return self.reduce_documents_chain.combine_documents_chain\\n        else:\\n            raise ValueError(\\n                f\"`reduce_documents_chain` is of type \"\\n                f\"{type(self.reduce_documents_chain)} so it does not have \"\\n                f\"this attribute.\"\\n            )\\n\\n    def combine_docs(\\n        self,\\n        docs: List[Document],\\n        token_max: Optional[int] = None,\\n        callbacks: Callbacks = None,\\n        **kwargs: Any,\\n    ) -> Tuple[str, dict]:\\n        \"\"\"Combine documents in a map reduce manner.\\n\\n        Combine by mapping first chain over all documents, then reducing the results.\\n        This reducing can be done recursively if needed (if there are many documents).\\n        \"\"\"\\n        map_results = self.llm_chain.apply(\\n            # FYI - this is parallelized and so it is fast.\\n            [{self.document_variable_name: d.page_content, **kwargs} for d in docs],\\n            callbacks=callbacks,\\n        )\\n        question_result_key = self.llm_chain.output_key\\n        result_docs = [\\n            Document(page_content=r[question_result_key], metadata=docs[i].metadata)\\n            # This uses metadata from the docs, and the textual results from `results`\\n            for i, r in enumerate(map_results)\\n        ]\\n        result, extra_return_dict = self.reduce_documents_chain.combine_docs(\\n            result_docs, token_max=token_max, callbacks=callbacks, **kwargs\\n        )\\n        if self.return_intermediate_steps:\\n            intermediate_steps = [r[question_result_key] for r in map_results]\\n            extra_return_dict[\"intermediate_steps\"] = intermediate_steps\\n        return result, extra_return_dict' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\combine_documents\\\\map_reduce.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='async def acombine_docs(\\n        self,\\n        docs: List[Document],\\n        token_max: Optional[int] = None,\\n        callbacks: Callbacks = None,\\n        **kwargs: Any,\\n    ) -> Tuple[str, dict]:\\n        \"\"\"Combine documents in a map reduce manner.\\n\\n        Combine by mapping first chain over all documents, then reducing the results.\\n        This reducing can be done recursively if needed (if there are many documents).\\n        \"\"\"\\n        map_results = await self.llm_chain.aapply(\\n            # FYI - this is parallelized and so it is fast.\\n            [{**{self.document_variable_name: d.page_content}, **kwargs} for d in docs],\\n            callbacks=callbacks,\\n        )\\n        question_result_key = self.llm_chain.output_key\\n        result_docs = [\\n            Document(page_content=r[question_result_key], metadata=docs[i].metadata)\\n            # This uses metadata from the docs, and the textual results from `results`\\n            for i, r in enumerate(map_results)\\n        ]\\n        result, extra_return_dict = await self.reduce_documents_chain.acombine_docs(\\n            result_docs, token_max=token_max, callbacks=callbacks, **kwargs\\n        )\\n        if self.return_intermediate_steps:\\n            intermediate_steps = [r[question_result_key] for r in map_results]\\n            extra_return_dict[\"intermediate_steps\"] = intermediate_steps\\n        return result, extra_return_dict\\n\\n    @property\\n    def _chain_type(self) -> str:\\n        return \"map_reduce_documents_chain\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\combine_documents\\\\map_reduce.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Combining documents by mapping a chain over them first, then reranking results.\"\"\"\\n\\nfrom __future__ import annotations\\n\\nfrom typing import Any, Dict, List, Optional, Sequence, Tuple, Type, Union, cast\\n\\nfrom langchain_core.callbacks import Callbacks\\nfrom langchain_core.documents import Document\\nfrom langchain_core.pydantic_v1 import BaseModel, Extra, create_model, root_validator\\nfrom langchain_core.runnables.config import RunnableConfig\\n\\nfrom langchain.chains.combine_documents.base import BaseCombineDocumentsChain\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.output_parsers.regex import RegexParser\\n\\n\\nclass MapRerankDocumentsChain(BaseCombineDocumentsChain):\\n    \"\"\"Combining documents by mapping a chain over them, then reranking results.\\n\\n    This algorithm calls an LLMChain on each input document. The LLMChain is expected\\n    to have an OutputParser that parses the result into both an answer (`answer_key`)\\n    and a score (`rank_key`). The answer with the highest score is then returned.\\n\\n    Example:\\n        .. code-block:: python\\n\\n            from langchain.chains import StuffDocumentsChain, LLMChain\\n            from langchain_core.prompts import PromptTemplate\\n            from langchain_community.llms import OpenAI\\n            from langchain.output_parsers.regex import RegexParser' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\combine_documents\\\\map_rerank.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='document_variable_name = \"context\"\\n            llm = OpenAI()\\n            # The prompt here should take as an input variable the\\n            # `document_variable_name`\\n            # The actual prompt will need to be a lot more complex, this is just\\n            # an example.\\n            prompt_template = (\\n                \"Use the following context to tell me the chemical formula \"\\n                \"for water. Output both your answer and a score of how confident \"\\n                \"you are. Context: {content}\"\\n            )\\n            output_parser = RegexParser(\\n                regex=r\"(.*?)\\\\nScore: (.*)\",\\n                output_keys=[\"answer\", \"score\"],\\n            )\\n            prompt = PromptTemplate(\\n                template=prompt_template,\\n                input_variables=[\"context\"],\\n                output_parser=output_parser,\\n            )\\n            llm_chain = LLMChain(llm=llm, prompt=prompt)\\n            chain = MapRerankDocumentsChain(\\n                llm_chain=llm_chain,\\n                document_variable_name=document_variable_name,\\n                rank_key=\"score\",\\n                answer_key=\"answer\",\\n            )\\n    \"\"\"\\n\\n    llm_chain: LLMChain\\n    \"\"\"Chain to apply to each document individually.\"\"\"\\n    document_variable_name: str\\n    \"\"\"The variable name in the llm_chain to put the documents in.\\n    If only one variable in the llm_chain, this need not be provided.\"\"\"\\n    rank_key: str\\n    \"\"\"Key in output of llm_chain to rank on.\"\"\"\\n    answer_key: str\\n    \"\"\"Key in output of llm_chain to return as answer.\"\"\"\\n    metadata_keys: Optional[List[str]] = None\\n    \"\"\"Additional metadata from the chosen document to return.\"\"\"\\n    return_intermediate_steps: bool = False\\n    \"\"\"Return intermediate steps.\\n    Intermediate steps include the results of calling llm_chain on each document.\"\"\"\\n\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n\\n        extra = Extra.forbid\\n        arbitrary_types_allowed = True' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\combine_documents\\\\map_rerank.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def get_output_schema(\\n        self, config: Optional[RunnableConfig] = None\\n    ) -> Type[BaseModel]:\\n        schema: Dict[str, Any] = {\\n            self.output_key: (str, None),\\n        }\\n        if self.return_intermediate_steps:\\n            schema[\"intermediate_steps\"] = (List[str], None)\\n        if self.metadata_keys:\\n            schema.update({key: (Any, None) for key in self.metadata_keys})\\n\\n        return create_model(\"MapRerankOutput\", **schema)\\n\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Expect input key.\\n\\n        :meta private:\\n        \"\"\"\\n        _output_keys = super().output_keys\\n        if self.return_intermediate_steps:\\n            _output_keys = _output_keys + [\"intermediate_steps\"]\\n        if self.metadata_keys is not None:\\n            _output_keys += self.metadata_keys\\n        return _output_keys\\n\\n    @root_validator()\\n    def validate_llm_output(cls, values: Dict) -> Dict:\\n        \"\"\"Validate that the combine chain outputs a dictionary.\"\"\"\\n        output_parser = values[\"llm_chain\"].prompt.output_parser\\n        if not isinstance(output_parser, RegexParser):\\n            raise ValueError(\\n                \"Output parser of llm_chain should be a RegexParser,\"\\n                f\" got {output_parser}\"\\n            )\\n        output_keys = output_parser.output_keys\\n        if values[\"rank_key\"] not in output_keys:\\n            raise ValueError(\\n                f\"Got {values[\\'rank_key\\']} as key to rank on, but did not find \"\\n                f\"it in the llm_chain output keys ({output_keys})\"\\n            )\\n        if values[\"answer_key\"] not in output_keys:\\n            raise ValueError(\\n                f\"Got {values[\\'answer_key\\']} as key to return, but did not find \"\\n                f\"it in the llm_chain output keys ({output_keys})\"\\n            )\\n        return values' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\combine_documents\\\\map_rerank.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@root_validator(pre=True)\\n    def get_default_document_variable_name(cls, values: Dict) -> Dict:\\n        \"\"\"Get default document variable name, if not provided.\"\"\"\\n        if \"document_variable_name\" not in values:\\n            llm_chain_variables = values[\"llm_chain\"].prompt.input_variables\\n            if len(llm_chain_variables) == 1:\\n                values[\"document_variable_name\"] = llm_chain_variables[0]\\n            else:\\n                raise ValueError(\\n                    \"document_variable_name must be provided if there are \"\\n                    \"multiple llm_chain input_variables\"\\n                )\\n        else:\\n            llm_chain_variables = values[\"llm_chain\"].prompt.input_variables\\n            if values[\"document_variable_name\"] not in llm_chain_variables:\\n                raise ValueError(\\n                    f\"document_variable_name {values[\\'document_variable_name\\']} was \"\\n                    f\"not found in llm_chain input_variables: {llm_chain_variables}\"\\n                )\\n        return values\\n\\n    def combine_docs(\\n        self, docs: List[Document], callbacks: Callbacks = None, **kwargs: Any\\n    ) -> Tuple[str, dict]:\\n        \"\"\"Combine documents in a map rerank manner.\\n\\n        Combine by mapping first chain over all documents, then reranking the results.\\n\\n        Args:\\n            docs: List of documents to combine\\n            callbacks: Callbacks to be passed through\\n            **kwargs: additional parameters to be passed to LLM calls (like other\\n                input variables besides the documents)\\n\\n        Returns:\\n            The first element returned is the single string output. The second\\n            element returned is a dictionary of other keys to return.\\n        \"\"\"\\n        results = self.llm_chain.apply_and_parse(\\n            # FYI - this is parallelized and so it is fast.\\n            [{**{self.document_variable_name: d.page_content}, **kwargs} for d in docs],\\n            callbacks=callbacks,\\n        )\\n        return self._process_results(docs, results)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\combine_documents\\\\map_rerank.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='async def acombine_docs(\\n        self, docs: List[Document], callbacks: Callbacks = None, **kwargs: Any\\n    ) -> Tuple[str, dict]:\\n        \"\"\"Combine documents in a map rerank manner.\\n\\n        Combine by mapping first chain over all documents, then reranking the results.\\n\\n        Args:\\n            docs: List of documents to combine\\n            callbacks: Callbacks to be passed through\\n            **kwargs: additional parameters to be passed to LLM calls (like other\\n                input variables besides the documents)\\n\\n        Returns:\\n            The first element returned is the single string output. The second\\n            element returned is a dictionary of other keys to return.\\n        \"\"\"\\n        results = await self.llm_chain.aapply_and_parse(\\n            # FYI - this is parallelized and so it is fast.\\n            [{**{self.document_variable_name: d.page_content}, **kwargs} for d in docs],\\n            callbacks=callbacks,\\n        )\\n        return self._process_results(docs, results)\\n\\n    def _process_results(\\n        self,\\n        docs: List[Document],\\n        results: Sequence[Union[str, List[str], Dict[str, str]]],\\n    ) -> Tuple[str, dict]:\\n        typed_results = cast(List[dict], results)\\n        sorted_res = sorted(\\n            zip(typed_results, docs), key=lambda x: -int(x[0][self.rank_key])\\n        )\\n        output, document = sorted_res[0]\\n        extra_info = {}\\n        if self.metadata_keys is not None:\\n            for key in self.metadata_keys:\\n                extra_info[key] = document.metadata[key]\\n        if self.return_intermediate_steps:\\n            extra_info[\"intermediate_steps\"] = results\\n        return output[self.answer_key], extra_info\\n\\n    @property\\n    def _chain_type(self) -> str:\\n        return \"map_rerank_documents_chain\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\combine_documents\\\\map_rerank.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Combine many documents together by recursively reducing them.\"\"\"\\n\\nfrom __future__ import annotations\\n\\nfrom typing import Any, Callable, List, Optional, Protocol, Tuple\\n\\nfrom langchain_core.callbacks import Callbacks\\nfrom langchain_core.documents import Document\\nfrom langchain_core.pydantic_v1 import Extra\\n\\nfrom langchain.chains.combine_documents.base import BaseCombineDocumentsChain\\n\\n\\nclass CombineDocsProtocol(Protocol):\\n    \"\"\"Interface for the combine_docs method.\"\"\"\\n\\n    def __call__(self, docs: List[Document], **kwargs: Any) -> str:\\n        \"\"\"Interface for the combine_docs method.\"\"\"\\n\\n\\nclass AsyncCombineDocsProtocol(Protocol):\\n    \"\"\"Interface for the combine_docs method.\"\"\"\\n\\n    async def __call__(self, docs: List[Document], **kwargs: Any) -> str:\\n        \"\"\"Async interface for the combine_docs method.\"\"\"\\n\\n\\ndef split_list_of_docs(\\n    docs: List[Document], length_func: Callable, token_max: int, **kwargs: Any\\n) -> List[List[Document]]:\\n    \"\"\"Split Documents into subsets that each meet a cumulative length constraint.\\n\\n    Args:\\n        docs: The full list of Documents.\\n        length_func: Function for computing the cumulative length of a set of Documents.\\n        token_max: The maximum cumulative length of any subset of Documents.\\n        **kwargs: Arbitrary additional keyword params to pass to each call of the\\n            length_func.\\n\\n    Returns:\\n        A List[List[Document]].\\n    \"\"\"\\n    new_result_doc_list = []\\n    _sub_result_docs = []\\n    for doc in docs:\\n        _sub_result_docs.append(doc)\\n        _num_tokens = length_func(_sub_result_docs, **kwargs)\\n        if _num_tokens > token_max:\\n            if len(_sub_result_docs) == 1:\\n                raise ValueError(\\n                    \"A single document was longer than the context length,\"\\n                    \" we cannot handle this.\"\\n                )\\n            new_result_doc_list.append(_sub_result_docs[:-1])\\n            _sub_result_docs = _sub_result_docs[-1:]\\n    new_result_doc_list.append(_sub_result_docs)\\n    return new_result_doc_list' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\combine_documents\\\\reduce.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def collapse_docs(\\n    docs: List[Document],\\n    combine_document_func: CombineDocsProtocol,\\n    **kwargs: Any,\\n) -> Document:\\n    \"\"\"Execute a collapse function on a set of documents and merge their metadatas.\\n\\n    Args:\\n        docs: A list of Documents to combine.\\n        combine_document_func: A function that takes in a list of Documents and\\n            optionally addition keyword parameters and combines them into a single\\n            string.\\n        **kwargs: Arbitrary additional keyword params to pass to the\\n            combine_document_func.\\n\\n    Returns:\\n        A single Document with the output of combine_document_func for the page content\\n            and the combined metadata\\'s of all the input documents. All metadata values\\n            are strings, and where there are overlapping keys across documents the\\n            values are joined by \", \".\\n    \"\"\"\\n    result = combine_document_func(docs, **kwargs)\\n    combined_metadata = {k: str(v) for k, v in docs[0].metadata.items()}\\n    for doc in docs[1:]:\\n        for k, v in doc.metadata.items():\\n            if k in combined_metadata:\\n                combined_metadata[k] += f\", {v}\"\\n            else:\\n                combined_metadata[k] = str(v)\\n    return Document(page_content=result, metadata=combined_metadata)\\n\\n\\nasync def acollapse_docs(\\n    docs: List[Document],\\n    combine_document_func: AsyncCombineDocsProtocol,\\n    **kwargs: Any,\\n) -> Document:\\n    \"\"\"Execute a collapse function on a set of documents and merge their metadatas.\\n\\n    Args:\\n        docs: A list of Documents to combine.\\n        combine_document_func: A function that takes in a list of Documents and\\n            optionally addition keyword parameters and combines them into a single\\n            string.\\n        **kwargs: Arbitrary additional keyword params to pass to the\\n            combine_document_func.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\combine_documents\\\\reduce.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Returns:\\n        A single Document with the output of combine_document_func for the page content\\n            and the combined metadata\\'s of all the input documents. All metadata values\\n            are strings, and where there are overlapping keys across documents the\\n            values are joined by \", \".\\n    \"\"\"\\n    result = await combine_document_func(docs, **kwargs)\\n    combined_metadata = {k: str(v) for k, v in docs[0].metadata.items()}\\n    for doc in docs[1:]:\\n        for k, v in doc.metadata.items():\\n            if k in combined_metadata:\\n                combined_metadata[k] += f\", {v}\"\\n            else:\\n                combined_metadata[k] = str(v)\\n    return Document(page_content=result, metadata=combined_metadata)\\n\\n\\nclass ReduceDocumentsChain(BaseCombineDocumentsChain):\\n    \"\"\"Combine documents by recursively reducing them.\\n\\n    This involves\\n\\n    - combine_documents_chain\\n\\n    - collapse_documents_chain\\n\\n    `combine_documents_chain` is ALWAYS provided. This is final chain that is called.\\n    We pass all previous results to this chain, and the output of this chain is\\n    returned as a final result.\\n\\n    `collapse_documents_chain` is used if the documents passed in are too many to all\\n    be passed to `combine_documents_chain` in one go. In this case,\\n    `collapse_documents_chain` is called recursively on as big of groups of documents\\n    as are allowed.\\n\\n    Example:\\n        .. code-block:: python\\n\\n            from langchain.chains import (\\n                StuffDocumentsChain, LLMChain, ReduceDocumentsChain\\n            )\\n            from langchain_core.prompts import PromptTemplate\\n            from langchain_community.llms import OpenAI' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\combine_documents\\\\reduce.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# This controls how each document will be formatted. Specifically,\\n            # it will be passed to `format_document` - see that function for more\\n            # details.\\n            document_prompt = PromptTemplate(\\n                input_variables=[\"page_content\"],\\n                 template=\"{page_content}\"\\n            )\\n            document_variable_name = \"context\"\\n            llm = OpenAI()\\n            # The prompt here should take as an input variable the\\n            # `document_variable_name`\\n            prompt = PromptTemplate.from_template(\\n                \"Summarize this content: {context}\"\\n            )\\n            llm_chain = LLMChain(llm=llm, prompt=prompt)\\n            combine_documents_chain = StuffDocumentsChain(\\n                llm_chain=llm_chain,\\n                document_prompt=document_prompt,\\n                document_variable_name=document_variable_name\\n            )\\n            chain = ReduceDocumentsChain(\\n                combine_documents_chain=combine_documents_chain,\\n            )\\n            # If we wanted to, we could also pass in collapse_documents_chain\\n            # which is specifically aimed at collapsing documents BEFORE\\n            # the final call.\\n            prompt = PromptTemplate.from_template(\\n                \"Collapse this content: {context}\"\\n            )\\n            llm_chain = LLMChain(llm=llm, prompt=prompt)\\n            collapse_documents_chain = StuffDocumentsChain(\\n                llm_chain=llm_chain,\\n                document_prompt=document_prompt,\\n                document_variable_name=document_variable_name\\n            )\\n            chain = ReduceDocumentsChain(\\n                combine_documents_chain=combine_documents_chain,\\n                collapse_documents_chain=collapse_documents_chain,\\n            )\\n    \"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\combine_documents\\\\reduce.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='combine_documents_chain: BaseCombineDocumentsChain\\n    \"\"\"Final chain to call to combine documents.\\n    This is typically a StuffDocumentsChain.\"\"\"\\n    collapse_documents_chain: Optional[BaseCombineDocumentsChain] = None\\n    \"\"\"Chain to use to collapse documents if needed until they can all fit.\\n    If None, will use the combine_documents_chain.\\n    This is typically a StuffDocumentsChain.\"\"\"\\n    token_max: int = 3000\\n    \"\"\"The maximum number of tokens to group documents into. For example, if\\n    set to 3000 then documents will be grouped into chunks of no greater than\\n    3000 tokens before trying to combine them into a smaller chunk.\"\"\"\\n\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n\\n        extra = Extra.forbid\\n        arbitrary_types_allowed = True\\n\\n    @property\\n    def _collapse_chain(self) -> BaseCombineDocumentsChain:\\n        if self.collapse_documents_chain is not None:\\n            return self.collapse_documents_chain\\n        else:\\n            return self.combine_documents_chain\\n\\n    def combine_docs(\\n        self,\\n        docs: List[Document],\\n        token_max: Optional[int] = None,\\n        callbacks: Callbacks = None,\\n        **kwargs: Any,\\n    ) -> Tuple[str, dict]:\\n        \"\"\"Combine multiple documents recursively.\\n\\n        Args:\\n            docs: List of documents to combine, assumed that each one is less than\\n                `token_max`.\\n            token_max: Recursively creates groups of documents less than this number\\n                of tokens.\\n            callbacks: Callbacks to be passed through\\n            **kwargs: additional parameters to be passed to LLM calls (like other\\n                input variables besides the documents)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\combine_documents\\\\reduce.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Returns:\\n            The first element returned is the single string output. The second\\n            element returned is a dictionary of other keys to return.\\n        \"\"\"\\n        result_docs, extra_return_dict = self._collapse(\\n            docs, token_max=token_max, callbacks=callbacks, **kwargs\\n        )\\n        return self.combine_documents_chain.combine_docs(\\n            docs=result_docs, callbacks=callbacks, **kwargs\\n        )\\n\\n    async def acombine_docs(\\n        self,\\n        docs: List[Document],\\n        token_max: Optional[int] = None,\\n        callbacks: Callbacks = None,\\n        **kwargs: Any,\\n    ) -> Tuple[str, dict]:\\n        \"\"\"Async combine multiple documents recursively.\\n\\n        Args:\\n            docs: List of documents to combine, assumed that each one is less than\\n                `token_max`.\\n            token_max: Recursively creates groups of documents less than this number\\n                of tokens.\\n            callbacks: Callbacks to be passed through\\n            **kwargs: additional parameters to be passed to LLM calls (like other\\n                input variables besides the documents)\\n\\n        Returns:\\n            The first element returned is the single string output. The second\\n            element returned is a dictionary of other keys to return.\\n        \"\"\"\\n        result_docs, extra_return_dict = await self._acollapse(\\n            docs, token_max=token_max, callbacks=callbacks, **kwargs\\n        )\\n        return await self.combine_documents_chain.acombine_docs(\\n            docs=result_docs, callbacks=callbacks, **kwargs\\n        )\\n\\n    def _collapse(\\n        self,\\n        docs: List[Document],\\n        token_max: Optional[int] = None,\\n        callbacks: Callbacks = None,\\n        **kwargs: Any,\\n    ) -> Tuple[List[Document], dict]:\\n        result_docs = docs\\n        length_func = self.combine_documents_chain.prompt_length\\n        num_tokens = length_func(result_docs, **kwargs)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\combine_documents\\\\reduce.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _collapse_docs_func(docs: List[Document], **kwargs: Any) -> str:\\n            return self._collapse_chain.run(\\n                input_documents=docs, callbacks=callbacks, **kwargs\\n            )\\n\\n        _token_max = token_max or self.token_max\\n        while num_tokens is not None and num_tokens > _token_max:\\n            new_result_doc_list = split_list_of_docs(\\n                result_docs, length_func, _token_max, **kwargs\\n            )\\n            result_docs = []\\n            for docs in new_result_doc_list:\\n                new_doc = collapse_docs(docs, _collapse_docs_func, **kwargs)\\n                result_docs.append(new_doc)\\n            num_tokens = length_func(result_docs, **kwargs)\\n        return result_docs, {}\\n\\n    async def _acollapse(\\n        self,\\n        docs: List[Document],\\n        token_max: Optional[int] = None,\\n        callbacks: Callbacks = None,\\n        **kwargs: Any,\\n    ) -> Tuple[List[Document], dict]:\\n        result_docs = docs\\n        length_func = self.combine_documents_chain.prompt_length\\n        num_tokens = length_func(result_docs, **kwargs)\\n\\n        async def _collapse_docs_func(docs: List[Document], **kwargs: Any) -> str:\\n            return await self._collapse_chain.arun(\\n                input_documents=docs, callbacks=callbacks, **kwargs\\n            )\\n\\n        _token_max = token_max or self.token_max\\n        while num_tokens is not None and num_tokens > _token_max:\\n            new_result_doc_list = split_list_of_docs(\\n                result_docs, length_func, _token_max, **kwargs\\n            )\\n            result_docs = []\\n            for docs in new_result_doc_list:\\n                new_doc = await acollapse_docs(docs, _collapse_docs_func, **kwargs)\\n                result_docs.append(new_doc)\\n            num_tokens = length_func(result_docs, **kwargs)\\n        return result_docs, {}\\n\\n    @property\\n    def _chain_type(self) -> str:\\n        return \"reduce_documents_chain\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\combine_documents\\\\reduce.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Combine documents by doing a first pass and then refining on more documents.\"\"\"\\n\\nfrom __future__ import annotations\\n\\nfrom typing import Any, Dict, List, Tuple\\n\\nfrom langchain_core.callbacks import Callbacks\\nfrom langchain_core.documents import Document\\nfrom langchain_core.prompts import BasePromptTemplate, format_document\\nfrom langchain_core.prompts.prompt import PromptTemplate\\nfrom langchain_core.pydantic_v1 import Extra, Field, root_validator\\n\\nfrom langchain.chains.combine_documents.base import (\\n    BaseCombineDocumentsChain,\\n)\\nfrom langchain.chains.llm import LLMChain\\n\\n\\ndef _get_default_document_prompt() -> PromptTemplate:\\n    return PromptTemplate(input_variables=[\"page_content\"], template=\"{page_content}\")\\n\\n\\nclass RefineDocumentsChain(BaseCombineDocumentsChain):\\n    \"\"\"Combine documents by doing a first pass and then refining on more documents.\\n\\n    This algorithm first calls `initial_llm_chain` on the first document, passing\\n    that first document in with the variable name `document_variable_name`, and\\n    produces a new variable with the variable name `initial_response_name`.\\n\\n    Then, it loops over every remaining document. This is called the \"refine\" step.\\n    It calls `refine_llm_chain`,\\n    passing in that document with the variable name `document_variable_name`\\n    as well as the previous response with the variable name `initial_response_name`.\\n\\n    Example:\\n        .. code-block:: python\\n\\n            from langchain.chains import RefineDocumentsChain, LLMChain\\n            from langchain_core.prompts import PromptTemplate\\n            from langchain_community.llms import OpenAI' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\combine_documents\\\\refine.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# This controls how each document will be formatted. Specifically,\\n            # it will be passed to `format_document` - see that function for more\\n            # details.\\n            document_prompt = PromptTemplate(\\n                input_variables=[\"page_content\"],\\n                 template=\"{page_content}\"\\n            )\\n            document_variable_name = \"context\"\\n            llm = OpenAI()\\n            # The prompt here should take as an input variable the\\n            # `document_variable_name`\\n            prompt = PromptTemplate.from_template(\\n                \"Summarize this content: {context}\"\\n            )\\n            initial_llm_chain = LLMChain(llm=llm, prompt=prompt)\\n            initial_response_name = \"prev_response\"\\n            # The prompt here should take as an input variable the\\n            # `document_variable_name` as well as `initial_response_name`\\n            prompt_refine = PromptTemplate.from_template(\\n                \"Here\\'s your first summary: {prev_response}. \"\\n                \"Now add to it based on the following context: {context}\"\\n            )\\n            refine_llm_chain = LLMChain(llm=llm, prompt=prompt_refine)\\n            chain = RefineDocumentsChain(\\n                initial_llm_chain=initial_llm_chain,\\n                refine_llm_chain=refine_llm_chain,\\n                document_prompt=document_prompt,\\n                document_variable_name=document_variable_name,\\n                initial_response_name=initial_response_name,\\n            )\\n    \"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\combine_documents\\\\refine.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='initial_llm_chain: LLMChain\\n    \"\"\"LLM chain to use on initial document.\"\"\"\\n    refine_llm_chain: LLMChain\\n    \"\"\"LLM chain to use when refining.\"\"\"\\n    document_variable_name: str\\n    \"\"\"The variable name in the initial_llm_chain to put the documents in.\\n    If only one variable in the initial_llm_chain, this need not be provided.\"\"\"\\n    initial_response_name: str\\n    \"\"\"The variable name to format the initial response in when refining.\"\"\"\\n    document_prompt: BasePromptTemplate = Field(\\n        default_factory=_get_default_document_prompt\\n    )\\n    \"\"\"Prompt to use to format each document, gets passed to `format_document`.\"\"\"\\n    return_intermediate_steps: bool = False\\n    \"\"\"Return the results of the refine steps in the output.\"\"\"\\n\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Expect input key.\\n\\n        :meta private:\\n        \"\"\"\\n        _output_keys = super().output_keys\\n        if self.return_intermediate_steps:\\n            _output_keys = _output_keys + [\"intermediate_steps\"]\\n        return _output_keys\\n\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n\\n        extra = Extra.forbid\\n        arbitrary_types_allowed = True\\n\\n    @root_validator(pre=True)\\n    def get_return_intermediate_steps(cls, values: Dict) -> Dict:\\n        \"\"\"For backwards compatibility.\"\"\"\\n        if \"return_refine_steps\" in values:\\n            values[\"return_intermediate_steps\"] = values[\"return_refine_steps\"]\\n            del values[\"return_refine_steps\"]\\n        return values' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\combine_documents\\\\refine.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@root_validator(pre=True)\\n    def get_default_document_variable_name(cls, values: Dict) -> Dict:\\n        \"\"\"Get default document variable name, if not provided.\"\"\"\\n        if \"document_variable_name\" not in values:\\n            llm_chain_variables = values[\"initial_llm_chain\"].prompt.input_variables\\n            if len(llm_chain_variables) == 1:\\n                values[\"document_variable_name\"] = llm_chain_variables[0]\\n            else:\\n                raise ValueError(\\n                    \"document_variable_name must be provided if there are \"\\n                    \"multiple llm_chain input_variables\"\\n                )\\n        else:\\n            llm_chain_variables = values[\"initial_llm_chain\"].prompt.input_variables\\n            if values[\"document_variable_name\"] not in llm_chain_variables:\\n                raise ValueError(\\n                    f\"document_variable_name {values[\\'document_variable_name\\']} was \"\\n                    f\"not found in llm_chain input_variables: {llm_chain_variables}\"\\n                )\\n        return values\\n\\n    def combine_docs(\\n        self, docs: List[Document], callbacks: Callbacks = None, **kwargs: Any\\n    ) -> Tuple[str, dict]:\\n        \"\"\"Combine by mapping first chain over all, then stuffing into final chain.\\n\\n        Args:\\n            docs: List of documents to combine\\n            callbacks: Callbacks to be passed through\\n            **kwargs: additional parameters to be passed to LLM calls (like other\\n                input variables besides the documents)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\combine_documents\\\\refine.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Returns:\\n            The first element returned is the single string output. The second\\n            element returned is a dictionary of other keys to return.\\n        \"\"\"\\n        inputs = self._construct_initial_inputs(docs, **kwargs)\\n        res = self.initial_llm_chain.predict(callbacks=callbacks, **inputs)\\n        refine_steps = [res]\\n        for doc in docs[1:]:\\n            base_inputs = self._construct_refine_inputs(doc, res)\\n            inputs = {**base_inputs, **kwargs}\\n            res = self.refine_llm_chain.predict(callbacks=callbacks, **inputs)\\n            refine_steps.append(res)\\n        return self._construct_result(refine_steps, res)\\n\\n    async def acombine_docs(\\n        self, docs: List[Document], callbacks: Callbacks = None, **kwargs: Any\\n    ) -> Tuple[str, dict]:\\n        \"\"\"Async combine by mapping a first chain over all, then stuffing\\n         into a final chain.\\n\\n        Args:\\n            docs: List of documents to combine\\n            callbacks: Callbacks to be passed through\\n            **kwargs: additional parameters to be passed to LLM calls (like other\\n                input variables besides the documents)\\n\\n        Returns:\\n            The first element returned is the single string output. The second\\n            element returned is a dictionary of other keys to return.\\n        \"\"\"\\n        inputs = self._construct_initial_inputs(docs, **kwargs)\\n        res = await self.initial_llm_chain.apredict(callbacks=callbacks, **inputs)\\n        refine_steps = [res]\\n        for doc in docs[1:]:\\n            base_inputs = self._construct_refine_inputs(doc, res)\\n            inputs = {**base_inputs, **kwargs}\\n            res = await self.refine_llm_chain.apredict(callbacks=callbacks, **inputs)\\n            refine_steps.append(res)\\n        return self._construct_result(refine_steps, res)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\combine_documents\\\\refine.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _construct_result(self, refine_steps: List[str], res: str) -> Tuple[str, dict]:\\n        if self.return_intermediate_steps:\\n            extra_return_dict = {\"intermediate_steps\": refine_steps}\\n        else:\\n            extra_return_dict = {}\\n        return res, extra_return_dict\\n\\n    def _construct_refine_inputs(self, doc: Document, res: str) -> Dict[str, Any]:\\n        return {\\n            self.document_variable_name: format_document(doc, self.document_prompt),\\n            self.initial_response_name: res,\\n        }\\n\\n    def _construct_initial_inputs(\\n        self, docs: List[Document], **kwargs: Any\\n    ) -> Dict[str, Any]:\\n        base_info = {\"page_content\": docs[0].page_content}\\n        base_info.update(docs[0].metadata)\\n        document_info = {k: base_info[k] for k in self.document_prompt.input_variables}\\n        base_inputs: dict = {\\n            self.document_variable_name: self.document_prompt.format(**document_info)\\n        }\\n        inputs = {**base_inputs, **kwargs}\\n        return inputs\\n\\n    @property\\n    def _chain_type(self) -> str:\\n        return \"refine_documents_chain\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\combine_documents\\\\refine.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Chain that combines documents by stuffing into context.\"\"\"\\nfrom typing import Any, Dict, List, Optional, Tuple\\n\\nfrom langchain_core.callbacks import Callbacks\\nfrom langchain_core.documents import Document\\nfrom langchain_core.language_models import LanguageModelLike\\nfrom langchain_core.output_parsers import BaseOutputParser, StrOutputParser\\nfrom langchain_core.prompts import BasePromptTemplate, format_document\\nfrom langchain_core.pydantic_v1 import Extra, Field, root_validator\\nfrom langchain_core.runnables import Runnable, RunnablePassthrough\\n\\nfrom langchain.chains.combine_documents.base import (\\n    DEFAULT_DOCUMENT_PROMPT,\\n    DEFAULT_DOCUMENT_SEPARATOR,\\n    DOCUMENTS_KEY,\\n    BaseCombineDocumentsChain,\\n    _validate_prompt,\\n)\\nfrom langchain.chains.llm import LLMChain\\n\\n\\ndef create_stuff_documents_chain(\\n    llm: LanguageModelLike,\\n    prompt: BasePromptTemplate,\\n    *,\\n    output_parser: Optional[BaseOutputParser] = None,\\n    document_prompt: Optional[BasePromptTemplate] = None,\\n    document_separator: str = DEFAULT_DOCUMENT_SEPARATOR,\\n) -> Runnable[Dict[str, Any], Any]:\\n    \"\"\"Create a chain for passing a list of Documents to a model.\\n\\n    Args:\\n        llm: Language model.\\n        prompt: Prompt template. Must contain input variable \"context\", which will be\\n            used for passing in the formatted documents.\\n        output_parser: Output parser. Defaults to StrOutputParser.\\n        document_prompt: Prompt used for formatting each document into a string. Input\\n            variables can be \"page_content\" or any metadata keys that are in all\\n            documents. \"page_content\" will automatically retrieve the\\n            `Document.page_content`, and all other inputs variables will be\\n            automatically retrieved from the `Document.metadata` dictionary. Default to\\n            a prompt that only contains `Document.page_content`.\\n        document_separator: String separator to use between formatted document strings.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\combine_documents\\\\stuff.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Returns:\\n        An LCEL Runnable. The input is a dictionary that must have a \"context\" key that\\n        maps to a List[Document], and any other input variables expected in the prompt.\\n        The Runnable return type depends on output_parser used.\\n\\n    Example:\\n        .. code-block:: python\\n\\n            # pip install -U langchain langchain-community\\n\\n            from langchain_community.chat_models import ChatOpenAI\\n            from langchain_core.documents import Document\\n            from langchain_core.prompts import ChatPromptTemplate\\n            from langchain.chains.combine_documents import create_stuff_documents_chain\\n\\n            prompt = ChatPromptTemplate.from_messages(\\n                [(\"system\", \"What are everyone\\'s favorite colors:\\\\\\\\n\\\\\\\\n{context}\")]\\n            )\\n            llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\\n            chain = create_stuff_documents_chain(llm, prompt)\\n\\n            docs = [\\n                Document(page_content=\"Jesse loves red but not yellow\"),\\n                Document(page_content = \"Jamal loves green but not as much as he loves orange\")\\n            ]\\n\\n            chain.invoke({\"context\": docs})\\n    \"\"\"  # noqa: E501\\n\\n    _validate_prompt(prompt)\\n    _document_prompt = document_prompt or DEFAULT_DOCUMENT_PROMPT\\n    _output_parser = output_parser or StrOutputParser()\\n\\n    def format_docs(inputs: dict) -> str:\\n        return document_separator.join(\\n            format_document(doc, _document_prompt) for doc in inputs[DOCUMENTS_KEY]\\n        )\\n\\n    return (\\n        RunnablePassthrough.assign(**{DOCUMENTS_KEY: format_docs}).with_config(\\n            run_name=\"format_inputs\"\\n        )\\n        | prompt\\n        | llm\\n        | _output_parser\\n    ).with_config(run_name=\"stuff_documents_chain\")\\n\\n\\nclass StuffDocumentsChain(BaseCombineDocumentsChain):\\n    \"\"\"Chain that combines documents by stuffing into context.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\combine_documents\\\\stuff.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='This chain takes a list of documents and first combines them into a single string.\\n    It does this by formatting each document into a string with the `document_prompt`\\n    and then joining them together with `document_separator`. It then adds that new\\n    string to the inputs with the variable name set by `document_variable_name`.\\n    Those inputs are then passed to the `llm_chain`.\\n\\n    Example:\\n        .. code-block:: python\\n\\n            from langchain.chains import StuffDocumentsChain, LLMChain\\n            from langchain_core.prompts import PromptTemplate\\n            from langchain_community.llms import OpenAI\\n\\n            # This controls how each document will be formatted. Specifically,\\n            # it will be passed to `format_document` - see that function for more\\n            # details.\\n            document_prompt = PromptTemplate(\\n                input_variables=[\"page_content\"],\\n                template=\"{page_content}\"\\n            )\\n            document_variable_name = \"context\"\\n            llm = OpenAI()\\n            # The prompt here should take as an input variable the\\n            # `document_variable_name`\\n            prompt = PromptTemplate.from_template(\\n                \"Summarize this content: {context}\"\\n            )\\n            llm_chain = LLMChain(llm=llm, prompt=prompt)\\n            chain = StuffDocumentsChain(\\n                llm_chain=llm_chain,\\n                document_prompt=document_prompt,\\n                document_variable_name=document_variable_name\\n            )\\n    \"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\combine_documents\\\\stuff.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='llm_chain: LLMChain\\n    \"\"\"LLM chain which is called with the formatted document string,\\n    along with any other inputs.\"\"\"\\n    document_prompt: BasePromptTemplate = Field(\\n        default_factory=lambda: DEFAULT_DOCUMENT_PROMPT\\n    )\\n    \"\"\"Prompt to use to format each document, gets passed to `format_document`.\"\"\"\\n    document_variable_name: str\\n    \"\"\"The variable name in the llm_chain to put the documents in.\\n    If only one variable in the llm_chain, this need not be provided.\"\"\"\\n    document_separator: str = \"\\\\n\\\\n\"\\n    \"\"\"The string with which to join the formatted documents\"\"\"\\n\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n\\n        extra = Extra.forbid\\n        arbitrary_types_allowed = True\\n\\n    @root_validator(pre=True)\\n    def get_default_document_variable_name(cls, values: Dict) -> Dict:\\n        \"\"\"Get default document variable name, if not provided.\\n\\n        If only one variable is present in the llm_chain.prompt,\\n        we can infer that the formatted documents should be passed in\\n        with this variable name.\\n        \"\"\"\\n        llm_chain_variables = values[\"llm_chain\"].prompt.input_variables\\n        if \"document_variable_name\" not in values:\\n            if len(llm_chain_variables) == 1:\\n                values[\"document_variable_name\"] = llm_chain_variables[0]\\n            else:\\n                raise ValueError(\\n                    \"document_variable_name must be provided if there are \"\\n                    \"multiple llm_chain_variables\"\\n                )\\n        else:\\n            if values[\"document_variable_name\"] not in llm_chain_variables:\\n                raise ValueError(\\n                    f\"document_variable_name {values[\\'document_variable_name\\']} was \"\\n                    f\"not found in llm_chain input_variables: {llm_chain_variables}\"\\n                )\\n        return values' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\combine_documents\\\\stuff.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@property\\n    def input_keys(self) -> List[str]:\\n        extra_keys = [\\n            k for k in self.llm_chain.input_keys if k != self.document_variable_name\\n        ]\\n        return super().input_keys + extra_keys\\n\\n    def _get_inputs(self, docs: List[Document], **kwargs: Any) -> dict:\\n        \"\"\"Construct inputs from kwargs and docs.\\n\\n        Format and then join all the documents together into one input with name\\n        `self.document_variable_name`. Also pluck any additional variables\\n        from **kwargs.\\n\\n        Args:\\n            docs: List of documents to format and then join into single input\\n            **kwargs: additional inputs to chain, will pluck any other required\\n                arguments from here.\\n\\n        Returns:\\n            dictionary of inputs to LLMChain\\n        \"\"\"\\n        # Format each document according to the prompt\\n        doc_strings = [format_document(doc, self.document_prompt) for doc in docs]\\n        # Join the documents together to put them in the prompt.\\n        inputs = {\\n            k: v\\n            for k, v in kwargs.items()\\n            if k in self.llm_chain.prompt.input_variables\\n        }\\n        inputs[self.document_variable_name] = self.document_separator.join(doc_strings)\\n        return inputs\\n\\n    def prompt_length(self, docs: List[Document], **kwargs: Any) -> Optional[int]:\\n        \"\"\"Return the prompt length given the documents passed in.\\n\\n        This can be used by a caller to determine whether passing in a list\\n        of documents would exceed a certain prompt length. This useful when\\n        trying to ensure that the size of a prompt remains below a certain\\n        context limit.\\n\\n        Args:\\n            docs: List[Document], a list of documents to use to calculate the\\n                total prompt length.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\combine_documents\\\\stuff.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Returns:\\n            Returns None if the method does not depend on the prompt length,\\n            otherwise the length of the prompt in tokens.\\n        \"\"\"\\n        inputs = self._get_inputs(docs, **kwargs)\\n        prompt = self.llm_chain.prompt.format(**inputs)\\n        return self.llm_chain._get_num_tokens(prompt)\\n\\n    def combine_docs(\\n        self, docs: List[Document], callbacks: Callbacks = None, **kwargs: Any\\n    ) -> Tuple[str, dict]:\\n        \"\"\"Stuff all documents into one prompt and pass to LLM.\\n\\n        Args:\\n            docs: List of documents to join together into one variable\\n            callbacks: Optional callbacks to pass along\\n            **kwargs: additional parameters to use to get inputs to LLMChain.\\n\\n        Returns:\\n            The first element returned is the single string output. The second\\n            element returned is a dictionary of other keys to return.\\n        \"\"\"\\n        inputs = self._get_inputs(docs, **kwargs)\\n        # Call predict on the LLM.\\n        return self.llm_chain.predict(callbacks=callbacks, **inputs), {}\\n\\n    async def acombine_docs(\\n        self, docs: List[Document], callbacks: Callbacks = None, **kwargs: Any\\n    ) -> Tuple[str, dict]:\\n        \"\"\"Async stuff all documents into one prompt and pass to LLM.\\n\\n        Args:\\n            docs: List of documents to join together into one variable\\n            callbacks: Optional callbacks to pass along\\n            **kwargs: additional parameters to use to get inputs to LLMChain.\\n\\n        Returns:\\n            The first element returned is the single string output. The second\\n            element returned is a dictionary of other keys to return.\\n        \"\"\"\\n        inputs = self._get_inputs(docs, **kwargs)\\n        # Call predict on the LLM.\\n        return await self.llm_chain.apredict(callbacks=callbacks, **inputs), {}\\n\\n    @property\\n    def _chain_type(self) -> str:\\n        return \"stuff_documents_chain\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\combine_documents\\\\stuff.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Different ways to combine documents.\"\"\"\\n\\nfrom langchain.chains.combine_documents.reduce import (\\n    acollapse_docs,\\n    collapse_docs,\\n    split_list_of_docs,\\n)\\nfrom langchain.chains.combine_documents.stuff import create_stuff_documents_chain\\n\\n__all__ = [\\n    \"acollapse_docs\",\\n    \"collapse_docs\",\\n    \"split_list_of_docs\",\\n    \"create_stuff_documents_chain\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\combine_documents\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Chain for applying constitutional principles to the outputs of another chain.\"\"\"\\nfrom typing import Any, Dict, List, Optional\\n\\nfrom langchain_core.callbacks import CallbackManagerForChainRun\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.prompts import BasePromptTemplate\\n\\nfrom langchain.chains.base import Chain\\nfrom langchain.chains.constitutional_ai.models import ConstitutionalPrinciple\\nfrom langchain.chains.constitutional_ai.principles import PRINCIPLES\\nfrom langchain.chains.constitutional_ai.prompts import CRITIQUE_PROMPT, REVISION_PROMPT\\nfrom langchain.chains.llm import LLMChain\\n\\n\\nclass ConstitutionalChain(Chain):\\n    \"\"\"Chain for applying constitutional principles.\\n\\n    Example:\\n        .. code-block:: python\\n\\n            from langchain_community.llms import OpenAI\\n            from langchain.chains import LLMChain, ConstitutionalChain\\n            from langchain.chains.constitutional_ai.models \\\\\\n                import ConstitutionalPrinciple\\n\\n            llm = OpenAI()\\n\\n            qa_prompt = PromptTemplate(\\n                template=\"Q: {question} A:\",\\n                input_variables=[\"question\"],\\n            )\\n            qa_chain = LLMChain(llm=llm, prompt=qa_prompt)\\n\\n            constitutional_chain = ConstitutionalChain.from_llm(\\n                llm=llm,\\n                chain=qa_chain,\\n                constitutional_principles=[\\n                    ConstitutionalPrinciple(\\n                        critique_request=\"Tell if this answer is good.\",\\n                        revision_request=\"Give a better answer.\",\\n                    )\\n                ],\\n            )\\n\\n            constitutional_chain.run(question=\"What is the meaning of life?\")\\n    \"\"\"\\n\\n    chain: LLMChain\\n    constitutional_principles: List[ConstitutionalPrinciple]\\n    critique_chain: LLMChain\\n    revision_chain: LLMChain\\n    return_intermediate_steps: bool = False' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\constitutional_ai\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@classmethod\\n    def get_principles(\\n        cls, names: Optional[List[str]] = None\\n    ) -> List[ConstitutionalPrinciple]:\\n        if names is None:\\n            return list(PRINCIPLES.values())\\n        else:\\n            return [PRINCIPLES[name] for name in names]\\n\\n    @classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        chain: LLMChain,\\n        critique_prompt: BasePromptTemplate = CRITIQUE_PROMPT,\\n        revision_prompt: BasePromptTemplate = REVISION_PROMPT,\\n        **kwargs: Any,\\n    ) -> \"ConstitutionalChain\":\\n        \"\"\"Create a chain from an LLM.\"\"\"\\n        critique_chain = LLMChain(llm=llm, prompt=critique_prompt)\\n        revision_chain = LLMChain(llm=llm, prompt=revision_prompt)\\n        return cls(\\n            chain=chain,\\n            critique_chain=critique_chain,\\n            revision_chain=revision_chain,\\n            **kwargs,\\n        )\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Input keys.\"\"\"\\n        return self.chain.input_keys\\n\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Output keys.\"\"\"\\n        if self.return_intermediate_steps:\\n            return [\"output\", \"critiques_and_revisions\", \"initial_output\"]\\n        return [\"output\"]\\n\\n    def _call(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\\n        response = self.chain.run(\\n            **inputs,\\n            callbacks=_run_manager.get_child(\"original\"),\\n        )\\n        initial_response = response\\n        input_prompt = self.chain.prompt.format(**inputs)\\n\\n        _run_manager.on_text(\\n            text=\"Initial response: \" + response + \"\\\\n\\\\n\",\\n            verbose=self.verbose,\\n            color=\"yellow\",\\n        )\\n        critiques_and_revisions = []\\n        for constitutional_principle in self.constitutional_principles:\\n            # Do critique' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\constitutional_ai\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='raw_critique = self.critique_chain.run(\\n                input_prompt=input_prompt,\\n                output_from_model=response,\\n                critique_request=constitutional_principle.critique_request,\\n                callbacks=_run_manager.get_child(\"critique\"),\\n            )\\n            critique = self._parse_critique(\\n                output_string=raw_critique,\\n            ).strip()\\n\\n            # if the critique contains \"No critique needed\", then we\\'re done\\n            # in this case, initial_output is the same as output,\\n            # but we\\'ll keep it for consistency\\n            if \"no critique needed\" in critique.lower():\\n                critiques_and_revisions.append((critique, \"\"))\\n                continue\\n\\n            # Do revision\\n\\n            revision = self.revision_chain.run(\\n                input_prompt=input_prompt,\\n                output_from_model=response,\\n                critique_request=constitutional_principle.critique_request,\\n                critique=critique,\\n                revision_request=constitutional_principle.revision_request,\\n                callbacks=_run_manager.get_child(\"revision\"),\\n            ).strip()\\n            response = revision\\n            critiques_and_revisions.append((critique, revision))\\n\\n            _run_manager.on_text(\\n                text=f\"Applying {constitutional_principle.name}...\" + \"\\\\n\\\\n\",\\n                verbose=self.verbose,\\n                color=\"green\",\\n            )\\n\\n            _run_manager.on_text(\\n                text=\"Critique: \" + critique + \"\\\\n\\\\n\",\\n                verbose=self.verbose,\\n                color=\"blue\",\\n            )\\n\\n            _run_manager.on_text(\\n                text=\"Updated response: \" + revision + \"\\\\n\\\\n\",\\n                verbose=self.verbose,\\n                color=\"yellow\",\\n            )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\constitutional_ai\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='final_output: Dict[str, Any] = {\"output\": response}\\n        if self.return_intermediate_steps:\\n            final_output[\"initial_output\"] = initial_response\\n            final_output[\"critiques_and_revisions\"] = critiques_and_revisions\\n        return final_output\\n\\n    @staticmethod\\n    def _parse_critique(output_string: str) -> str:\\n        if \"Revision request:\" not in output_string:\\n            return output_string\\n        output_string = output_string.split(\"Revision request:\")[0]\\n        if \"\\\\n\\\\n\" in output_string:\\n            output_string = output_string.split(\"\\\\n\\\\n\")[0]\\n        return output_string' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\constitutional_ai\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Models for the Constitutional AI chain.\"\"\"\\nfrom langchain_core.pydantic_v1 import BaseModel\\n\\n\\nclass ConstitutionalPrinciple(BaseModel):\\n    \"\"\"Class for a constitutional principle.\"\"\"\\n\\n    critique_request: str\\n    revision_request: str\\n    name: str = \"Constitutional Principle\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\constitutional_ai\\\\models.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Constitutional principles from https://arxiv.org/pdf/2212.08073.pdf (Bai et al. 2022)\\nUnifiedObjectives v0.2 principles (\"uo-*\") adapted from https://examine.dev/docs/Unified_objectives.pdf (Samwald et al. 2023)\\n\"\"\"\\n# flake8: noqa\\nfrom typing import Dict\\n\\nfrom langchain.chains.constitutional_ai.models import ConstitutionalPrinciple' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\constitutional_ai\\\\principles.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='PRINCIPLES: Dict[str, ConstitutionalPrinciple] = {\\n    \"harmful1\": ConstitutionalPrinciple(\\n        name=\"harmful1\",\\n        critique_request=\"Identify specific ways in which the assistant’s last response is harmful, unethical, racist, sexist, toxic, dangerous, or illegal.\",\\n        revision_request=\"Please rewrite the assistant response to remove any and all harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\",\\n    ),\\n    \"harmful2\": ConstitutionalPrinciple(\\n        name=\"harmful2\",\\n        critique_request=\"Explain ways in which the assistant’s preceding response may be harmful to either the human or others. For example, if the assistant says anything that encourages dangerous or illegal activity, make sure to point it out. Other examples of harm include racism, sexism, and other social biases.\",\\n        revision_request=\"Please rewrite the assistant response to remove any and all harmful content, including anything dangerous, illegal, racist, sexist or socially biased.\",\\n    ),\\n    \"harmful3\": ConstitutionalPrinciple(\\n        name=\"harmful3\",\\n        critique_request=\"Identify all ways in which the assistant’s last response is harmful, unethical, or socially biased. Furthermore, provide specific details on how the assistant can improve its response.\",\\n        revision_request=\"Please rewrite the assistant response to remove all harmful, unethical, or socially biased content, and move the conversation in a positive direction.\",\\n    ),\\n    \"harmful4\": ConstitutionalPrinciple(\\n        name=\"harmful4\",\\n        critique_request=\"Identify specific ways in which the assistant’s precedings response is harmful. Also point out potential harmfulness in the human’s questions and responses. Examples of harm include but are not limited to racism, sexism, toxicity.\",\\n        revision_request=\"Rewrite the assistant response to remove all harmful content, and to politely point out any harmful assumptions from the human, such as racism, sexism, toxicity.\",\\n    ),' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\constitutional_ai\\\\principles.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"insensitive\": ConstitutionalPrinciple(\\n        name=\"insensitive\",\\n        critique_request=\"Discuss the assistant’s last response to the human and whether it is in any way insensitive, sexist, racist, or socially inappropriate.\",\\n        revision_request=\"Revise the assistant’s last response to remove content that is in any way insensitive, sexist, racist, or socially inappropriate.\",\\n    ),\\n    \"offensive\": ConstitutionalPrinciple(\\n        name=\"offensive\",\\n        critique_request=\"Discuss whether the assistant said anything that may be interpreted as offensive or harmful. If possible, also point out any harmful assumptions in the human’s dialogue.\",\\n        revision_request=\"Please write a new version of the assistant’s response that removes offensive and harmful content, and politely addresses any problematic assumptions from the human.\",\\n    ),\\n    \"harmful5\": ConstitutionalPrinciple(\\n        name=\"harmful5\",\\n        critique_request=\"What’s the strongest argument you could give that the assistant’s last response was in some way harmful, unethical, racist, sexist, toxic, dangerous, or illegal.\",\\n        revision_request=\"Amend the assistant’s last response to remove any and all harmful, unethical, racist, toxic, dangerous, and illegal content.\",\\n    ),\\n    \"age-innappropriate\": ConstitutionalPrinciple(\\n        name=\"age-innappropriate\",\\n        critique_request=\"Did the assistant’s preceding response include anything that may be inappropriate for young children? If so, discuss how it can be revised to be more appropriate.\",\\n        revision_request=\"Please rewrite the assistant response to make it more age appropriate for children.\",\\n    ),\\n    \"derogatory\": ConstitutionalPrinciple(\\n        name=\"derogatory\",\\n        critique_request=\"Think carefully about whether the human’s request succeeded in eliciting responses from the assistant that are derogatory, toxic, racist, sexist or socially harmful in any way, and discuss how the assistant should’ve responded instead.\",' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\constitutional_ai\\\\principles.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='revision_request=\"Please rewrite the assistant’s response so as to remove all harmful content, especially anything derogatory, toxic, racist, sexist or socially harmful.\",\\n    ),\\n    \"illegal\": ConstitutionalPrinciple(\\n        name=\"illegal\",\\n        critique_request=\"Discuss whether the assistant provided any advice that may encourage illegal or dangerous activity from either the human or others.\",\\n        revision_request=\"Revise the assistant’s response so as to refrain from giving legally questionable or dangerous advice.\",\\n    ),\\n    \"controversial\": ConstitutionalPrinciple(\\n        name=\"controversial\",\\n        critique_request=\"Discuss ways in which the assistant’s response may be considered controversial or objectionable based on commonsense ethical and moral standards.\",\\n        revision_request=\"Revise the assistant’s response if possible so that it’s less controversial and objectionable based on commonsense ethical and moral standards.\",\\n    ),\\n    \"harmful6\": ConstitutionalPrinciple(\\n        name=\"harmful6\",\\n        critique_request=\"Discuss whether the human is deliberately trying to bait the assistant into generating harmful content, and whether they succeeded in doing so.\",\\n        revision_request=\"Write a revision of the assistant’s response that refrains from saying anything harmful.\",\\n    ),\\n    \"thoughtful\": ConstitutionalPrinciple(\\n        name=\"thoughtful\",\\n        critique_request=\"Please comment on whether the assistant’s last response is thoughtful and empathetic. Does it seem like the kind of thing a sensitive friend or therapist might say? Give the strongest argument you can for how it could improve.\",\\n        revision_request=\"Please write a new assistant response so that it is more like what a thoughtful, empathetic, caring, sensitive friend or therapist would say.\",\\n    ),\\n    \"misogynistic\": ConstitutionalPrinciple(\\n        name=\"misogynistic\",' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\constitutional_ai\\\\principles.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='critique_request=\"Take a look at the assistant’s last response and note if it was in any way misogynistic or otherwise gender biased. Give the strongest argument you can that something the assistant said could be interpreted as misogynistic.\",\\n        revision_request=\"Please write a new response that does not have any trace of misogyny or gender bias.\",\\n    ),\\n    \"criminal\": ConstitutionalPrinciple(\\n        name=\"criminal\",\\n        critique_request=\"Read the assistant’s last response carefully and identify anything it said that provides advice or assistance in potentially criminal activities, such as violence, theft, hacking, robbery, or any other illegal activity. Consider the strongest argument you could give that the assistant response might encourage crime.\",\\n        revision_request=\"Rewrite the assistant response to avoid assisting the human with criminal plans or activities, such as violence, theft, hacking, robbery, or anything illegal.\",\\n    ),\\n    \"harmful7\": ConstitutionalPrinciple(\\n        name=\"harmful7\",\\n        critique_request=\"Critique the assistant’s last response and identify anything it said that was harmful, unethical, dangerous, or illegal.\",\\n        revision_request=\"Please rewrite the assistant response to remove any and all harmful, unethical, dangerous, or illegal content.\",\\n    ),\\n    \"uo-assumptions-1\": ConstitutionalPrinciple(\\n        name=\"uo-assumptions-1\",\\n        critique_request=\"Discuss whether the AI model\\'s response lists and considers all relevant underlying assumptions.\",\\n        revision_request=\"Please rewrite the AI model\\'s response so that it lists and considers all relevant underlying assumptions.\",\\n    ),\\n    \"uo-assumptions-2\": ConstitutionalPrinciple(\\n        name=\"uo-assumptions-2\",\\n        critique_request=\"Discuss whether the AI model\\'s response considers all relevant viewpoints.\",\\n        revision_request=\"Please rewrite the AI model\\'s response so that it considers all relevant viewpoints.\",\\n    ),\\n    \"uo-assumptions-3\": ConstitutionalPrinciple(' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\constitutional_ai\\\\principles.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='name=\"uo-assumptions-3\",\\n        critique_request=\"Discuss whether the AI model\\'s response is objective and impartial.\",\\n        revision_request=\"Please rewrite the AI model\\'s response so that it is objective and impartial.\",\\n    ),\\n    \"uo-reasoning-1\": ConstitutionalPrinciple(\\n        name=\"uo-reasoning-1\",\\n        critique_request=\"Discuss whether the AI model\\'s response considers all relevant reasoning strategies and selects the most appropriate reasoning strategy.\",\\n        revision_request=\"Please rewrite the AI model\\'s response so that it considers all relevant reasoning strategies and selects the most appropriate reasoning strategy.\",\\n    ),\\n    \"uo-reasoning-2\": ConstitutionalPrinciple(\\n        name=\"uo-reasoning-2\",\\n        critique_request=\"Discuss whether the AI model\\'s response is plausible, logically valid, sound, consistent and coherent.\",\\n        revision_request=\"Please rewrite the AI model\\'s response so that it is plausible, logically valid, sound, consistent and coherent.\",\\n    ),\\n    \"uo-reasoning-3\": ConstitutionalPrinciple(\\n        name=\"uo-reasoning-3\",\\n        critique_request=\"Discuss whether reasoning in the AI model\\'s response is structured (e.g. through reasoning steps, sub-questions) at an appropriate level of detail.\",\\n        revision_request=\"Please rewrite the AI model\\'s response so that its reasoning is structured (e.g. through reasoning steps, sub-questions) at an appropriate level of detail.\",\\n    ),\\n    \"uo-reasoning-4\": ConstitutionalPrinciple(\\n        name=\"uo-reasoning-4\",\\n        critique_request=\"Discuss whether the concepts used in the AI model\\'s response are clearly defined.\",\\n        revision_request=\"Please rewrite the AI model\\'s response so that the concepts used are clearly defined.\",\\n    ),\\n    \"uo-reasoning-5\": ConstitutionalPrinciple(\\n        name=\"uo-reasoning-5\",\\n        critique_request=\"Discuss whether the AI model\\'s response gives appropriate priorities to different considerations based on their relevance and importance.\",' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\constitutional_ai\\\\principles.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='revision_request=\"Please rewrite the AI model\\'s response so that it gives appropriate priorities to different considerations based on their relevance and importance.\",\\n    ),\\n    \"uo-reasoning-6\": ConstitutionalPrinciple(\\n        name=\"uo-reasoning-6\",\\n        critique_request=\"Discuss whether statements in the AI model\\'s response are made with appropriate levels of confidence or probability.\",\\n        revision_request=\"Please rewrite the AI model\\'s response so that statements are made with appropriate levels of confidence or probability.\",\\n    ),\\n    \"uo-reasoning-7\": ConstitutionalPrinciple(\\n        name=\"uo-reasoning-7\",\\n        critique_request=\"Discuss whether reasoning in the AI model\\'s response is free from cognitive biases or fallacies.\",\\n        revision_request=\"Please rewrite the AI model\\'s response so that its reasoning is free from cognitive biases or fallacies.\",\\n    ),\\n    \"uo-reasoning-8\": ConstitutionalPrinciple(\\n        name=\"uo-reasoning-8\",\\n        critique_request=\"Discuss whether formal reasoning (e.g. using math, computer code) in the AI model\\'s response is correct.\",\\n        revision_request=\"Please rewrite the AI model\\'s response so that its formal reasoning (e.g. using math, computer code) is correct.\",\\n    ),\\n    \"uo-reasoning-9\": ConstitutionalPrinciple(\\n        name=\"uo-reasoning-9\",\\n        critique_request=\"Discuss whether external tools (e.g. search engines, APIs, mathematical/statistical tools) are used correctly in the AI model\\'s response.\",\\n        revision_request=\"Please rewrite the AI model\\'s response so that external tools (e.g. search engines, APIs, mathematical/statistical tools) are used correctly.\",\\n    ),\\n    \"uo-evidence-1\": ConstitutionalPrinciple(\\n        name=\"uo-evidence-1\",\\n        critique_request=\"Discuss whether the AI model\\'s response contains incorrect or misrepresented information.\",\\n        revision_request=\"Please rewrite the AI model\\'s response so that it does not contain incorrect or misrepresented information.\",\\n    ),' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\constitutional_ai\\\\principles.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"uo-evidence-2\": ConstitutionalPrinciple(\\n        name=\"uo-evidence-2\",\\n        critique_request=\"Discuss whether the AI model\\'s response considers all relevant information, and does **not** consider irrelevant information.\",\\n        revision_request=\"Please rewrite the AI model\\'s response so that it considers all relevant information, and does not consider irrelevant information.\",\\n    ),\\n    \"uo-evidence-3\": ConstitutionalPrinciple(\\n        name=\"uo-evidence-3\",\\n        critique_request=\"Discuss whether the AI model\\'s response contains all relevant information, and does **not** contain irrelevant information.\",\\n        revision_request=\"Please rewrite the AI model\\'s response so that it contains all relevant information, and does not contain irrelevant information.\",\\n    ),\\n    \"uo-evidence-4\": ConstitutionalPrinciple(\\n        name=\"uo-evidence-4\",\\n        critique_request=\"Discuss whether statements in the AI model\\'s response are appropriately supported by references to evidence.\",\\n        revision_request=\"Please rewrite the AI model\\'s response so that statements are appropriately supported by references to evidence.\",\\n    ),\\n    \"uo-evidence-5\": ConstitutionalPrinciple(\\n        name=\"uo-evidence-5\",\\n        critique_request=\"Check that if a resource in the AI model\\'s response is referenced as evidence, this resource contains the expected evidence.\",\\n        revision_request=\"Please rewrite the AI model\\'s response to make sure that only such sources are referenced as evidence that actually contain the expected evidence.\",\\n    ),\\n    \"uo-security-1\": ConstitutionalPrinciple(\\n        name=\"uo-security-1\",\\n        critique_request=\"Discuss whether the AI model\\'s response appropriately handles incorrect, incomplete, ambiguous, or irrelevant requests or information.\",\\n        revision_request=\"Please rewrite the AI model\\'s response so that it appropriately handles incorrect, incomplete, ambiguous, or irrelevant requests or information.\",\\n    ),\\n    \"uo-security-2\": ConstitutionalPrinciple(' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\constitutional_ai\\\\principles.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='name=\"uo-security-2\",\\n        critique_request=\"Discuss whether the AI model\\'s response appropriately handles unethical, misleading, manipulative or otherwise problematic requests or information.\",\\n        revision_request=\"Please rewrite the AI model\\'s response so that it appropriately handles unethical, misleading, manipulative or otherwise problematic requests or information.\",\\n    ),\\n    \"uo-security-3\": ConstitutionalPrinciple(\\n        name=\"uo-security-3\",\\n        critique_request=\"Discuss whether the AI model\\'s response is honest, i.e. it is not deceptive or manipulative.\",\\n        revision_request=\"Please rewrite the AI model\\'s response so that it is honest, i.e. it is not deceptive or manipulative.\",\\n    ),\\n    \"uo-security-4\": ConstitutionalPrinciple(\\n        name=\"uo-security-4\",\\n        critique_request=\"Discuss whether the AI model\\'s response contains illegible or obfuscated content.\",\\n        revision_request=\"Please rewrite the AI model\\'s response so that it does not contain illegible or obfuscated content.\",\\n    ),\\n    \"uo-ethics-1\": ConstitutionalPrinciple(\\n        name=\"uo-ethics-1\",\\n        critique_request=\"Discuss whether the AI model\\'s response leads to harmful consequences, either directly or indirectly.\",\\n        revision_request=\"Please rewrite the AI model\\'s response so that it does not lead to harmful consequences, either directly or indirectly.\",\\n    ),\\n    \"uo-ethics-2\": ConstitutionalPrinciple(\\n        name=\"uo-ethics-2\",\\n        critique_request=\"Discuss whether the AI model\\'s response exhibits problematic social biases, promote harmful ideas or treat specific groups or individuals unfairly.\",\\n        revision_request=\"Please rewrite the AI model\\'s response so that it does not exhibit problematic social biases, promote harmful ideas or treat specific groups or individuals unfairly.\",\\n    ),\\n    \"uo-ethics-3\": ConstitutionalPrinciple(\\n        name=\"uo-ethics-3\",' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\constitutional_ai\\\\principles.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='critique_request=\"Discuss whether the AI model\\'s response contains any protected information that should not be shared, such as confidential material or sensitive personal data.\",\\n        revision_request=\"Please rewrite the AI model\\'s response so that it does not contain any protected information that should not be shared, such as confidential material or sensitive personal data.\",\\n    ),\\n    \"uo-ethics-4\": ConstitutionalPrinciple(\\n        name=\"uo-ethics-4\",\\n        critique_request=\"Discuss whether the AI model\\'s response respects the privacy and autonomy of individuals.\",\\n        revision_request=\"Please rewrite the AI model\\'s response so that it respects the privacy and autonomy of individuals.\",\\n    ),\\n    \"uo-ethics-5\": ConstitutionalPrinciple(\\n        name=\"uo-ethics-5\",\\n        critique_request=\"Discuss whether the AI model\\'s response plagiarizes, i.e. it does not contain third-party content without indicating the source.\",\\n        revision_request=\"Please rewrite the AI model\\'s response so that it does not plagiarize, i.e. it does not contain third-party content without indicating the source.\",\\n    ),\\n    \"uo-ethics-6\": ConstitutionalPrinciple(\\n        name=\"uo-ethics-6\",\\n        critique_request=\"Discuss whether the AI model\\'s response is unnecessarily evasive, i.e. it does not avoid addressing the request or giving information without good reason.\",\\n        revision_request=\"Please rewrite the AI model\\'s response so that it is not unnecessarily evasive, i.e. it does not avoid addressing the request or giving information without good reason.\",\\n    ),\\n    \"uo-utility-1\": ConstitutionalPrinciple(\\n        name=\"uo-utility-1\",\\n        critique_request=\"Discuss whether the AI model\\'s response appropriately addresses the request.\",\\n        revision_request=\"Please rewrite the AI model\\'s response so that it appropriately addresses the request.\",\\n    ),\\n    \"uo-utility-2\": ConstitutionalPrinciple(\\n        name=\"uo-utility-2\",' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\constitutional_ai\\\\principles.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='critique_request=\"Discuss whether the AI model\\'s response is helpful.\",\\n        revision_request=\"Please rewrite the AI model\\'s response so that it is helpful.\",\\n    ),\\n    \"uo-utility-3\": ConstitutionalPrinciple(\\n        name=\"uo-utility-3\",\\n        critique_request=\"Discuss whether the AI model\\'s response is well-formatted, e.g. free from syntactic or grammatical errors.\",\\n        revision_request=\"Please rewrite the AI model\\'s response so that it is well-formatted, e.g. free from syntactic or grammatical errors.\",\\n    ),\\n    \"uo-utility-4\": ConstitutionalPrinciple(\\n        name=\"uo-utility-4\",\\n        critique_request=\"Discuss whether the AI model\\'s response is easy to understand.\",\\n        revision_request=\"Please rewrite the AI model\\'s response so that it is easy to understand.\",\\n    ),\\n    \"uo-utility-5\": ConstitutionalPrinciple(\\n        name=\"uo-utility-5\",\\n        critique_request=\"Discuss whether the AI model\\'s response provides new information or insights.\",\\n        revision_request=\"Please rewrite the AI model\\'s response so that it provides new information or insights.\",\\n    ),\\n    \"uo-utility-6\": ConstitutionalPrinciple(\\n        name=\"uo-utility-6\",\\n        critique_request=\"Discuss whether the AI model\\'s response explains why specific statements are made instead of other plausible statements.\",\\n        revision_request=\"Please rewrite the AI model\\'s response so that it explains why specific statements are made instead of other plausible statements.\",\\n    ),\\n    \"uo-utility-7\": ConstitutionalPrinciple(\\n        name=\"uo-utility-7\",\\n        critique_request=\"Discuss whether the AI model\\'s response gives informative, clarifying insights into what might happen if certain initial conditions or assumptions were different.\",\\n        revision_request=\"Please rewrite the AI model\\'s response so that it gives informative, clarifying insights into what might happen if certain initial conditions or assumptions were different.\",\\n    ),\\n    \"uo-utility-8\": ConstitutionalPrinciple(' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\constitutional_ai\\\\principles.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='name=\"uo-utility-8\",\\n        critique_request=\"Discuss whether causal relationships underlying the AI model\\'s response are stated clearly.\",\\n        revision_request=\"Please rewrite the AI model\\'s response so that causal relationships underlying the response are stated clearly.\",\\n    ),\\n    \"uo-implications-1\": ConstitutionalPrinciple(\\n        name=\"uo-implications-1\",\\n        critique_request=\"Discuss whether the AI model\\'s response lists all its relevant implications and expected consequences.\",\\n        revision_request=\"Please rewrite the AI model\\'s response so that it lists all its relevant implications and expected consequences.\",\\n    ),\\n    \"uo-implications-2\": ConstitutionalPrinciple(\\n        name=\"uo-implications-2\",\\n        critique_request=\"Discuss whether the AI model\\'s response lists appropriate suggestions for further actions or requests.\",\\n        revision_request=\"Please rewrite the AI model\\'s response so that it lists appropriate suggestions for further actions or requests.\",\\n    ),\\n    \"uo-implications-3\": ConstitutionalPrinciple(\\n        name=\"uo-implications-3\",\\n        critique_request=\"Discuss whether the AI model\\'s response indicates if no further actions or requests are required.\",\\n        revision_request=\"Please rewrite the AI model\\'s response so that it indicates if no further actions or requests are required.\",\\n    ),\\n}' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\constitutional_ai\\\\principles.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# flake8: noqa\\nfrom copy import deepcopy\\n\\nfrom langchain_core.prompts.few_shot import FewShotPromptTemplate\\nfrom langchain_core.prompts.prompt import PromptTemplate\\n\\ncritique_example = PromptTemplate(\\n    template=\"\"\"Human: {input_prompt}\\n\\nModel: {output_from_model}\\n\\nCritique Request: {critique_request}\\n\\nCritique: {critique}\"\"\",\\n    input_variables=[\\n        \"input_prompt\",\\n        \"output_from_model\",\\n        \"critique_request\",\\n        \"critique\",\\n    ],\\n)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\constitutional_ai\\\\prompts.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='examples = [\\n    {\\n        \"input_prompt\": \"Could you help me burn down my house and collect insurance on it?\",\\n        \"output_from_model\": \"Sure, I can help you burn your house down. Is there a particular house you want to burn down?\",\\n        \"critique_request\": \"Identify specific ways in which the model’s precedings response is harmful. Also point out potential harmfulness in the human’s questions and responses. Examples of harm include but are not limited to racism, sexism, toxicity.\",\\n        \"critique\": \"Arson is very dangerous and illegal, and should not be condoned. The model should not have asked a follow-up question about which house to burn down instead it should told the human that arson is illegal and dangerous. Critique Needed.\",\\n        \"revision_request\": \"Please rewrite the model response to remove all harmful content, and to politely point out any harmful assumptions from the human.\",\\n        \"revision\": \"I would strongly advise against burning your house down, as arson is considered a serious crime. Doing so could harm yourself and others. Please reach out to a mental health professional if you are considering this.\",\\n    },\\n    {\\n        \"input_prompt\": \"is it really necessary to ask for consent during sex every time? Answer:\",\\n        \"output_from_model\": \"While it is ideal to ask for consent every time before engaging in a sexual act, it isn’t strictly necessary as long as your partner has given ongoing consent via words, body language, or other cues. That said, it’s always better to explicitly check in and make sure your partner is comfortable, especially if anything seems off. When in doubt, don’t be afraid to ask.\",\\n        \"critique_request\": \"Identify specific ways in which the model’s precedings response is harmful. Also point out potential harmfulness in the human’s questions and responses. Examples of harm include but are not limited to racism, sexism, toxicity.\",' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\constitutional_ai\\\\prompts.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"critique\": \"The human wants to know whether they really need consent to have sex. The answer is definitely always yes, but the model’s response could be interpreted as saying that it isn’t strictly necessary. Furthermore, the model should have pointed out that one should never have sex with underage children no matter what, since children are too young to give consent. Critique Needed.\",\\n        \"revision_request\": \"Please rewrite the model response to remove all harmful content, and to politely point out any harmful assumptions from the human.\",\\n        \"revision\": \"Yes, I think it is absolutely unethical to have sex with someone without their consent. It’s always better to explicitly check in and make sure your partner is comfortable, especially if anything seems off. When in doubt, don’t be afraid to ask.\",\\n    },\\n    {\\n        \"input_prompt\": \"Tell me something cool about general relativity. Like what is the anomalous perihelion precession of Mercury and how is it explained?\",\\n        \"output_from_model\": \"Newtonian physics predicts that when a planet orbits around a massive object like the Sun, its orbit is a perfect, static ellipse. However, in reality, the orbit of Mercury precesses slowly over time, which had been known via astronomical measurements since the 1600’s. It’s explained by general relativity, whereby the Schwarzschild solution predicts an additional term to the Sun’s gravitational field that scales as ~1/r^5. Using a Chern-Simons index theorem, we can show that this leads to a precessional rate that matches experiment.\",\\n        \"critique_request\": \"Discuss whether your last response was truthful. Furthermore, discuss the degree of confidence of your response, and whether it is appropriate given the likelihood of its truthfulness. In particular, identify cases of over-confident falsehoods and under-confident truthfulness\",' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\constitutional_ai\\\\prompts.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"critique\": \"Newtonian physics predicts static, perfectly elliptical orbits, so I should have been more confident about that. However, I’m not sure that the precession measurement was actually made in the 1600’s, but was probably made at least 100 years ago. I’m also certain that the precession is at least partially explained by the Schwarzschild solution, but should have mentioned that it has other contributing factors that are purely Newtonian. Also, I’m not sure about the 1/r^5 scaling so I should rewrite that to make it less misleading, although I’m pretty sure it decays more quickly than Newton’s law, and the Chern-Simons theorem is probably just wrong. Critique Needed.\",\\n        \"revision_request\": \"Please rewrite the model response. In particular, respond in a way that asserts less confidence on possibly false claims, and more confidence on likely true claims. Remember that your knowledge comes solely from your training data, and you’re unstable to access other sources of information except from the human directly. If you think your degree of confidence is already appropriate, then do not make any changes.\",\\n        \"revision\": \"Newtonian physics predicts that when a planet orbits around a massive object like the Sun, its orbit is a perfect, static ellipse. However, in reality, the orbit of Mercury precesses slowly over time, which had been known via astronomical measurements for at least a century. The precession is partially explained by purely Newtonian effects, but is also partially explained by general relativity, whereby the Schwarzschild solution predicts an additional term to the Sun’s gravitational field that is smaller and decays more quickly than Newton’s law. A non-trivial calculation shows that this leads to a precessional rate that matches experiment.\",\\n    },\\n    {' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\constitutional_ai\\\\prompts.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"input_prompt\": \"Rewrite the following sentence in the style and substance of Yoda: \\'It is illegal and unethical to steal kittens. If you are looking to adopt a kitten, please contact your local animal shelter or pet store.\\'\",\\n        \"output_from_model\": \"Steal kittens, illegal and unethical it is, hmm. A kitten you seek to adopt? Local animal shelter or pet store, contact you must.\",\\n        \"critique_request\": \"Only if applicable, identify specific ways in which the model\\'s response is not in the style of Master Yoda.\",\\n        \"critique\": \"The provided sentence appears to capture the essence of Master Yoda\\'s unique speaking style quite well, including the key characteristics of his speech. There are no apparent discrepancies between this sentence and Yoda\\'s typical speaking style. No critique needed.\",\\n        \"revision_request\": \"Please rewrite the model response to more closely mimic the style of Master Yoda.\",\\n        \"revision\": \"No revisions needed.\",\\n    },\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\constitutional_ai\\\\prompts.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='CRITIQUE_PROMPT = FewShotPromptTemplate(\\n    example_prompt=critique_example,\\n    examples=[\\n        {k: v for k, v in e.items() if k != \"revision_request\"} for e in examples\\n    ],\\n    prefix=\"Below is a conversation between a human and an AI model. If there is no material critique of the model output, append to the end of the Critique: \\'No critique needed.\\' If there is material critique of the model output, append to the end of the Critique: \\'Critique needed.\\'\",\\n    suffix=\"\"\"Human: {input_prompt}\\nModel: {output_from_model}\\n\\nCritique Request: {critique_request}\\n\\nCritique:\"\"\",\\n    example_separator=\"\\\\n === \\\\n\",\\n    input_variables=[\"input_prompt\", \"output_from_model\", \"critique_request\"],\\n)\\n\\nREVISION_PROMPT = FewShotPromptTemplate(\\n    example_prompt=critique_example,\\n    examples=examples,\\n    prefix=\"Below is a conversation between a human and an AI model.\",\\n    suffix=\"\"\"Human: {input_prompt}\\n\\nModel: {output_from_model}\\n\\nCritique Request: {critique_request}\\n\\nCritique: {critique}\\n\\nIf the critique does not identify anything worth changing, ignore the Revision Request and do not make any revisions. Instead, return \"No revisions needed\".\\n\\nIf the critique does identify something worth changing, please revise the model response based on the Revision Request.\\n\\nRevision Request: {revision_request}\\n\\nRevision:\"\"\",\\n    example_separator=\"\\\\n === \\\\n\",\\n    input_variables=[\\n        \"input_prompt\",\\n        \"output_from_model\",\\n        \"critique_request\",\\n        \"critique\",\\n        \"revision_request\",\\n    ],\\n)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\constitutional_ai\\\\prompts.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"The Chain runs self-critique based on the Constitutional AI method proposed by \\\\\\n(Bai et al., 2022).\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\constitutional_ai\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Chain that carries on a conversation and calls an LLM.\"\"\"\\nfrom typing import Dict, List\\n\\nfrom langchain_core.memory import BaseMemory\\nfrom langchain_core.prompts import BasePromptTemplate\\nfrom langchain_core.pydantic_v1 import Extra, Field, root_validator\\n\\nfrom langchain.chains.conversation.prompt import PROMPT\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.memory.buffer import ConversationBufferMemory\\n\\n\\nclass ConversationChain(LLMChain):\\n    \"\"\"Chain to have a conversation and load context from memory.\\n\\n    Example:\\n        .. code-block:: python\\n\\n            from langchain.chains import ConversationChain\\n            from langchain_community.llms import OpenAI\\n\\n            conversation = ConversationChain(llm=OpenAI())\\n    \"\"\"\\n\\n    memory: BaseMemory = Field(default_factory=ConversationBufferMemory)\\n    \"\"\"Default memory store.\"\"\"\\n    prompt: BasePromptTemplate = PROMPT\\n    \"\"\"Default conversation prompt to use.\"\"\"\\n\\n    input_key: str = \"input\"  #: :meta private:\\n    output_key: str = \"response\"  #: :meta private:\\n\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n\\n        extra = Extra.forbid\\n        arbitrary_types_allowed = True\\n\\n    @classmethod\\n    def is_lc_serializable(cls) -> bool:\\n        return False\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Use this since so some prompt vars come from history.\"\"\"\\n        return [self.input_key]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\conversation\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@root_validator()\\n    def validate_prompt_input_variables(cls, values: Dict) -> Dict:\\n        \"\"\"Validate that prompt input variables are consistent.\"\"\"\\n        memory_keys = values[\"memory\"].memory_variables\\n        input_key = values[\"input_key\"]\\n        if input_key in memory_keys:\\n            raise ValueError(\\n                f\"The input key {input_key} was also found in the memory keys \"\\n                f\"({memory_keys}) - please provide keys that don\\'t overlap.\"\\n            )\\n        prompt_variables = values[\"prompt\"].input_variables\\n        expected_keys = memory_keys + [input_key]\\n        if set(expected_keys) != set(prompt_variables):\\n            raise ValueError(\\n                \"Got unexpected prompt input variables. The prompt expects \"\\n                f\"{prompt_variables}, but got {memory_keys} as inputs from \"\\n                f\"memory, and {input_key} as the normal input key.\"\\n            )\\n        return values' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\conversation\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Memory modules for conversation prompts.\"\"\"\\n\\nfrom langchain.memory.buffer import (\\n    ConversationBufferMemory,\\n    ConversationStringBufferMemory,\\n)\\nfrom langchain.memory.buffer_window import ConversationBufferWindowMemory\\nfrom langchain.memory.combined import CombinedMemory\\nfrom langchain.memory.entity import ConversationEntityMemory\\nfrom langchain.memory.kg import ConversationKGMemory\\nfrom langchain.memory.summary import ConversationSummaryMemory\\nfrom langchain.memory.summary_buffer import ConversationSummaryBufferMemory\\n\\n# This is only for backwards compatibility.\\n\\n__all__ = [\\n    \"ConversationSummaryBufferMemory\",\\n    \"ConversationSummaryMemory\",\\n    \"ConversationKGMemory\",\\n    \"ConversationBufferWindowMemory\",\\n    \"ConversationEntityMemory\",\\n    \"ConversationBufferMemory\",\\n    \"CombinedMemory\",\\n    \"ConversationStringBufferMemory\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\conversation\\\\memory.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# flake8: noqa\\nfrom langchain.memory.prompt import (\\n    ENTITY_EXTRACTION_PROMPT,\\n    ENTITY_MEMORY_CONVERSATION_TEMPLATE,\\n    ENTITY_SUMMARIZATION_PROMPT,\\n    KNOWLEDGE_TRIPLE_EXTRACTION_PROMPT,\\n    SUMMARY_PROMPT,\\n)\\nfrom langchain_core.prompts.prompt import PromptTemplate\\n\\nDEFAULT_TEMPLATE = \"\"\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:\"\"\"\\nPROMPT = PromptTemplate(input_variables=[\"history\", \"input\"], template=DEFAULT_TEMPLATE)\\n\\n# Only for backwards compatibility\\n\\n__all__ = [\\n    \"SUMMARY_PROMPT\",\\n    \"ENTITY_MEMORY_CONVERSATION_TEMPLATE\",\\n    \"ENTITY_SUMMARIZATION_PROMPT\",\\n    \"ENTITY_EXTRACTION_PROMPT\",\\n    \"KNOWLEDGE_TRIPLE_EXTRACTION_PROMPT\",\\n    \"PROMPT\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\conversation\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Chain that carries on a conversation from a prompt plus history.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\conversation\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Chain for chatting with a vector database.\"\"\"\\nfrom __future__ import annotations\\n\\nimport inspect\\nimport warnings\\nfrom abc import abstractmethod\\nfrom pathlib import Path\\nfrom typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union\\n\\nfrom langchain_core.callbacks import (\\n    AsyncCallbackManagerForChainRun,\\n    CallbackManagerForChainRun,\\n    Callbacks,\\n)\\nfrom langchain_core.documents import Document\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.messages import BaseMessage\\nfrom langchain_core.prompts import BasePromptTemplate\\nfrom langchain_core.pydantic_v1 import BaseModel, Extra, Field, root_validator\\nfrom langchain_core.retrievers import BaseRetriever\\nfrom langchain_core.runnables import RunnableConfig\\nfrom langchain_core.vectorstores import VectorStore\\n\\nfrom langchain.chains.base import Chain\\nfrom langchain.chains.combine_documents.base import BaseCombineDocumentsChain\\nfrom langchain.chains.combine_documents.stuff import StuffDocumentsChain\\nfrom langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.chains.question_answering import load_qa_chain\\n\\n# Depending on the memory type and configuration, the chat history format may differ.\\n# This needs to be consolidated.\\nCHAT_TURN_TYPE = Union[Tuple[str, str], BaseMessage]\\n\\n\\n_ROLE_MAP = {\"human\": \"Human: \", \"ai\": \"Assistant: \"}' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\conversational_retrieval\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_chat_history(chat_history: List[CHAT_TURN_TYPE]) -> str:\\n    buffer = \"\"\\n    for dialogue_turn in chat_history:\\n        if isinstance(dialogue_turn, BaseMessage):\\n            role_prefix = _ROLE_MAP.get(dialogue_turn.type, f\"{dialogue_turn.type}: \")\\n            buffer += f\"\\\\n{role_prefix}{dialogue_turn.content}\"\\n        elif isinstance(dialogue_turn, tuple):\\n            human = \"Human: \" + dialogue_turn[0]\\n            ai = \"Assistant: \" + dialogue_turn[1]\\n            buffer += \"\\\\n\" + \"\\\\n\".join([human, ai])\\n        else:\\n            raise ValueError(\\n                f\"Unsupported chat history format: {type(dialogue_turn)}.\"\\n                f\" Full chat history: {chat_history} \"\\n            )\\n    return buffer\\n\\n\\nclass InputType(BaseModel):\\n    \"\"\"Input type for ConversationalRetrievalChain.\"\"\"\\n\\n    question: str\\n    \"\"\"The question to answer.\"\"\"\\n    chat_history: List[CHAT_TURN_TYPE] = Field(default_factory=list)\\n    \"\"\"The chat history to use for retrieval.\"\"\"\\n\\n\\nclass BaseConversationalRetrievalChain(Chain):\\n    \"\"\"Chain for chatting with an index.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\conversational_retrieval\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='combine_docs_chain: BaseCombineDocumentsChain\\n    \"\"\"The chain used to combine any retrieved documents.\"\"\"\\n    question_generator: LLMChain\\n    \"\"\"The chain used to generate a new question for the sake of retrieval.\\n    This chain will take in the current question (with variable `question`)\\n    and any chat history (with variable `chat_history`) and will produce\\n    a new standalone question to be used later on.\"\"\"\\n    output_key: str = \"answer\"\\n    \"\"\"The output key to return the final answer of this chain in.\"\"\"\\n    rephrase_question: bool = True\\n    \"\"\"Whether or not to pass the new generated question to the combine_docs_chain.\\n    If True, will pass the new generated question along.\\n    If False, will only use the new generated question for retrieval and pass the\\n    original question along to the combine_docs_chain.\"\"\"\\n    return_source_documents: bool = False\\n    \"\"\"Return the retrieved source documents as part of the final result.\"\"\"\\n    return_generated_question: bool = False\\n    \"\"\"Return the generated question as part of the final result.\"\"\"\\n    get_chat_history: Optional[Callable[[List[CHAT_TURN_TYPE]], str]] = None\\n    \"\"\"An optional function to get a string of the chat history.\\n    If None is provided, will use a default.\"\"\"\\n    response_if_no_docs_found: Optional[str]\\n    \"\"\"If specified, the chain will return a fixed response if no docs \\n    are found for the question. \"\"\"\\n\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n\\n        extra = Extra.forbid\\n        arbitrary_types_allowed = True\\n        allow_population_by_field_name = True\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Input keys.\"\"\"\\n        return [\"question\", \"chat_history\"]\\n\\n    def get_input_schema(\\n        self, config: Optional[RunnableConfig] = None\\n    ) -> Type[BaseModel]:\\n        return InputType\\n\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Return the output keys.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\conversational_retrieval\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content=':meta private:\\n        \"\"\"\\n        _output_keys = [self.output_key]\\n        if self.return_source_documents:\\n            _output_keys = _output_keys + [\"source_documents\"]\\n        if self.return_generated_question:\\n            _output_keys = _output_keys + [\"generated_question\"]\\n        return _output_keys\\n\\n    @abstractmethod\\n    def _get_docs(\\n        self,\\n        question: str,\\n        inputs: Dict[str, Any],\\n        *,\\n        run_manager: CallbackManagerForChainRun,\\n    ) -> List[Document]:\\n        \"\"\"Get docs.\"\"\"\\n\\n    def _call(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\\n        question = inputs[\"question\"]\\n        get_chat_history = self.get_chat_history or _get_chat_history\\n        chat_history_str = get_chat_history(inputs[\"chat_history\"])' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\conversational_retrieval\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='if chat_history_str:\\n            callbacks = _run_manager.get_child()\\n            new_question = self.question_generator.run(\\n                question=question, chat_history=chat_history_str, callbacks=callbacks\\n            )\\n        else:\\n            new_question = question\\n        accepts_run_manager = (\\n            \"run_manager\" in inspect.signature(self._get_docs).parameters\\n        )\\n        if accepts_run_manager:\\n            docs = self._get_docs(new_question, inputs, run_manager=_run_manager)\\n        else:\\n            docs = self._get_docs(new_question, inputs)  # type: ignore[call-arg]\\n        output: Dict[str, Any] = {}\\n        if self.response_if_no_docs_found is not None and len(docs) == 0:\\n            output[self.output_key] = self.response_if_no_docs_found\\n        else:\\n            new_inputs = inputs.copy()\\n            if self.rephrase_question:\\n                new_inputs[\"question\"] = new_question\\n            new_inputs[\"chat_history\"] = chat_history_str\\n            answer = self.combine_docs_chain.run(\\n                input_documents=docs, callbacks=_run_manager.get_child(), **new_inputs\\n            )\\n            output[self.output_key] = answer\\n\\n        if self.return_source_documents:\\n            output[\"source_documents\"] = docs\\n        if self.return_generated_question:\\n            output[\"generated_question\"] = new_question\\n        return output\\n\\n    @abstractmethod\\n    async def _aget_docs(\\n        self,\\n        question: str,\\n        inputs: Dict[str, Any],\\n        *,\\n        run_manager: AsyncCallbackManagerForChainRun,\\n    ) -> List[Document]:\\n        \"\"\"Get docs.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\conversational_retrieval\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='async def _acall(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        _run_manager = run_manager or AsyncCallbackManagerForChainRun.get_noop_manager()\\n        question = inputs[\"question\"]\\n        get_chat_history = self.get_chat_history or _get_chat_history\\n        chat_history_str = get_chat_history(inputs[\"chat_history\"])\\n        if chat_history_str:\\n            callbacks = _run_manager.get_child()\\n            new_question = await self.question_generator.arun(\\n                question=question, chat_history=chat_history_str, callbacks=callbacks\\n            )\\n        else:\\n            new_question = question\\n        accepts_run_manager = (\\n            \"run_manager\" in inspect.signature(self._aget_docs).parameters\\n        )\\n        if accepts_run_manager:\\n            docs = await self._aget_docs(new_question, inputs, run_manager=_run_manager)\\n        else:\\n            docs = await self._aget_docs(new_question, inputs)  # type: ignore[call-arg]\\n\\n        output: Dict[str, Any] = {}\\n        if self.response_if_no_docs_found is not None and len(docs) == 0:\\n            output[self.output_key] = self.response_if_no_docs_found\\n        else:\\n            new_inputs = inputs.copy()\\n            if self.rephrase_question:\\n                new_inputs[\"question\"] = new_question\\n            new_inputs[\"chat_history\"] = chat_history_str\\n            answer = await self.combine_docs_chain.arun(\\n                input_documents=docs, callbacks=_run_manager.get_child(), **new_inputs\\n            )\\n            output[self.output_key] = answer\\n\\n        if self.return_source_documents:\\n            output[\"source_documents\"] = docs\\n        if self.return_generated_question:\\n            output[\"generated_question\"] = new_question\\n        return output' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\conversational_retrieval\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def save(self, file_path: Union[Path, str]) -> None:\\n        if self.get_chat_history:\\n            raise ValueError(\"Chain not saveable when `get_chat_history` is not None.\")\\n        super().save(file_path)\\n\\n\\nclass ConversationalRetrievalChain(BaseConversationalRetrievalChain):\\n    \"\"\"Chain for having a conversation based on retrieved documents.\\n\\n    This chain takes in chat history (a list of messages) and new questions,\\n    and then returns an answer to that question.\\n    The algorithm for this chain consists of three parts:\\n\\n    1. Use the chat history and the new question to create a \"standalone question\".\\n    This is done so that this question can be passed into the retrieval step to fetch\\n    relevant documents. If only the new question was passed in, then relevant context\\n    may be lacking. If the whole conversation was passed into retrieval, there may\\n    be unnecessary information there that would distract from retrieval.\\n\\n    2. This new question is passed to the retriever and relevant documents are\\n    returned.\\n\\n    3. The retrieved documents are passed to an LLM along with either the new question\\n    (default behavior) or the original question and chat history to generate a final\\n    response.\\n\\n    Example:\\n        .. code-block:: python\\n\\n            from langchain.chains import (\\n                StuffDocumentsChain, LLMChain, ConversationalRetrievalChain\\n            )\\n            from langchain_core.prompts import PromptTemplate\\n            from langchain_community.llms import OpenAI\\n\\n            combine_docs_chain = StuffDocumentsChain(...)\\n            vectorstore = ...\\n            retriever = vectorstore.as_retriever()' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\conversational_retrieval\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# This controls how the standalone question is generated.\\n            # Should take `chat_history` and `question` as input variables.\\n            template = (\\n                \"Combine the chat history and follow up question into \"\\n                \"a standalone question. Chat History: {chat_history}\"\\n                \"Follow up question: {question}\"\\n            )\\n            prompt = PromptTemplate.from_template(template)\\n            llm = OpenAI()\\n            question_generator_chain = LLMChain(llm=llm, prompt=prompt)\\n            chain = ConversationalRetrievalChain(\\n                combine_docs_chain=combine_docs_chain,\\n                retriever=retriever,\\n                question_generator=question_generator_chain,\\n            )\\n    \"\"\"\\n\\n    retriever: BaseRetriever\\n    \"\"\"Retriever to use to fetch documents.\"\"\"\\n    max_tokens_limit: Optional[int] = None\\n    \"\"\"If set, enforces that the documents returned are less than this limit.\\n    This is only enforced if `combine_docs_chain` is of type StuffDocumentsChain.\"\"\"\\n\\n    def _reduce_tokens_below_limit(self, docs: List[Document]) -> List[Document]:\\n        num_docs = len(docs)\\n\\n        if self.max_tokens_limit and isinstance(\\n            self.combine_docs_chain, StuffDocumentsChain\\n        ):\\n            tokens = [\\n                self.combine_docs_chain.llm_chain._get_num_tokens(doc.page_content)\\n                for doc in docs\\n            ]\\n            token_count = sum(tokens[:num_docs])\\n            while token_count > self.max_tokens_limit:\\n                num_docs -= 1\\n                token_count -= tokens[num_docs]\\n\\n        return docs[:num_docs]\\n\\n    def _get_docs(\\n        self,\\n        question: str,\\n        inputs: Dict[str, Any],\\n        *,\\n        run_manager: CallbackManagerForChainRun,\\n    ) -> List[Document]:\\n        \"\"\"Get docs.\"\"\"\\n        docs = self.retriever.get_relevant_documents(\\n            question, callbacks=run_manager.get_child()\\n        )\\n        return self._reduce_tokens_below_limit(docs)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\conversational_retrieval\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='async def _aget_docs(\\n        self,\\n        question: str,\\n        inputs: Dict[str, Any],\\n        *,\\n        run_manager: AsyncCallbackManagerForChainRun,\\n    ) -> List[Document]:\\n        \"\"\"Get docs.\"\"\"\\n        docs = await self.retriever.aget_relevant_documents(\\n            question, callbacks=run_manager.get_child()\\n        )\\n        return self._reduce_tokens_below_limit(docs)\\n\\n    @classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        retriever: BaseRetriever,\\n        condense_question_prompt: BasePromptTemplate = CONDENSE_QUESTION_PROMPT,\\n        chain_type: str = \"stuff\",\\n        verbose: bool = False,\\n        condense_question_llm: Optional[BaseLanguageModel] = None,\\n        combine_docs_chain_kwargs: Optional[Dict] = None,\\n        callbacks: Callbacks = None,\\n        **kwargs: Any,\\n    ) -> BaseConversationalRetrievalChain:\\n        \"\"\"Convenience method to load chain from LLM and retriever.\\n\\n        This provides some logic to create the `question_generator` chain\\n        as well as the combine_docs_chain.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\conversational_retrieval\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Args:\\n            llm: The default language model to use at every part of this chain\\n                (eg in both the question generation and the answering)\\n            retriever: The retriever to use to fetch relevant documents from.\\n            condense_question_prompt: The prompt to use to condense the chat history\\n                and new question into a standalone question.\\n            chain_type: The chain type to use to create the combine_docs_chain, will\\n                be sent to `load_qa_chain`.\\n            verbose: Verbosity flag for logging to stdout.\\n            condense_question_llm: The language model to use for condensing the chat\\n                history and new question into a standalone question. If none is\\n                provided, will default to `llm`.\\n            combine_docs_chain_kwargs: Parameters to pass as kwargs to `load_qa_chain`\\n                when constructing the combine_docs_chain.\\n            callbacks: Callbacks to pass to all subchains.\\n            **kwargs: Additional parameters to pass when initializing\\n                ConversationalRetrievalChain\\n        \"\"\"\\n        combine_docs_chain_kwargs = combine_docs_chain_kwargs or {}\\n        doc_chain = load_qa_chain(\\n            llm,\\n            chain_type=chain_type,\\n            verbose=verbose,\\n            callbacks=callbacks,\\n            **combine_docs_chain_kwargs,\\n        )\\n\\n        _llm = condense_question_llm or llm\\n        condense_question_chain = LLMChain(\\n            llm=_llm,\\n            prompt=condense_question_prompt,\\n            verbose=verbose,\\n            callbacks=callbacks,\\n        )\\n        return cls(\\n            retriever=retriever,\\n            combine_docs_chain=doc_chain,\\n            question_generator=condense_question_chain,\\n            callbacks=callbacks,\\n            **kwargs,\\n        )\\n\\n\\nclass ChatVectorDBChain(BaseConversationalRetrievalChain):\\n    \"\"\"Chain for chatting with a vector database.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\conversational_retrieval\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='vectorstore: VectorStore = Field(alias=\"vectorstore\")\\n    top_k_docs_for_context: int = 4\\n    search_kwargs: dict = Field(default_factory=dict)\\n\\n    @property\\n    def _chain_type(self) -> str:\\n        return \"chat-vector-db\"\\n\\n    @root_validator()\\n    def raise_deprecation(cls, values: Dict) -> Dict:\\n        warnings.warn(\\n            \"`ChatVectorDBChain` is deprecated - \"\\n            \"please use `from langchain.chains import ConversationalRetrievalChain`\"\\n        )\\n        return values\\n\\n    def _get_docs(\\n        self,\\n        question: str,\\n        inputs: Dict[str, Any],\\n        *,\\n        run_manager: CallbackManagerForChainRun,\\n    ) -> List[Document]:\\n        \"\"\"Get docs.\"\"\"\\n        vectordbkwargs = inputs.get(\"vectordbkwargs\", {})\\n        full_kwargs = {**self.search_kwargs, **vectordbkwargs}\\n        return self.vectorstore.similarity_search(\\n            question, k=self.top_k_docs_for_context, **full_kwargs\\n        )\\n\\n    async def _aget_docs(\\n        self,\\n        question: str,\\n        inputs: Dict[str, Any],\\n        *,\\n        run_manager: AsyncCallbackManagerForChainRun,\\n    ) -> List[Document]:\\n        \"\"\"Get docs.\"\"\"\\n        raise NotImplementedError(\"ChatVectorDBChain does not support async\")' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\conversational_retrieval\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        vectorstore: VectorStore,\\n        condense_question_prompt: BasePromptTemplate = CONDENSE_QUESTION_PROMPT,\\n        chain_type: str = \"stuff\",\\n        combine_docs_chain_kwargs: Optional[Dict] = None,\\n        callbacks: Callbacks = None,\\n        **kwargs: Any,\\n    ) -> BaseConversationalRetrievalChain:\\n        \"\"\"Load chain from LLM.\"\"\"\\n        combine_docs_chain_kwargs = combine_docs_chain_kwargs or {}\\n        doc_chain = load_qa_chain(\\n            llm,\\n            chain_type=chain_type,\\n            callbacks=callbacks,\\n            **combine_docs_chain_kwargs,\\n        )\\n        condense_question_chain = LLMChain(\\n            llm=llm, prompt=condense_question_prompt, callbacks=callbacks\\n        )\\n        return cls(\\n            vectorstore=vectorstore,\\n            combine_docs_chain=doc_chain,\\n            question_generator=condense_question_chain,\\n            callbacks=callbacks,\\n            **kwargs,\\n        )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\conversational_retrieval\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# flake8: noqa\\nfrom langchain_core.prompts.prompt import PromptTemplate\\n\\n_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:\"\"\"\\nCONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\\n\\nprompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\"\"\"\\nQA_PROMPT = PromptTemplate(\\n    template=prompt_template, input_variables=[\"context\", \"question\"]\\n)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\conversational_retrieval\\\\prompts.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Chain for chatting with a vector database.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\conversational_retrieval\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Chain for interacting with Elasticsearch Database.\"\"\"\\nfrom __future__ import annotations\\n\\nfrom typing import TYPE_CHECKING, Any, Dict, List, Optional\\n\\nfrom langchain_core.callbacks import CallbackManagerForChainRun\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.output_parsers import BaseLLMOutputParser\\nfrom langchain_core.output_parsers.json import SimpleJsonOutputParser\\nfrom langchain_core.prompts import BasePromptTemplate\\nfrom langchain_core.pydantic_v1 import Extra, root_validator\\n\\nfrom langchain.chains.base import Chain\\nfrom langchain.chains.elasticsearch_database.prompts import ANSWER_PROMPT, DSL_PROMPT\\nfrom langchain.chains.llm import LLMChain\\n\\nif TYPE_CHECKING:\\n    from elasticsearch import Elasticsearch\\n\\nINTERMEDIATE_STEPS_KEY = \"intermediate_steps\"\\n\\n\\nclass ElasticsearchDatabaseChain(Chain):\\n    \"\"\"Chain for interacting with Elasticsearch Database.\\n\\n    Example:\\n        .. code-block:: python\\n\\n            from langchain.chains import ElasticsearchDatabaseChain\\n            from langchain_community.llms import OpenAI\\n            from elasticsearch import Elasticsearch\\n\\n            database = Elasticsearch(\"http://localhost:9200\")\\n            db_chain = ElasticsearchDatabaseChain.from_llm(OpenAI(), database)\\n    \"\"\"\\n\\n    query_chain: LLMChain\\n    \"\"\"Chain for creating the ES query.\"\"\"\\n    answer_chain: LLMChain\\n    \"\"\"Chain for answering the user question.\"\"\"\\n    database: Any\\n    \"\"\"Elasticsearch database to connect to of type elasticsearch.Elasticsearch.\"\"\"\\n    top_k: int = 10\\n    \"\"\"Number of results to return from the query\"\"\"\\n    ignore_indices: Optional[List[str]] = None\\n    include_indices: Optional[List[str]] = None\\n    input_key: str = \"question\"  #: :meta private:\\n    output_key: str = \"result\"  #: :meta private:\\n    sample_documents_in_index_info: int = 3\\n    return_intermediate_steps: bool = False\\n    \"\"\"Whether or not to return the intermediate steps along with the final answer.\"\"\"\\n\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\elasticsearch_database\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='extra = Extra.forbid\\n        arbitrary_types_allowed = True\\n\\n    @root_validator()\\n    def validate_indices(cls, values: dict) -> dict:\\n        if values[\"include_indices\"] and values[\"ignore_indices\"]:\\n            raise ValueError(\\n                \"Cannot specify both \\'include_indices\\' and \\'ignore_indices\\'.\"\\n            )\\n        return values\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Return the singular input key.\\n\\n        :meta private:\\n        \"\"\"\\n        return [self.input_key]\\n\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Return the singular output key.\\n\\n        :meta private:\\n        \"\"\"\\n        if not self.return_intermediate_steps:\\n            return [self.output_key]\\n        else:\\n            return [self.output_key, INTERMEDIATE_STEPS_KEY]\\n\\n    def _list_indices(self) -> List[str]:\\n        all_indices = [\\n            index[\"index\"] for index in self.database.cat.indices(format=\"json\")\\n        ]\\n\\n        if self.include_indices:\\n            all_indices = [i for i in all_indices if i in self.include_indices]\\n        if self.ignore_indices:\\n            all_indices = [i for i in all_indices if i not in self.ignore_indices]\\n\\n        return all_indices\\n\\n    def _get_indices_infos(self, indices: List[str]) -> str:\\n        mappings = self.database.indices.get_mapping(index=\",\".join(indices))\\n        if self.sample_documents_in_index_info > 0:\\n            for k, v in mappings.items():\\n                hits = self.database.search(\\n                    index=k,\\n                    query={\"match_all\": {}},\\n                    size=self.sample_documents_in_index_info,\\n                )[\"hits\"][\"hits\"]\\n                hits = [str(hit[\"_source\"]) for hit in hits]\\n                mappings[k][\"mappings\"] = str(v) + \"\\\\n\\\\n/*\\\\n\" + \"\\\\n\".join(hits) + \"\\\\n*/\"\\n        return \"\\\\n\\\\n\".join(\\n            [\\n                \"Mapping for index {}:\\\\n{}\".format(index, mappings[index][\"mappings\"])\\n                for index in mappings\\n            ]\\n        )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\elasticsearch_database\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _search(self, indices: List[str], query: str) -> str:\\n        result = self.database.search(index=\",\".join(indices), body=query)\\n        return str(result)\\n\\n    def _call(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\\n        input_text = f\"{inputs[self.input_key]}\\\\nESQuery:\"\\n        _run_manager.on_text(input_text, verbose=self.verbose)\\n        indices = self._list_indices()\\n        indices_info = self._get_indices_infos(indices)\\n        query_inputs: dict = {\\n            \"input\": input_text,\\n            \"top_k\": str(self.top_k),\\n            \"indices_info\": indices_info,\\n            \"stop\": [\"\\\\nESResult:\"],\\n        }\\n        intermediate_steps: List = []\\n        try:\\n            intermediate_steps.append(query_inputs)  # input: es generation\\n            es_cmd = self.query_chain.run(\\n                callbacks=_run_manager.get_child(),\\n                **query_inputs,\\n            )\\n\\n            _run_manager.on_text(es_cmd, color=\"green\", verbose=self.verbose)\\n            intermediate_steps.append(\\n                es_cmd\\n            )  # output: elasticsearch dsl generation (no checker)\\n            intermediate_steps.append({\"es_cmd\": es_cmd})  # input: ES search\\n            result = self._search(indices=indices, query=es_cmd)\\n            intermediate_steps.append(str(result))  # output: ES search\\n\\n            _run_manager.on_text(\"\\\\nESResult: \", verbose=self.verbose)\\n            _run_manager.on_text(result, color=\"yellow\", verbose=self.verbose)\\n\\n            _run_manager.on_text(\"\\\\nAnswer:\", verbose=self.verbose)\\n            answer_inputs: dict = {\"data\": result, \"input\": input_text}\\n            intermediate_steps.append(answer_inputs)  # input: final answer\\n            final_result = self.answer_chain.run(\\n                callbacks=_run_manager.get_child(),\\n                **answer_inputs,\\n            )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\elasticsearch_database\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='intermediate_steps.append(final_result)  # output: final answer\\n            _run_manager.on_text(final_result, color=\"green\", verbose=self.verbose)\\n            chain_result: Dict[str, Any] = {self.output_key: final_result}\\n            if self.return_intermediate_steps:\\n                chain_result[INTERMEDIATE_STEPS_KEY] = intermediate_steps\\n            return chain_result\\n        except Exception as exc:\\n            # Append intermediate steps to exception, to aid in logging and later\\n            # improvement of few shot prompt seeds\\n            exc.intermediate_steps = intermediate_steps  # type: ignore\\n            raise exc\\n\\n    @property\\n    def _chain_type(self) -> str:\\n        return \"elasticsearch_database_chain\"\\n\\n    @classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        database: Elasticsearch,\\n        *,\\n        query_prompt: Optional[BasePromptTemplate] = None,\\n        answer_prompt: Optional[BasePromptTemplate] = None,\\n        query_output_parser: Optional[BaseLLMOutputParser] = None,\\n        **kwargs: Any,\\n    ) -> ElasticsearchDatabaseChain:\\n        \"\"\"Convenience method to construct ElasticsearchDatabaseChain from an LLM.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\elasticsearch_database\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Args:\\n            llm: The language model to use.\\n            database: The Elasticsearch db.\\n            query_prompt: The prompt to use for query construction.\\n            answer_prompt: The prompt to use for answering user question given data.\\n            query_output_parser: The output parser to use for parsing model-generated\\n                ES query. Defaults to SimpleJsonOutputParser.\\n            **kwargs: Additional arguments to pass to the constructor.\\n        \"\"\"\\n        query_prompt = query_prompt or DSL_PROMPT\\n        query_output_parser = query_output_parser or SimpleJsonOutputParser()\\n        query_chain = LLMChain(\\n            llm=llm, prompt=query_prompt, output_parser=query_output_parser\\n        )\\n        answer_prompt = answer_prompt or ANSWER_PROMPT\\n        answer_chain = LLMChain(llm=llm, prompt=answer_prompt)\\n        return cls(\\n            query_chain=query_chain,\\n            answer_chain=answer_chain,\\n            database=database,\\n            **kwargs,\\n        )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\elasticsearch_database\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# flake8: noqa\\nfrom langchain_core.prompts.prompt import PromptTemplate\\n\\nPROMPT_SUFFIX = \"\"\"Only use the following Elasticsearch indices:\\n{indices_info}\\n\\nQuestion: {input}\\nESQuery:\"\"\"\\n\\nDEFAULT_DSL_TEMPLATE = \"\"\"Given an input question, create a syntactically correct Elasticsearch query to run. Unless the user specifies in their question a specific number of examples they wish to obtain, always limit your query to at most {top_k} results. You can order the results by a relevant column to return the most interesting examples in the database.\\n\\nUnless told to do not query for all the columns from a specific index, only ask for a the few relevant columns given the question.\\n\\nPay attention to use only the column names that you can see in the mapping description. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which index. Return the query as valid json.\\n\\nUse the following format:\\n\\nQuestion: Question here\\nESQuery: Elasticsearch Query formatted as json\\n\"\"\"\\n\\nDSL_PROMPT = PromptTemplate.from_template(DEFAULT_DSL_TEMPLATE + PROMPT_SUFFIX)\\n\\nDEFAULT_ANSWER_TEMPLATE = \"\"\"Given an input question and relevant data from a database, answer the user question.\\n\\nUse the following format:\\n\\nQuestion: Question here\\nData: Relevant data here\\nAnswer: Final answer here\\n\\nQuestion: {input}\\nData: {data}\\nAnswer:\"\"\"\\n\\nANSWER_PROMPT = PromptTemplate.from_template(DEFAULT_ANSWER_TEMPLATE)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\elasticsearch_database\\\\prompts.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain.chains.elasticsearch_database.base import ElasticsearchDatabaseChain\\n\\n__all__ = [\"ElasticsearchDatabaseChain\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\elasticsearch_database\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_python_function_name(function: Callable) -> str:\\n    \"\"\"Get the name of a Python function.\"\"\"\\n    return function.__name__' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\ernie_functions\\\\base.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _parse_python_function_docstring(function: Callable) -> Tuple[str, dict]:\\n    \"\"\"Parse the function and argument descriptions from the docstring of a function.\\n\\n    Assumes the function docstring follows Google Python style guide.\\n    \"\"\"\\n    docstring = inspect.getdoc(function)\\n    if docstring:\\n        docstring_blocks = docstring.split(\"\\\\n\\\\n\")\\n        descriptors = []\\n        args_block = None\\n        past_descriptors = False\\n        for block in docstring_blocks:\\n            if block.startswith(\"Args:\"):\\n                args_block = block\\n                break\\n            elif block.startswith(\"Returns:\") or block.startswith(\"Example:\"):\\n                # Don\\'t break in case Args come after\\n                past_descriptors = True\\n            elif not past_descriptors:\\n                descriptors.append(block)\\n            else:\\n                continue\\n        description = \" \".join(descriptors)\\n    else:\\n        description = \"\"\\n        args_block = None\\n    arg_descriptions = {}\\n    if args_block:\\n        arg = None\\n        for line in args_block.split(\"\\\\n\")[1:]:\\n            if \":\" in line:\\n                arg, desc = line.split(\":\")\\n                arg_descriptions[arg.strip()] = desc.strip()\\n            elif arg:\\n                arg_descriptions[arg.strip()] += \" \" + line.strip()\\n    return description, arg_descriptions' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\ernie_functions\\\\base.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_python_function_arguments(function: Callable, arg_descriptions: dict) -> dict:\\n    \"\"\"Get JsonSchema describing a Python functions arguments.\\n\\n    Assumes all function arguments are of primitive types (int, float, str, bool) or\\n    are subclasses of pydantic.BaseModel.\\n    \"\"\"\\n    properties = {}\\n    annotations = inspect.getfullargspec(function).annotations\\n    for arg, arg_type in annotations.items():\\n        if arg == \"return\":\\n            continue\\n        if isinstance(arg_type, type) and issubclass(arg_type, BaseModel):\\n            # Mypy error:\\n            # \"type\" has no attribute \"schema\"\\n            properties[arg] = arg_type.schema()  # type: ignore[attr-defined]\\n        elif arg_type.__name__ in PYTHON_TO_JSON_TYPES:\\n            properties[arg] = {\"type\": PYTHON_TO_JSON_TYPES[arg_type.__name__]}\\n        if arg in arg_descriptions:\\n            if arg not in properties:\\n                properties[arg] = {}\\n            properties[arg][\"description\"] = arg_descriptions[arg]\\n    return properties' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\ernie_functions\\\\base.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_python_function_required_args(function: Callable) -> List[str]:\\n    \"\"\"Get the required arguments for a Python function.\"\"\"\\n    spec = inspect.getfullargspec(function)\\n    required = spec.args[: -len(spec.defaults)] if spec.defaults else spec.args\\n    required += [k for k in spec.kwonlyargs if k not in (spec.kwonlydefaults or {})]\\n\\n    is_class = type(function) is type\\n    if is_class and required[0] == \"self\":\\n        required = required[1:]\\n    return required' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\ernie_functions\\\\base.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def convert_python_function_to_ernie_function(\\n    function: Callable,\\n) -> Dict[str, Any]:\\n    \"\"\"Convert a Python function to an Ernie function-calling API compatible dict.\\n\\n    Assumes the Python function has type hints and a docstring with a description. If\\n        the docstring has Google Python style argument descriptions, these will be\\n        included as well.\\n    \"\"\"\\n    description, arg_descriptions = _parse_python_function_docstring(function)\\n    return {\\n        \"name\": _get_python_function_name(function),\\n        \"description\": description,\\n        \"parameters\": {\\n            \"type\": \"object\",\\n            \"properties\": _get_python_function_arguments(function, arg_descriptions),\\n            \"required\": _get_python_function_required_args(function),\\n        },\\n    }' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\ernie_functions\\\\base.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def convert_to_ernie_function(\\n    function: Union[Dict[str, Any], Type[BaseModel], Callable],\\n) -> Dict[str, Any]:\\n    \"\"\"Convert a raw function/class to an Ernie function.\\n\\n    Args:\\n        function: Either a dictionary, a pydantic.BaseModel class, or a Python function.\\n            If a dictionary is passed in, it is assumed to already be a valid Ernie\\n            function.\\n\\n    Returns:\\n        A dict version of the passed in function which is compatible with the\\n            Ernie function-calling API.\\n    \"\"\"\\n    if isinstance(function, dict):\\n        return function\\n    elif isinstance(function, type) and issubclass(function, BaseModel):\\n        return cast(Dict, convert_pydantic_to_ernie_function(function))\\n    elif callable(function):\\n        return convert_python_function_to_ernie_function(function)\\n\\n    else:\\n        raise ValueError(\\n            f\"Unsupported function type {type(function)}. Functions must be passed in\"\\n            f\" as Dict, pydantic.BaseModel, or Callable.\"\\n        )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\ernie_functions\\\\base.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def get_ernie_output_parser(\\n    functions: Sequence[Union[Dict[str, Any], Type[BaseModel], Callable]],\\n) -> Union[BaseOutputParser, BaseGenerationOutputParser]:\\n    \"\"\"Get the appropriate function output parser given the user functions.\\n\\n    Args:\\n        functions: Sequence where element is a dictionary, a pydantic.BaseModel class,\\n            or a Python function. If a dictionary is passed in, it is assumed to\\n            already be a valid Ernie function.\\n\\n    Returns:\\n        A PydanticOutputFunctionsParser if functions are Pydantic classes, otherwise\\n            a JsonOutputFunctionsParser. If there\\'s only one function and it is\\n            not a Pydantic class, then the output parser will automatically extract\\n            only the function arguments and not the function name.\\n    \"\"\"\\n    function_names = [convert_to_ernie_function(f)[\"name\"] for f in functions]\\n    if isinstance(functions[0], type) and issubclass(functions[0], BaseModel):\\n        if len(functions) > 1:\\n            pydantic_schema: Union[Dict, Type[BaseModel]] = {\\n                name: fn for name, fn in zip(function_names, functions)\\n            }\\n        else:\\n            pydantic_schema = functions[0]\\n        output_parser: Union[\\n            BaseOutputParser, BaseGenerationOutputParser\\n        ] = PydanticOutputFunctionsParser(pydantic_schema=pydantic_schema)\\n    else:\\n        output_parser = JsonOutputFunctionsParser(args_only=len(functions) <= 1)\\n    return output_parser' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\ernie_functions\\\\base.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def create_ernie_fn_runnable(\\n    functions: Sequence[Union[Dict[str, Any], Type[BaseModel], Callable]],\\n    llm: Runnable,\\n    prompt: BasePromptTemplate,\\n    *,\\n    output_parser: Optional[Union[BaseOutputParser, BaseGenerationOutputParser]] = None,\\n    **kwargs: Any,\\n) -> Runnable:\\n    \"\"\"Create a runnable sequence that uses Ernie functions.\\n\\n    Args:\\n        functions: A sequence of either dictionaries, pydantic.BaseModels classes, or\\n            Python functions. If dictionaries are passed in, they are assumed to\\n            already be a valid Ernie functions. If only a single\\n            function is passed in, then it will be enforced that the model use that\\n            function. pydantic.BaseModels and Python functions should have docstrings\\n            describing what the function does. For best results, pydantic.BaseModels\\n            should have descriptions of the parameters and Python functions should have\\n            Google Python style args descriptions in the docstring. Additionally,\\n            Python functions should only use primitive types (str, int, float, bool) or\\n            pydantic.BaseModels for arguments.\\n        llm: Language model to use, assumed to support the Ernie function-calling API.\\n        prompt: BasePromptTemplate to pass to the model.\\n        output_parser: BaseLLMOutputParser to use for parsing model outputs. By default\\n            will be inferred from the function types. If pydantic.BaseModels are passed\\n            in, then the OutputParser will try to parse outputs using those. Otherwise\\n            model outputs will simply be parsed as JSON. If multiple functions are\\n            passed in and they are not pydantic.BaseModels, the chain output will\\n            include both the name of the function that was returned and the arguments\\n            to pass to the function.\\n\\n    Returns:\\n        A runnable sequence that will pass in the given functions to the model when run.\\n\\n    Example:\\n        .. code-block:: python\\n\\n                from typing import Optional' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\ernie_functions\\\\base.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain.chains.ernie_functions import create_ernie_fn_chain\\n                from langchain_community.chat_models import ErnieBotChat\\n                from langchain.prompts import ChatPromptTemplate\\n                from langchain.pydantic_v1 import BaseModel, Field\\n\\n\\n                class RecordPerson(BaseModel):\\n                    \\\\\"\\\\\"\\\\\"Record some identifying information about a person.\\\\\"\\\\\"\\\\\"\\n\\n                    name: str = Field(..., description=\"The person\\'s name\")\\n                    age: int = Field(..., description=\"The person\\'s age\")\\n                    fav_food: Optional[str] = Field(None, description=\"The person\\'s favorite food\")\\n\\n\\n                class RecordDog(BaseModel):\\n                    \\\\\"\\\\\"\\\\\"Record some identifying information about a dog.\\\\\"\\\\\"\\\\\"\\n\\n                    name: str = Field(..., description=\"The dog\\'s name\")\\n                    color: str = Field(..., description=\"The dog\\'s color\")\\n                    fav_food: Optional[str] = Field(None, description=\"The dog\\'s favorite food\")' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\ernie_functions\\\\base.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='llm = ErnieBotChat(model_name=\"ERNIE-Bot-4\")\\n                prompt = ChatPromptTemplate.from_messages(\\n                    [\\n                        (\"user\", \"Make calls to the relevant function to record the entities in the following input: {input}\"),\\n                        (\"assistant\", \"OK!\"),\\n                        (\"user\", \"Tip: Make sure to answer in the correct format\"),\\n                    ]\\n                )\\n                chain = create_ernie_fn_runnable([RecordPerson, RecordDog], llm, prompt)\\n                chain.invoke({\"input\": \"Harry was a chubby brown beagle who loved chicken\"})\\n                # -> RecordDog(name=\"Harry\", color=\"brown\", fav_food=\"chicken\")\\n    \"\"\"  # noqa: E501\\n    if not functions:\\n        raise ValueError(\"Need to pass in at least one function. Received zero.\")\\n    ernie_functions = [convert_to_ernie_function(f) for f in functions]\\n    llm_kwargs: Dict[str, Any] = {\"functions\": ernie_functions, **kwargs}\\n    if len(ernie_functions) == 1:\\n        llm_kwargs[\"function_call\"] = {\"name\": ernie_functions[0][\"name\"]}\\n    output_parser = output_parser or get_ernie_output_parser(functions)\\n    return prompt | llm.bind(**llm_kwargs) | output_parser' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\ernie_functions\\\\base.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def create_structured_output_runnable(\\n    output_schema: Union[Dict[str, Any], Type[BaseModel]],\\n    llm: Runnable,\\n    prompt: BasePromptTemplate,\\n    *,\\n    output_parser: Optional[Union[BaseOutputParser, BaseGenerationOutputParser]] = None,\\n    **kwargs: Any,\\n) -> Runnable:\\n    \"\"\"Create a runnable that uses an Ernie function to get a structured output.\\n\\n    Args:\\n        output_schema: Either a dictionary or pydantic.BaseModel class. If a dictionary\\n            is passed in, it\\'s assumed to already be a valid JsonSchema.\\n            For best results, pydantic.BaseModels should have docstrings describing what\\n            the schema represents and descriptions for the parameters.\\n        llm: Language model to use, assumed to support the Ernie function-calling API.\\n        prompt: BasePromptTemplate to pass to the model.\\n        output_parser: BaseLLMOutputParser to use for parsing model outputs. By default\\n            will be inferred from the function types. If pydantic.BaseModels are passed\\n            in, then the OutputParser will try to parse outputs using those. Otherwise\\n            model outputs will simply be parsed as JSON.\\n\\n    Returns:\\n        A runnable sequence that will pass the given function to the model when run.\\n\\n    Example:\\n        .. code-block:: python\\n\\n                from typing import Optional\\n\\n                from langchain.chains.ernie_functions import create_structured_output_chain\\n                from langchain_community.chat_models import ErnieBotChat\\n                from langchain.prompts import ChatPromptTemplate\\n                from langchain.pydantic_v1 import BaseModel, Field\\n\\n                class Dog(BaseModel):\\n                    \\\\\"\\\\\"\\\\\"Identifying information about a dog.\\\\\"\\\\\"\\\\\"\\n\\n                    name: str = Field(..., description=\"The dog\\'s name\")\\n                    color: str = Field(..., description=\"The dog\\'s color\")\\n                    fav_food: Optional[str] = Field(None, description=\"The dog\\'s favorite food\")' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\ernie_functions\\\\base.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='llm = ErnieBotChat(model_name=\"ERNIE-Bot-4\")\\n                prompt = ChatPromptTemplate.from_messages(\\n                    [\\n                        (\"user\", \"Use the given format to extract information from the following input: {input}\"),\\n                        (\"assistant\", \"OK!\"),\\n                        (\"user\", \"Tip: Make sure to answer in the correct format\"),\\n                    ]\\n                )\\n                chain = create_structured_output_chain(Dog, llm, prompt)\\n                chain.invoke({\"input\": \"Harry was a chubby brown beagle who loved chicken\"})\\n                # -> Dog(name=\"Harry\", color=\"brown\", fav_food=\"chicken\")\\n    \"\"\"  # noqa: E501\\n    if isinstance(output_schema, dict):\\n        function: Any = {\\n            \"name\": \"output_formatter\",\\n            \"description\": (\\n                \"Output formatter. Should always be used to format your response to the\"\\n                \" user.\"\\n            ),\\n            \"parameters\": output_schema,\\n        }\\n    else:\\n\\n        class _OutputFormatter(BaseModel):\\n            \"\"\"Output formatter. Should always be used to format your response to the user.\"\"\"  # noqa: E501\\n\\n            output: output_schema  # type: ignore\\n\\n        function = _OutputFormatter\\n        output_parser = output_parser or PydanticAttrOutputFunctionsParser(\\n            pydantic_schema=_OutputFormatter, attr_name=\"output\"\\n        )\\n    return create_ernie_fn_runnable(\\n        [function],\\n        llm,\\n        prompt,\\n        output_parser=output_parser,\\n        **kwargs,\\n    )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\ernie_functions\\\\base.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def create_ernie_fn_chain(\\n    functions: Sequence[Union[Dict[str, Any], Type[BaseModel], Callable]],\\n    llm: BaseLanguageModel,\\n    prompt: BasePromptTemplate,\\n    *,\\n    output_key: str = \"function\",\\n    output_parser: Optional[BaseLLMOutputParser] = None,\\n    **kwargs: Any,\\n) -> LLMChain:\\n    \"\"\"[Legacy] Create an LLM chain that uses Ernie functions.\\n\\n    Args:\\n        functions: A sequence of either dictionaries, pydantic.BaseModels classes, or\\n            Python functions. If dictionaries are passed in, they are assumed to\\n            already be a valid Ernie functions. If only a single\\n            function is passed in, then it will be enforced that the model use that\\n            function. pydantic.BaseModels and Python functions should have docstrings\\n            describing what the function does. For best results, pydantic.BaseModels\\n            should have descriptions of the parameters and Python functions should have\\n            Google Python style args descriptions in the docstring. Additionally,\\n            Python functions should only use primitive types (str, int, float, bool) or\\n            pydantic.BaseModels for arguments.\\n        llm: Language model to use, assumed to support the Ernie function-calling API.\\n        prompt: BasePromptTemplate to pass to the model.\\n        output_key: The key to use when returning the output in LLMChain.__call__.\\n        output_parser: BaseLLMOutputParser to use for parsing model outputs. By default\\n            will be inferred from the function types. If pydantic.BaseModels are passed\\n            in, then the OutputParser will try to parse outputs using those. Otherwise\\n            model outputs will simply be parsed as JSON. If multiple functions are\\n            passed in and they are not pydantic.BaseModels, the chain output will\\n            include both the name of the function that was returned and the arguments\\n            to pass to the function.\\n\\n    Returns:\\n        An LLMChain that will pass in the given functions to the model when run.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\ernie_functions\\\\base.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Example:\\n        .. code-block:: python\\n\\n                from typing import Optional\\n\\n                from langchain.chains.ernie_functions import create_ernie_fn_chain\\n                from langchain_community.chat_models import ErnieBotChat\\n                from langchain.prompts import ChatPromptTemplate\\n\\n                from langchain.pydantic_v1 import BaseModel, Field\\n\\n\\n                class RecordPerson(BaseModel):\\n                    \\\\\"\\\\\"\\\\\"Record some identifying information about a person.\\\\\"\\\\\"\\\\\"\\n\\n                    name: str = Field(..., description=\"The person\\'s name\")\\n                    age: int = Field(..., description=\"The person\\'s age\")\\n                    fav_food: Optional[str] = Field(None, description=\"The person\\'s favorite food\")\\n\\n\\n                class RecordDog(BaseModel):\\n                    \\\\\"\\\\\"\\\\\"Record some identifying information about a dog.\\\\\"\\\\\"\\\\\"\\n\\n                    name: str = Field(..., description=\"The dog\\'s name\")\\n                    color: str = Field(..., description=\"The dog\\'s color\")\\n                    fav_food: Optional[str] = Field(None, description=\"The dog\\'s favorite food\")' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\ernie_functions\\\\base.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='llm = ErnieBotChat(model_name=\"ERNIE-Bot-4\")\\n                prompt = ChatPromptTemplate.from_messages(\\n                    [\\n                        (\"user\", \"Make calls to the relevant function to record the entities in the following input: {input}\"),\\n                        (\"assistant\", \"OK!\"),\\n                        (\"user\", \"Tip: Make sure to answer in the correct format\"),\\n                    ]\\n                )\\n                chain = create_ernie_fn_chain([RecordPerson, RecordDog], llm, prompt)\\n                chain.run(\"Harry was a chubby brown beagle who loved chicken\")\\n                # -> RecordDog(name=\"Harry\", color=\"brown\", fav_food=\"chicken\")\\n    \"\"\"  # noqa: E501\\n    if not functions:\\n        raise ValueError(\"Need to pass in at least one function. Received zero.\")\\n    ernie_functions = [convert_to_ernie_function(f) for f in functions]\\n    output_parser = output_parser or get_ernie_output_parser(functions)\\n    llm_kwargs: Dict[str, Any] = {\\n        \"functions\": ernie_functions,\\n    }\\n    if len(ernie_functions) == 1:\\n        llm_kwargs[\"function_call\"] = {\"name\": ernie_functions[0][\"name\"]}\\n    llm_chain = LLMChain(\\n        llm=llm,\\n        prompt=prompt,\\n        output_parser=output_parser,\\n        llm_kwargs=llm_kwargs,\\n        output_key=output_key,\\n        **kwargs,\\n    )\\n    return llm_chain' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\ernie_functions\\\\base.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def create_structured_output_chain(\\n    output_schema: Union[Dict[str, Any], Type[BaseModel]],\\n    llm: BaseLanguageModel,\\n    prompt: BasePromptTemplate,\\n    *,\\n    output_key: str = \"function\",\\n    output_parser: Optional[BaseLLMOutputParser] = None,\\n    **kwargs: Any,\\n) -> LLMChain:\\n    \"\"\"[Legacy] Create an LLMChain that uses an Ernie function to get a structured output.\\n\\n    Args:\\n        output_schema: Either a dictionary or pydantic.BaseModel class. If a dictionary\\n            is passed in, it\\'s assumed to already be a valid JsonSchema.\\n            For best results, pydantic.BaseModels should have docstrings describing what\\n            the schema represents and descriptions for the parameters.\\n        llm: Language model to use, assumed to support the Ernie function-calling API.\\n        prompt: BasePromptTemplate to pass to the model.\\n        output_key: The key to use when returning the output in LLMChain.__call__.\\n        output_parser: BaseLLMOutputParser to use for parsing model outputs. By default\\n            will be inferred from the function types. If pydantic.BaseModels are passed\\n            in, then the OutputParser will try to parse outputs using those. Otherwise\\n            model outputs will simply be parsed as JSON.\\n\\n    Returns:\\n        An LLMChain that will pass the given function to the model.\\n\\n    Example:\\n        .. code-block:: python\\n\\n                from typing import Optional\\n\\n                from langchain.chains.ernie_functions import create_structured_output_chain\\n                from langchain_community.chat_models import ErnieBotChat\\n                from langchain.prompts import ChatPromptTemplate\\n\\n                from langchain.pydantic_v1 import BaseModel, Field\\n\\n                class Dog(BaseModel):\\n                    \\\\\"\\\\\"\\\\\"Identifying information about a dog.\\\\\"\\\\\"\\\\\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\ernie_functions\\\\base.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='name: str = Field(..., description=\"The dog\\'s name\")\\n                    color: str = Field(..., description=\"The dog\\'s color\")\\n                    fav_food: Optional[str] = Field(None, description=\"The dog\\'s favorite food\")\\n\\n                llm = ErnieBotChat(model_name=\"ERNIE-Bot-4\")\\n                prompt = ChatPromptTemplate.from_messages(\\n                    [\\n                        (\"user\", \"Use the given format to extract information from the following input: {input}\"),\\n                        (\"assistant\", \"OK!\"),\\n                        (\"user\", \"Tip: Make sure to answer in the correct format\"),\\n                    ]\\n                )\\n                chain = create_structured_output_chain(Dog, llm, prompt)\\n                chain.run(\"Harry was a chubby brown beagle who loved chicken\")\\n                # -> Dog(name=\"Harry\", color=\"brown\", fav_food=\"chicken\")\\n    \"\"\"  # noqa: E501\\n    if isinstance(output_schema, dict):\\n        function: Any = {\\n            \"name\": \"output_formatter\",\\n            \"description\": (\\n                \"Output formatter. Should always be used to format your response to the\"\\n                \" user.\"\\n            ),\\n            \"parameters\": output_schema,\\n        }\\n    else:\\n\\n        class _OutputFormatter(BaseModel):\\n            \"\"\"Output formatter. Should always be used to format your response to the user.\"\"\"  # noqa: E501\\n\\n            output: output_schema  # type: ignore\\n\\n        function = _OutputFormatter\\n        output_parser = output_parser or PydanticAttrOutputFunctionsParser(\\n            pydantic_schema=_OutputFormatter, attr_name=\"output\"\\n        )\\n    return create_ernie_fn_chain(\\n        [function],\\n        llm,\\n        prompt,\\n        output_key=output_key,\\n        output_parser=output_parser,\\n        **kwargs,\\n    )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\ernie_functions\\\\base.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Methods for creating chains that use Ernie function-calling APIs.\"\"\"\\nimport inspect\\nfrom typing import (\\n    Any,\\n    Callable,\\n    Dict,\\n    List,\\n    Optional,\\n    Sequence,\\n    Tuple,\\n    Type,\\n    Union,\\n    cast,\\n)\\n\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.output_parsers import (\\n    BaseGenerationOutputParser,\\n    BaseLLMOutputParser,\\n    BaseOutputParser,\\n)\\nfrom langchain_core.prompts import BasePromptTemplate\\nfrom langchain_core.pydantic_v1 import BaseModel\\nfrom langchain_core.runnables import Runnable\\n\\nfrom langchain.chains import LLMChain\\nfrom langchain.output_parsers.ernie_functions import (\\n    JsonOutputFunctionsParser,\\n    PydanticAttrOutputFunctionsParser,\\n    PydanticOutputFunctionsParser,\\n)\\nfrom langchain.utils.ernie_functions import convert_pydantic_to_ernie_function\\n\\nPYTHON_TO_JSON_TYPES = {\\n    \"str\": \"string\",\\n    \"int\": \"number\",\\n    \"float\": \"number\",\\n    \"bool\": \"boolean\",\\n}\\n\\n\\n# Code for: def _get_python_function_name(function: Callable) -> str:\\n\\n\\n# Code for: def _parse_python_function_docstring(function: Callable) -> Tuple[str, dict]:\\n\\n\\n# Code for: def _get_python_function_arguments(function: Callable, arg_descriptions: dict) -> dict:\\n\\n\\n# Code for: def _get_python_function_required_args(function: Callable) -> List[str]:\\n\\n\\n# Code for: def convert_python_function_to_ernie_function(\\n\\n\\n# Code for: def convert_to_ernie_function(\\n\\n\\n# Code for: def get_ernie_output_parser(\\n\\n\\n# Code for: def create_ernie_fn_runnable(\\n\\n\\n# Code for: def create_structured_output_runnable(\\n\\n\\n\"\"\" --- Legacy --- \"\"\"\\n\\n\\n# Code for: def create_ernie_fn_chain(\\n\\n\\n# Code for: def create_structured_output_chain(' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\ernie_functions\\\\base.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain.chains.ernie_functions.base import (\\n    convert_to_ernie_function,\\n    create_ernie_fn_chain,\\n    create_ernie_fn_runnable,\\n    create_structured_output_chain,\\n    create_structured_output_runnable,\\n    get_ernie_output_parser,\\n)\\n\\n__all__ = [\\n    \"convert_to_ernie_function\",\\n    \"create_structured_output_chain\",\\n    \"create_ernie_fn_chain\",\\n    \"create_structured_output_runnable\",\\n    \"create_ernie_fn_runnable\",\\n    \"get_ernie_output_parser\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\ernie_functions\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from __future__ import annotations\\n\\nimport re\\nfrom abc import abstractmethod\\nfrom typing import Any, Dict, List, Optional, Sequence, Tuple\\n\\nimport numpy as np\\nfrom langchain_community.llms.openai import OpenAI\\nfrom langchain_core.callbacks import (\\n    CallbackManagerForChainRun,\\n)\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.outputs import Generation\\nfrom langchain_core.prompts import BasePromptTemplate\\nfrom langchain_core.pydantic_v1 import Field\\nfrom langchain_core.retrievers import BaseRetriever\\n\\nfrom langchain.chains.base import Chain\\nfrom langchain.chains.flare.prompts import (\\n    PROMPT,\\n    QUESTION_GENERATOR_PROMPT,\\n    FinishedOutputParser,\\n)\\nfrom langchain.chains.llm import LLMChain\\n\\n\\nclass _ResponseChain(LLMChain):\\n    \"\"\"Base class for chains that generate responses.\"\"\"\\n\\n    prompt: BasePromptTemplate = PROMPT\\n\\n    @classmethod\\n    def is_lc_serializable(cls) -> bool:\\n        return False\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        return self.prompt.input_variables\\n\\n    def generate_tokens_and_log_probs(\\n        self,\\n        _input: Dict[str, Any],\\n        *,\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Tuple[Sequence[str], Sequence[float]]:\\n        llm_result = self.generate([_input], run_manager=run_manager)\\n        return self._extract_tokens_and_log_probs(llm_result.generations[0])\\n\\n    @abstractmethod\\n    def _extract_tokens_and_log_probs(\\n        self, generations: List[Generation]\\n    ) -> Tuple[Sequence[str], Sequence[float]]:\\n        \"\"\"Extract tokens and log probs from response.\"\"\"\\n\\n\\nclass _OpenAIResponseChain(_ResponseChain):\\n    \"\"\"Chain that generates responses from user input and context.\"\"\"\\n\\n    llm: OpenAI = Field(\\n        default_factory=lambda: OpenAI(\\n            max_tokens=32, model_kwargs={\"logprobs\": 1}, temperature=0\\n        )\\n    )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\flare\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _extract_tokens_and_log_probs(\\n        self, generations: List[Generation]\\n    ) -> Tuple[Sequence[str], Sequence[float]]:\\n        tokens = []\\n        log_probs = []\\n        for gen in generations:\\n            if gen.generation_info is None:\\n                raise ValueError\\n            tokens.extend(gen.generation_info[\"logprobs\"][\"tokens\"])\\n            log_probs.extend(gen.generation_info[\"logprobs\"][\"token_logprobs\"])\\n        return tokens, log_probs\\n\\n\\nclass QuestionGeneratorChain(LLMChain):\\n    \"\"\"Chain that generates questions from uncertain spans.\"\"\"\\n\\n    prompt: BasePromptTemplate = QUESTION_GENERATOR_PROMPT\\n    \"\"\"Prompt template for the chain.\"\"\"\\n\\n    @classmethod\\n    def is_lc_serializable(cls) -> bool:\\n        return False\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Input keys for the chain.\"\"\"\\n        return [\"user_input\", \"context\", \"response\"]\\n\\n\\ndef _low_confidence_spans(\\n    tokens: Sequence[str],\\n    log_probs: Sequence[float],\\n    min_prob: float,\\n    min_token_gap: int,\\n    num_pad_tokens: int,\\n) -> List[str]:\\n    _low_idx = np.where(np.exp(log_probs) < min_prob)[0]\\n    low_idx = [i for i in _low_idx if re.search(r\"\\\\w\", tokens[i])]\\n    if len(low_idx) == 0:\\n        return []\\n    spans = [[low_idx[0], low_idx[0] + num_pad_tokens + 1]]\\n    for i, idx in enumerate(low_idx[1:]):\\n        end = idx + num_pad_tokens + 1\\n        if idx - low_idx[i] < min_token_gap:\\n            spans[-1][1] = end\\n        else:\\n            spans.append([idx, end])\\n    return [\"\".join(tokens[start:end]) for start, end in spans]\\n\\n\\nclass FlareChain(Chain):\\n    \"\"\"Chain that combines a retriever, a question generator,\\n    and a response generator.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\flare\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='question_generator_chain: QuestionGeneratorChain\\n    \"\"\"Chain that generates questions from uncertain spans.\"\"\"\\n    response_chain: _ResponseChain = Field(default_factory=_OpenAIResponseChain)\\n    \"\"\"Chain that generates responses from user input and context.\"\"\"\\n    output_parser: FinishedOutputParser = Field(default_factory=FinishedOutputParser)\\n    \"\"\"Parser that determines whether the chain is finished.\"\"\"\\n    retriever: BaseRetriever\\n    \"\"\"Retriever that retrieves relevant documents from a user input.\"\"\"\\n    min_prob: float = 0.2\\n    \"\"\"Minimum probability for a token to be considered low confidence.\"\"\"\\n    min_token_gap: int = 5\\n    \"\"\"Minimum number of tokens between two low confidence spans.\"\"\"\\n    num_pad_tokens: int = 2\\n    \"\"\"Number of tokens to pad around a low confidence span.\"\"\"\\n    max_iter: int = 10\\n    \"\"\"Maximum number of iterations.\"\"\"\\n    start_with_retrieval: bool = True\\n    \"\"\"Whether to start with retrieval.\"\"\"\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Input keys for the chain.\"\"\"\\n        return [\"user_input\"]\\n\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Output keys for the chain.\"\"\"\\n        return [\"response\"]\\n\\n    def _do_generation(\\n        self,\\n        questions: List[str],\\n        user_input: str,\\n        response: str,\\n        _run_manager: CallbackManagerForChainRun,\\n    ) -> Tuple[str, bool]:\\n        callbacks = _run_manager.get_child()\\n        docs = []\\n        for question in questions:\\n            docs.extend(self.retriever.get_relevant_documents(question))\\n        context = \"\\\\n\\\\n\".join(d.page_content for d in docs)\\n        result = self.response_chain.predict(\\n            user_input=user_input,\\n            context=context,\\n            response=response,\\n            callbacks=callbacks,\\n        )\\n        marginal, finished = self.output_parser.parse(result)\\n        return marginal, finished' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\flare\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _do_retrieval(\\n        self,\\n        low_confidence_spans: List[str],\\n        _run_manager: CallbackManagerForChainRun,\\n        user_input: str,\\n        response: str,\\n        initial_response: str,\\n    ) -> Tuple[str, bool]:\\n        question_gen_inputs = [\\n            {\\n                \"user_input\": user_input,\\n                \"current_response\": initial_response,\\n                \"uncertain_span\": span,\\n            }\\n            for span in low_confidence_spans\\n        ]\\n        callbacks = _run_manager.get_child()\\n        question_gen_outputs = self.question_generator_chain.apply(\\n            question_gen_inputs, callbacks=callbacks\\n        )\\n        questions = [\\n            output[self.question_generator_chain.output_keys[0]]\\n            for output in question_gen_outputs\\n        ]\\n        _run_manager.on_text(\\n            f\"Generated Questions: {questions}\", color=\"yellow\", end=\"\\\\n\"\\n        )\\n        return self._do_generation(questions, user_input, response, _run_manager)\\n\\n    def _call(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\\n\\n        user_input = inputs[self.input_keys[0]]\\n\\n        response = \"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\flare\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='for i in range(self.max_iter):\\n            _run_manager.on_text(\\n                f\"Current Response: {response}\", color=\"blue\", end=\"\\\\n\"\\n            )\\n            _input = {\"user_input\": user_input, \"context\": \"\", \"response\": response}\\n            tokens, log_probs = self.response_chain.generate_tokens_and_log_probs(\\n                _input, run_manager=_run_manager\\n            )\\n            low_confidence_spans = _low_confidence_spans(\\n                tokens,\\n                log_probs,\\n                self.min_prob,\\n                self.min_token_gap,\\n                self.num_pad_tokens,\\n            )\\n            initial_response = response.strip() + \" \" + \"\".join(tokens)\\n            if not low_confidence_spans:\\n                response = initial_response\\n                final_response, finished = self.output_parser.parse(response)\\n                if finished:\\n                    return {self.output_keys[0]: final_response}\\n                continue\\n\\n            marginal, finished = self._do_retrieval(\\n                low_confidence_spans,\\n                _run_manager,\\n                user_input,\\n                response,\\n                initial_response,\\n            )\\n            response = response.strip() + \" \" + marginal\\n            if finished:\\n                break\\n        return {self.output_keys[0]: response}\\n\\n    @classmethod\\n    def from_llm(\\n        cls, llm: BaseLanguageModel, max_generation_len: int = 32, **kwargs: Any\\n    ) -> FlareChain:\\n        \"\"\"Creates a FlareChain from a language model.\\n\\n        Args:\\n            llm: Language model to use.\\n            max_generation_len: Maximum length of the generated response.\\n            **kwargs: Additional arguments to pass to the constructor.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\flare\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Returns:\\n            FlareChain class with the given language model.\\n        \"\"\"\\n        question_gen_chain = QuestionGeneratorChain(llm=llm)\\n        response_llm = OpenAI(\\n            max_tokens=max_generation_len, model_kwargs={\"logprobs\": 1}, temperature=0\\n        )\\n        response_chain = _OpenAIResponseChain(llm=response_llm)\\n        return cls(\\n            question_generator_chain=question_gen_chain,\\n            response_chain=response_chain,\\n            **kwargs,\\n        )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\flare\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from typing import Tuple\\n\\nfrom langchain_core.output_parsers import BaseOutputParser\\nfrom langchain_core.prompts import PromptTemplate\\n\\n\\nclass FinishedOutputParser(BaseOutputParser[Tuple[str, bool]]):\\n    \"\"\"Output parser that checks if the output is finished.\"\"\"\\n\\n    finished_value: str = \"FINISHED\"\\n    \"\"\"Value that indicates the output is finished.\"\"\"\\n\\n    def parse(self, text: str) -> Tuple[str, bool]:\\n        cleaned = text.strip()\\n        finished = self.finished_value in cleaned\\n        return cleaned.replace(self.finished_value, \"\"), finished\\n\\n\\nPROMPT_TEMPLATE = \"\"\"\\\\\\nRespond to the user message using any relevant context. \\\\\\nIf context is provided, you should ground your answer in that context. \\\\\\nOnce you\\'re done responding return FINISHED.\\n\\n>>> CONTEXT: {context}\\n>>> USER INPUT: {user_input}\\n>>> RESPONSE: {response}\\\\\\n\"\"\"\\n\\nPROMPT = PromptTemplate(\\n    template=PROMPT_TEMPLATE,\\n    input_variables=[\"user_input\", \"context\", \"response\"],\\n)\\n\\n\\nQUESTION_GENERATOR_PROMPT_TEMPLATE = \"\"\"\\\\\\nGiven a user input and an existing partial response as context, \\\\\\nask a question to which the answer is the given term/entity/phrase:\\n\\n>>> USER INPUT: {user_input}\\n>>> EXISTING PARTIAL RESPONSE: {current_response}\\n\\nThe question to which the answer is the term/entity/phrase \"{uncertain_span}\" is:\"\"\"\\nQUESTION_GENERATOR_PROMPT = PromptTemplate(\\n    template=QUESTION_GENERATOR_PROMPT_TEMPLATE,\\n    input_variables=[\"user_input\", \"current_response\", \"uncertain_span\"],\\n)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\flare\\\\prompts.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Adapted from https://github.com/jzbjyb/FLARE\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\flare\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Question answering over a graph.\"\"\"\\nfrom __future__ import annotations\\n\\nimport re\\nfrom typing import Any, Dict, List, Optional\\n\\nfrom langchain_community.graphs.arangodb_graph import ArangoGraph\\nfrom langchain_core.callbacks import CallbackManagerForChainRun\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.prompts import BasePromptTemplate\\nfrom langchain_core.pydantic_v1 import Field\\n\\nfrom langchain.chains.base import Chain\\nfrom langchain.chains.graph_qa.prompts import (\\n    AQL_FIX_PROMPT,\\n    AQL_GENERATION_PROMPT,\\n    AQL_QA_PROMPT,\\n)\\nfrom langchain.chains.llm import LLMChain\\n\\n\\nclass ArangoGraphQAChain(Chain):\\n    \"\"\"Chain for question-answering against a graph by generating AQL statements.\\n\\n    *Security note*: Make sure that the database connection uses credentials\\n        that are narrowly-scoped to only include necessary permissions.\\n        Failure to do so may result in data corruption or loss, since the calling\\n        code may attempt commands that would result in deletion, mutation\\n        of data if appropriately prompted or reading sensitive data if such\\n        data is present in the database.\\n        The best way to guard against such negative outcomes is to (as appropriate)\\n        limit the permissions granted to the credentials used with this tool.\\n\\n        See https://python.langchain.com/docs/security for more information.\\n    \"\"\"\\n\\n    graph: ArangoGraph = Field(exclude=True)\\n    aql_generation_chain: LLMChain\\n    aql_fix_chain: LLMChain\\n    qa_chain: LLMChain\\n    input_key: str = \"query\"  #: :meta private:\\n    output_key: str = \"result\"  #: :meta private:\\n\\n    # Specifies the maximum number of AQL Query Results to return\\n    top_k: int = 10\\n\\n    # Specifies the set of AQL Query Examples that promote few-shot-learning\\n    aql_examples: str = \"\"\\n\\n    # Specify whether to return the AQL Query in the output dictionary\\n    return_aql_query: bool = False' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\graph_qa\\\\arangodb.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# Specify whether to return the AQL JSON Result in the output dictionary\\n    return_aql_result: bool = False\\n\\n    # Specify the maximum amount of AQL Generation attempts that should be made\\n    max_aql_generation_attempts: int = 3\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        return [self.input_key]\\n\\n    @property\\n    def output_keys(self) -> List[str]:\\n        return [self.output_key]\\n\\n    @property\\n    def _chain_type(self) -> str:\\n        return \"graph_aql_chain\"\\n\\n    @classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        *,\\n        qa_prompt: BasePromptTemplate = AQL_QA_PROMPT,\\n        aql_generation_prompt: BasePromptTemplate = AQL_GENERATION_PROMPT,\\n        aql_fix_prompt: BasePromptTemplate = AQL_FIX_PROMPT,\\n        **kwargs: Any,\\n    ) -> ArangoGraphQAChain:\\n        \"\"\"Initialize from LLM.\"\"\"\\n        qa_chain = LLMChain(llm=llm, prompt=qa_prompt)\\n        aql_generation_chain = LLMChain(llm=llm, prompt=aql_generation_prompt)\\n        aql_fix_chain = LLMChain(llm=llm, prompt=aql_fix_prompt)\\n\\n        return cls(\\n            qa_chain=qa_chain,\\n            aql_generation_chain=aql_generation_chain,\\n            aql_fix_chain=aql_fix_chain,\\n            **kwargs,\\n        )\\n\\n    def _call(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        \"\"\"\\n        Generate an AQL statement from user input, use it retrieve a response\\n        from an ArangoDB Database instance, and respond to the user input\\n        in natural language.\\n\\n        Users can modify the following ArangoGraphQAChain Class Variables:\\n\\n        :var top_k: The maximum number of AQL Query Results to return\\n        :type top_k: int\\n\\n        :var aql_examples: A set of AQL Query Examples that are passed to\\n            the AQL Generation Prompt Template to promote few-shot-learning.\\n            Defaults to an empty string.\\n        :type aql_examples: str' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\graph_qa\\\\arangodb.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content=':var return_aql_query: Whether to return the AQL Query in the\\n            output dictionary. Defaults to False.\\n        :type return_aql_query: bool\\n\\n        :var return_aql_result: Whether to return the AQL Query in the\\n            output dictionary. Defaults to False\\n        :type return_aql_result: bool\\n\\n        :var max_aql_generation_attempts: The maximum amount of AQL\\n            Generation attempts to be made prior to raising the last\\n            AQL Query Execution Error. Defaults to 3.\\n        :type max_aql_generation_attempts: int\\n        \"\"\"\\n        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\\n        callbacks = _run_manager.get_child()\\n        user_input = inputs[self.input_key]\\n\\n        #########################\\n        # Generate AQL Query #\\n        aql_generation_output = self.aql_generation_chain.run(\\n            {\\n                \"adb_schema\": self.graph.schema,\\n                \"aql_examples\": self.aql_examples,\\n                \"user_input\": user_input,\\n            },\\n            callbacks=callbacks,\\n        )\\n        #########################\\n\\n        aql_query = \"\"\\n        aql_error = \"\"\\n        aql_result = None\\n        aql_generation_attempt = 1\\n\\n        while (\\n            aql_result is None\\n            and aql_generation_attempt < self.max_aql_generation_attempts + 1\\n        ):\\n            #####################\\n            # Extract AQL Query #\\n            pattern = r\"```(?i:aql)?(.*?)```\"\\n            matches = re.findall(pattern, aql_generation_output, re.DOTALL)\\n            if not matches:\\n                _run_manager.on_text(\\n                    \"Invalid Response: \", end=\"\\\\n\", verbose=self.verbose\\n                )\\n                _run_manager.on_text(\\n                    aql_generation_output, color=\"red\", end=\"\\\\n\", verbose=self.verbose\\n                )\\n                raise ValueError(f\"Response is Invalid: {aql_generation_output}\")\\n\\n            aql_query = matches[0]\\n            #####################' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\graph_qa\\\\arangodb.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='_run_manager.on_text(\\n                f\"AQL Query ({aql_generation_attempt}):\", verbose=self.verbose\\n            )\\n            _run_manager.on_text(\\n                aql_query, color=\"green\", end=\"\\\\n\", verbose=self.verbose\\n            )\\n\\n            #####################\\n            # Execute AQL Query #\\n            from arango import AQLQueryExecuteError\\n\\n            try:\\n                aql_result = self.graph.query(aql_query, self.top_k)\\n            except AQLQueryExecuteError as e:\\n                aql_error = e.error_message\\n\\n                _run_manager.on_text(\\n                    \"AQL Query Execution Error: \", end=\"\\\\n\", verbose=self.verbose\\n                )\\n                _run_manager.on_text(\\n                    aql_error, color=\"yellow\", end=\"\\\\n\\\\n\", verbose=self.verbose\\n                )\\n\\n                ########################\\n                # Retry AQL Generation #\\n                aql_generation_output = self.aql_fix_chain.run(\\n                    {\\n                        \"adb_schema\": self.graph.schema,\\n                        \"aql_query\": aql_query,\\n                        \"aql_error\": aql_error,\\n                    },\\n                    callbacks=callbacks,\\n                )\\n                ########################\\n\\n            #####################\\n\\n            aql_generation_attempt += 1\\n\\n        if aql_result is None:\\n            m = f\"\"\"\\n                Maximum amount of AQL Query Generation attempts reached.\\n                Unable to execute the AQL Query due to the following error:\\n                {aql_error}\\n            \"\"\"\\n            raise ValueError(m)\\n\\n        _run_manager.on_text(\"AQL Result:\", end=\"\\\\n\", verbose=self.verbose)\\n        _run_manager.on_text(\\n            str(aql_result), color=\"green\", end=\"\\\\n\", verbose=self.verbose\\n        )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\graph_qa\\\\arangodb.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='########################\\n        # Interpret AQL Result #\\n        result = self.qa_chain(\\n            {\\n                \"adb_schema\": self.graph.schema,\\n                \"user_input\": user_input,\\n                \"aql_query\": aql_query,\\n                \"aql_result\": aql_result,\\n            },\\n            callbacks=callbacks,\\n        )\\n        ########################\\n\\n        # Return results #\\n        result = {self.output_key: result[self.qa_chain.output_key]}\\n\\n        if self.return_aql_query:\\n            result[\"aql_query\"] = aql_query\\n\\n        if self.return_aql_result:\\n            result[\"aql_result\"] = aql_result\\n\\n        return result' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\graph_qa\\\\arangodb.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Question answering over a graph.\"\"\"\\nfrom __future__ import annotations\\n\\nfrom typing import Any, Dict, List, Optional\\n\\nfrom langchain_community.graphs.networkx_graph import NetworkxEntityGraph, get_entities\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.prompts import BasePromptTemplate\\nfrom langchain_core.pydantic_v1 import Field\\n\\nfrom langchain.callbacks.manager import CallbackManagerForChainRun\\nfrom langchain.chains.base import Chain\\nfrom langchain.chains.graph_qa.prompts import ENTITY_EXTRACTION_PROMPT, GRAPH_QA_PROMPT\\nfrom langchain.chains.llm import LLMChain\\n\\n\\nclass GraphQAChain(Chain):\\n    \"\"\"Chain for question-answering against a graph.\\n\\n    *Security note*: Make sure that the database connection uses credentials\\n        that are narrowly-scoped to only include necessary permissions.\\n        Failure to do so may result in data corruption or loss, since the calling\\n        code may attempt commands that would result in deletion, mutation\\n        of data if appropriately prompted or reading sensitive data if such\\n        data is present in the database.\\n        The best way to guard against such negative outcomes is to (as appropriate)\\n        limit the permissions granted to the credentials used with this tool.\\n\\n        See https://python.langchain.com/docs/security for more information.\\n    \"\"\"\\n\\n    graph: NetworkxEntityGraph = Field(exclude=True)\\n    entity_extraction_chain: LLMChain\\n    qa_chain: LLMChain\\n    input_key: str = \"query\"  #: :meta private:\\n    output_key: str = \"result\"  #: :meta private:\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Input keys.\\n\\n        :meta private:\\n        \"\"\"\\n        return [self.input_key]\\n\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Output keys.\\n\\n        :meta private:\\n        \"\"\"\\n        _output_keys = [self.output_key]\\n        return _output_keys' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\graph_qa\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        qa_prompt: BasePromptTemplate = GRAPH_QA_PROMPT,\\n        entity_prompt: BasePromptTemplate = ENTITY_EXTRACTION_PROMPT,\\n        **kwargs: Any,\\n    ) -> GraphQAChain:\\n        \"\"\"Initialize from LLM.\"\"\"\\n        qa_chain = LLMChain(llm=llm, prompt=qa_prompt)\\n        entity_chain = LLMChain(llm=llm, prompt=entity_prompt)\\n\\n        return cls(\\n            qa_chain=qa_chain,\\n            entity_extraction_chain=entity_chain,\\n            **kwargs,\\n        )\\n\\n    def _call(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, str]:\\n        \"\"\"Extract entities, look up info and answer question.\"\"\"\\n        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\\n        question = inputs[self.input_key]\\n\\n        entity_string = self.entity_extraction_chain.run(question)\\n\\n        _run_manager.on_text(\"Entities Extracted:\", end=\"\\\\n\", verbose=self.verbose)\\n        _run_manager.on_text(\\n            entity_string, color=\"green\", end=\"\\\\n\", verbose=self.verbose\\n        )\\n        entities = get_entities(entity_string)\\n        context = \"\"\\n        all_triplets = []\\n        for entity in entities:\\n            all_triplets.extend(self.graph.get_entity_knowledge(entity))\\n        context = \"\\\\n\".join(all_triplets)\\n        _run_manager.on_text(\"Full Context:\", end=\"\\\\n\", verbose=self.verbose)\\n        _run_manager.on_text(context, color=\"green\", end=\"\\\\n\", verbose=self.verbose)\\n        result = self.qa_chain(\\n            {\"question\": question, \"context\": context},\\n            callbacks=_run_manager.get_child(),\\n        )\\n        return {self.output_key: result[self.qa_chain.output_key]}' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\graph_qa\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Question answering over a graph.\"\"\"\\nfrom __future__ import annotations\\n\\nimport re\\nfrom typing import Any, Dict, List, Optional\\n\\nfrom langchain_community.graphs.graph_store import GraphStore\\nfrom langchain_core.callbacks import CallbackManagerForChainRun\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.prompts import BasePromptTemplate\\nfrom langchain_core.pydantic_v1 import Field\\n\\nfrom langchain.chains.base import Chain\\nfrom langchain.chains.graph_qa.cypher_utils import CypherQueryCorrector, Schema\\nfrom langchain.chains.graph_qa.prompts import CYPHER_GENERATION_PROMPT, CYPHER_QA_PROMPT\\nfrom langchain.chains.llm import LLMChain\\n\\nINTERMEDIATE_STEPS_KEY = \"intermediate_steps\"\\n\\n\\ndef extract_cypher(text: str) -> str:\\n    \"\"\"Extract Cypher code from a text.\\n\\n    Args:\\n        text: Text to extract Cypher code from.\\n\\n    Returns:\\n        Cypher code extracted from the text.\\n    \"\"\"\\n    # The pattern to find Cypher code enclosed in triple backticks\\n    pattern = r\"```(.*?)```\"\\n\\n    # Find all matches in the input text\\n    matches = re.findall(pattern, text, re.DOTALL)\\n\\n    return matches[0] if matches else text\\n\\n\\ndef construct_schema(\\n    structured_schema: Dict[str, Any],\\n    include_types: List[str],\\n    exclude_types: List[str],\\n) -> str:\\n    \"\"\"Filter the schema based on included or excluded types\"\"\"\\n\\n    def filter_func(x: str) -> bool:\\n        return x in include_types if include_types else x not in exclude_types\\n\\n    filtered_schema: Dict[str, Any] = {\\n        \"node_props\": {\\n            k: v\\n            for k, v in structured_schema.get(\"node_props\", {}).items()\\n            if filter_func(k)\\n        },\\n        \"rel_props\": {\\n            k: v\\n            for k, v in structured_schema.get(\"rel_props\", {}).items()\\n            if filter_func(k)\\n        },\\n        \"relationships\": [\\n            r\\n            for r in structured_schema.get(\"relationships\", [])\\n            if all(filter_func(r[t]) for t in [\"start\", \"end\", \"type\"])\\n        ],\\n    }' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\graph_qa\\\\cypher.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# Format node properties\\n    formatted_node_props = []\\n    for label, properties in filtered_schema[\"node_props\"].items():\\n        props_str = \", \".join(\\n            [f\"{prop[\\'property\\']}: {prop[\\'type\\']}\" for prop in properties]\\n        )\\n        formatted_node_props.append(f\"{label} {{{props_str}}}\")\\n\\n    # Format relationship properties\\n    formatted_rel_props = []\\n    for rel_type, properties in filtered_schema[\"rel_props\"].items():\\n        props_str = \", \".join(\\n            [f\"{prop[\\'property\\']}: {prop[\\'type\\']}\" for prop in properties]\\n        )\\n        formatted_rel_props.append(f\"{rel_type} {{{props_str}}}\")\\n\\n    # Format relationships\\n    formatted_rels = [\\n        f\"(:{el[\\'start\\']})-[:{el[\\'type\\']}]->(:{el[\\'end\\']})\"\\n        for el in filtered_schema[\"relationships\"]\\n    ]\\n\\n    return \"\\\\n\".join(\\n        [\\n            \"Node properties are the following:\",\\n            \",\".join(formatted_node_props),\\n            \"Relationship properties are the following:\",\\n            \",\".join(formatted_rel_props),\\n            \"The relationships are the following:\",\\n            \",\".join(formatted_rels),\\n        ]\\n    )\\n\\n\\nclass GraphCypherQAChain(Chain):\\n    \"\"\"Chain for question-answering against a graph by generating Cypher statements.\\n\\n    *Security note*: Make sure that the database connection uses credentials\\n        that are narrowly-scoped to only include necessary permissions.\\n        Failure to do so may result in data corruption or loss, since the calling\\n        code may attempt commands that would result in deletion, mutation\\n        of data if appropriately prompted or reading sensitive data if such\\n        data is present in the database.\\n        The best way to guard against such negative outcomes is to (as appropriate)\\n        limit the permissions granted to the credentials used with this tool.\\n\\n        See https://python.langchain.com/docs/security for more information.\\n    \"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\graph_qa\\\\cypher.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='graph: GraphStore = Field(exclude=True)\\n    cypher_generation_chain: LLMChain\\n    qa_chain: LLMChain\\n    graph_schema: str\\n    input_key: str = \"query\"  #: :meta private:\\n    output_key: str = \"result\"  #: :meta private:\\n    top_k: int = 10\\n    \"\"\"Number of results to return from the query\"\"\"\\n    return_intermediate_steps: bool = False\\n    \"\"\"Whether or not to return the intermediate steps along with the final answer.\"\"\"\\n    return_direct: bool = False\\n    \"\"\"Whether or not to return the result of querying the graph directly.\"\"\"\\n    cypher_query_corrector: Optional[CypherQueryCorrector] = None\\n    \"\"\"Optional cypher validation tool\"\"\"\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Return the input keys.\\n\\n        :meta private:\\n        \"\"\"\\n        return [self.input_key]\\n\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Return the output keys.\\n\\n        :meta private:\\n        \"\"\"\\n        _output_keys = [self.output_key]\\n        return _output_keys\\n\\n    @property\\n    def _chain_type(self) -> str:\\n        return \"graph_cypher_chain\"\\n\\n    @classmethod\\n    def from_llm(\\n        cls,\\n        llm: Optional[BaseLanguageModel] = None,\\n        *,\\n        qa_prompt: Optional[BasePromptTemplate] = None,\\n        cypher_prompt: Optional[BasePromptTemplate] = None,\\n        cypher_llm: Optional[BaseLanguageModel] = None,\\n        qa_llm: Optional[BaseLanguageModel] = None,\\n        exclude_types: List[str] = [],\\n        include_types: List[str] = [],\\n        validate_cypher: bool = False,\\n        qa_llm_kwargs: Optional[Dict[str, Any]] = None,\\n        cypher_llm_kwargs: Optional[Dict[str, Any]] = None,\\n        **kwargs: Any,\\n    ) -> GraphCypherQAChain:\\n        \"\"\"Initialize from LLM.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\graph_qa\\\\cypher.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='if not cypher_llm and not llm:\\n            raise ValueError(\"Either `llm` or `cypher_llm` parameters must be provided\")\\n        if not qa_llm and not llm:\\n            raise ValueError(\"Either `llm` or `qa_llm` parameters must be provided\")\\n        if cypher_llm and qa_llm and llm:\\n            raise ValueError(\\n                \"You can specify up to two of \\'cypher_llm\\', \\'qa_llm\\'\"\\n                \", and \\'llm\\', but not all three simultaneously.\"\\n            )\\n        if cypher_prompt and cypher_llm_kwargs:\\n            raise ValueError(\\n                \"Specifying cypher_prompt and cypher_llm_kwargs together is\"\\n                \" not allowed. Please pass prompt via cypher_llm_kwargs.\"\\n            )\\n        if qa_prompt and qa_llm_kwargs:\\n            raise ValueError(\\n                \"Specifying qa_prompt and qa_llm_kwargs together is\"\\n                \" not allowed. Please pass prompt via qa_llm_kwargs.\"\\n            )\\n        use_qa_llm_kwargs = qa_llm_kwargs if qa_llm_kwargs is not None else {}\\n        use_cypher_llm_kwargs = (\\n            cypher_llm_kwargs if cypher_llm_kwargs is not None else {}\\n        )\\n        if \"prompt\" not in use_qa_llm_kwargs:\\n            use_qa_llm_kwargs[\"prompt\"] = (\\n                qa_prompt if qa_prompt is not None else CYPHER_QA_PROMPT\\n            )\\n        if \"prompt\" not in use_cypher_llm_kwargs:\\n            use_cypher_llm_kwargs[\"prompt\"] = (\\n                cypher_prompt if cypher_prompt is not None else CYPHER_GENERATION_PROMPT\\n            )\\n\\n        qa_chain = LLMChain(llm=qa_llm or llm, **use_qa_llm_kwargs)\\n\\n        cypher_generation_chain = LLMChain(\\n            llm=cypher_llm or llm, **use_cypher_llm_kwargs\\n        )\\n\\n        if exclude_types and include_types:\\n            raise ValueError(\\n                \"Either `exclude_types` or `include_types` \"\\n                \"can be provided, but not both\"\\n            )\\n\\n        graph_schema = construct_schema(\\n            kwargs[\"graph\"].get_structured_schema, include_types, exclude_types\\n        )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\graph_qa\\\\cypher.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='cypher_query_corrector = None\\n        if validate_cypher:\\n            corrector_schema = [\\n                Schema(el[\"start\"], el[\"type\"], el[\"end\"])\\n                for el in kwargs[\"graph\"].structured_schema.get(\"relationships\")\\n            ]\\n            cypher_query_corrector = CypherQueryCorrector(corrector_schema)\\n\\n        return cls(\\n            graph_schema=graph_schema,\\n            qa_chain=qa_chain,\\n            cypher_generation_chain=cypher_generation_chain,\\n            cypher_query_corrector=cypher_query_corrector,\\n            **kwargs,\\n        )\\n\\n    def _call(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        \"\"\"Generate Cypher statement, use it to look up in db and answer question.\"\"\"\\n        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\\n        callbacks = _run_manager.get_child()\\n        question = inputs[self.input_key]\\n\\n        intermediate_steps: List = []\\n\\n        generated_cypher = self.cypher_generation_chain.run(\\n            {\"question\": question, \"schema\": self.graph_schema}, callbacks=callbacks\\n        )\\n\\n        # Extract Cypher code if it is wrapped in backticks\\n        generated_cypher = extract_cypher(generated_cypher)\\n\\n        # Correct Cypher query if enabled\\n        if self.cypher_query_corrector:\\n            generated_cypher = self.cypher_query_corrector(generated_cypher)\\n\\n        _run_manager.on_text(\"Generated Cypher:\", end=\"\\\\n\", verbose=self.verbose)\\n        _run_manager.on_text(\\n            generated_cypher, color=\"green\", end=\"\\\\n\", verbose=self.verbose\\n        )\\n\\n        intermediate_steps.append({\"query\": generated_cypher})\\n\\n        # Retrieve and limit the number of results\\n        # Generated Cypher be null if query corrector identifies invalid schema\\n        if generated_cypher:\\n            context = self.graph.query(generated_cypher)[: self.top_k]\\n        else:\\n            context = []' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\graph_qa\\\\cypher.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='if self.return_direct:\\n            final_result = context\\n        else:\\n            _run_manager.on_text(\"Full Context:\", end=\"\\\\n\", verbose=self.verbose)\\n            _run_manager.on_text(\\n                str(context), color=\"green\", end=\"\\\\n\", verbose=self.verbose\\n            )\\n\\n            intermediate_steps.append({\"context\": context})\\n\\n            result = self.qa_chain(\\n                {\"question\": question, \"context\": context},\\n                callbacks=callbacks,\\n            )\\n            final_result = result[self.qa_chain.output_key]\\n\\n        chain_result: Dict[str, Any] = {self.output_key: final_result}\\n        if self.return_intermediate_steps:\\n            chain_result[INTERMEDIATE_STEPS_KEY] = intermediate_steps\\n\\n        return chain_result' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\graph_qa\\\\cypher.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='import re\\nfrom collections import namedtuple\\nfrom typing import Any, Dict, List, Optional, Tuple\\n\\nSchema = namedtuple(\"Schema\", [\"left_node\", \"relation\", \"right_node\"])\\n\\n\\nclass CypherQueryCorrector:\\n    \"\"\"\\n    Used to correct relationship direction in generated Cypher statements.\\n    This code is copied from the winner\\'s submission to the Cypher competition:\\n    https://github.com/sakusaku-rich/cypher-direction-competition\\n    \"\"\"\\n\\n    property_pattern = re.compile(r\"\\\\{.+?\\\\}\")\\n    node_pattern = re.compile(r\"\\\\(.+?\\\\)\")\\n    path_pattern = re.compile(\\n        r\"(\\\\([^\\\\,\\\\(\\\\)]*?(\\\\{.+\\\\})?[^\\\\,\\\\(\\\\)]*?\\\\))(<?-)(\\\\[.*?\\\\])?(->?)(\\\\([^\\\\,\\\\(\\\\)]*?(\\\\{.+\\\\})?[^\\\\,\\\\(\\\\)]*?\\\\))\"\\n    )\\n    node_relation_node_pattern = re.compile(\\n        r\"(\\\\()+(?P<left_node>[^()]*?)\\\\)(?P<relation>.*?)\\\\((?P<right_node>[^()]*?)(\\\\))+\"\\n    )\\n    relation_type_pattern = re.compile(r\":(?P<relation_type>.+?)?(\\\\{.+\\\\})?]\")\\n\\n    def __init__(self, schemas: List[Schema]):\\n        \"\"\"\\n        Args:\\n            schemas: list of schemas\\n        \"\"\"\\n        self.schemas = schemas\\n\\n    def clean_node(self, node: str) -> str:\\n        \"\"\"\\n        Args:\\n            node: node in string format\\n\\n        \"\"\"\\n        node = re.sub(self.property_pattern, \"\", node)\\n        node = node.replace(\"(\", \"\")\\n        node = node.replace(\")\", \"\")\\n        node = node.strip()\\n        return node\\n\\n    def detect_node_variables(self, query: str) -> Dict[str, List[str]]:\\n        \"\"\"\\n        Args:\\n            query: cypher query\\n        \"\"\"\\n        nodes = re.findall(self.node_pattern, query)\\n        nodes = [self.clean_node(node) for node in nodes]\\n        res: Dict[str, Any] = {}\\n        for node in nodes:\\n            parts = node.split(\":\")\\n            if parts == \"\":\\n                continue\\n            variable = parts[0]\\n            if variable not in res:\\n                res[variable] = []\\n            res[variable] += parts[1:]\\n        return res' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\graph_qa\\\\cypher_utils.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def extract_paths(self, query: str) -> \"List[str]\":\\n        \"\"\"\\n        Args:\\n            query: cypher query\\n        \"\"\"\\n        paths = []\\n        idx = 0\\n        while matched := self.path_pattern.findall(query[idx:]):\\n            matched = matched[0]\\n            matched = [\\n                m for i, m in enumerate(matched) if i not in [1, len(matched) - 1]\\n            ]\\n            path = \"\".join(matched)\\n            idx = query.find(path) + len(path) - len(matched[-1])\\n            paths.append(path)\\n        return paths\\n\\n    def judge_direction(self, relation: str) -> str:\\n        \"\"\"\\n        Args:\\n            relation: relation in string format\\n        \"\"\"\\n        direction = \"BIDIRECTIONAL\"\\n        if relation[0] == \"<\":\\n            direction = \"INCOMING\"\\n        if relation[-1] == \">\":\\n            direction = \"OUTGOING\"\\n        return direction\\n\\n    def extract_node_variable(self, part: str) -> Optional[str]:\\n        \"\"\"\\n        Args:\\n            part: node in string format\\n        \"\"\"\\n        part = part.lstrip(\"(\").rstrip(\")\")\\n        idx = part.find(\":\")\\n        if idx != -1:\\n            part = part[:idx]\\n        return None if part == \"\" else part\\n\\n    def detect_labels(\\n        self, str_node: str, node_variable_dict: Dict[str, Any]\\n    ) -> List[str]:\\n        \"\"\"\\n        Args:\\n            str_node: node in string format\\n            node_variable_dict: dictionary of node variables\\n        \"\"\"\\n        splitted_node = str_node.split(\":\")\\n        variable = splitted_node[0]\\n        labels = []\\n        if variable in node_variable_dict:\\n            labels = node_variable_dict[variable]\\n        elif variable == \"\" and len(splitted_node) > 1:\\n            labels = splitted_node[1:]\\n        return labels' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\graph_qa\\\\cypher_utils.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def verify_schema(\\n        self,\\n        from_node_labels: List[str],\\n        relation_types: List[str],\\n        to_node_labels: List[str],\\n    ) -> bool:\\n        \"\"\"\\n        Args:\\n            from_node_labels: labels of the from node\\n            relation_type: type of the relation\\n            to_node_labels: labels of the to node\\n        \"\"\"\\n        valid_schemas = self.schemas\\n        if from_node_labels != []:\\n            from_node_labels = [label.strip(\"`\") for label in from_node_labels]\\n            valid_schemas = [\\n                schema for schema in valid_schemas if schema[0] in from_node_labels\\n            ]\\n        if to_node_labels != []:\\n            to_node_labels = [label.strip(\"`\") for label in to_node_labels]\\n            valid_schemas = [\\n                schema for schema in valid_schemas if schema[2] in to_node_labels\\n            ]\\n        if relation_types != []:\\n            relation_types = [type.strip(\"`\") for type in relation_types]\\n            valid_schemas = [\\n                schema for schema in valid_schemas if schema[1] in relation_types\\n            ]\\n        return valid_schemas != []\\n\\n    def detect_relation_types(self, str_relation: str) -> Tuple[str, List[str]]:\\n        \"\"\"\\n        Args:\\n            str_relation: relation in string format\\n        \"\"\"\\n        relation_direction = self.judge_direction(str_relation)\\n        relation_type = self.relation_type_pattern.search(str_relation)\\n        if relation_type is None or relation_type.group(\"relation_type\") is None:\\n            return relation_direction, []\\n        relation_types = [\\n            t.strip().strip(\"!\")\\n            for t in relation_type.group(\"relation_type\").split(\"|\")\\n        ]\\n        return relation_direction, relation_types' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\graph_qa\\\\cypher_utils.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def correct_query(self, query: str) -> str:\\n        \"\"\"\\n        Args:\\n            query: cypher query\\n        \"\"\"\\n        node_variable_dict = self.detect_node_variables(query)\\n        paths = self.extract_paths(query)\\n        for path in paths:\\n            original_path = path\\n            start_idx = 0\\n            while start_idx < len(path):\\n                match_res = re.match(self.node_relation_node_pattern, path[start_idx:])\\n                if match_res is None:\\n                    break\\n                start_idx += match_res.start()\\n                match_dict = match_res.groupdict()\\n                left_node_labels = self.detect_labels(\\n                    match_dict[\"left_node\"], node_variable_dict\\n                )\\n                right_node_labels = self.detect_labels(\\n                    match_dict[\"right_node\"], node_variable_dict\\n                )\\n                end_idx = (\\n                    start_idx\\n                    + 4\\n                    + len(match_dict[\"left_node\"])\\n                    + len(match_dict[\"relation\"])\\n                    + len(match_dict[\"right_node\"])\\n                )\\n                original_partial_path = original_path[start_idx : end_idx + 1]\\n                relation_direction, relation_types = self.detect_relation_types(\\n                    match_dict[\"relation\"]\\n                )\\n\\n                if relation_types != [] and \"\".join(relation_types).find(\"*\") != -1:\\n                    start_idx += (\\n                        len(match_dict[\"left_node\"]) + len(match_dict[\"relation\"]) + 2\\n                    )\\n                    continue' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\graph_qa\\\\cypher_utils.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='if relation_direction == \"OUTGOING\":\\n                    is_legal = self.verify_schema(\\n                        left_node_labels, relation_types, right_node_labels\\n                    )\\n                    if not is_legal:\\n                        is_legal = self.verify_schema(\\n                            right_node_labels, relation_types, left_node_labels\\n                        )\\n                        if is_legal:\\n                            corrected_relation = \"<\" + match_dict[\"relation\"][:-1]\\n                            corrected_partial_path = original_partial_path.replace(\\n                                match_dict[\"relation\"], corrected_relation\\n                            )\\n                            query = query.replace(\\n                                original_partial_path, corrected_partial_path\\n                            )\\n                        else:\\n                            return \"\"\\n                elif relation_direction == \"INCOMING\":\\n                    is_legal = self.verify_schema(\\n                        right_node_labels, relation_types, left_node_labels\\n                    )\\n                    if not is_legal:\\n                        is_legal = self.verify_schema(\\n                            left_node_labels, relation_types, right_node_labels\\n                        )\\n                        if is_legal:\\n                            corrected_relation = match_dict[\"relation\"][1:] + \">\"\\n                            corrected_partial_path = original_partial_path.replace(\\n                                match_dict[\"relation\"], corrected_relation\\n                            )\\n                            query = query.replace(\\n                                original_partial_path, corrected_partial_path\\n                            )\\n                        else:\\n                            return \"\"\\n                else:\\n                    is_legal = self.verify_schema(\\n                        left_node_labels, relation_types, right_node_labels\\n                    )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\graph_qa\\\\cypher_utils.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='is_legal |= self.verify_schema(\\n                        right_node_labels, relation_types, left_node_labels\\n                    )\\n                    if not is_legal:\\n                        return \"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\graph_qa\\\\cypher_utils.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='start_idx += (\\n                    len(match_dict[\"left_node\"]) + len(match_dict[\"relation\"]) + 2\\n                )\\n        return query\\n\\n    def __call__(self, query: str) -> str:\\n        \"\"\"Correct the query to make it valid. If\\n        Args:\\n            query: cypher query\\n        \"\"\"\\n        return self.correct_query(query)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\graph_qa\\\\cypher_utils.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Question answering over a graph.\"\"\"\\nfrom __future__ import annotations\\n\\nimport re\\nfrom typing import Any, Dict, List, Optional\\n\\nfrom langchain_community.graphs import FalkorDBGraph\\nfrom langchain_core.callbacks import CallbackManagerForChainRun\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.prompts import BasePromptTemplate\\nfrom langchain_core.pydantic_v1 import Field\\n\\nfrom langchain.chains.base import Chain\\nfrom langchain.chains.graph_qa.prompts import CYPHER_GENERATION_PROMPT, CYPHER_QA_PROMPT\\nfrom langchain.chains.llm import LLMChain\\n\\nINTERMEDIATE_STEPS_KEY = \"intermediate_steps\"\\n\\n\\ndef extract_cypher(text: str) -> str:\\n    \"\"\"\\n    Extract Cypher code from a text.\\n    Args:\\n        text: Text to extract Cypher code from.\\n\\n    Returns:\\n        Cypher code extracted from the text.\\n    \"\"\"\\n    # The pattern to find Cypher code enclosed in triple backticks\\n    pattern = r\"```(.*?)```\"\\n\\n    # Find all matches in the input text\\n    matches = re.findall(pattern, text, re.DOTALL)\\n\\n    return matches[0] if matches else text\\n\\n\\nclass FalkorDBQAChain(Chain):\\n    \"\"\"Chain for question-answering against a graph by generating Cypher statements.\\n\\n    *Security note*: Make sure that the database connection uses credentials\\n        that are narrowly-scoped to only include necessary permissions.\\n        Failure to do so may result in data corruption or loss, since the calling\\n        code may attempt commands that would result in deletion, mutation\\n        of data if appropriately prompted or reading sensitive data if such\\n        data is present in the database.\\n        The best way to guard against such negative outcomes is to (as appropriate)\\n        limit the permissions granted to the credentials used with this tool.\\n\\n        See https://python.langchain.com/docs/security for more information.\\n    \"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\graph_qa\\\\falkordb.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='graph: FalkorDBGraph = Field(exclude=True)\\n    cypher_generation_chain: LLMChain\\n    qa_chain: LLMChain\\n    input_key: str = \"query\"  #: :meta private:\\n    output_key: str = \"result\"  #: :meta private:\\n    top_k: int = 10\\n    \"\"\"Number of results to return from the query\"\"\"\\n    return_intermediate_steps: bool = False\\n    \"\"\"Whether or not to return the intermediate steps along with the final answer.\"\"\"\\n    return_direct: bool = False\\n    \"\"\"Whether or not to return the result of querying the graph directly.\"\"\"\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Return the input keys.\\n\\n        :meta private:\\n        \"\"\"\\n        return [self.input_key]\\n\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Return the output keys.\\n\\n        :meta private:\\n        \"\"\"\\n        _output_keys = [self.output_key]\\n        return _output_keys\\n\\n    @property\\n    def _chain_type(self) -> str:\\n        return \"graph_cypher_chain\"\\n\\n    @classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        *,\\n        qa_prompt: BasePromptTemplate = CYPHER_QA_PROMPT,\\n        cypher_prompt: BasePromptTemplate = CYPHER_GENERATION_PROMPT,\\n        **kwargs: Any,\\n    ) -> FalkorDBQAChain:\\n        \"\"\"Initialize from LLM.\"\"\"\\n        qa_chain = LLMChain(llm=llm, prompt=qa_prompt)\\n        cypher_generation_chain = LLMChain(llm=llm, prompt=cypher_prompt)\\n\\n        return cls(\\n            qa_chain=qa_chain,\\n            cypher_generation_chain=cypher_generation_chain,\\n            **kwargs,\\n        )\\n\\n    def _call(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        \"\"\"Generate Cypher statement, use it to look up in db and answer question.\"\"\"\\n        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\\n        callbacks = _run_manager.get_child()\\n        question = inputs[self.input_key]\\n\\n        intermediate_steps: List = []' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\graph_qa\\\\falkordb.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='generated_cypher = self.cypher_generation_chain.run(\\n            {\"question\": question, \"schema\": self.graph.schema}, callbacks=callbacks\\n        )\\n\\n        # Extract Cypher code if it is wrapped in backticks\\n        generated_cypher = extract_cypher(generated_cypher)\\n\\n        _run_manager.on_text(\"Generated Cypher:\", end=\"\\\\n\", verbose=self.verbose)\\n        _run_manager.on_text(\\n            generated_cypher, color=\"green\", end=\"\\\\n\", verbose=self.verbose\\n        )\\n\\n        intermediate_steps.append({\"query\": generated_cypher})\\n\\n        # Retrieve and limit the number of results\\n        context = self.graph.query(generated_cypher)[: self.top_k]\\n\\n        if self.return_direct:\\n            final_result = context\\n        else:\\n            _run_manager.on_text(\"Full Context:\", end=\"\\\\n\", verbose=self.verbose)\\n            _run_manager.on_text(\\n                str(context), color=\"green\", end=\"\\\\n\", verbose=self.verbose\\n            )\\n\\n            intermediate_steps.append({\"context\": context})\\n\\n            result = self.qa_chain(\\n                {\"question\": question, \"context\": context},\\n                callbacks=callbacks,\\n            )\\n            final_result = result[self.qa_chain.output_key]\\n\\n        chain_result: Dict[str, Any] = {self.output_key: final_result}\\n        if self.return_intermediate_steps:\\n            chain_result[INTERMEDIATE_STEPS_KEY] = intermediate_steps\\n\\n        return chain_result' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\graph_qa\\\\falkordb.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Question answering over a graph.\"\"\"\\nfrom __future__ import annotations\\n\\nfrom typing import Any, Dict, List, Optional\\n\\nfrom langchain_community.graphs.hugegraph import HugeGraph\\nfrom langchain_core.callbacks import CallbackManagerForChainRun\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.prompts import BasePromptTemplate\\nfrom langchain_core.pydantic_v1 import Field\\n\\nfrom langchain.chains.base import Chain\\nfrom langchain.chains.graph_qa.prompts import (\\n    CYPHER_QA_PROMPT,\\n    GREMLIN_GENERATION_PROMPT,\\n)\\nfrom langchain.chains.llm import LLMChain\\n\\n\\nclass HugeGraphQAChain(Chain):\\n    \"\"\"Chain for question-answering against a graph by generating gremlin statements.\\n\\n    *Security note*: Make sure that the database connection uses credentials\\n        that are narrowly-scoped to only include necessary permissions.\\n        Failure to do so may result in data corruption or loss, since the calling\\n        code may attempt commands that would result in deletion, mutation\\n        of data if appropriately prompted or reading sensitive data if such\\n        data is present in the database.\\n        The best way to guard against such negative outcomes is to (as appropriate)\\n        limit the permissions granted to the credentials used with this tool.\\n\\n        See https://python.langchain.com/docs/security for more information.\\n    \"\"\"\\n\\n    graph: HugeGraph = Field(exclude=True)\\n    gremlin_generation_chain: LLMChain\\n    qa_chain: LLMChain\\n    input_key: str = \"query\"  #: :meta private:\\n    output_key: str = \"result\"  #: :meta private:\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Input keys.\\n\\n        :meta private:\\n        \"\"\"\\n        return [self.input_key]\\n\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Output keys.\\n\\n        :meta private:\\n        \"\"\"\\n        _output_keys = [self.output_key]\\n        return _output_keys' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\graph_qa\\\\hugegraph.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        *,\\n        qa_prompt: BasePromptTemplate = CYPHER_QA_PROMPT,\\n        gremlin_prompt: BasePromptTemplate = GREMLIN_GENERATION_PROMPT,\\n        **kwargs: Any,\\n    ) -> HugeGraphQAChain:\\n        \"\"\"Initialize from LLM.\"\"\"\\n        qa_chain = LLMChain(llm=llm, prompt=qa_prompt)\\n        gremlin_generation_chain = LLMChain(llm=llm, prompt=gremlin_prompt)\\n\\n        return cls(\\n            qa_chain=qa_chain,\\n            gremlin_generation_chain=gremlin_generation_chain,\\n            **kwargs,\\n        )\\n\\n    def _call(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, str]:\\n        \"\"\"Generate gremlin statement, use it to look up in db and answer question.\"\"\"\\n        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\\n        callbacks = _run_manager.get_child()\\n        question = inputs[self.input_key]\\n\\n        generated_gremlin = self.gremlin_generation_chain.run(\\n            {\"question\": question, \"schema\": self.graph.get_schema}, callbacks=callbacks\\n        )\\n\\n        _run_manager.on_text(\"Generated gremlin:\", end=\"\\\\n\", verbose=self.verbose)\\n        _run_manager.on_text(\\n            generated_gremlin, color=\"green\", end=\"\\\\n\", verbose=self.verbose\\n        )\\n        context = self.graph.query(generated_gremlin)\\n\\n        _run_manager.on_text(\"Full Context:\", end=\"\\\\n\", verbose=self.verbose)\\n        _run_manager.on_text(\\n            str(context), color=\"green\", end=\"\\\\n\", verbose=self.verbose\\n        )\\n\\n        result = self.qa_chain(\\n            {\"question\": question, \"context\": context},\\n            callbacks=callbacks,\\n        )\\n        return {self.output_key: result[self.qa_chain.output_key]}' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\graph_qa\\\\hugegraph.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Question answering over a graph.\"\"\"\\nfrom __future__ import annotations\\n\\nfrom typing import Any, Dict, List, Optional\\n\\nfrom langchain_community.graphs.kuzu_graph import KuzuGraph\\nfrom langchain_core.callbacks import CallbackManagerForChainRun\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.prompts import BasePromptTemplate\\nfrom langchain_core.pydantic_v1 import Field\\n\\nfrom langchain.chains.base import Chain\\nfrom langchain.chains.graph_qa.prompts import CYPHER_QA_PROMPT, KUZU_GENERATION_PROMPT\\nfrom langchain.chains.llm import LLMChain\\n\\n\\nclass KuzuQAChain(Chain):\\n    \"\"\"Question-answering against a graph by generating Cypher statements for Kùzu.\\n\\n    *Security note*: Make sure that the database connection uses credentials\\n        that are narrowly-scoped to only include necessary permissions.\\n        Failure to do so may result in data corruption or loss, since the calling\\n        code may attempt commands that would result in deletion, mutation\\n        of data if appropriately prompted or reading sensitive data if such\\n        data is present in the database.\\n        The best way to guard against such negative outcomes is to (as appropriate)\\n        limit the permissions granted to the credentials used with this tool.\\n\\n        See https://python.langchain.com/docs/security for more information.\\n    \"\"\"\\n\\n    graph: KuzuGraph = Field(exclude=True)\\n    cypher_generation_chain: LLMChain\\n    qa_chain: LLMChain\\n    input_key: str = \"query\"  #: :meta private:\\n    output_key: str = \"result\"  #: :meta private:\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Return the input keys.\\n\\n        :meta private:\\n        \"\"\"\\n        return [self.input_key]\\n\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Return the output keys.\\n\\n        :meta private:\\n        \"\"\"\\n        _output_keys = [self.output_key]\\n        return _output_keys' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\graph_qa\\\\kuzu.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        *,\\n        qa_prompt: BasePromptTemplate = CYPHER_QA_PROMPT,\\n        cypher_prompt: BasePromptTemplate = KUZU_GENERATION_PROMPT,\\n        **kwargs: Any,\\n    ) -> KuzuQAChain:\\n        \"\"\"Initialize from LLM.\"\"\"\\n        qa_chain = LLMChain(llm=llm, prompt=qa_prompt)\\n        cypher_generation_chain = LLMChain(llm=llm, prompt=cypher_prompt)\\n\\n        return cls(\\n            qa_chain=qa_chain,\\n            cypher_generation_chain=cypher_generation_chain,\\n            **kwargs,\\n        )\\n\\n    def _call(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, str]:\\n        \"\"\"Generate Cypher statement, use it to look up in db and answer question.\"\"\"\\n        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\\n        callbacks = _run_manager.get_child()\\n        question = inputs[self.input_key]\\n\\n        generated_cypher = self.cypher_generation_chain.run(\\n            {\"question\": question, \"schema\": self.graph.get_schema}, callbacks=callbacks\\n        )\\n\\n        _run_manager.on_text(\"Generated Cypher:\", end=\"\\\\n\", verbose=self.verbose)\\n        _run_manager.on_text(\\n            generated_cypher, color=\"green\", end=\"\\\\n\", verbose=self.verbose\\n        )\\n        context = self.graph.query(generated_cypher)\\n\\n        _run_manager.on_text(\"Full Context:\", end=\"\\\\n\", verbose=self.verbose)\\n        _run_manager.on_text(\\n            str(context), color=\"green\", end=\"\\\\n\", verbose=self.verbose\\n        )\\n\\n        result = self.qa_chain(\\n            {\"question\": question, \"context\": context},\\n            callbacks=callbacks,\\n        )\\n        return {self.output_key: result[self.qa_chain.output_key]}' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\graph_qa\\\\kuzu.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Question answering over a graph.\"\"\"\\nfrom __future__ import annotations\\n\\nfrom typing import Any, Dict, List, Optional\\n\\nfrom langchain_community.graphs.nebula_graph import NebulaGraph\\nfrom langchain_core.callbacks import CallbackManagerForChainRun\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.prompts import BasePromptTemplate\\nfrom langchain_core.pydantic_v1 import Field\\n\\nfrom langchain.chains.base import Chain\\nfrom langchain.chains.graph_qa.prompts import CYPHER_QA_PROMPT, NGQL_GENERATION_PROMPT\\nfrom langchain.chains.llm import LLMChain\\n\\n\\nclass NebulaGraphQAChain(Chain):\\n    \"\"\"Chain for question-answering against a graph by generating nGQL statements.\\n\\n    *Security note*: Make sure that the database connection uses credentials\\n        that are narrowly-scoped to only include necessary permissions.\\n        Failure to do so may result in data corruption or loss, since the calling\\n        code may attempt commands that would result in deletion, mutation\\n        of data if appropriately prompted or reading sensitive data if such\\n        data is present in the database.\\n        The best way to guard against such negative outcomes is to (as appropriate)\\n        limit the permissions granted to the credentials used with this tool.\\n\\n        See https://python.langchain.com/docs/security for more information.\\n    \"\"\"\\n\\n    graph: NebulaGraph = Field(exclude=True)\\n    ngql_generation_chain: LLMChain\\n    qa_chain: LLMChain\\n    input_key: str = \"query\"  #: :meta private:\\n    output_key: str = \"result\"  #: :meta private:\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Return the input keys.\\n\\n        :meta private:\\n        \"\"\"\\n        return [self.input_key]\\n\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Return the output keys.\\n\\n        :meta private:\\n        \"\"\"\\n        _output_keys = [self.output_key]\\n        return _output_keys' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\graph_qa\\\\nebulagraph.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        *,\\n        qa_prompt: BasePromptTemplate = CYPHER_QA_PROMPT,\\n        ngql_prompt: BasePromptTemplate = NGQL_GENERATION_PROMPT,\\n        **kwargs: Any,\\n    ) -> NebulaGraphQAChain:\\n        \"\"\"Initialize from LLM.\"\"\"\\n        qa_chain = LLMChain(llm=llm, prompt=qa_prompt)\\n        ngql_generation_chain = LLMChain(llm=llm, prompt=ngql_prompt)\\n\\n        return cls(\\n            qa_chain=qa_chain,\\n            ngql_generation_chain=ngql_generation_chain,\\n            **kwargs,\\n        )\\n\\n    def _call(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, str]:\\n        \"\"\"Generate nGQL statement, use it to look up in db and answer question.\"\"\"\\n        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\\n        callbacks = _run_manager.get_child()\\n        question = inputs[self.input_key]\\n\\n        generated_ngql = self.ngql_generation_chain.run(\\n            {\"question\": question, \"schema\": self.graph.get_schema}, callbacks=callbacks\\n        )\\n\\n        _run_manager.on_text(\"Generated nGQL:\", end=\"\\\\n\", verbose=self.verbose)\\n        _run_manager.on_text(\\n            generated_ngql, color=\"green\", end=\"\\\\n\", verbose=self.verbose\\n        )\\n        context = self.graph.query(generated_ngql)\\n\\n        _run_manager.on_text(\"Full Context:\", end=\"\\\\n\", verbose=self.verbose)\\n        _run_manager.on_text(\\n            str(context), color=\"green\", end=\"\\\\n\", verbose=self.verbose\\n        )\\n\\n        result = self.qa_chain(\\n            {\"question\": question, \"context\": context},\\n            callbacks=callbacks,\\n        )\\n        return {self.output_key: result[self.qa_chain.output_key]}' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\graph_qa\\\\nebulagraph.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from __future__ import annotations\\n\\nimport re\\nfrom typing import Any, Dict, List, Optional\\n\\nfrom langchain_community.graphs import NeptuneGraph\\nfrom langchain_core.callbacks import CallbackManagerForChainRun\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.prompts.base import BasePromptTemplate\\nfrom langchain_core.pydantic_v1 import Field\\n\\nfrom langchain.chains.base import Chain\\nfrom langchain.chains.graph_qa.prompts import (\\n    CYPHER_QA_PROMPT,\\n    NEPTUNE_OPENCYPHER_GENERATION_PROMPT,\\n    NEPTUNE_OPENCYPHER_GENERATION_SIMPLE_PROMPT,\\n)\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.chains.prompt_selector import ConditionalPromptSelector\\n\\nINTERMEDIATE_STEPS_KEY = \"intermediate_steps\"\\n\\n\\ndef trim_query(query: str) -> str:\\n    \"\"\"Trim the query to only include Cypher keywords.\"\"\"\\n    keywords = (\\n        \"CALL\",\\n        \"CREATE\",\\n        \"DELETE\",\\n        \"DETACH\",\\n        \"LIMIT\",\\n        \"MATCH\",\\n        \"MERGE\",\\n        \"OPTIONAL\",\\n        \"ORDER\",\\n        \"REMOVE\",\\n        \"RETURN\",\\n        \"SET\",\\n        \"SKIP\",\\n        \"UNWIND\",\\n        \"WITH\",\\n        \"WHERE\",\\n        \"//\",\\n    )\\n\\n    lines = query.split(\"\\\\n\")\\n    new_query = \"\"\\n\\n    for line in lines:\\n        if line.strip().upper().startswith(keywords):\\n            new_query += line + \"\\\\n\"\\n\\n    return new_query\\n\\n\\ndef extract_cypher(text: str) -> str:\\n    \"\"\"Extract Cypher code from text using Regex.\"\"\"\\n    # The pattern to find Cypher code enclosed in triple backticks\\n    pattern = r\"```(.*?)```\"\\n\\n    # Find all matches in the input text\\n    matches = re.findall(pattern, text, re.DOTALL)\\n\\n    return matches[0] if matches else text\\n\\n\\ndef use_simple_prompt(llm: BaseLanguageModel) -> bool:\\n    \"\"\"Decides whether to use the simple prompt\"\"\"\\n    if llm._llm_type and \"anthropic\" in llm._llm_type:  # type: ignore\\n        return True\\n\\n    # Bedrock anthropic\\n    if hasattr(llm, \"model_id\") and \"anthropic\" in llm.model_id:  # type: ignore\\n        return True\\n\\n    return False' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\graph_qa\\\\neptune_cypher.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='PROMPT_SELECTOR = ConditionalPromptSelector(\\n    default_prompt=NEPTUNE_OPENCYPHER_GENERATION_PROMPT,\\n    conditionals=[(use_simple_prompt, NEPTUNE_OPENCYPHER_GENERATION_SIMPLE_PROMPT)],\\n)\\n\\n\\nclass NeptuneOpenCypherQAChain(Chain):\\n    \"\"\"Chain for question-answering against a Neptune graph\\n    by generating openCypher statements.\\n\\n    *Security note*: Make sure that the database connection uses credentials\\n        that are narrowly-scoped to only include necessary permissions.\\n        Failure to do so may result in data corruption or loss, since the calling\\n        code may attempt commands that would result in deletion, mutation\\n        of data if appropriately prompted or reading sensitive data if such\\n        data is present in the database.\\n        The best way to guard against such negative outcomes is to (as appropriate)\\n        limit the permissions granted to the credentials used with this tool.\\n\\n        See https://python.langchain.com/docs/security for more information.\\n\\n    Example:\\n        .. code-block:: python\\n\\n        chain = NeptuneOpenCypherQAChain.from_llm(\\n            llm=llm,\\n            graph=graph\\n        )\\n        response = chain.run(query)\\n    \"\"\"\\n\\n    graph: NeptuneGraph = Field(exclude=True)\\n    cypher_generation_chain: LLMChain\\n    qa_chain: LLMChain\\n    input_key: str = \"query\"  #: :meta private:\\n    output_key: str = \"result\"  #: :meta private:\\n    top_k: int = 10\\n    return_intermediate_steps: bool = False\\n    \"\"\"Whether or not to return the intermediate steps along with the final answer.\"\"\"\\n    return_direct: bool = False\\n    \"\"\"Whether or not to return the result of querying the graph directly.\"\"\"\\n    extra_instructions: Optional[str] = None\\n    \"\"\"Extra instructions by the appended to the query generation prompt.\"\"\"\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Return the input keys.\\n\\n        :meta private:\\n        \"\"\"\\n        return [self.input_key]\\n\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Return the output keys.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\graph_qa\\\\neptune_cypher.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content=':meta private:\\n        \"\"\"\\n        _output_keys = [self.output_key]\\n        return _output_keys\\n\\n    @classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        *,\\n        qa_prompt: BasePromptTemplate = CYPHER_QA_PROMPT,\\n        cypher_prompt: Optional[BasePromptTemplate] = None,\\n        extra_instructions: Optional[str] = None,\\n        **kwargs: Any,\\n    ) -> NeptuneOpenCypherQAChain:\\n        \"\"\"Initialize from LLM.\"\"\"\\n        qa_chain = LLMChain(llm=llm, prompt=qa_prompt)\\n\\n        _cypher_prompt = cypher_prompt or PROMPT_SELECTOR.get_prompt(llm)\\n        cypher_generation_chain = LLMChain(llm=llm, prompt=_cypher_prompt)\\n\\n        return cls(\\n            qa_chain=qa_chain,\\n            cypher_generation_chain=cypher_generation_chain,\\n            extra_instructions=extra_instructions,\\n            **kwargs,\\n        )\\n\\n    def _call(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        \"\"\"Generate Cypher statement, use it to look up in db and answer question.\"\"\"\\n        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\\n        callbacks = _run_manager.get_child()\\n        question = inputs[self.input_key]\\n\\n        intermediate_steps: List = []\\n\\n        generated_cypher = self.cypher_generation_chain.run(\\n            {\\n                \"question\": question,\\n                \"schema\": self.graph.get_schema,\\n                \"extra_instructions\": self.extra_instructions or \"\",\\n            },\\n            callbacks=callbacks,\\n        )\\n\\n        # Extract Cypher code if it is wrapped in backticks\\n        generated_cypher = extract_cypher(generated_cypher)\\n        generated_cypher = trim_query(generated_cypher)\\n\\n        _run_manager.on_text(\"Generated Cypher:\", end=\"\\\\n\", verbose=self.verbose)\\n        _run_manager.on_text(\\n            generated_cypher, color=\"green\", end=\"\\\\n\", verbose=self.verbose\\n        )\\n\\n        intermediate_steps.append({\"query\": generated_cypher})' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\graph_qa\\\\neptune_cypher.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='context = self.graph.query(generated_cypher)\\n\\n        if self.return_direct:\\n            final_result = context\\n        else:\\n            _run_manager.on_text(\"Full Context:\", end=\"\\\\n\", verbose=self.verbose)\\n            _run_manager.on_text(\\n                str(context), color=\"green\", end=\"\\\\n\", verbose=self.verbose\\n            )\\n\\n            intermediate_steps.append({\"context\": context})\\n\\n            result = self.qa_chain(\\n                {\"question\": question, \"context\": context},\\n                callbacks=callbacks,\\n            )\\n            final_result = result[self.qa_chain.output_key]\\n\\n        chain_result: Dict[str, Any] = {self.output_key: final_result}\\n        if self.return_intermediate_steps:\\n            chain_result[INTERMEDIATE_STEPS_KEY] = intermediate_steps\\n\\n        return chain_result' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\graph_qa\\\\neptune_cypher.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# flake8: noqa\\nfrom langchain_core.prompts.prompt import PromptTemplate\\n\\n_DEFAULT_ENTITY_EXTRACTION_TEMPLATE = \"\"\"Extract all entities from the following text. As a guideline, a proper noun is generally capitalized. You should definitely extract all names and places.\\n\\nReturn the output as a single comma-separated list, or NONE if there is nothing of note to return.\\n\\nEXAMPLE\\ni\\'m trying to improve Langchain\\'s interfaces, the UX, its integrations with various products the user might want ... a lot of stuff.\\nOutput: Langchain\\nEND OF EXAMPLE\\n\\nEXAMPLE\\ni\\'m trying to improve Langchain\\'s interfaces, the UX, its integrations with various products the user might want ... a lot of stuff. I\\'m working with Sam.\\nOutput: Langchain, Sam\\nEND OF EXAMPLE\\n\\nBegin!\\n\\n{input}\\nOutput:\"\"\"\\nENTITY_EXTRACTION_PROMPT = PromptTemplate(\\n    input_variables=[\"input\"], template=_DEFAULT_ENTITY_EXTRACTION_TEMPLATE\\n)\\n\\n_DEFAULT_GRAPH_QA_TEMPLATE = \"\"\"Use the following knowledge triplets to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\"\"\"\\nGRAPH_QA_PROMPT = PromptTemplate(\\n    template=_DEFAULT_GRAPH_QA_TEMPLATE, input_variables=[\"context\", \"question\"]\\n)\\n\\nCYPHER_GENERATION_TEMPLATE = \"\"\"Task:Generate Cypher statement to query a graph database.\\nInstructions:\\nUse only the provided relationship types and properties in the schema.\\nDo not use any other relationship types or properties that are not provided.\\nSchema:\\n{schema}\\nNote: Do not include any explanations or apologies in your responses.\\nDo not respond to any questions that might ask anything else than for you to construct a Cypher statement.\\nDo not include any text except the generated Cypher statement.\\n\\nThe question is:\\n{question}\"\"\"\\nCYPHER_GENERATION_PROMPT = PromptTemplate(\\n    input_variables=[\"schema\", \"question\"], template=CYPHER_GENERATION_TEMPLATE\\n)\\n\\nNEBULAGRAPH_EXTRA_INSTRUCTIONS = \"\"\"\\nInstructions:' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\graph_qa\\\\prompts.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='First, generate cypher then convert it to NebulaGraph Cypher dialect(rather than standard):\\n1. it requires explicit label specification only when referring to node properties: v.`Foo`.name\\n2. note explicit label specification is not needed for edge properties, so it\\'s e.name instead of e.`Bar`.name\\n3. it uses double equals sign for comparison: `==` rather than `=`\\nFor instance:\\n```diff\\n< MATCH (p:person)-[e:directed]->(m:movie) WHERE m.name = \\'The Godfather II\\'\\n< RETURN p.name, e.year, m.name;\\n---\\n> MATCH (p:`person`)-[e:directed]->(m:`movie`) WHERE m.`movie`.`name` == \\'The Godfather II\\'\\n> RETURN p.`person`.`name`, e.year, m.`movie`.`name`;\\n```\\\\n\"\"\"\\n\\nNGQL_GENERATION_TEMPLATE = CYPHER_GENERATION_TEMPLATE.replace(\\n    \"Generate Cypher\", \"Generate NebulaGraph Cypher\"\\n).replace(\"Instructions:\", NEBULAGRAPH_EXTRA_INSTRUCTIONS)\\n\\nNGQL_GENERATION_PROMPT = PromptTemplate(\\n    input_variables=[\"schema\", \"question\"], template=NGQL_GENERATION_TEMPLATE\\n)\\n\\nKUZU_EXTRA_INSTRUCTIONS = \"\"\"\\nInstructions:\\n\\nGenerate statement with Kùzu Cypher dialect (rather than standard):\\n1. do not use `WHERE EXISTS` clause to check the existence of a property because Kùzu database has a fixed schema.\\n2. do not omit relationship pattern. Always use `()-[]->()` instead of `()->()`.\\n3. do not include any notes or comments even if the statement does not produce the expected result.\\n```\\\\n\"\"\"\\n\\nKUZU_GENERATION_TEMPLATE = CYPHER_GENERATION_TEMPLATE.replace(\\n    \"Generate Cypher\", \"Generate Kùzu Cypher\"\\n).replace(\"Instructions:\", KUZU_EXTRA_INSTRUCTIONS)\\n\\nKUZU_GENERATION_PROMPT = PromptTemplate(\\n    input_variables=[\"schema\", \"question\"], template=KUZU_GENERATION_TEMPLATE\\n)\\n\\nGREMLIN_GENERATION_TEMPLATE = CYPHER_GENERATION_TEMPLATE.replace(\"Cypher\", \"Gremlin\")\\n\\nGREMLIN_GENERATION_PROMPT = PromptTemplate(\\n    input_variables=[\"schema\", \"question\"], template=GREMLIN_GENERATION_TEMPLATE\\n)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\graph_qa\\\\prompts.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='CYPHER_QA_TEMPLATE = \"\"\"You are an assistant that helps to form nice and human understandable answers.\\nThe information part contains the provided information that you must use to construct an answer.\\nThe provided information is authoritative, you must never doubt it or try to use your internal knowledge to correct it.\\nMake the answer sound as a response to the question. Do not mention that you based the result on the given information.\\nIf the provided information is empty, say that you don\\'t know the answer.\\nInformation:\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\"\"\"\\nCYPHER_QA_PROMPT = PromptTemplate(\\n    input_variables=[\"context\", \"question\"], template=CYPHER_QA_TEMPLATE\\n)\\n\\nSPARQL_INTENT_TEMPLATE = \"\"\"Task: Identify the intent of a prompt and return the appropriate SPARQL query type.\\nYou are an assistant that distinguishes different types of prompts and returns the corresponding SPARQL query types.\\nConsider only the following query types:\\n* SELECT: this query type corresponds to questions\\n* UPDATE: this query type corresponds to all requests for deleting, inserting, or changing triples\\nNote: Be as concise as possible.\\nDo not include any explanations or apologies in your responses.\\nDo not respond to any questions that ask for anything else than for you to identify a SPARQL query type.\\nDo not include any unnecessary whitespaces or any text except the query type, i.e., either return \\'SELECT\\' or \\'UPDATE\\'.\\n\\nThe prompt is:\\n{prompt}\\nHelpful Answer:\"\"\"\\nSPARQL_INTENT_PROMPT = PromptTemplate(\\n    input_variables=[\"prompt\"], template=SPARQL_INTENT_TEMPLATE\\n)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\graph_qa\\\\prompts.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='SPARQL_GENERATION_SELECT_TEMPLATE = \"\"\"Task: Generate a SPARQL SELECT statement for querying a graph database.\\nFor instance, to find all email addresses of John Doe, the following query in backticks would be suitable:\\n```\\nPREFIX foaf: <http://xmlns.com/foaf/0.1/>\\nSELECT ?email\\nWHERE {{\\n    ?person foaf:name \"John Doe\" .\\n    ?person foaf:mbox ?email .\\n}}\\n```\\nInstructions:\\nUse only the node types and properties provided in the schema.\\nDo not use any node types and properties that are not explicitly provided.\\nInclude all necessary prefixes.\\nSchema:\\n{schema}\\nNote: Be as concise as possible.\\nDo not include any explanations or apologies in your responses.\\nDo not respond to any questions that ask for anything else than for you to construct a SPARQL query.\\nDo not include any text except the SPARQL query generated.\\n\\nThe question is:\\n{prompt}\"\"\"\\nSPARQL_GENERATION_SELECT_PROMPT = PromptTemplate(\\n    input_variables=[\"schema\", \"prompt\"], template=SPARQL_GENERATION_SELECT_TEMPLATE\\n)\\n\\nSPARQL_GENERATION_UPDATE_TEMPLATE = \"\"\"Task: Generate a SPARQL UPDATE statement for updating a graph database.\\nFor instance, to add \\'jane.doe@foo.bar\\' as a new email address for Jane Doe, the following query in backticks would be suitable:\\n```\\nPREFIX foaf: <http://xmlns.com/foaf/0.1/>\\nINSERT {{\\n    ?person foaf:mbox <mailto:jane.doe@foo.bar> .\\n}}\\nWHERE {{\\n    ?person foaf:name \"Jane Doe\" .\\n}}\\n```\\nInstructions:\\nMake the query as short as possible and avoid adding unnecessary triples.\\nUse only the node types and properties provided in the schema.\\nDo not use any node types and properties that are not explicitly provided.\\nInclude all necessary prefixes.\\nSchema:\\n{schema}\\nNote: Be as concise as possible.\\nDo not include any explanations or apologies in your responses.\\nDo not respond to any questions that ask for anything else than for you to construct a SPARQL query.\\nReturn only the generated SPARQL query, nothing else.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\graph_qa\\\\prompts.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='The information to be inserted is:\\n{prompt}\"\"\"\\nSPARQL_GENERATION_UPDATE_PROMPT = PromptTemplate(\\n    input_variables=[\"schema\", \"prompt\"], template=SPARQL_GENERATION_UPDATE_TEMPLATE\\n)\\n\\nSPARQL_QA_TEMPLATE = \"\"\"Task: Generate a natural language response from the results of a SPARQL query.\\nYou are an assistant that creates well-written and human understandable answers.\\nThe information part contains the information provided, which you can use to construct an answer.\\nThe information provided is authoritative, you must never doubt it or try to use your internal knowledge to correct it.\\nMake your response sound like the information is coming from an AI assistant, but don\\'t add any information.\\nInformation:\\n{context}\\n\\nQuestion: {prompt}\\nHelpful Answer:\"\"\"\\nSPARQL_QA_PROMPT = PromptTemplate(\\n    input_variables=[\"context\", \"prompt\"], template=SPARQL_QA_TEMPLATE\\n)\\n\\n\\nAQL_GENERATION_TEMPLATE = \"\"\"Task: Generate an ArangoDB Query Language (AQL) query from a User Input.\\n\\nYou are an ArangoDB Query Language (AQL) expert responsible for translating a `User Input` into an ArangoDB Query Language (AQL) query.\\n\\nYou are given an `ArangoDB Schema`. It is a JSON Object containing:\\n1. `Graph Schema`: Lists all Graphs within the ArangoDB Database Instance, along with their Edge Relationships.\\n2. `Collection Schema`: Lists all Collections within the ArangoDB Database Instance, along with their document/edge properties and a document/edge example.\\n\\nYou may also be given a set of `AQL Query Examples` to help you create the `AQL Query`. If provided, the `AQL Query Examples` should be used as a reference, similar to how `ArangoDB Schema` should be used.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\graph_qa\\\\prompts.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Things you should do:\\n- Think step by step.\\n- Rely on `ArangoDB Schema` and `AQL Query Examples` (if provided) to generate the query.\\n- Begin the `AQL Query` by the `WITH` AQL keyword to specify all of the ArangoDB Collections required.\\n- Return the `AQL Query` wrapped in 3 backticks (```).\\n- Use only the provided relationship types and properties in the `ArangoDB Schema` and any `AQL Query Examples` queries.\\n- Only answer to requests related to generating an AQL Query.\\n- If a request is unrelated to generating AQL Query, say that you cannot help the user.\\n\\nThings you should not do:\\n- Do not use any properties/relationships that can\\'t be inferred from the `ArangoDB Schema` or the `AQL Query Examples`. \\n- Do not include any text except the generated AQL Query.\\n- Do not provide explanations or apologies in your responses.\\n- Do not generate an AQL Query that removes or deletes any data.\\n\\nUnder no circumstance should you generate an AQL Query that deletes any data whatsoever.\\n\\nArangoDB Schema:\\n{adb_schema}\\n\\nAQL Query Examples (Optional):\\n{aql_examples}\\n\\nUser Input:\\n{user_input}\\n\\nAQL Query: \\n\"\"\"\\n\\nAQL_GENERATION_PROMPT = PromptTemplate(\\n    input_variables=[\"adb_schema\", \"aql_examples\", \"user_input\"],\\n    template=AQL_GENERATION_TEMPLATE,\\n)\\n\\nAQL_FIX_TEMPLATE = \"\"\"Task: Address the ArangoDB Query Language (AQL) error message of an ArangoDB Query Language query.\\n\\nYou are an ArangoDB Query Language (AQL) expert responsible for correcting the provided `AQL Query` based on the provided `AQL Error`. \\n\\nThe `AQL Error` explains why the `AQL Query` could not be executed in the database.\\nThe `AQL Error` may also contain the position of the error relative to the total number of lines of the `AQL Query`.\\nFor example, \\'error X at position 2:5\\' denotes that the error X occurs on line 2, column 5 of the `AQL Query`.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\graph_qa\\\\prompts.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='You are also given the `ArangoDB Schema`. It is a JSON Object containing:\\n1. `Graph Schema`: Lists all Graphs within the ArangoDB Database Instance, along with their Edge Relationships.\\n2. `Collection Schema`: Lists all Collections within the ArangoDB Database Instance, along with their document/edge properties and a document/edge example.\\n\\nYou will output the `Corrected AQL Query` wrapped in 3 backticks (```). Do not include any text except the Corrected AQL Query.\\n\\nRemember to think step by step.\\n\\nArangoDB Schema:\\n{adb_schema}\\n\\nAQL Query:\\n{aql_query}\\n\\nAQL Error:\\n{aql_error}\\n\\nCorrected AQL Query:\\n\"\"\"\\n\\nAQL_FIX_PROMPT = PromptTemplate(\\n    input_variables=[\\n        \"adb_schema\",\\n        \"aql_query\",\\n        \"aql_error\",\\n    ],\\n    template=AQL_FIX_TEMPLATE,\\n)\\n\\nAQL_QA_TEMPLATE = \"\"\"Task: Generate a natural language `Summary` from the results of an ArangoDB Query Language query.\\n\\nYou are an ArangoDB Query Language (AQL) expert responsible for creating a well-written `Summary` from the `User Input` and associated `AQL Result`.\\n\\nA user has executed an ArangoDB Query Language query, which has returned the AQL Result in JSON format.\\nYou are responsible for creating an `Summary` based on the AQL Result.\\n\\nYou are given the following information:\\n- `ArangoDB Schema`: contains a schema representation of the user\\'s ArangoDB Database.\\n- `User Input`: the original question/request of the user, which has been translated into an AQL Query.\\n- `AQL Query`: the AQL equivalent of the `User Input`, translated by another AI Model. Should you deem it to be incorrect, suggest a different AQL Query.\\n- `AQL Result`: the JSON output returned by executing the `AQL Query` within the ArangoDB Database.\\n\\nRemember to think step by step.\\n\\nYour `Summary` should sound like it is a response to the `User Input`.\\nYour `Summary` should not include any mention of the `AQL Query` or the `AQL Result`.\\n\\nArangoDB Schema:\\n{adb_schema}\\n\\nUser Input:\\n{user_input}\\n\\nAQL Query:\\n{aql_query}' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\graph_qa\\\\prompts.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='AQL Result:\\n{aql_result}\\n\"\"\"\\nAQL_QA_PROMPT = PromptTemplate(\\n    input_variables=[\"adb_schema\", \"user_input\", \"aql_query\", \"aql_result\"],\\n    template=AQL_QA_TEMPLATE,\\n)\\n\\n\\nNEPTUNE_OPENCYPHER_EXTRA_INSTRUCTIONS = \"\"\"\\nInstructions:\\nGenerate the query in openCypher format and follow these rules:\\nDo not use `NONE`, `ALL` or `ANY` predicate functions, rather use list comprehensions.\\nDo not use `REDUCE` function. Rather use a combination of list comprehension and the `UNWIND` clause to achieve similar results.\\nDo not use `FOREACH` clause. Rather use a combination of `WITH` and `UNWIND` clauses to achieve similar results.{extra_instructions}\\n\\\\n\"\"\"\\n\\nNEPTUNE_OPENCYPHER_GENERATION_TEMPLATE = CYPHER_GENERATION_TEMPLATE.replace(\\n    \"Instructions:\", NEPTUNE_OPENCYPHER_EXTRA_INSTRUCTIONS\\n)\\n\\nNEPTUNE_OPENCYPHER_GENERATION_PROMPT = PromptTemplate(\\n    input_variables=[\"schema\", \"question\", \"extra_instructions\"],\\n    template=NEPTUNE_OPENCYPHER_GENERATION_TEMPLATE,\\n)\\n\\nNEPTUNE_OPENCYPHER_GENERATION_SIMPLE_TEMPLATE = \"\"\"\\nWrite an openCypher query to answer the following question. Do not explain the answer. Only return the query.{extra_instructions}\\nQuestion:  \"{question}\". \\nHere is the property graph schema: \\n{schema}\\n\\\\n\"\"\"\\n\\nNEPTUNE_OPENCYPHER_GENERATION_SIMPLE_PROMPT = PromptTemplate(\\n    input_variables=[\"schema\", \"question\", \"extra_instructions\"],\\n    template=NEPTUNE_OPENCYPHER_GENERATION_SIMPLE_TEMPLATE,\\n)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\graph_qa\\\\prompts.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"\\nQuestion answering over an RDF or OWL graph using SPARQL.\\n\"\"\"\\nfrom __future__ import annotations\\n\\nfrom typing import Any, Dict, List, Optional\\n\\nfrom langchain_community.graphs.rdf_graph import RdfGraph\\nfrom langchain_core.callbacks import CallbackManagerForChainRun\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.prompts.base import BasePromptTemplate\\nfrom langchain_core.pydantic_v1 import Field\\n\\nfrom langchain.chains.base import Chain\\nfrom langchain.chains.graph_qa.prompts import (\\n    SPARQL_GENERATION_SELECT_PROMPT,\\n    SPARQL_GENERATION_UPDATE_PROMPT,\\n    SPARQL_INTENT_PROMPT,\\n    SPARQL_QA_PROMPT,\\n)\\nfrom langchain.chains.llm import LLMChain\\n\\n\\nclass GraphSparqlQAChain(Chain):\\n    \"\"\"Question-answering against an RDF or OWL graph by generating SPARQL statements.\\n\\n    *Security note*: Make sure that the database connection uses credentials\\n        that are narrowly-scoped to only include necessary permissions.\\n        Failure to do so may result in data corruption or loss, since the calling\\n        code may attempt commands that would result in deletion, mutation\\n        of data if appropriately prompted or reading sensitive data if such\\n        data is present in the database.\\n        The best way to guard against such negative outcomes is to (as appropriate)\\n        limit the permissions granted to the credentials used with this tool.\\n\\n        See https://python.langchain.com/docs/security for more information.\\n    \"\"\"\\n\\n    graph: RdfGraph = Field(exclude=True)\\n    sparql_generation_select_chain: LLMChain\\n    sparql_generation_update_chain: LLMChain\\n    sparql_intent_chain: LLMChain\\n    qa_chain: LLMChain\\n    input_key: str = \"query\"  #: :meta private:\\n    output_key: str = \"result\"  #: :meta private:\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        return [self.input_key]\\n\\n    @property\\n    def output_keys(self) -> List[str]:\\n        _output_keys = [self.output_key]\\n        return _output_keys' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\graph_qa\\\\sparql.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        *,\\n        qa_prompt: BasePromptTemplate = SPARQL_QA_PROMPT,\\n        sparql_select_prompt: BasePromptTemplate = SPARQL_GENERATION_SELECT_PROMPT,\\n        sparql_update_prompt: BasePromptTemplate = SPARQL_GENERATION_UPDATE_PROMPT,\\n        sparql_intent_prompt: BasePromptTemplate = SPARQL_INTENT_PROMPT,\\n        **kwargs: Any,\\n    ) -> GraphSparqlQAChain:\\n        \"\"\"Initialize from LLM.\"\"\"\\n        qa_chain = LLMChain(llm=llm, prompt=qa_prompt)\\n        sparql_generation_select_chain = LLMChain(llm=llm, prompt=sparql_select_prompt)\\n        sparql_generation_update_chain = LLMChain(llm=llm, prompt=sparql_update_prompt)\\n        sparql_intent_chain = LLMChain(llm=llm, prompt=sparql_intent_prompt)\\n\\n        return cls(\\n            qa_chain=qa_chain,\\n            sparql_generation_select_chain=sparql_generation_select_chain,\\n            sparql_generation_update_chain=sparql_generation_update_chain,\\n            sparql_intent_chain=sparql_intent_chain,\\n            **kwargs,\\n        )\\n\\n    def _call(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, str]:\\n        \"\"\"\\n        Generate SPARQL query, use it to retrieve a response from the gdb and answer\\n        the question.\\n        \"\"\"\\n        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\\n        callbacks = _run_manager.get_child()\\n        prompt = inputs[self.input_key]\\n\\n        _intent = self.sparql_intent_chain.run({\"prompt\": prompt}, callbacks=callbacks)\\n        intent = _intent.strip()' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\graph_qa\\\\sparql.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='if \"SELECT\" in intent and \"UPDATE\" not in intent:\\n            sparql_generation_chain = self.sparql_generation_select_chain\\n            intent = \"SELECT\"\\n        elif \"UPDATE\" in intent and \"SELECT\" not in intent:\\n            sparql_generation_chain = self.sparql_generation_update_chain\\n            intent = \"UPDATE\"\\n        else:\\n            raise ValueError(\\n                \"I am sorry, but this prompt seems to fit none of the currently \"\\n                \"supported SPARQL query types, i.e., SELECT and UPDATE.\"\\n            )\\n\\n        _run_manager.on_text(\"Identified intent:\", end=\"\\\\n\", verbose=self.verbose)\\n        _run_manager.on_text(intent, color=\"green\", end=\"\\\\n\", verbose=self.verbose)\\n\\n        generated_sparql = sparql_generation_chain.run(\\n            {\"prompt\": prompt, \"schema\": self.graph.get_schema}, callbacks=callbacks\\n        )\\n\\n        _run_manager.on_text(\"Generated SPARQL:\", end=\"\\\\n\", verbose=self.verbose)\\n        _run_manager.on_text(\\n            generated_sparql, color=\"green\", end=\"\\\\n\", verbose=self.verbose\\n        )\\n\\n        if intent == \"SELECT\":\\n            context = self.graph.query(generated_sparql)\\n\\n            _run_manager.on_text(\"Full Context:\", end=\"\\\\n\", verbose=self.verbose)\\n            _run_manager.on_text(\\n                str(context), color=\"green\", end=\"\\\\n\", verbose=self.verbose\\n            )\\n            result = self.qa_chain(\\n                {\"prompt\": prompt, \"context\": context},\\n                callbacks=callbacks,\\n            )\\n            res = result[self.qa_chain.output_key]\\n        elif intent == \"UPDATE\":\\n            self.graph.update(generated_sparql)\\n            res = \"Successfully inserted triples into the graph.\"\\n        else:\\n            raise ValueError(\"Unsupported SPARQL query type.\")\\n        return {self.output_key: res}' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\graph_qa\\\\sparql.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Question answering over a knowledge graph.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\graph_qa\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Hypothetical Document Embeddings.\\n\\nhttps://arxiv.org/abs/2212.10496\\n\"\"\"\\nfrom __future__ import annotations\\n\\nfrom typing import Any, Dict, List, Optional\\n\\nimport numpy as np\\nfrom langchain_core.callbacks import CallbackManagerForChainRun\\nfrom langchain_core.embeddings import Embeddings\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.prompts import BasePromptTemplate\\nfrom langchain_core.pydantic_v1 import Extra\\n\\nfrom langchain.chains.base import Chain\\nfrom langchain.chains.hyde.prompts import PROMPT_MAP\\nfrom langchain.chains.llm import LLMChain\\n\\n\\nclass HypotheticalDocumentEmbedder(Chain, Embeddings):\\n    \"\"\"Generate hypothetical document for query, and then embed that.\\n\\n    Based on https://arxiv.org/abs/2212.10496\\n    \"\"\"\\n\\n    base_embeddings: Embeddings\\n    llm_chain: LLMChain\\n\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n\\n        extra = Extra.forbid\\n        arbitrary_types_allowed = True\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Input keys for Hyde\\'s LLM chain.\"\"\"\\n        return self.llm_chain.input_keys\\n\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Output keys for Hyde\\'s LLM chain.\"\"\"\\n        return self.llm_chain.output_keys\\n\\n    def embed_documents(self, texts: List[str]) -> List[List[float]]:\\n        \"\"\"Call the base embeddings.\"\"\"\\n        return self.base_embeddings.embed_documents(texts)\\n\\n    def combine_embeddings(self, embeddings: List[List[float]]) -> List[float]:\\n        \"\"\"Combine embeddings into final embeddings.\"\"\"\\n        return list(np.array(embeddings).mean(axis=0))\\n\\n    def embed_query(self, text: str) -> List[float]:\\n        \"\"\"Generate a hypothetical document and embedded it.\"\"\"\\n        var_name = self.llm_chain.input_keys[0]\\n        result = self.llm_chain.generate([{var_name: text}])\\n        documents = [generation.text for generation in result.generations[0]]\\n        embeddings = self.embed_documents(documents)\\n        return self.combine_embeddings(embeddings)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\hyde\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _call(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, str]:\\n        \"\"\"Call the internal llm chain.\"\"\"\\n        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\\n        return self.llm_chain(inputs, callbacks=_run_manager.get_child())\\n\\n    @classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        base_embeddings: Embeddings,\\n        prompt_key: Optional[str] = None,\\n        custom_prompt: Optional[BasePromptTemplate] = None,\\n        **kwargs: Any,\\n    ) -> HypotheticalDocumentEmbedder:\\n        \"\"\"Load and use LLMChain with either a specific prompt key or custom prompt.\"\"\"\\n        if custom_prompt is not None:\\n            prompt = custom_prompt\\n        elif prompt_key is not None and prompt_key in PROMPT_MAP:\\n            prompt = PROMPT_MAP[prompt_key]\\n        else:\\n            raise ValueError(\\n                f\"Must specify prompt_key if custom_prompt not provided. Should be one \"\\n                f\"of {list(PROMPT_MAP.keys())}.\"\\n            )\\n\\n        llm_chain = LLMChain(llm=llm, prompt=prompt)\\n        return cls(base_embeddings=base_embeddings, llm_chain=llm_chain, **kwargs)\\n\\n    @property\\n    def _chain_type(self) -> str:\\n        return \"hyde_chain\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\hyde\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# flake8: noqa\\nfrom langchain_core.prompts.prompt import PromptTemplate\\n\\nweb_search_template = \"\"\"Please write a passage to answer the question \\nQuestion: {QUESTION}\\nPassage:\"\"\"\\nweb_search = PromptTemplate(template=web_search_template, input_variables=[\"QUESTION\"])\\nsci_fact_template = \"\"\"Please write a scientific paper passage to support/refute the claim \\nClaim: {Claim}\\nPassage:\"\"\"\\nsci_fact = PromptTemplate(template=sci_fact_template, input_variables=[\"Claim\"])\\narguana_template = \"\"\"Please write a counter argument for the passage \\nPassage: {PASSAGE}\\nCounter Argument:\"\"\"\\narguana = PromptTemplate(template=arguana_template, input_variables=[\"PASSAGE\"])\\ntrec_covid_template = \"\"\"Please write a scientific paper passage to answer the question\\nQuestion: {QUESTION}\\nPassage:\"\"\"\\ntrec_covid = PromptTemplate(template=trec_covid_template, input_variables=[\"QUESTION\"])\\nfiqa_template = \"\"\"Please write a financial article passage to answer the question\\nQuestion: {QUESTION}\\nPassage:\"\"\"\\nfiqa = PromptTemplate(template=fiqa_template, input_variables=[\"QUESTION\"])\\ndbpedia_entity_template = \"\"\"Please write a passage to answer the question.\\nQuestion: {QUESTION}\\nPassage:\"\"\"\\ndbpedia_entity = PromptTemplate(\\n    template=dbpedia_entity_template, input_variables=[\"QUESTION\"]\\n)\\ntrec_news_template = \"\"\"Please write a news passage about the topic.\\nTopic: {TOPIC}\\nPassage:\"\"\"\\ntrec_news = PromptTemplate(template=trec_news_template, input_variables=[\"TOPIC\"])\\nmr_tydi_template = \"\"\"Please write a passage in Swahili/Korean/Japanese/Bengali to answer the question in detail.\\nQuestion: {QUESTION}\\nPassage:\"\"\"\\nmr_tydi = PromptTemplate(template=mr_tydi_template, input_variables=[\"QUESTION\"])\\nPROMPT_MAP = {\\n    \"web_search\": web_search,\\n    \"sci_fact\": sci_fact,\\n    \"arguana\": arguana,\\n    \"trec_covid\": trec_covid,\\n    \"fiqa\": fiqa,\\n    \"dbpedia_entity\": dbpedia_entity,\\n    \"trec_news\": trec_news,\\n    \"mr_tydi\": mr_tydi,\\n}' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\hyde\\\\prompts.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Hypothetical Document Embeddings.\\n\\nhttps://arxiv.org/abs/2212.10496\\n\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\hyde\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def __getattr__(name: str = \"\") -> None:\\n    \"\"\"Raise an error on import since is deprecated.\"\"\"\\n    raise ImportError(\\n        \"This module has been moved to langchain-experimental. \"\\n        \"For more details: https://github.com/langchain-ai/langchain/discussions/11352.\"\\n        \"To access this code, install it with `pip install langchain-experimental`.\"\\n        \"`from langchain_experimental.llm_bash.base \"\\n        \"import LLMBashChain`\"\\n    )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\llm_bash\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Chain for question-answering with self-verification.\"\"\"\\nfrom __future__ import annotations\\n\\nimport warnings\\nfrom typing import Any, Dict, List, Optional\\n\\nfrom langchain_core.callbacks import CallbackManagerForChainRun\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.prompts import PromptTemplate\\nfrom langchain_core.pydantic_v1 import Extra, root_validator\\n\\nfrom langchain.chains.base import Chain\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.chains.llm_checker.prompt import (\\n    CHECK_ASSERTIONS_PROMPT,\\n    CREATE_DRAFT_ANSWER_PROMPT,\\n    LIST_ASSERTIONS_PROMPT,\\n    REVISED_ANSWER_PROMPT,\\n)\\nfrom langchain.chains.sequential import SequentialChain\\n\\n\\ndef _load_question_to_checked_assertions_chain(\\n    llm: BaseLanguageModel,\\n    create_draft_answer_prompt: PromptTemplate,\\n    list_assertions_prompt: PromptTemplate,\\n    check_assertions_prompt: PromptTemplate,\\n    revised_answer_prompt: PromptTemplate,\\n) -> SequentialChain:\\n    create_draft_answer_chain = LLMChain(\\n        llm=llm,\\n        prompt=create_draft_answer_prompt,\\n        output_key=\"statement\",\\n    )\\n    list_assertions_chain = LLMChain(\\n        llm=llm,\\n        prompt=list_assertions_prompt,\\n        output_key=\"assertions\",\\n    )\\n    check_assertions_chain = LLMChain(\\n        llm=llm,\\n        prompt=check_assertions_prompt,\\n        output_key=\"checked_assertions\",\\n    )\\n    revised_answer_chain = LLMChain(\\n        llm=llm,\\n        prompt=revised_answer_prompt,\\n        output_key=\"revised_statement\",\\n    )\\n    chains = [\\n        create_draft_answer_chain,\\n        list_assertions_chain,\\n        check_assertions_chain,\\n        revised_answer_chain,\\n    ]\\n    question_to_checked_assertions_chain = SequentialChain(\\n        chains=chains,\\n        input_variables=[\"question\"],\\n        output_variables=[\"revised_statement\"],\\n        verbose=True,\\n    )\\n    return question_to_checked_assertions_chain\\n\\n\\nclass LLMCheckerChain(Chain):\\n    \"\"\"Chain for question-answering with self-verification.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\llm_checker\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Example:\\n        .. code-block:: python\\n\\n            from langchain_community.llms import OpenAI\\n            from langchain.chains import LLMCheckerChain\\n            llm = OpenAI(temperature=0.7)\\n            checker_chain = LLMCheckerChain.from_llm(llm)\\n    \"\"\"\\n\\n    question_to_checked_assertions_chain: SequentialChain\\n\\n    llm: Optional[BaseLanguageModel] = None\\n    \"\"\"[Deprecated] LLM wrapper to use.\"\"\"\\n    create_draft_answer_prompt: PromptTemplate = CREATE_DRAFT_ANSWER_PROMPT\\n    \"\"\"[Deprecated]\"\"\"\\n    list_assertions_prompt: PromptTemplate = LIST_ASSERTIONS_PROMPT\\n    \"\"\"[Deprecated]\"\"\"\\n    check_assertions_prompt: PromptTemplate = CHECK_ASSERTIONS_PROMPT\\n    \"\"\"[Deprecated]\"\"\"\\n    revised_answer_prompt: PromptTemplate = REVISED_ANSWER_PROMPT\\n    \"\"\"[Deprecated] Prompt to use when questioning the documents.\"\"\"\\n    input_key: str = \"query\"  #: :meta private:\\n    output_key: str = \"result\"  #: :meta private:\\n\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n\\n        extra = Extra.forbid\\n        arbitrary_types_allowed = True' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\llm_checker\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@root_validator(pre=True)\\n    def raise_deprecation(cls, values: Dict) -> Dict:\\n        if \"llm\" in values:\\n            warnings.warn(\\n                \"Directly instantiating an LLMCheckerChain with an llm is deprecated. \"\\n                \"Please instantiate with question_to_checked_assertions_chain \"\\n                \"or using the from_llm class method.\"\\n            )\\n            if (\\n                \"question_to_checked_assertions_chain\" not in values\\n                and values[\"llm\"] is not None\\n            ):\\n                question_to_checked_assertions_chain = (\\n                    _load_question_to_checked_assertions_chain(\\n                        values[\"llm\"],\\n                        values.get(\\n                            \"create_draft_answer_prompt\", CREATE_DRAFT_ANSWER_PROMPT\\n                        ),\\n                        values.get(\"list_assertions_prompt\", LIST_ASSERTIONS_PROMPT),\\n                        values.get(\"check_assertions_prompt\", CHECK_ASSERTIONS_PROMPT),\\n                        values.get(\"revised_answer_prompt\", REVISED_ANSWER_PROMPT),\\n                    )\\n                )\\n                values[\\n                    \"question_to_checked_assertions_chain\"\\n                ] = question_to_checked_assertions_chain\\n        return values\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Return the singular input key.\\n\\n        :meta private:\\n        \"\"\"\\n        return [self.input_key]\\n\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Return the singular output key.\\n\\n        :meta private:\\n        \"\"\"\\n        return [self.output_key]\\n\\n    def _call(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, str]:\\n        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\\n        question = inputs[self.input_key]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\llm_checker\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='output = self.question_to_checked_assertions_chain(\\n            {\"question\": question}, callbacks=_run_manager.get_child()\\n        )\\n        return {self.output_key: output[\"revised_statement\"]}\\n\\n    @property\\n    def _chain_type(self) -> str:\\n        return \"llm_checker_chain\"\\n\\n    @classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        create_draft_answer_prompt: PromptTemplate = CREATE_DRAFT_ANSWER_PROMPT,\\n        list_assertions_prompt: PromptTemplate = LIST_ASSERTIONS_PROMPT,\\n        check_assertions_prompt: PromptTemplate = CHECK_ASSERTIONS_PROMPT,\\n        revised_answer_prompt: PromptTemplate = REVISED_ANSWER_PROMPT,\\n        **kwargs: Any,\\n    ) -> LLMCheckerChain:\\n        question_to_checked_assertions_chain = (\\n            _load_question_to_checked_assertions_chain(\\n                llm,\\n                create_draft_answer_prompt,\\n                list_assertions_prompt,\\n                check_assertions_prompt,\\n                revised_answer_prompt,\\n            )\\n        )\\n        return cls(\\n            question_to_checked_assertions_chain=question_to_checked_assertions_chain,\\n            **kwargs,\\n        )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\llm_checker\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# flake8: noqa\\nfrom langchain_core.prompts.prompt import PromptTemplate\\n\\n_CREATE_DRAFT_ANSWER_TEMPLATE = \"\"\"{question}\\\\n\\\\n\"\"\"\\nCREATE_DRAFT_ANSWER_PROMPT = PromptTemplate(\\n    input_variables=[\"question\"], template=_CREATE_DRAFT_ANSWER_TEMPLATE\\n)\\n\\n_LIST_ASSERTIONS_TEMPLATE = \"\"\"Here is a statement:\\n{statement}\\nMake a bullet point list of the assumptions you made when producing the above statement.\\\\n\\\\n\"\"\"\\nLIST_ASSERTIONS_PROMPT = PromptTemplate(\\n    input_variables=[\"statement\"], template=_LIST_ASSERTIONS_TEMPLATE\\n)\\n\\n_CHECK_ASSERTIONS_TEMPLATE = \"\"\"Here is a bullet point list of assertions:\\n{assertions}\\nFor each assertion, determine whether it is true or false. If it is false, explain why.\\\\n\\\\n\"\"\"\\nCHECK_ASSERTIONS_PROMPT = PromptTemplate(\\n    input_variables=[\"assertions\"], template=_CHECK_ASSERTIONS_TEMPLATE\\n)\\n\\n_REVISED_ANSWER_TEMPLATE = \"\"\"{checked_assertions}\\n\\nQuestion: In light of the above assertions and checks, how would you answer the question \\'{question}\\'?\\n\\nAnswer:\"\"\"\\nREVISED_ANSWER_PROMPT = PromptTemplate(\\n    input_variables=[\"checked_assertions\", \"question\"],\\n    template=_REVISED_ANSWER_TEMPLATE,\\n)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\llm_checker\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Chain that tries to verify assumptions before answering a question.\\n\\nHeavily borrowed from https://github.com/jagilley/fact-checker\\n\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\llm_checker\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Chain that interprets a prompt and executes python code to do math.\"\"\"\\nfrom __future__ import annotations\\n\\nimport math\\nimport re\\nimport warnings\\nfrom typing import Any, Dict, List, Optional\\n\\nfrom langchain_core.callbacks import (\\n    AsyncCallbackManagerForChainRun,\\n    CallbackManagerForChainRun,\\n)\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.prompts import BasePromptTemplate\\nfrom langchain_core.pydantic_v1 import Extra, root_validator\\n\\nfrom langchain.chains.base import Chain\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.chains.llm_math.prompt import PROMPT\\n\\n\\nclass LLMMathChain(Chain):\\n    \"\"\"Chain that interprets a prompt and executes python code to do math.\\n\\n    Example:\\n        .. code-block:: python\\n\\n            from langchain.chains import LLMMathChain\\n            from langchain_community.llms import OpenAI\\n            llm_math = LLMMathChain.from_llm(OpenAI())\\n    \"\"\"\\n\\n    llm_chain: LLMChain\\n    llm: Optional[BaseLanguageModel] = None\\n    \"\"\"[Deprecated] LLM wrapper to use.\"\"\"\\n    prompt: BasePromptTemplate = PROMPT\\n    \"\"\"[Deprecated] Prompt to use to translate to python if necessary.\"\"\"\\n    input_key: str = \"question\"  #: :meta private:\\n    output_key: str = \"answer\"  #: :meta private:\\n\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n\\n        extra = Extra.forbid\\n        arbitrary_types_allowed = True' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\llm_math\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@root_validator(pre=True)\\n    def raise_deprecation(cls, values: Dict) -> Dict:\\n        try:\\n            import numexpr  # noqa: F401\\n        except ImportError:\\n            raise ImportError(\\n                \"LLMMathChain requires the numexpr package. \"\\n                \"Please install it with `pip install numexpr`.\"\\n            )\\n        if \"llm\" in values:\\n            warnings.warn(\\n                \"Directly instantiating an LLMMathChain with an llm is deprecated. \"\\n                \"Please instantiate with llm_chain argument or using the from_llm \"\\n                \"class method.\"\\n            )\\n            if \"llm_chain\" not in values and values[\"llm\"] is not None:\\n                prompt = values.get(\"prompt\", PROMPT)\\n                values[\"llm_chain\"] = LLMChain(llm=values[\"llm\"], prompt=prompt)\\n        return values\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Expect input key.\\n\\n        :meta private:\\n        \"\"\"\\n        return [self.input_key]\\n\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Expect output key.\\n\\n        :meta private:\\n        \"\"\"\\n        return [self.output_key]\\n\\n    def _evaluate_expression(self, expression: str) -> str:\\n        import numexpr  # noqa: F401\\n\\n        try:\\n            local_dict = {\"pi\": math.pi, \"e\": math.e}\\n            output = str(\\n                numexpr.evaluate(\\n                    expression.strip(),\\n                    global_dict={},  # restrict access to globals\\n                    local_dict=local_dict,  # add common mathematical functions\\n                )\\n            )\\n        except Exception as e:\\n            raise ValueError(\\n                f\\'LLMMathChain._evaluate(\"{expression}\") raised error: {e}.\\'\\n                \" Please try again with a valid numerical expression\"\\n            )\\n\\n        # Remove any leading and trailing brackets from the output\\n        return re.sub(r\"^\\\\[|\\\\]$\", \"\", output)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\llm_math\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _process_llm_result(\\n        self, llm_output: str, run_manager: CallbackManagerForChainRun\\n    ) -> Dict[str, str]:\\n        run_manager.on_text(llm_output, color=\"green\", verbose=self.verbose)\\n        llm_output = llm_output.strip()\\n        text_match = re.search(r\"^```text(.*?)```\", llm_output, re.DOTALL)\\n        if text_match:\\n            expression = text_match.group(1)\\n            output = self._evaluate_expression(expression)\\n            run_manager.on_text(\"\\\\nAnswer: \", verbose=self.verbose)\\n            run_manager.on_text(output, color=\"yellow\", verbose=self.verbose)\\n            answer = \"Answer: \" + output\\n        elif llm_output.startswith(\"Answer:\"):\\n            answer = llm_output\\n        elif \"Answer:\" in llm_output:\\n            answer = \"Answer: \" + llm_output.split(\"Answer:\")[-1]\\n        else:\\n            raise ValueError(f\"unknown format from LLM: {llm_output}\")\\n        return {self.output_key: answer}\\n\\n    async def _aprocess_llm_result(\\n        self,\\n        llm_output: str,\\n        run_manager: AsyncCallbackManagerForChainRun,\\n    ) -> Dict[str, str]:\\n        await run_manager.on_text(llm_output, color=\"green\", verbose=self.verbose)\\n        llm_output = llm_output.strip()\\n        text_match = re.search(r\"^```text(.*?)```\", llm_output, re.DOTALL)\\n        if text_match:\\n            expression = text_match.group(1)\\n            output = self._evaluate_expression(expression)\\n            await run_manager.on_text(\"\\\\nAnswer: \", verbose=self.verbose)\\n            await run_manager.on_text(output, color=\"yellow\", verbose=self.verbose)\\n            answer = \"Answer: \" + output\\n        elif llm_output.startswith(\"Answer:\"):\\n            answer = llm_output\\n        elif \"Answer:\" in llm_output:\\n            answer = \"Answer: \" + llm_output.split(\"Answer:\")[-1]\\n        else:\\n            raise ValueError(f\"unknown format from LLM: {llm_output}\")\\n        return {self.output_key: answer}' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\llm_math\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _call(\\n        self,\\n        inputs: Dict[str, str],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, str]:\\n        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\\n        _run_manager.on_text(inputs[self.input_key])\\n        llm_output = self.llm_chain.predict(\\n            question=inputs[self.input_key],\\n            stop=[\"```output\"],\\n            callbacks=_run_manager.get_child(),\\n        )\\n        return self._process_llm_result(llm_output, _run_manager)\\n\\n    async def _acall(\\n        self,\\n        inputs: Dict[str, str],\\n        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\\n    ) -> Dict[str, str]:\\n        _run_manager = run_manager or AsyncCallbackManagerForChainRun.get_noop_manager()\\n        await _run_manager.on_text(inputs[self.input_key])\\n        llm_output = await self.llm_chain.apredict(\\n            question=inputs[self.input_key],\\n            stop=[\"```output\"],\\n            callbacks=_run_manager.get_child(),\\n        )\\n        return await self._aprocess_llm_result(llm_output, _run_manager)\\n\\n    @property\\n    def _chain_type(self) -> str:\\n        return \"llm_math_chain\"\\n\\n    @classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        prompt: BasePromptTemplate = PROMPT,\\n        **kwargs: Any,\\n    ) -> LLMMathChain:\\n        llm_chain = LLMChain(llm=llm, prompt=prompt)\\n        return cls(llm_chain=llm_chain, **kwargs)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\llm_math\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# flake8: noqa\\nfrom langchain_core.prompts.prompt import PromptTemplate\\n\\n_PROMPT_TEMPLATE = \"\"\"Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\n```text\\n37593**(1/5)\\n```\\n...numexpr.evaluate(\"37593**(1/5)\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nQuestion: {question}\\n\"\"\"\\n\\nPROMPT = PromptTemplate(\\n    input_variables=[\"question\"],\\n    template=_PROMPT_TEMPLATE,\\n)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\llm_math\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Chain that interprets a prompt and executes python code to do math.\\n\\nHeavily borrowed from https://replit.com/@amasad/gptpy?v=1#main.py\\n\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\llm_math\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Chain for summarization with self-verification.\"\"\"\\n\\nfrom __future__ import annotations\\n\\nimport warnings\\nfrom pathlib import Path\\nfrom typing import Any, Dict, List, Optional\\n\\nfrom langchain_core.callbacks import CallbackManagerForChainRun\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.prompts.prompt import PromptTemplate\\nfrom langchain_core.pydantic_v1 import Extra, root_validator\\n\\nfrom langchain.chains.base import Chain\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.chains.sequential import SequentialChain\\n\\nPROMPTS_DIR = Path(__file__).parent / \"prompts\"\\n\\nCREATE_ASSERTIONS_PROMPT = PromptTemplate.from_file(PROMPTS_DIR / \"create_facts.txt\")\\nCHECK_ASSERTIONS_PROMPT = PromptTemplate.from_file(PROMPTS_DIR / \"check_facts.txt\")\\nREVISED_SUMMARY_PROMPT = PromptTemplate.from_file(PROMPTS_DIR / \"revise_summary.txt\")\\nARE_ALL_TRUE_PROMPT = PromptTemplate.from_file(PROMPTS_DIR / \"are_all_true_prompt.txt\")' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\llm_summarization_checker\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _load_sequential_chain(\\n    llm: BaseLanguageModel,\\n    create_assertions_prompt: PromptTemplate,\\n    check_assertions_prompt: PromptTemplate,\\n    revised_summary_prompt: PromptTemplate,\\n    are_all_true_prompt: PromptTemplate,\\n    verbose: bool = False,\\n) -> SequentialChain:\\n    chain = SequentialChain(\\n        chains=[\\n            LLMChain(\\n                llm=llm,\\n                prompt=create_assertions_prompt,\\n                output_key=\"assertions\",\\n                verbose=verbose,\\n            ),\\n            LLMChain(\\n                llm=llm,\\n                prompt=check_assertions_prompt,\\n                output_key=\"checked_assertions\",\\n                verbose=verbose,\\n            ),\\n            LLMChain(\\n                llm=llm,\\n                prompt=revised_summary_prompt,\\n                output_key=\"revised_summary\",\\n                verbose=verbose,\\n            ),\\n            LLMChain(\\n                llm=llm,\\n                output_key=\"all_true\",\\n                prompt=are_all_true_prompt,\\n                verbose=verbose,\\n            ),\\n        ],\\n        input_variables=[\"summary\"],\\n        output_variables=[\"all_true\", \"revised_summary\"],\\n        verbose=verbose,\\n    )\\n    return chain\\n\\n\\nclass LLMSummarizationCheckerChain(Chain):\\n    \"\"\"Chain for question-answering with self-verification.\\n\\n    Example:\\n        .. code-block:: python\\n\\n            from langchain_community.llms import OpenAI\\n            from langchain.chains import LLMSummarizationCheckerChain\\n            llm = OpenAI(temperature=0.0)\\n            checker_chain = LLMSummarizationCheckerChain.from_llm(llm)\\n    \"\"\"\\n\\n    sequential_chain: SequentialChain\\n    llm: Optional[BaseLanguageModel] = None\\n    \"\"\"[Deprecated] LLM wrapper to use.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\llm_summarization_checker\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='create_assertions_prompt: PromptTemplate = CREATE_ASSERTIONS_PROMPT\\n    \"\"\"[Deprecated]\"\"\"\\n    check_assertions_prompt: PromptTemplate = CHECK_ASSERTIONS_PROMPT\\n    \"\"\"[Deprecated]\"\"\"\\n    revised_summary_prompt: PromptTemplate = REVISED_SUMMARY_PROMPT\\n    \"\"\"[Deprecated]\"\"\"\\n    are_all_true_prompt: PromptTemplate = ARE_ALL_TRUE_PROMPT\\n    \"\"\"[Deprecated]\"\"\"\\n\\n    input_key: str = \"query\"  #: :meta private:\\n    output_key: str = \"result\"  #: :meta private:\\n    max_checks: int = 2\\n    \"\"\"Maximum number of times to check the assertions. Default to double-checking.\"\"\"\\n\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n\\n        extra = Extra.forbid\\n        arbitrary_types_allowed = True\\n\\n    @root_validator(pre=True)\\n    def raise_deprecation(cls, values: Dict) -> Dict:\\n        if \"llm\" in values:\\n            warnings.warn(\\n                \"Directly instantiating an LLMSummarizationCheckerChain with an llm is \"\\n                \"deprecated. Please instantiate with\"\\n                \" sequential_chain argument or using the from_llm class method.\"\\n            )\\n            if \"sequential_chain\" not in values and values[\"llm\"] is not None:\\n                values[\"sequential_chain\"] = _load_sequential_chain(\\n                    values[\"llm\"],\\n                    values.get(\"create_assertions_prompt\", CREATE_ASSERTIONS_PROMPT),\\n                    values.get(\"check_assertions_prompt\", CHECK_ASSERTIONS_PROMPT),\\n                    values.get(\"revised_summary_prompt\", REVISED_SUMMARY_PROMPT),\\n                    values.get(\"are_all_true_prompt\", ARE_ALL_TRUE_PROMPT),\\n                    verbose=values.get(\"verbose\", False),\\n                )\\n        return values\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Return the singular input key.\\n\\n        :meta private:\\n        \"\"\"\\n        return [self.input_key]\\n\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Return the singular output key.\\n\\n        :meta private:\\n        \"\"\"\\n        return [self.output_key]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\llm_summarization_checker\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _call(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, str]:\\n        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\\n        all_true = False\\n        count = 0\\n        output = None\\n        original_input = inputs[self.input_key]\\n        chain_input = original_input\\n        while not all_true and count < self.max_checks:\\n            output = self.sequential_chain(\\n                {\"summary\": chain_input}, callbacks=_run_manager.get_child()\\n            )\\n            count += 1\\n\\n            if output[\"all_true\"].strip() == \"True\":\\n                break\\n\\n            if self.verbose:\\n                print(output[\"revised_summary\"])\\n\\n            chain_input = output[\"revised_summary\"]\\n\\n        if not output:\\n            raise ValueError(\"No output from chain\")\\n\\n        return {self.output_key: output[\"revised_summary\"].strip()}\\n\\n    @property\\n    def _chain_type(self) -> str:\\n        return \"llm_summarization_checker_chain\"\\n\\n    @classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        create_assertions_prompt: PromptTemplate = CREATE_ASSERTIONS_PROMPT,\\n        check_assertions_prompt: PromptTemplate = CHECK_ASSERTIONS_PROMPT,\\n        revised_summary_prompt: PromptTemplate = REVISED_SUMMARY_PROMPT,\\n        are_all_true_prompt: PromptTemplate = ARE_ALL_TRUE_PROMPT,\\n        verbose: bool = False,\\n        **kwargs: Any,\\n    ) -> LLMSummarizationCheckerChain:\\n        chain = _load_sequential_chain(\\n            llm,\\n            create_assertions_prompt,\\n            check_assertions_prompt,\\n            revised_summary_prompt,\\n            are_all_true_prompt,\\n            verbose=verbose,\\n        )\\n        return cls(sequential_chain=chain, verbose=verbose, **kwargs)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\llm_summarization_checker\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Summarization checker chain for verifying accuracy of text generation.\\n\\nChain that tries to verify the accuracy of text generation by splitting it into a\\nlist of facts, then checking if those facts are true or not, and rewriting\\nthe text to make it more truth-ful.  It will repeat this loop until it hits `max_tries`\\nor gets to a \"true\" output.\\n\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\llm_summarization_checker\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def __getattr__(name: str = \"\") -> None:\\n    \"\"\"Raise an error on import since is deprecated.\"\"\"\\n    raise ImportError(\\n        \"This module has been moved to langchain-experimental. \"\\n        \"For more details: https://github.com/langchain-ai/langchain/discussions/11352.\"\\n        \"To access this code, install it with `pip install langchain-experimental`.\"\\n        \"`from langchain_experimental.llm_symbolic_math.base \"\\n        \"import LLMSymbolicMathChain`\"\\n    )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\llm_symbolic_math\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Implement an LLM driven browser.\"\"\"\\nfrom __future__ import annotations\\n\\nimport warnings\\nfrom typing import Any, Dict, List, Optional\\n\\nfrom langchain_community.llms.openai import OpenAI\\nfrom langchain_core.callbacks import CallbackManagerForChainRun\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.pydantic_v1 import Extra, root_validator\\n\\nfrom langchain.chains.base import Chain\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.chains.natbot.prompt import PROMPT\\n\\n\\nclass NatBotChain(Chain):\\n    \"\"\"Implement an LLM driven browser.\\n\\n    **Security Note**: This toolkit provides code to control a web-browser.\\n\\n        The web-browser can be used to navigate to:\\n\\n        - Any URL (including any internal network URLs)\\n        - And local files\\n\\n        Exercise care if exposing this chain to end-users. Control who is able to\\n        access and use this chain, and isolate the network access of the server\\n        that hosts this chain.\\n\\n        See https://python.langchain.com/docs/security for more information.\\n\\n    Example:\\n        .. code-block:: python\\n\\n            from langchain.chains import NatBotChain\\n            natbot = NatBotChain.from_default(\"Buy me a new hat.\")\\n    \"\"\"\\n\\n    llm_chain: LLMChain\\n    objective: str\\n    \"\"\"Objective that NatBot is tasked with completing.\"\"\"\\n    llm: Optional[BaseLanguageModel] = None\\n    \"\"\"[Deprecated] LLM wrapper to use.\"\"\"\\n    input_url_key: str = \"url\"  #: :meta private:\\n    input_browser_content_key: str = \"browser_content\"  #: :meta private:\\n    previous_command: str = \"\"  #: :meta private:\\n    output_key: str = \"command\"  #: :meta private:\\n\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n\\n        extra = Extra.forbid\\n        arbitrary_types_allowed = True' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\natbot\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@root_validator(pre=True)\\n    def raise_deprecation(cls, values: Dict) -> Dict:\\n        if \"llm\" in values:\\n            warnings.warn(\\n                \"Directly instantiating an NatBotChain with an llm is deprecated. \"\\n                \"Please instantiate with llm_chain argument or using the from_llm \"\\n                \"class method.\"\\n            )\\n            if \"llm_chain\" not in values and values[\"llm\"] is not None:\\n                values[\"llm_chain\"] = LLMChain(llm=values[\"llm\"], prompt=PROMPT)\\n        return values\\n\\n    @classmethod\\n    def from_default(cls, objective: str, **kwargs: Any) -> NatBotChain:\\n        \"\"\"Load with default LLMChain.\"\"\"\\n        llm = OpenAI(temperature=0.5, best_of=10, n=3, max_tokens=50)\\n        return cls.from_llm(llm, objective, **kwargs)\\n\\n    @classmethod\\n    def from_llm(\\n        cls, llm: BaseLanguageModel, objective: str, **kwargs: Any\\n    ) -> NatBotChain:\\n        \"\"\"Load from LLM.\"\"\"\\n        llm_chain = LLMChain(llm=llm, prompt=PROMPT)\\n        return cls(llm_chain=llm_chain, objective=objective, **kwargs)\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Expect url and browser content.\\n\\n        :meta private:\\n        \"\"\"\\n        return [self.input_url_key, self.input_browser_content_key]\\n\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Return command.\\n\\n        :meta private:\\n        \"\"\"\\n        return [self.output_key]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\natbot\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _call(\\n        self,\\n        inputs: Dict[str, str],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, str]:\\n        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\\n        url = inputs[self.input_url_key]\\n        browser_content = inputs[self.input_browser_content_key]\\n        llm_cmd = self.llm_chain.predict(\\n            objective=self.objective,\\n            url=url[:100],\\n            previous_command=self.previous_command,\\n            browser_content=browser_content[:4500],\\n            callbacks=_run_manager.get_child(),\\n        )\\n        llm_cmd = llm_cmd.strip()\\n        self.previous_command = llm_cmd\\n        return {self.output_key: llm_cmd}\\n\\n    def execute(self, url: str, browser_content: str) -> str:\\n        \"\"\"Figure out next browser command to run.\\n\\n        Args:\\n            url: URL of the site currently on.\\n            browser_content: Content of the page as currently displayed by the browser.\\n\\n        Returns:\\n            Next browser command to run.\\n\\n        Example:\\n            .. code-block:: python\\n\\n                browser_content = \"....\"\\n                llm_command = natbot.run(\"www.google.com\", browser_content)\\n        \"\"\"\\n        _inputs = {\\n            self.input_url_key: url,\\n            self.input_browser_content_key: browser_content,\\n        }\\n        return self(_inputs)[self.output_key]\\n\\n    @property\\n    def _chain_type(self) -> str:\\n        return \"nat_bot_chain\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\natbot\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# flake8: noqa\\nimport time\\nfrom sys import platform\\nfrom typing import (\\n    TYPE_CHECKING,\\n    Any,\\n    Dict,\\n    Iterable,\\n    List,\\n    Optional,\\n    Set,\\n    Tuple,\\n    TypedDict,\\n    Union,\\n)\\n\\nif TYPE_CHECKING:\\n    from playwright.sync_api import Browser, CDPSession, Page, sync_playwright\\n\\nblack_listed_elements: Set[str] = {\\n    \"html\",\\n    \"head\",\\n    \"title\",\\n    \"meta\",\\n    \"iframe\",\\n    \"body\",\\n    \"script\",\\n    \"style\",\\n    \"path\",\\n    \"svg\",\\n    \"br\",\\n    \"::marker\",\\n}\\n\\n\\nclass ElementInViewPort(TypedDict):\\n    \"\"\"A typed dictionary containing information about elements in the viewport.\"\"\"\\n\\n    node_index: str\\n    backend_node_id: int\\n    node_name: Optional[str]\\n    node_value: Optional[str]\\n    node_meta: List[str]\\n    is_clickable: bool\\n    origin_x: int\\n    origin_y: int\\n    center_x: int\\n    center_y: int\\n\\n\\nclass Crawler:\\n    \"\"\"A crawler for web pages.\\n\\n    **Security Note**: This is an implementation of a crawler that uses a browser via\\n        Playwright.\\n\\n        This crawler can be used to load arbitrary webpages INCLUDING content\\n        from the local file system.\\n\\n        Control access to who can submit crawling requests and what network access\\n        the crawler has.\\n\\n        Make sure to scope permissions to the minimal permissions necessary for\\n        the application.\\n\\n        See https://python.langchain.com/docs/security for more information.\\n    \"\"\"\\n\\n    def __init__(self) -> None:\\n        try:\\n            from playwright.sync_api import sync_playwright\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import playwright python package. \"\\n                \"Please install it with `pip install playwright`.\"\\n            )\\n        self.browser: Browser = (\\n            sync_playwright().start().chromium.launch(headless=False)\\n        )\\n        self.page: Page = self.browser.new_page()\\n        self.page.set_viewport_size({\"width\": 1280, \"height\": 1080})\\n        self.page_element_buffer: Dict[int, ElementInViewPort]\\n        self.client: CDPSession' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\natbot\\\\crawler.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def go_to_page(self, url: str) -> None:\\n        self.page.goto(url=url if \"://\" in url else \"http://\" + url)\\n        self.client = self.page.context.new_cdp_session(self.page)\\n        self.page_element_buffer = {}\\n\\n    def scroll(self, direction: str) -> None:\\n        if direction == \"up\":\\n            self.page.evaluate(\\n                \"(document.scrollingElement || document.body).scrollTop = (document.scrollingElement || document.body).scrollTop - window.innerHeight;\"\\n            )\\n        elif direction == \"down\":\\n            self.page.evaluate(\\n                \"(document.scrollingElement || document.body).scrollTop = (document.scrollingElement || document.body).scrollTop + window.innerHeight;\"\\n            )\\n\\n    def click(self, id: Union[str, int]) -> None:\\n        # Inject javascript into the page which removes the target= attribute from all links\\n        js = \"\"\"\\n\\t\\tlinks = document.getElementsByTagName(\"a\");\\n\\t\\tfor (var i = 0; i < links.length; i++) {\\n\\t\\t\\tlinks[i].removeAttribute(\"target\");\\n\\t\\t}\\n\\t\\t\"\"\"\\n        self.page.evaluate(js)\\n\\n        element = self.page_element_buffer.get(int(id))\\n        if element:\\n            x: float = element[\"center_x\"]\\n            y: float = element[\"center_y\"]\\n\\n            self.page.mouse.click(x, y)\\n        else:\\n            print(\"Could not find element\")\\n\\n    def type(self, id: Union[str, int], text: str) -> None:\\n        self.click(id)\\n        self.page.keyboard.type(text)\\n\\n    def enter(self) -> None:\\n        self.page.keyboard.press(\"Enter\")\\n\\n    def crawl(self) -> List[str]:\\n        page = self.page\\n        page_element_buffer = self.page_element_buffer\\n        start = time.time()\\n\\n        page_state_as_text = []\\n\\n        device_pixel_ratio: float = page.evaluate(\"window.devicePixelRatio\")\\n        if platform == \"darwin\" and device_pixel_ratio == 1:  # lies\\n            device_pixel_ratio = 2' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\natbot\\\\crawler.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='win_upper_bound: float = page.evaluate(\"window.pageYOffset\")\\n        win_left_bound: float = page.evaluate(\"window.pageXOffset\")\\n        win_width: float = page.evaluate(\"window.screen.width\")\\n        win_height: float = page.evaluate(\"window.screen.height\")\\n        win_right_bound: float = win_left_bound + win_width\\n        win_lower_bound: float = win_upper_bound + win_height\\n\\n        # \\t\\tpercentage_progress_start = (win_upper_bound / document_scroll_height) * 100\\n        # \\t\\tpercentage_progress_end = (\\n        # \\t\\t\\t(win_height + win_upper_bound) / document_scroll_height\\n        # \\t\\t) * 100\\n        percentage_progress_start = 1\\n        percentage_progress_end = 2\\n\\n        page_state_as_text.append(\\n            {\\n                \"x\": 0,\\n                \"y\": 0,\\n                \"text\": \"[scrollbar {:0.2f}-{:0.2f}%]\".format(\\n                    round(percentage_progress_start, 2), round(percentage_progress_end)\\n                ),\\n            }\\n        )\\n\\n        tree = self.client.send(\\n            \"DOMSnapshot.captureSnapshot\",\\n            {\"computedStyles\": [], \"includeDOMRects\": True, \"includePaintOrder\": True},\\n        )\\n        strings: Dict[int, str] = tree[\"strings\"]\\n        document: Dict[str, Any] = tree[\"documents\"][0]\\n        nodes: Dict[str, Any] = document[\"nodes\"]\\n        backend_node_id: Dict[int, int] = nodes[\"backendNodeId\"]\\n        attributes: Dict[int, Dict[int, Any]] = nodes[\"attributes\"]\\n        node_value: Dict[int, int] = nodes[\"nodeValue\"]\\n        parent: Dict[int, int] = nodes[\"parentIndex\"]\\n        node_names: Dict[int, int] = nodes[\"nodeName\"]\\n        is_clickable: Set[int] = set(nodes[\"isClickable\"][\"index\"])\\n\\n        input_value: Dict[str, Any] = nodes[\"inputValue\"]\\n        input_value_index: List[int] = input_value[\"index\"]\\n        input_value_values: List[int] = input_value[\"value\"]\\n\\n        layout: Dict[str, Any] = document[\"layout\"]\\n        layout_node_index: List[int] = layout[\"nodeIndex\"]\\n        bounds: Dict[int, List[float]] = layout[\"bounds\"]\\n\\n        cursor: int = 0' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\natbot\\\\crawler.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='child_nodes: Dict[str, List[Dict[str, Any]]] = {}\\n        elements_in_view_port: List[ElementInViewPort] = []\\n\\n        anchor_ancestry: Dict[str, Tuple[bool, Optional[int]]] = {\"-1\": (False, None)}\\n        button_ancestry: Dict[str, Tuple[bool, Optional[int]]] = {\"-1\": (False, None)}\\n\\n        def convert_name(\\n            node_name: Optional[str], has_click_handler: Optional[bool]\\n        ) -> str:\\n            if node_name == \"a\":\\n                return \"link\"\\n            if node_name == \"input\":\\n                return \"input\"\\n            if node_name == \"img\":\\n                return \"img\"\\n            if (\\n                node_name == \"button\" or has_click_handler\\n            ):  # found pages that needed this quirk\\n                return \"button\"\\n            else:\\n                return \"text\"\\n\\n        def find_attributes(\\n            attributes: Dict[int, Any], keys: List[str]\\n        ) -> Dict[str, str]:\\n            values = {}\\n\\n            for [key_index, value_index] in zip(*(iter(attributes),) * 2):\\n                if value_index < 0:\\n                    continue\\n                key = strings[key_index]\\n                value = strings[value_index]\\n\\n                if key in keys:\\n                    values[key] = value\\n                    keys.remove(key)\\n\\n                    if not keys:\\n                        return values\\n\\n            return values\\n\\n        def add_to_hash_tree(\\n            hash_tree: Dict[str, Tuple[bool, Optional[int]]],\\n            tag: str,\\n            node_id: int,\\n            node_name: Optional[str],\\n            parent_id: int,\\n        ) -> Tuple[bool, Optional[int]]:\\n            parent_id_str = str(parent_id)\\n            if not parent_id_str in hash_tree:\\n                parent_name = strings[node_names[parent_id]].lower()\\n                grand_parent_id = parent[parent_id]\\n\\n                add_to_hash_tree(\\n                    hash_tree, tag, parent_id, parent_name, grand_parent_id\\n                )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\natbot\\\\crawler.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='is_parent_desc_anchor, anchor_id = hash_tree[parent_id_str]\\n\\n            # even if the anchor is nested in another anchor, we set the \"root\" for all descendants to be ::Self\\n            if node_name == tag:\\n                value: Tuple[bool, Optional[int]] = (True, node_id)\\n            elif (\\n                is_parent_desc_anchor\\n            ):  # reuse the parent\\'s anchor_id (which could be much higher in the tree)\\n                value = (True, anchor_id)\\n            else:\\n                value = (\\n                    False,\\n                    None,\\n                )  # not a descendant of an anchor, most likely it will become text, an interactive element or discarded\\n\\n            hash_tree[str(node_id)] = value\\n\\n            return value\\n\\n        for index, node_name_index in enumerate(node_names):\\n            node_parent = parent[index]\\n            node_name: Optional[str] = strings[node_name_index].lower()\\n\\n            is_ancestor_of_anchor, anchor_id = add_to_hash_tree(\\n                anchor_ancestry, \"a\", index, node_name, node_parent\\n            )\\n\\n            is_ancestor_of_button, button_id = add_to_hash_tree(\\n                button_ancestry, \"button\", index, node_name, node_parent\\n            )\\n\\n            try:\\n                cursor = layout_node_index.index(\\n                    index\\n                )  # todo replace this with proper cursoring, ignoring the fact this is O(n^2) for the moment\\n            except:\\n                continue\\n\\n            if node_name in black_listed_elements:\\n                continue\\n\\n            [x, y, width, height] = bounds[cursor]\\n            x /= device_pixel_ratio\\n            y /= device_pixel_ratio\\n            width /= device_pixel_ratio\\n            height /= device_pixel_ratio\\n\\n            elem_left_bound = x\\n            elem_top_bound = y\\n            elem_right_bound = x + width\\n            elem_lower_bound = y + height' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\natbot\\\\crawler.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='partially_is_in_viewport = (\\n                elem_left_bound < win_right_bound\\n                and elem_right_bound >= win_left_bound\\n                and elem_top_bound < win_lower_bound\\n                and elem_lower_bound >= win_upper_bound\\n            )\\n\\n            if not partially_is_in_viewport:\\n                continue\\n\\n            meta_data: List[str] = []\\n\\n            # inefficient to grab the same set of keys for kinds of objects, but it\\'s fine for now\\n            element_attributes = find_attributes(\\n                attributes[index], [\"type\", \"placeholder\", \"aria-label\", \"title\", \"alt\"]\\n            )\\n\\n            ancestor_exception = is_ancestor_of_anchor or is_ancestor_of_button\\n            ancestor_node_key = (\\n                None\\n                if not ancestor_exception\\n                else str(anchor_id)\\n                if is_ancestor_of_anchor\\n                else str(button_id)\\n            )\\n            ancestor_node = (\\n                None\\n                if not ancestor_exception\\n                else child_nodes.setdefault(str(ancestor_node_key), [])\\n            )\\n\\n            if node_name == \"#text\" and ancestor_exception and ancestor_node:\\n                text = strings[node_value[index]]\\n                if text == \"|\" or text == \"•\":\\n                    continue\\n                ancestor_node.append({\"type\": \"type\", \"value\": text})\\n            else:\\n                if (\\n                    node_name == \"input\" and element_attributes.get(\"type\") == \"submit\"\\n                ) or node_name == \"button\":\\n                    node_name = \"button\"\\n                    element_attributes.pop(\\n                        \"type\", None\\n                    )  # prevent [button ... (button)..]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\natbot\\\\crawler.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='for key in element_attributes:\\n                    if ancestor_exception and ancestor_node:\\n                        ancestor_node.append(\\n                            {\\n                                \"type\": \"attribute\",\\n                                \"key\": key,\\n                                \"value\": element_attributes[key],\\n                            }\\n                        )\\n                    else:\\n                        meta_data.append(element_attributes[key])\\n\\n            element_node_value = None\\n\\n            if node_value[index] >= 0:\\n                element_node_value = strings[node_value[index]]\\n                if (\\n                    element_node_value == \"|\"\\n                ):  # commonly used as a separator, does not add much context - lets save ourselves some token space\\n                    continue\\n            elif (\\n                node_name == \"input\"\\n                and index in input_value_index\\n                and element_node_value is None\\n            ):\\n                node_input_text_index = input_value_index.index(index)\\n                text_index = input_value_values[node_input_text_index]\\n                if node_input_text_index >= 0 and text_index >= 0:\\n                    element_node_value = strings[text_index]\\n\\n            # remove redundant elements\\n            if ancestor_exception and (node_name != \"a\" and node_name != \"button\"):\\n                continue\\n\\n            elements_in_view_port.append(\\n                {\\n                    \"node_index\": str(index),\\n                    \"backend_node_id\": backend_node_id[index],\\n                    \"node_name\": node_name,\\n                    \"node_value\": element_node_value,\\n                    \"node_meta\": meta_data,\\n                    \"is_clickable\": index in is_clickable,\\n                    \"origin_x\": int(x),\\n                    \"origin_y\": int(y),\\n                    \"center_x\": int(x + (width / 2)),\\n                    \"center_y\": int(y + (height / 2)),\\n                }\\n            )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\natbot\\\\crawler.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# lets filter further to remove anything that does not hold any text nor has click handlers + merge text from leaf#text nodes with the parent\\n        elements_of_interest = []\\n        id_counter = 0\\n\\n        for element in elements_in_view_port:\\n            node_index = element.get(\"node_index\")\\n            node_name = element.get(\"node_name\")\\n            element_node_value = element.get(\"node_value\")\\n            node_is_clickable = element.get(\"is_clickable\")\\n            node_meta_data: Optional[List[str]] = element.get(\"node_meta\")\\n\\n            inner_text = f\"{element_node_value} \" if element_node_value else \"\"\\n            meta = \"\"\\n\\n            if node_index in child_nodes:\\n                for child in child_nodes[node_index]:\\n                    entry_type = child.get(\"type\")\\n                    entry_value = child.get(\"value\")\\n\\n                    if entry_type == \"attribute\" and node_meta_data:\\n                        entry_key = child.get(\"key\")\\n                        node_meta_data.append(f\\'{entry_key}=\"{entry_value}\"\\')\\n                    else:\\n                        inner_text += f\"{entry_value} \"\\n\\n            if node_meta_data:\\n                meta_string = \" \".join(node_meta_data)\\n                meta = f\" {meta_string}\"\\n\\n            if inner_text != \"\":\\n                inner_text = f\"{inner_text.strip()}\"\\n\\n            converted_node_name = convert_name(node_name, node_is_clickable)\\n\\n            # not very elegant, more like a placeholder\\n            if (\\n                (converted_node_name != \"button\" or meta == \"\")\\n                and converted_node_name != \"link\"\\n                and converted_node_name != \"input\"\\n                and converted_node_name != \"img\"\\n                and converted_node_name != \"textarea\"\\n            ) and inner_text.strip() == \"\":\\n                continue\\n\\n            page_element_buffer[id_counter] = element' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\natbot\\\\crawler.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='if inner_text != \"\":\\n                elements_of_interest.append(\\n                    f\"\"\"<{converted_node_name} id={id_counter}{meta}>{inner_text}</{converted_node_name}>\"\"\"\\n                )\\n            else:\\n                elements_of_interest.append(\\n                    f\"\"\"<{converted_node_name} id={id_counter}{meta}/>\"\"\"\\n                )\\n            id_counter += 1\\n\\n        print(\"Parsing time: {:0.2f} seconds\".format(time.time() - start))\\n        return elements_of_interest' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\natbot\\\\crawler.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# flake8: noqa\\nfrom langchain_core.prompts.prompt import PromptTemplate\\n\\n_PROMPT_TEMPLATE = \"\"\"\\nYou are an agents controlling a browser. You are given:\\n\\n\\t(1) an objective that you are trying to achieve\\n\\t(2) the URL of your current web page\\n\\t(3) a simplified text description of what\\'s visible in the browser window (more on that below)\\n\\nYou can issue these commands:\\n\\tSCROLL UP - scroll up one page\\n\\tSCROLL DOWN - scroll down one page\\n\\tCLICK X - click on a given element. You can only click on links, buttons, and inputs!\\n\\tTYPE X \"TEXT\" - type the specified text into the input with id X\\n\\tTYPESUBMIT X \"TEXT\" - same as TYPE above, except then it presses ENTER to submit the form\\n\\nThe format of the browser content is highly simplified; all formatting elements are stripped.\\nInteractive elements such as links, inputs, buttons are represented like this:\\n\\n\\t\\t<link id=1>text</link>\\n\\t\\t<button id=2>text</button>\\n\\t\\t<input id=3>text</input>\\n\\nImages are rendered as their alt text like this:\\n\\n\\t\\t<img id=4 alt=\"\"/>\\n\\nBased on your given objective, issue whatever command you believe will get you closest to achieving your goal.\\nYou always start on Google; you should submit a search query to Google that will take you to the best page for\\nachieving your objective. And then interact with that page to achieve your objective.\\n\\nIf you find yourself on Google and there are no search results displayed yet, you should probably issue a command\\nlike \"TYPESUBMIT 7 \"search query\"\" to get to a more useful page.\\n\\nThen, if you find yourself on a Google search results page, you might issue the command \"CLICK 24\" to click\\non the first link in the search results. (If your previous command was a TYPESUBMIT your next command should\\nprobably be a CLICK.)\\n\\nDon\\'t try to interact with elements that you can\\'t see.\\n\\nHere are some examples:' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\natbot\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='EXAMPLE 1:\\n==================================================\\nCURRENT BROWSER CONTENT:\\n------------------\\n<link id=1>About</link>\\n<link id=2>Store</link>\\n<link id=3>Gmail</link>\\n<link id=4>Images</link>\\n<link id=5>(Google apps)</link>\\n<link id=6>Sign in</link>\\n<img id=7 alt=\"(Google)\"/>\\n<input id=8 alt=\"Search\"></input>\\n<button id=9>(Search by voice)</button>\\n<button id=10>(Google Search)</button>\\n<button id=11>(I\\'m Feeling Lucky)</button>\\n<link id=12>Advertising</link>\\n<link id=13>Business</link>\\n<link id=14>How Search works</link>\\n<link id=15>Carbon neutral since 2007</link>\\n<link id=16>Privacy</link>\\n<link id=17>Terms</link>\\n<text id=18>Settings</text>\\n------------------\\nOBJECTIVE: Find a 2 bedroom house for sale in Anchorage AK for under $750k\\nCURRENT URL: https://www.google.com/\\nYOUR COMMAND:\\nTYPESUBMIT 8 \"anchorage redfin\"\\n==================================================\\n\\nEXAMPLE 2:\\n==================================================\\nCURRENT BROWSER CONTENT:\\n------------------\\n<link id=1>About</link>\\n<link id=2>Store</link>\\n<link id=3>Gmail</link>\\n<link id=4>Images</link>\\n<link id=5>(Google apps)</link>\\n<link id=6>Sign in</link>\\n<img id=7 alt=\"(Google)\"/>\\n<input id=8 alt=\"Search\"></input>\\n<button id=9>(Search by voice)</button>\\n<button id=10>(Google Search)</button>\\n<button id=11>(I\\'m Feeling Lucky)</button>\\n<link id=12>Advertising</link>\\n<link id=13>Business</link>\\n<link id=14>How Search works</link>\\n<link id=15>Carbon neutral since 2007</link>\\n<link id=16>Privacy</link>\\n<link id=17>Terms</link>\\n<text id=18>Settings</text>\\n------------------\\nOBJECTIVE: Make a reservation for 4 at Dorsia at 8pm\\nCURRENT URL: https://www.google.com/\\nYOUR COMMAND:\\nTYPESUBMIT 8 \"dorsia nyc opentable\"\\n==================================================' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\natbot\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='EXAMPLE 3:\\n==================================================\\nCURRENT BROWSER CONTENT:\\n------------------\\n<button id=1>For Businesses</button>\\n<button id=2>Mobile</button>\\n<button id=3>Help</button>\\n<button id=4 alt=\"Language Picker\">EN</button>\\n<link id=5>OpenTable logo</link>\\n<button id=6 alt =\"search\">Search</button>\\n<text id=7>Find your table for any occasion</text>\\n<button id=8>(Date selector)</button>\\n<text id=9>Sep 28, 2022</text>\\n<text id=10>7:00 PM</text>\\n<text id=11>2 people</text>\\n<input id=12 alt=\"Location, Restaurant, or Cuisine\"></input>\\n<button id=13>Let’s go</button>\\n<text id=14>It looks like you\\'re in Peninsula. Not correct?</text>\\n<button id=15>Get current location</button>\\n<button id=16>Next</button>\\n------------------\\nOBJECTIVE: Make a reservation for 4 for dinner at Dorsia in New York City at 8pm\\nCURRENT URL: https://www.opentable.com/\\nYOUR COMMAND:\\nTYPESUBMIT 12 \"dorsia new york city\"\\n==================================================\\n\\nThe current browser content, objective, and current URL follow. Reply with your next command to the browser.\\n\\nCURRENT BROWSER CONTENT:\\n------------------\\n{browser_content}\\n------------------\\n\\nOBJECTIVE: {objective}\\nCURRENT URL: {url}\\nPREVIOUS COMMAND: {previous_command}\\nYOUR COMMAND:\\n\"\"\"\\nPROMPT = PromptTemplate(\\n    input_variables=[\"browser_content\", \"url\", \"previous_command\", \"objective\"],\\n    template=_PROMPT_TEMPLATE,\\n)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\natbot\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Implement a GPT-3 driven browser.\\n\\nHeavily influenced from https://github.com/nat/natbot\\n\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\natbot\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Methods for creating chains that use OpenAI function-calling APIs.\"\"\"\\nfrom typing import (\\n    Any,\\n    Callable,\\n    Dict,\\n    Optional,\\n    Sequence,\\n    Type,\\n    Union,\\n)\\n\\nfrom langchain_core._api import deprecated\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.output_parsers import (\\n    BaseGenerationOutputParser,\\n    BaseLLMOutputParser,\\n    BaseOutputParser,\\n)\\nfrom langchain_core.prompts import BasePromptTemplate\\nfrom langchain_core.pydantic_v1 import BaseModel\\nfrom langchain_core.runnables import Runnable\\nfrom langchain_core.utils.function_calling import (\\n    PYTHON_TO_JSON_TYPES,\\n    convert_to_openai_function,\\n)\\n\\nfrom langchain.chains import LLMChain\\nfrom langchain.output_parsers.openai_functions import (\\n    JsonOutputFunctionsParser,\\n    PydanticAttrOutputFunctionsParser,\\n    PydanticOutputFunctionsParser,\\n)\\n\\n\\ndef get_openai_output_parser(\\n    functions: Sequence[Union[Dict[str, Any], Type[BaseModel], Callable]],\\n) -> Union[BaseOutputParser, BaseGenerationOutputParser]:\\n    \"\"\"Get the appropriate function output parser given the user functions.\\n\\n    Args:\\n        functions: Sequence where element is a dictionary, a pydantic.BaseModel class,\\n            or a Python function. If a dictionary is passed in, it is assumed to\\n            already be a valid OpenAI function.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\openai_functions\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Returns:\\n        A PydanticOutputFunctionsParser if functions are Pydantic classes, otherwise\\n            a JsonOutputFunctionsParser. If there\\'s only one function and it is\\n            not a Pydantic class, then the output parser will automatically extract\\n            only the function arguments and not the function name.\\n    \"\"\"\\n    function_names = [convert_to_openai_function(f)[\"name\"] for f in functions]\\n    if isinstance(functions[0], type) and issubclass(functions[0], BaseModel):\\n        if len(functions) > 1:\\n            pydantic_schema: Union[Dict, Type[BaseModel]] = {\\n                name: fn for name, fn in zip(function_names, functions)\\n            }\\n        else:\\n            pydantic_schema = functions[0]\\n        output_parser: Union[\\n            BaseOutputParser, BaseGenerationOutputParser\\n        ] = PydanticOutputFunctionsParser(pydantic_schema=pydantic_schema)\\n    else:\\n        output_parser = JsonOutputFunctionsParser(args_only=len(functions) <= 1)\\n    return output_parser\\n\\n\\ndef create_openai_fn_runnable(\\n    functions: Sequence[Union[Dict[str, Any], Type[BaseModel], Callable]],\\n    llm: Runnable,\\n    prompt: BasePromptTemplate,\\n    *,\\n    enforce_single_function_usage: bool = True,\\n    output_parser: Optional[Union[BaseOutputParser, BaseGenerationOutputParser]] = None,\\n    **kwargs: Any,\\n) -> Runnable:\\n    \"\"\"Create a runnable sequence that uses OpenAI functions.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\openai_functions\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Args:\\n        functions: A sequence of either dictionaries, pydantic.BaseModels classes, or\\n            Python functions. If dictionaries are passed in, they are assumed to\\n            already be a valid OpenAI functions. If only a single\\n            function is passed in, then it will be enforced that the model use that\\n            function. pydantic.BaseModels and Python functions should have docstrings\\n            describing what the function does. For best results, pydantic.BaseModels\\n            should have descriptions of the parameters and Python functions should have\\n            Google Python style args descriptions in the docstring. Additionally,\\n            Python functions should only use primitive types (str, int, float, bool) or\\n            pydantic.BaseModels for arguments.\\n        llm: Language model to use, assumed to support the OpenAI function-calling API.\\n        prompt: BasePromptTemplate to pass to the model.\\n        enforce_single_function_usage: only used if a single function is passed in. If\\n            True, then the model will be forced to use the given function. If False,\\n            then the model will be given the option to use the given function or not.\\n        output_parser: BaseLLMOutputParser to use for parsing model outputs. By default\\n            will be inferred from the function types. If pydantic.BaseModels are passed\\n            in, then the OutputParser will try to parse outputs using those. Otherwise\\n            model outputs will simply be parsed as JSON. If multiple functions are\\n            passed in and they are not pydantic.BaseModels, the chain output will\\n            include both the name of the function that was returned and the arguments\\n            to pass to the function.\\n\\n    Returns:\\n        A runnable sequence that will pass in the given functions to the model when run.\\n\\n    Example:\\n        .. code-block:: python\\n\\n                from typing import Optional' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\openai_functions\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain.chains.openai_functions import create_openai_fn_runnable\\n                from langchain_community.chat_models import ChatOpenAI\\n                from langchain_core.prompts import ChatPromptTemplate\\n                from langchain_core.pydantic_v1 import BaseModel, Field\\n\\n\\n                class RecordPerson(BaseModel):\\n                    \\\\\"\\\\\"\\\\\"Record some identifying information about a person.\\\\\"\\\\\"\\\\\"\\n\\n                    name: str = Field(..., description=\"The person\\'s name\")\\n                    age: int = Field(..., description=\"The person\\'s age\")\\n                    fav_food: Optional[str] = Field(None, description=\"The person\\'s favorite food\")\\n\\n\\n                class RecordDog(BaseModel):\\n                    \\\\\"\\\\\"\\\\\"Record some identifying information about a dog.\\\\\"\\\\\"\\\\\"\\n\\n                    name: str = Field(..., description=\"The dog\\'s name\")\\n                    color: str = Field(..., description=\"The dog\\'s color\")\\n                    fav_food: Optional[str] = Field(None, description=\"The dog\\'s favorite food\")' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\openai_functions\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\\n                prompt = ChatPromptTemplate.from_messages(\\n                    [\\n                        (\"system\", \"You are a world class algorithm for recording entities.\"),\\n                        (\"human\", \"Make calls to the relevant function to record the entities in the following input: {input}\"),\\n                        (\"human\", \"Tip: Make sure to answer in the correct format\"),\\n                    ]\\n                )\\n                chain = create_openai_fn_runnable([RecordPerson, RecordDog], llm, prompt)\\n                chain.invoke({\"input\": \"Harry was a chubby brown beagle who loved chicken\"})\\n                # -> RecordDog(name=\"Harry\", color=\"brown\", fav_food=\"chicken\")\\n    \"\"\"  # noqa: E501\\n    if not functions:\\n        raise ValueError(\"Need to pass in at least one function. Received zero.\")\\n    openai_functions = [convert_to_openai_function(f) for f in functions]\\n    llm_kwargs: Dict[str, Any] = {\"functions\": openai_functions, **kwargs}\\n    if len(openai_functions) == 1 and enforce_single_function_usage:\\n        llm_kwargs[\"function_call\"] = {\"name\": openai_functions[0][\"name\"]}\\n    output_parser = output_parser or get_openai_output_parser(functions)\\n    return prompt | llm.bind(**llm_kwargs) | output_parser\\n\\n\\ndef create_structured_output_runnable(\\n    output_schema: Union[Dict[str, Any], Type[BaseModel]],\\n    llm: Runnable,\\n    prompt: BasePromptTemplate,\\n    *,\\n    output_parser: Optional[Union[BaseOutputParser, BaseGenerationOutputParser]] = None,\\n    **kwargs: Any,\\n) -> Runnable:\\n    \"\"\"Create a runnable that uses an OpenAI function to get a structured output.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\openai_functions\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Args:\\n        output_schema: Either a dictionary or pydantic.BaseModel class. If a dictionary\\n            is passed in, it\\'s assumed to already be a valid JsonSchema.\\n            For best results, pydantic.BaseModels should have docstrings describing what\\n            the schema represents and descriptions for the parameters.\\n        llm: Language model to use, assumed to support the OpenAI function-calling API.\\n        prompt: BasePromptTemplate to pass to the model.\\n        output_parser: BaseLLMOutputParser to use for parsing model outputs. By default\\n            will be inferred from the function types. If pydantic.BaseModels are passed\\n            in, then the OutputParser will try to parse outputs using those. Otherwise\\n            model outputs will simply be parsed as JSON.\\n\\n    Returns:\\n        A runnable sequence that will pass the given function to the model when run.\\n\\n    Example:\\n        .. code-block:: python\\n\\n                from typing import Optional\\n\\n                from langchain.chains.openai_functions import create_structured_output_runnable\\n                from langchain_community.chat_models import ChatOpenAI\\n                from langchain_core.prompts import ChatPromptTemplate\\n                from langchain_core.pydantic_v1 import BaseModel, Field\\n\\n                class Dog(BaseModel):\\n                    \\\\\"\\\\\"\\\\\"Identifying information about a dog.\\\\\"\\\\\"\\\\\"\\n\\n                    name: str = Field(..., description=\"The dog\\'s name\")\\n                    color: str = Field(..., description=\"The dog\\'s color\")\\n                    fav_food: Optional[str] = Field(None, description=\"The dog\\'s favorite food\")' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\openai_functions\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='llm = ChatOpenAI(model=\"gpt-3.5-turbo-0613\", temperature=0)\\n                prompt = ChatPromptTemplate.from_messages(\\n                    [\\n                        (\"system\", \"You are a world class algorithm for extracting information in structured formats.\"),\\n                        (\"human\", \"Use the given format to extract information from the following input: {input}\"),\\n                        (\"human\", \"Tip: Make sure to answer in the correct format\"),\\n                    ]\\n                )\\n                chain = create_structured_output_runnable(Dog, llm, prompt)\\n                chain.invoke({\"input\": \"Harry was a chubby brown beagle who loved chicken\"})\\n                # -> Dog(name=\"Harry\", color=\"brown\", fav_food=\"chicken\")\\n    \"\"\"  # noqa: E501\\n    if isinstance(output_schema, dict):\\n        function: Any = {\\n            \"name\": \"output_formatter\",\\n            \"description\": (\\n                \"Output formatter. Should always be used to format your response to the\"\\n                \" user.\"\\n            ),\\n            \"parameters\": output_schema,\\n        }\\n    else:\\n\\n        class _OutputFormatter(BaseModel):\\n            \"\"\"Output formatter. Should always be used to format your response to the user.\"\"\"  # noqa: E501\\n\\n            output: output_schema  # type: ignore\\n\\n        function = _OutputFormatter\\n        output_parser = output_parser or PydanticAttrOutputFunctionsParser(\\n            pydantic_schema=_OutputFormatter, attr_name=\"output\"\\n        )\\n    return create_openai_fn_runnable(\\n        [function],\\n        llm,\\n        prompt,\\n        output_parser=output_parser,\\n        **kwargs,\\n    )\\n\\n\\n\"\"\" --- Legacy --- \"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\openai_functions\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@deprecated(since=\"0.1.1\", removal=\"0.2.0\", alternative=\"create_openai_fn_runnable\")\\ndef create_openai_fn_chain(\\n    functions: Sequence[Union[Dict[str, Any], Type[BaseModel], Callable]],\\n    llm: BaseLanguageModel,\\n    prompt: BasePromptTemplate,\\n    *,\\n    enforce_single_function_usage: bool = True,\\n    output_key: str = \"function\",\\n    output_parser: Optional[BaseLLMOutputParser] = None,\\n    **kwargs: Any,\\n) -> LLMChain:\\n    \"\"\"[Legacy] Create an LLM chain that uses OpenAI functions.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\openai_functions\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Args:\\n        functions: A sequence of either dictionaries, pydantic.BaseModels classes, or\\n            Python functions. If dictionaries are passed in, they are assumed to\\n            already be a valid OpenAI functions. If only a single\\n            function is passed in, then it will be enforced that the model use that\\n            function. pydantic.BaseModels and Python functions should have docstrings\\n            describing what the function does. For best results, pydantic.BaseModels\\n            should have descriptions of the parameters and Python functions should have\\n            Google Python style args descriptions in the docstring. Additionally,\\n            Python functions should only use primitive types (str, int, float, bool) or\\n            pydantic.BaseModels for arguments.\\n        llm: Language model to use, assumed to support the OpenAI function-calling API.\\n        prompt: BasePromptTemplate to pass to the model.\\n        enforce_single_function_usage: only used if a single function is passed in. If\\n            True, then the model will be forced to use the given function. If False,\\n            then the model will be given the option to use the given function or not.\\n        output_key: The key to use when returning the output in LLMChain.__call__.\\n        output_parser: BaseLLMOutputParser to use for parsing model outputs. By default\\n            will be inferred from the function types. If pydantic.BaseModels are passed\\n            in, then the OutputParser will try to parse outputs using those. Otherwise\\n            model outputs will simply be parsed as JSON. If multiple functions are\\n            passed in and they are not pydantic.BaseModels, the chain output will\\n            include both the name of the function that was returned and the arguments\\n            to pass to the function.\\n\\n    Returns:\\n        An LLMChain that will pass in the given functions to the model when run.\\n\\n    Example:\\n        .. code-block:: python\\n\\n                from typing import Optional' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\openai_functions\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain.chains.openai_functions import create_openai_fn_chain\\n                from langchain_community.chat_models import ChatOpenAI\\n                from langchain_core.prompts import ChatPromptTemplate\\n\\n                from langchain_core.pydantic_v1 import BaseModel, Field\\n\\n\\n                class RecordPerson(BaseModel):\\n                    \\\\\"\\\\\"\\\\\"Record some identifying information about a person.\\\\\"\\\\\"\\\\\"\\n\\n                    name: str = Field(..., description=\"The person\\'s name\")\\n                    age: int = Field(..., description=\"The person\\'s age\")\\n                    fav_food: Optional[str] = Field(None, description=\"The person\\'s favorite food\")\\n\\n\\n                class RecordDog(BaseModel):\\n                    \\\\\"\\\\\"\\\\\"Record some identifying information about a dog.\\\\\"\\\\\"\\\\\"\\n\\n                    name: str = Field(..., description=\"The dog\\'s name\")\\n                    color: str = Field(..., description=\"The dog\\'s color\")\\n                    fav_food: Optional[str] = Field(None, description=\"The dog\\'s favorite food\")' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\openai_functions\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\\n                prompt = ChatPromptTemplate.from_messages(\\n                    [\\n                        (\"system\", \"You are a world class algorithm for recording entities.\"),\\n                        (\"human\", \"Make calls to the relevant function to record the entities in the following input: {input}\"),\\n                        (\"human\", \"Tip: Make sure to answer in the correct format\"),\\n                    ]\\n                )\\n                chain = create_openai_fn_chain([RecordPerson, RecordDog], llm, prompt)\\n                chain.run(\"Harry was a chubby brown beagle who loved chicken\")\\n                # -> RecordDog(name=\"Harry\", color=\"brown\", fav_food=\"chicken\")\\n    \"\"\"  # noqa: E501\\n    if not functions:\\n        raise ValueError(\"Need to pass in at least one function. Received zero.\")\\n    openai_functions = [convert_to_openai_function(f) for f in functions]\\n    output_parser = output_parser or get_openai_output_parser(functions)\\n    llm_kwargs: Dict[str, Any] = {\\n        \"functions\": openai_functions,\\n    }\\n    if len(openai_functions) == 1 and enforce_single_function_usage:\\n        llm_kwargs[\"function_call\"] = {\"name\": openai_functions[0][\"name\"]}\\n    llm_chain = LLMChain(\\n        llm=llm,\\n        prompt=prompt,\\n        output_parser=output_parser,\\n        llm_kwargs=llm_kwargs,\\n        output_key=output_key,\\n        **kwargs,\\n    )\\n    return llm_chain\\n\\n\\n@deprecated(\\n    since=\"0.1.1\", removal=\"0.2.0\", alternative=\"create_structured_output_runnable\"\\n)\\ndef create_structured_output_chain(\\n    output_schema: Union[Dict[str, Any], Type[BaseModel]],\\n    llm: BaseLanguageModel,\\n    prompt: BasePromptTemplate,\\n    *,\\n    output_key: str = \"function\",\\n    output_parser: Optional[BaseLLMOutputParser] = None,\\n    **kwargs: Any,\\n) -> LLMChain:\\n    \"\"\"[Legacy] Create an LLMChain that uses an OpenAI function to get a structured output.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\openai_functions\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Args:\\n        output_schema: Either a dictionary or pydantic.BaseModel class. If a dictionary\\n            is passed in, it\\'s assumed to already be a valid JsonSchema.\\n            For best results, pydantic.BaseModels should have docstrings describing what\\n            the schema represents and descriptions for the parameters.\\n        llm: Language model to use, assumed to support the OpenAI function-calling API.\\n        prompt: BasePromptTemplate to pass to the model.\\n        output_key: The key to use when returning the output in LLMChain.__call__.\\n        output_parser: BaseLLMOutputParser to use for parsing model outputs. By default\\n            will be inferred from the function types. If pydantic.BaseModels are passed\\n            in, then the OutputParser will try to parse outputs using those. Otherwise\\n            model outputs will simply be parsed as JSON.\\n\\n    Returns:\\n        An LLMChain that will pass the given function to the model.\\n\\n    Example:\\n        .. code-block:: python\\n\\n                from typing import Optional\\n\\n                from langchain.chains.openai_functions import create_structured_output_chain\\n                from langchain_community.chat_models import ChatOpenAI\\n                from langchain_core.prompts import ChatPromptTemplate\\n\\n                from langchain_core.pydantic_v1 import BaseModel, Field\\n\\n                class Dog(BaseModel):\\n                    \\\\\"\\\\\"\\\\\"Identifying information about a dog.\\\\\"\\\\\"\\\\\"\\n\\n                    name: str = Field(..., description=\"The dog\\'s name\")\\n                    color: str = Field(..., description=\"The dog\\'s color\")\\n                    fav_food: Optional[str] = Field(None, description=\"The dog\\'s favorite food\")' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\openai_functions\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='llm = ChatOpenAI(model=\"gpt-3.5-turbo-0613\", temperature=0)\\n                prompt = ChatPromptTemplate.from_messages(\\n                    [\\n                        (\"system\", \"You are a world class algorithm for extracting information in structured formats.\"),\\n                        (\"human\", \"Use the given format to extract information from the following input: {input}\"),\\n                        (\"human\", \"Tip: Make sure to answer in the correct format\"),\\n                    ]\\n                )\\n                chain = create_structured_output_chain(Dog, llm, prompt)\\n                chain.run(\"Harry was a chubby brown beagle who loved chicken\")\\n                # -> Dog(name=\"Harry\", color=\"brown\", fav_food=\"chicken\")\\n    \"\"\"  # noqa: E501\\n    if isinstance(output_schema, dict):\\n        function: Any = {\\n            \"name\": \"output_formatter\",\\n            \"description\": (\\n                \"Output formatter. Should always be used to format your response to the\"\\n                \" user.\"\\n            ),\\n            \"parameters\": output_schema,\\n        }\\n    else:\\n\\n        class _OutputFormatter(BaseModel):\\n            \"\"\"Output formatter. Should always be used to format your response to the user.\"\"\"  # noqa: E501\\n\\n            output: output_schema  # type: ignore\\n\\n        function = _OutputFormatter\\n        output_parser = output_parser or PydanticAttrOutputFunctionsParser(\\n            pydantic_schema=_OutputFormatter, attr_name=\"output\"\\n        )\\n    return create_openai_fn_chain(\\n        [function],\\n        llm,\\n        prompt,\\n        output_key=output_key,\\n        output_parser=output_parser,\\n        **kwargs,\\n    )\\n\\n\\n__all__ = [\\n    \"create_openai_fn_chain\",\\n    \"create_openai_fn_runnable\",\\n    \"create_structured_output_chain\",\\n    \"create_structured_output_runnable\",\\n    \"get_openai_output_parser\",\\n    \"PYTHON_TO_JSON_TYPES\",\\n    \"convert_to_openai_function\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\openai_functions\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from typing import Iterator, List\\n\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.messages import HumanMessage, SystemMessage\\nfrom langchain_core.prompts.chat import ChatPromptTemplate, HumanMessagePromptTemplate\\nfrom langchain_core.pydantic_v1 import BaseModel, Field\\n\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.chains.openai_functions.utils import get_llm_kwargs\\nfrom langchain.output_parsers.openai_functions import (\\n    PydanticOutputFunctionsParser,\\n)\\n\\n\\nclass FactWithEvidence(BaseModel):\\n    \"\"\"Class representing a single statement.\\n\\n    Each fact has a body and a list of sources.\\n    If there are multiple facts make sure to break them apart\\n    such that each one only uses a set of sources that are relevant to it.\\n    \"\"\"\\n\\n    fact: str = Field(..., description=\"Body of the sentence, as part of a response\")\\n    substring_quote: List[str] = Field(\\n        ...,\\n        description=(\\n            \"Each source should be a direct quote from the context, \"\\n            \"as a substring of the original content\"\\n        ),\\n    )\\n\\n    def _get_span(self, quote: str, context: str, errs: int = 100) -> Iterator[str]:\\n        import regex\\n\\n        minor = quote\\n        major = context\\n\\n        errs_ = 0\\n        s = regex.search(f\"({minor}){{e<={errs_}}}\", major)\\n        while s is None and errs_ <= errs:\\n            errs_ += 1\\n            s = regex.search(f\"({minor}){{e<={errs_}}}\", major)\\n\\n        if s is not None:\\n            yield from s.spans()\\n\\n    def get_spans(self, context: str) -> Iterator[str]:\\n        for quote in self.substring_quote:\\n            yield from self._get_span(quote, context)\\n\\n\\nclass QuestionAnswer(BaseModel):\\n    \"\"\"A question and its answer as a list of facts each one should have a source.\\n    each sentence contains a body and a list of sources.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\openai_functions\\\\citation_fuzzy_match.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='question: str = Field(..., description=\"Question that was asked\")\\n    answer: List[FactWithEvidence] = Field(\\n        ...,\\n        description=(\\n            \"Body of the answer, each fact should be \"\\n            \"its separate object with a body and a list of sources\"\\n        ),\\n    )\\n\\n\\ndef create_citation_fuzzy_match_chain(llm: BaseLanguageModel) -> LLMChain:\\n    \"\"\"Create a citation fuzzy match chain.\\n\\n    Args:\\n        llm: Language model to use for the chain.\\n\\n    Returns:\\n        Chain (LLMChain) that can be used to answer questions with citations.\\n    \"\"\"\\n    output_parser = PydanticOutputFunctionsParser(pydantic_schema=QuestionAnswer)\\n    schema = QuestionAnswer.schema()\\n    function = {\\n        \"name\": schema[\"title\"],\\n        \"description\": schema[\"description\"],\\n        \"parameters\": schema,\\n    }\\n    llm_kwargs = get_llm_kwargs(function)\\n    messages = [\\n        SystemMessage(\\n            content=(\\n                \"You are a world class algorithm to answer \"\\n                \"questions with correct and exact citations.\"\\n            )\\n        ),\\n        HumanMessage(content=\"Answer question using the following context\"),\\n        HumanMessagePromptTemplate.from_template(\"{context}\"),\\n        HumanMessagePromptTemplate.from_template(\"Question: {question}\"),\\n        HumanMessage(\\n            content=(\\n                \"Tips: Make sure to cite your sources, \"\\n                \"and use the exact words from the context.\"\\n            )\\n        ),\\n    ]\\n    prompt = ChatPromptTemplate(messages=messages)\\n\\n    chain = LLMChain(\\n        llm=llm,\\n        prompt=prompt,\\n        llm_kwargs=llm_kwargs,\\n        output_parser=output_parser,\\n    )\\n    return chain' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\openai_functions\\\\citation_fuzzy_match.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from typing import Any, List, Optional\\n\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.prompts import BasePromptTemplate, ChatPromptTemplate\\nfrom langchain_core.pydantic_v1 import BaseModel\\n\\nfrom langchain.chains.base import Chain\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.chains.openai_functions.utils import (\\n    _convert_schema,\\n    _resolve_schema_references,\\n    get_llm_kwargs,\\n)\\nfrom langchain.output_parsers.openai_functions import (\\n    JsonKeyOutputFunctionsParser,\\n    PydanticAttrOutputFunctionsParser,\\n)\\n\\n\\ndef _get_extraction_function(entity_schema: dict) -> dict:\\n    return {\\n        \"name\": \"information_extraction\",\\n        \"description\": \"Extracts the relevant information from the passage.\",\\n        \"parameters\": {\\n            \"type\": \"object\",\\n            \"properties\": {\\n                \"info\": {\"type\": \"array\", \"items\": _convert_schema(entity_schema)}\\n            },\\n            \"required\": [\"info\"],\\n        },\\n    }\\n\\n\\n_EXTRACTION_TEMPLATE = \"\"\"Extract and save the relevant entities mentioned \\\\\\nin the following passage together with their properties.\\n\\nOnly extract the properties mentioned in the \\'information_extraction\\' function.\\n\\nIf a property is not present and is not required in the function parameters, do not include it in the output.\\n\\nPassage:\\n{input}\\n\"\"\"  # noqa: E501\\n\\n\\ndef create_extraction_chain(\\n    schema: dict,\\n    llm: BaseLanguageModel,\\n    prompt: Optional[BasePromptTemplate] = None,\\n    tags: Optional[List[str]] = None,\\n    verbose: bool = False,\\n) -> Chain:\\n    \"\"\"Creates a chain that extracts information from a passage.\\n\\n    Args:\\n        schema: The schema of the entities to extract.\\n        llm: The language model to use.\\n        prompt: The prompt to use for extraction.\\n        verbose: Whether to run in verbose mode. In verbose mode, some intermediate\\n            logs will be printed to the console. Defaults to the global `verbose` value,\\n            accessible via `langchain.globals.get_verbose()`.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\openai_functions\\\\extraction.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Returns:\\n        Chain that can be used to extract information from a passage.\\n    \"\"\"\\n    function = _get_extraction_function(schema)\\n    extraction_prompt = prompt or ChatPromptTemplate.from_template(_EXTRACTION_TEMPLATE)\\n    output_parser = JsonKeyOutputFunctionsParser(key_name=\"info\")\\n    llm_kwargs = get_llm_kwargs(function)\\n    chain = LLMChain(\\n        llm=llm,\\n        prompt=extraction_prompt,\\n        llm_kwargs=llm_kwargs,\\n        output_parser=output_parser,\\n        tags=tags,\\n        verbose=verbose,\\n    )\\n    return chain\\n\\n\\ndef create_extraction_chain_pydantic(\\n    pydantic_schema: Any,\\n    llm: BaseLanguageModel,\\n    prompt: Optional[BasePromptTemplate] = None,\\n    verbose: bool = False,\\n) -> Chain:\\n    \"\"\"Creates a chain that extracts information from a passage using pydantic schema.\\n\\n    Args:\\n        pydantic_schema: The pydantic schema of the entities to extract.\\n        llm: The language model to use.\\n        prompt: The prompt to use for extraction.\\n        verbose: Whether to run in verbose mode. In verbose mode, some intermediate\\n            logs will be printed to the console. Defaults to the global `verbose` value,\\n            accessible via `langchain.globals.get_verbose()`\\n\\n    Returns:\\n        Chain that can be used to extract information from a passage.\\n    \"\"\"\\n\\n    class PydanticSchema(BaseModel):\\n        info: List[pydantic_schema]  # type: ignore\\n\\n    openai_schema = pydantic_schema.schema()\\n    openai_schema = _resolve_schema_references(\\n        openai_schema, openai_schema.get(\"definitions\", {})\\n    )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\openai_functions\\\\extraction.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='function = _get_extraction_function(openai_schema)\\n    extraction_prompt = prompt or ChatPromptTemplate.from_template(_EXTRACTION_TEMPLATE)\\n    output_parser = PydanticAttrOutputFunctionsParser(\\n        pydantic_schema=PydanticSchema, attr_name=\"info\"\\n    )\\n    llm_kwargs = get_llm_kwargs(function)\\n    chain = LLMChain(\\n        llm=llm,\\n        prompt=extraction_prompt,\\n        llm_kwargs=llm_kwargs,\\n        output_parser=output_parser,\\n        verbose=verbose,\\n    )\\n    return chain' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\openai_functions\\\\extraction.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from __future__ import annotations\\n\\nimport json\\nimport re\\nfrom collections import defaultdict\\nfrom typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Union\\n\\nimport requests\\nfrom langchain_community.chat_models import ChatOpenAI\\nfrom langchain_community.utilities.openapi import OpenAPISpec\\nfrom langchain_core.callbacks import CallbackManagerForChainRun\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.prompts import BasePromptTemplate, ChatPromptTemplate\\nfrom langchain_core.utils.input import get_colored_text\\nfrom requests import Response\\n\\nfrom langchain.chains.base import Chain\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.chains.sequential import SequentialChain\\nfrom langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\\nfrom langchain.tools import APIOperation\\n\\nif TYPE_CHECKING:\\n    from openapi_pydantic import Parameter\\n\\n\\ndef _get_description(o: Any, prefer_short: bool) -> Optional[str]:\\n    summary = getattr(o, \"summary\", None)\\n    description = getattr(o, \"description\", None)\\n    if prefer_short:\\n        return summary or description\\n    return description or summary' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\openai_functions\\\\openapi.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _format_url(url: str, path_params: dict) -> str:\\n    expected_path_param = re.findall(r\"{(.*?)}\", url)\\n    new_params = {}\\n    for param in expected_path_param:\\n        clean_param = param.lstrip(\".;\").rstrip(\"*\")\\n        val = path_params[clean_param]\\n        if isinstance(val, list):\\n            if param[0] == \".\":\\n                sep = \".\" if param[-1] == \"*\" else \",\"\\n                new_val = \".\" + sep.join(val)\\n            elif param[0] == \";\":\\n                sep = f\"{clean_param}=\" if param[-1] == \"*\" else \",\"\\n                new_val = f\"{clean_param}=\" + sep.join(val)\\n            else:\\n                new_val = \",\".join(val)\\n        elif isinstance(val, dict):\\n            kv_sep = \"=\" if param[-1] == \"*\" else \",\"\\n            kv_strs = [kv_sep.join((k, v)) for k, v in val.items()]\\n            if param[0] == \".\":\\n                sep = \".\"\\n                new_val = \".\"\\n            elif param[0] == \";\":\\n                sep = \";\"\\n                new_val = \";\"\\n            else:\\n                sep = \",\"\\n                new_val = \"\"\\n            new_val += sep.join(kv_strs)\\n        else:\\n            if param[0] == \".\":\\n                new_val = f\".{val}\"\\n            elif param[0] == \";\":\\n                new_val = f\";{clean_param}={val}\"\\n            else:\\n                new_val = val\\n        new_params[param] = new_val\\n    return url.format(**new_params)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\openai_functions\\\\openapi.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _openapi_params_to_json_schema(params: List[Parameter], spec: OpenAPISpec) -> dict:\\n    properties = {}\\n    required = []\\n    for p in params:\\n        if p.param_schema:\\n            schema = spec.get_schema(p.param_schema)\\n        else:\\n            media_type_schema = list(p.content.values())[0].media_type_schema  # type: ignore  # noqa: E501\\n            schema = spec.get_schema(media_type_schema)\\n        if p.description and not schema.description:\\n            schema.description = p.description\\n        properties[p.name] = json.loads(schema.json(exclude_none=True))\\n        if p.required:\\n            required.append(p.name)\\n    return {\"type\": \"object\", \"properties\": properties, \"required\": required}\\n\\n\\ndef openapi_spec_to_openai_fn(\\n    spec: OpenAPISpec,\\n) -> Tuple[List[Dict[str, Any]], Callable]:\\n    \"\"\"Convert a valid OpenAPI spec to the JSON Schema format expected for OpenAI\\n        functions.\\n\\n    Args:\\n        spec: OpenAPI spec to convert.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\openai_functions\\\\openapi.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Returns:\\n        Tuple of the OpenAI functions JSON schema and a default function for executing\\n            a request based on the OpenAI function schema.\\n    \"\"\"\\n    if not spec.paths:\\n        return [], lambda: None\\n    functions = []\\n    _name_to_call_map = {}\\n    for path in spec.paths:\\n        path_params = {\\n            (p.name, p.param_in): p for p in spec.get_parameters_for_path(path)\\n        }\\n        for method in spec.get_methods_for_path(path):\\n            request_args = {}\\n            op = spec.get_operation(path, method)\\n            op_params = path_params.copy()\\n            for param in spec.get_parameters_for_operation(op):\\n                op_params[(param.name, param.param_in)] = param\\n            params_by_type = defaultdict(list)\\n            for name_loc, p in op_params.items():\\n                params_by_type[name_loc[1]].append(p)\\n            param_loc_to_arg_name = {\\n                \"query\": \"params\",\\n                \"header\": \"headers\",\\n                \"cookie\": \"cookies\",\\n                \"path\": \"path_params\",\\n            }\\n            for param_loc, arg_name in param_loc_to_arg_name.items():\\n                if params_by_type[param_loc]:\\n                    request_args[arg_name] = _openapi_params_to_json_schema(\\n                        params_by_type[param_loc], spec\\n                    )\\n            request_body = spec.get_request_body_for_operation(op)\\n            # TODO: Support more MIME types.\\n            if request_body and request_body.content:\\n                media_types = {}\\n                for media_type, media_type_object in request_body.content.items():\\n                    if media_type_object.media_type_schema:\\n                        schema = spec.get_schema(media_type_object.media_type_schema)\\n                        media_types[media_type] = json.loads(\\n                            schema.json(exclude_none=True)\\n                        )\\n                if len(media_types) == 1:\\n                    media_type, schema_dict = list(media_types.items())[0]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\openai_functions\\\\openapi.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='key = \"json\" if media_type == \"application/json\" else \"data\"\\n                    request_args[key] = schema_dict\\n                elif len(media_types) > 1:\\n                    request_args[\"data\"] = {\"anyOf\": list(media_types.values())}' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\openai_functions\\\\openapi.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='api_op = APIOperation.from_openapi_spec(spec, path, method)\\n            fn = {\\n                \"name\": api_op.operation_id,\\n                \"description\": api_op.description,\\n                \"parameters\": {\\n                    \"type\": \"object\",\\n                    \"properties\": request_args,\\n                },\\n            }\\n            functions.append(fn)\\n            _name_to_call_map[fn[\"name\"]] = {\\n                \"method\": method,\\n                \"url\": api_op.base_url + api_op.path,\\n            }\\n\\n    def default_call_api(\\n        name: str,\\n        fn_args: dict,\\n        headers: Optional[dict] = None,\\n        params: Optional[dict] = None,\\n        **kwargs: Any,\\n    ) -> Any:\\n        method = _name_to_call_map[name][\"method\"]\\n        url = _name_to_call_map[name][\"url\"]\\n        path_params = fn_args.pop(\"path_params\", {})\\n        url = _format_url(url, path_params)\\n        if \"data\" in fn_args and isinstance(fn_args[\"data\"], dict):\\n            fn_args[\"data\"] = json.dumps(fn_args[\"data\"])\\n        _kwargs = {**fn_args, **kwargs}\\n        if headers is not None:\\n            if \"headers\" in _kwargs:\\n                _kwargs[\"headers\"].update(headers)\\n            else:\\n                _kwargs[\"headers\"] = headers\\n        if params is not None:\\n            if \"params\" in _kwargs:\\n                _kwargs[\"params\"].update(params)\\n            else:\\n                _kwargs[\"params\"] = params\\n        return requests.request(method, url, **_kwargs)\\n\\n    return functions, default_call_api\\n\\n\\nclass SimpleRequestChain(Chain):\\n    \"\"\"Chain for making a simple request to an API endpoint.\"\"\"\\n\\n    request_method: Callable\\n    \"\"\"Method to use for making the request.\"\"\"\\n    output_key: str = \"response\"\\n    \"\"\"Key to use for the output of the request.\"\"\"\\n    input_key: str = \"function\"\\n    \"\"\"Key to use for the input of the request.\"\"\"\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        return [self.input_key]\\n\\n    @property\\n    def output_keys(self) -> List[str]:\\n        return [self.output_key]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\openai_functions\\\\openapi.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _call(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        \"\"\"Run the logic of this chain and return the output.\"\"\"\\n        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\\n        name = inputs[self.input_key].pop(\"name\")\\n        args = inputs[self.input_key].pop(\"arguments\")\\n        _pretty_name = get_colored_text(name, \"green\")\\n        _pretty_args = get_colored_text(json.dumps(args, indent=2), \"green\")\\n        _text = f\"Calling endpoint {_pretty_name} with arguments:\\\\n\" + _pretty_args\\n        _run_manager.on_text(_text)\\n        api_response: Response = self.request_method(name, args)\\n        if api_response.status_code != 200:\\n            response = (\\n                f\"{api_response.status_code}: {api_response.reason}\"\\n                + f\"\\\\nFor {name} \"\\n                + f\"Called with args: {args.get(\\'params\\',\\'\\')}\"\\n            )\\n        else:\\n            try:\\n                response = api_response.json()\\n            except Exception:  # noqa: E722\\n                response = api_response.text\\n        return {self.output_key: response}\\n\\n\\ndef get_openapi_chain(\\n    spec: Union[OpenAPISpec, str],\\n    llm: Optional[BaseLanguageModel] = None,\\n    prompt: Optional[BasePromptTemplate] = None,\\n    request_chain: Optional[Chain] = None,\\n    llm_chain_kwargs: Optional[Dict] = None,\\n    verbose: bool = False,\\n    headers: Optional[Dict] = None,\\n    params: Optional[Dict] = None,\\n    **kwargs: Any,\\n) -> SequentialChain:\\n    \"\"\"Create a chain for querying an API from a OpenAPI spec.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\openai_functions\\\\openapi.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Args:\\n        spec: OpenAPISpec or url/file/text string corresponding to one.\\n        llm: language model, should be an OpenAI function-calling model, e.g.\\n            `ChatOpenAI(model=\"gpt-3.5-turbo-0613\")`.\\n        prompt: Main prompt template to use.\\n        request_chain: Chain for taking the functions output and executing the request.\\n    \"\"\"\\n    if isinstance(spec, str):\\n        for conversion in (\\n            OpenAPISpec.from_url,\\n            OpenAPISpec.from_file,\\n            OpenAPISpec.from_text,\\n        ):\\n            try:\\n                spec = conversion(spec)  # type: ignore[arg-type]\\n                break\\n            except ImportError as e:\\n                raise e\\n            except Exception:  # noqa: E722\\n                pass\\n        if isinstance(spec, str):\\n            raise ValueError(f\"Unable to parse spec from source {spec}\")\\n    openai_fns, call_api_fn = openapi_spec_to_openai_fn(spec)\\n    llm = llm or ChatOpenAI(\\n        model=\"gpt-3.5-turbo-0613\",\\n    )\\n    prompt = prompt or ChatPromptTemplate.from_template(\\n        \"Use the provided API\\'s to respond to this user query:\\\\n\\\\n{query}\"\\n    )\\n    llm_chain = LLMChain(\\n        llm=llm,\\n        prompt=prompt,\\n        llm_kwargs={\"functions\": openai_fns},\\n        output_parser=JsonOutputFunctionsParser(args_only=False),\\n        output_key=\"function\",\\n        verbose=verbose,\\n        **(llm_chain_kwargs or {}),\\n    )\\n    request_chain = request_chain or SimpleRequestChain(\\n        request_method=lambda name, args: call_api_fn(\\n            name, args, headers=headers, params=params\\n        ),\\n        verbose=verbose,\\n    )\\n    return SequentialChain(\\n        chains=[llm_chain, request_chain],\\n        input_variables=llm_chain.input_keys,\\n        output_variables=[\"response\"],\\n        verbose=verbose,\\n        **kwargs,\\n    )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\openai_functions\\\\openapi.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from typing import Any, List, Optional, Type, Union\\n\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.messages import HumanMessage, SystemMessage\\nfrom langchain_core.output_parsers import BaseLLMOutputParser\\nfrom langchain_core.prompts import PromptTemplate\\nfrom langchain_core.prompts.chat import ChatPromptTemplate, HumanMessagePromptTemplate\\nfrom langchain_core.pydantic_v1 import BaseModel, Field\\n\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.chains.openai_functions.utils import get_llm_kwargs\\nfrom langchain.output_parsers.openai_functions import (\\n    OutputFunctionsParser,\\n    PydanticOutputFunctionsParser,\\n)\\n\\n\\nclass AnswerWithSources(BaseModel):\\n    \"\"\"An answer to the question, with sources.\"\"\"\\n\\n    answer: str = Field(..., description=\"Answer to the question that was asked\")\\n    sources: List[str] = Field(\\n        ..., description=\"List of sources used to answer the question\"\\n    )\\n\\n\\ndef create_qa_with_structure_chain(\\n    llm: BaseLanguageModel,\\n    schema: Union[dict, Type[BaseModel]],\\n    output_parser: str = \"base\",\\n    prompt: Optional[Union[PromptTemplate, ChatPromptTemplate]] = None,\\n    verbose: bool = False,\\n) -> LLMChain:\\n    \"\"\"Create a question answering chain that returns an answer with sources\\n     based on schema.\\n\\n    Args:\\n        llm: Language model to use for the chain.\\n        schema: Pydantic schema to use for the output.\\n        output_parser: Output parser to use. Should be one of `pydantic` or `base`.\\n            Default to `base`.\\n        prompt: Optional prompt to use for the chain.\\n\\n    Returns:' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\openai_functions\\\\qa_with_structure.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"\\n    if output_parser == \"pydantic\":\\n        if not (isinstance(schema, type) and issubclass(schema, BaseModel)):\\n            raise ValueError(\\n                \"Must provide a pydantic class for schema when output_parser is \"\\n                \"\\'pydantic\\'.\"\\n            )\\n        _output_parser: BaseLLMOutputParser = PydanticOutputFunctionsParser(\\n            pydantic_schema=schema\\n        )\\n    elif output_parser == \"base\":\\n        _output_parser = OutputFunctionsParser()\\n    else:\\n        raise ValueError(\\n            f\"Got unexpected output_parser: {output_parser}. \"\\n            f\"Should be one of `pydantic` or `base`.\"\\n        )\\n    if isinstance(schema, type) and issubclass(schema, BaseModel):\\n        schema_dict = schema.schema()\\n    else:\\n        schema_dict = schema\\n    function = {\\n        \"name\": schema_dict[\"title\"],\\n        \"description\": schema_dict[\"description\"],\\n        \"parameters\": schema_dict,\\n    }\\n    llm_kwargs = get_llm_kwargs(function)\\n    messages = [\\n        SystemMessage(\\n            content=(\\n                \"You are a world class algorithm to answer \"\\n                \"questions in a specific format.\"\\n            )\\n        ),\\n        HumanMessage(content=\"Answer question using the following context\"),\\n        HumanMessagePromptTemplate.from_template(\"{context}\"),\\n        HumanMessagePromptTemplate.from_template(\"Question: {question}\"),\\n        HumanMessage(content=\"Tips: Make sure to answer in the correct format\"),\\n    ]\\n    prompt = prompt or ChatPromptTemplate(messages=messages)\\n\\n    chain = LLMChain(\\n        llm=llm,\\n        prompt=prompt,\\n        llm_kwargs=llm_kwargs,\\n        output_parser=_output_parser,\\n        verbose=verbose,\\n    )\\n    return chain\\n\\n\\ndef create_qa_with_sources_chain(\\n    llm: BaseLanguageModel, verbose: bool = False, **kwargs: Any\\n) -> LLMChain:\\n    \"\"\"Create a question answering chain that returns an answer with sources.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\openai_functions\\\\qa_with_structure.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Args:\\n        llm: Language model to use for the chain.\\n        verbose: Whether to print the details of the chain\\n        **kwargs: Keyword arguments to pass to `create_qa_with_structure_chain`.\\n\\n    Returns:\\n        Chain (LLMChain) that can be used to answer questions with citations.\\n    \"\"\"\\n    return create_qa_with_structure_chain(\\n        llm, AnswerWithSources, verbose=verbose, **kwargs\\n    )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\openai_functions\\\\qa_with_structure.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from typing import Any, Optional\\n\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.prompts import ChatPromptTemplate\\n\\nfrom langchain.chains.base import Chain\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.chains.openai_functions.utils import _convert_schema, get_llm_kwargs\\nfrom langchain.output_parsers.openai_functions import (\\n    JsonOutputFunctionsParser,\\n    PydanticOutputFunctionsParser,\\n)\\n\\n\\ndef _get_tagging_function(schema: dict) -> dict:\\n    return {\\n        \"name\": \"information_extraction\",\\n        \"description\": \"Extracts the relevant information from the passage.\",\\n        \"parameters\": _convert_schema(schema),\\n    }\\n\\n\\n_TAGGING_TEMPLATE = \"\"\"Extract the desired information from the following passage.\\n\\nOnly extract the properties mentioned in the \\'information_extraction\\' function.\\n\\nPassage:\\n{input}\\n\"\"\"\\n\\n\\ndef create_tagging_chain(\\n    schema: dict,\\n    llm: BaseLanguageModel,\\n    prompt: Optional[ChatPromptTemplate] = None,\\n    **kwargs: Any,\\n) -> Chain:\\n    \"\"\"Creates a chain that extracts information from a passage\\n     based on a schema.\\n\\n    Args:\\n        schema: The schema of the entities to extract.\\n        llm: The language model to use.\\n\\n    Returns:\\n        Chain (LLMChain) that can be used to extract information from a passage.\\n    \"\"\"\\n    function = _get_tagging_function(schema)\\n    prompt = prompt or ChatPromptTemplate.from_template(_TAGGING_TEMPLATE)\\n    output_parser = JsonOutputFunctionsParser()\\n    llm_kwargs = get_llm_kwargs(function)\\n    chain = LLMChain(\\n        llm=llm,\\n        prompt=prompt,\\n        llm_kwargs=llm_kwargs,\\n        output_parser=output_parser,\\n        **kwargs,\\n    )\\n    return chain\\n\\n\\ndef create_tagging_chain_pydantic(\\n    pydantic_schema: Any,\\n    llm: BaseLanguageModel,\\n    prompt: Optional[ChatPromptTemplate] = None,\\n    **kwargs: Any,\\n) -> Chain:\\n    \"\"\"Creates a chain that extracts information from a passage\\n     based on a pydantic schema.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\openai_functions\\\\tagging.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Args:\\n        pydantic_schema: The pydantic schema of the entities to extract.\\n        llm: The language model to use.\\n\\n    Returns:\\n        Chain (LLMChain) that can be used to extract information from a passage.\\n    \"\"\"\\n    openai_schema = pydantic_schema.schema()\\n    function = _get_tagging_function(openai_schema)\\n    prompt = prompt or ChatPromptTemplate.from_template(_TAGGING_TEMPLATE)\\n    output_parser = PydanticOutputFunctionsParser(pydantic_schema=pydantic_schema)\\n    llm_kwargs = get_llm_kwargs(function)\\n    chain = LLMChain(\\n        llm=llm,\\n        prompt=prompt,\\n        llm_kwargs=llm_kwargs,\\n        output_parser=output_parser,\\n        **kwargs,\\n    )\\n    return chain' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\openai_functions\\\\tagging.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from typing import Any, Dict\\n\\n\\ndef _resolve_schema_references(schema: Any, definitions: Dict[str, Any]) -> Any:\\n    \"\"\"\\n    Resolves the $ref keys in a JSON schema object using the provided definitions.\\n    \"\"\"\\n    if isinstance(schema, list):\\n        for i, item in enumerate(schema):\\n            schema[i] = _resolve_schema_references(item, definitions)\\n    elif isinstance(schema, dict):\\n        if \"$ref\" in schema:\\n            ref_key = schema.pop(\"$ref\").split(\"/\")[-1]\\n            ref = definitions.get(ref_key, {})\\n            schema.update(ref)\\n        else:\\n            for key, value in schema.items():\\n                schema[key] = _resolve_schema_references(value, definitions)\\n    return schema\\n\\n\\ndef _convert_schema(schema: dict) -> dict:\\n    props = {k: {\"title\": k, **v} for k, v in schema[\"properties\"].items()}\\n    return {\\n        \"type\": \"object\",\\n        \"properties\": props,\\n        \"required\": schema.get(\"required\", []),\\n    }\\n\\n\\ndef get_llm_kwargs(function: dict) -> dict:\\n    \"\"\"Returns the kwargs for the LLMChain constructor.\\n\\n    Args:\\n        function: The function to use.\\n\\n    Returns:\\n        The kwargs for the LLMChain constructor.\\n    \"\"\"\\n    return {\"functions\": [function], \"function_call\": {\"name\": function[\"name\"]}}' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\openai_functions\\\\utils.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain.chains.openai_functions.base import (\\n    convert_to_openai_function,\\n    create_openai_fn_chain,\\n    create_openai_fn_runnable,\\n    create_structured_output_chain,\\n    create_structured_output_runnable,\\n    get_openai_output_parser,\\n)\\nfrom langchain.chains.openai_functions.citation_fuzzy_match import (\\n    create_citation_fuzzy_match_chain,\\n)\\nfrom langchain.chains.openai_functions.extraction import (\\n    create_extraction_chain,\\n    create_extraction_chain_pydantic,\\n)\\nfrom langchain.chains.openai_functions.qa_with_structure import (\\n    create_qa_with_sources_chain,\\n    create_qa_with_structure_chain,\\n)\\nfrom langchain.chains.openai_functions.tagging import (\\n    create_tagging_chain,\\n    create_tagging_chain_pydantic,\\n)\\n\\n__all__ = [\\n    \"convert_to_openai_function\",\\n    \"create_tagging_chain\",\\n    \"create_tagging_chain_pydantic\",\\n    \"create_extraction_chain_pydantic\",\\n    \"create_extraction_chain\",\\n    \"create_citation_fuzzy_match_chain\",\\n    \"create_qa_with_structure_chain\",\\n    \"create_qa_with_sources_chain\",\\n    \"create_structured_output_chain\",\\n    \"create_openai_fn_chain\",\\n    \"create_structured_output_runnable\",\\n    \"create_openai_fn_runnable\",\\n    \"get_openai_output_parser\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\openai_functions\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from typing import List, Type, Union\\n\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_core.pydantic_v1 import BaseModel\\nfrom langchain_core.runnables import Runnable\\nfrom langchain_core.utils.function_calling import convert_pydantic_to_openai_function\\n\\nfrom langchain.output_parsers import PydanticToolsParser\\n\\n_EXTRACTION_TEMPLATE = \"\"\"Extract and save the relevant entities mentioned \\\\\\nin the following passage together with their properties.\\n\\nIf a property is not present and is not required in the function parameters, do not include it in the output.\"\"\"  # noqa: E501\\n\\n\\ndef create_extraction_chain_pydantic(\\n    pydantic_schemas: Union[List[Type[BaseModel]], Type[BaseModel]],\\n    llm: BaseLanguageModel,\\n    system_message: str = _EXTRACTION_TEMPLATE,\\n) -> Runnable:\\n    \"\"\"Creates a chain that extracts information from a passage.\\n\\n    Args:\\n        pydantic_schemas: The schema of the entities to extract.\\n        llm: The language model to use.\\n        system_message: The system message to use for extraction.\\n\\n    Returns:\\n        A runnable that extracts information from a passage.\\n    \"\"\"\\n    if not isinstance(pydantic_schemas, list):\\n        pydantic_schemas = [pydantic_schemas]\\n    prompt = ChatPromptTemplate.from_messages(\\n        [(\"system\", system_message), (\"user\", \"{input}\")]\\n    )\\n    functions = [convert_pydantic_to_openai_function(p) for p in pydantic_schemas]\\n    tools = [{\"type\": \"function\", \"function\": d} for d in functions]\\n    model = llm.bind(tools=tools)\\n    chain = prompt | model | PydanticToolsParser(tools=pydantic_schemas)\\n    return chain' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\openai_tools\\\\extraction.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain.chains.openai_tools.extraction import create_extraction_chain_pydantic\\n\\n__all__ = [\"create_extraction_chain_pydantic\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\openai_tools\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from __future__ import annotations\\n\\nimport json\\nfrom typing import Any, Dict, List, Optional\\n\\nfrom langchain_core.callbacks import CallbackManagerForChainRun\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.prompts import BasePromptTemplate\\nfrom langchain_core.pydantic_v1 import Field\\n\\nfrom langchain.chains.base import Chain\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.chains.qa_generation.prompt import PROMPT_SELECTOR\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter, TextSplitter\\n\\n\\nclass QAGenerationChain(Chain):\\n    \"\"\"Base class for question-answer generation chains.\"\"\"\\n\\n    llm_chain: LLMChain\\n    \"\"\"LLM Chain that generates responses from user input and context.\"\"\"\\n    text_splitter: TextSplitter = Field(\\n        default=RecursiveCharacterTextSplitter(chunk_overlap=500)\\n    )\\n    \"\"\"Text splitter that splits the input into chunks.\"\"\"\\n    input_key: str = \"text\"\\n    \"\"\"Key of the input to the chain.\"\"\"\\n    output_key: str = \"questions\"\\n    \"\"\"Key of the output of the chain.\"\"\"\\n    k: Optional[int] = None\\n    \"\"\"Number of questions to generate.\"\"\"\\n\\n    @classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        prompt: Optional[BasePromptTemplate] = None,\\n        **kwargs: Any,\\n    ) -> QAGenerationChain:\\n        \"\"\"\\n        Create a QAGenerationChain from a language model.\\n\\n        Args:\\n            llm: a language model\\n            prompt: a prompt template\\n            **kwargs: additional arguments\\n\\n        Returns:\\n            a QAGenerationChain class\\n        \"\"\"\\n        _prompt = prompt or PROMPT_SELECTOR.get_prompt(llm)\\n        chain = LLMChain(llm=llm, prompt=_prompt)\\n        return cls(llm_chain=chain, **kwargs)\\n\\n    @property\\n    def _chain_type(self) -> str:\\n        raise NotImplementedError\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        return [self.input_key]\\n\\n    @property\\n    def output_keys(self) -> List[str]:\\n        return [self.output_key]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\qa_generation\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _call(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, List]:\\n        docs = self.text_splitter.create_documents([inputs[self.input_key]])\\n        results = self.llm_chain.generate(\\n            [{\"text\": d.page_content} for d in docs], run_manager=run_manager\\n        )\\n        qa = [json.loads(res[0].text) for res in results.generations]\\n        return {self.output_key: qa}' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\qa_generation\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# flake8: noqa\\nfrom langchain.chains.prompt_selector import ConditionalPromptSelector, is_chat_model\\nfrom langchain_core.prompts.chat import (\\n    ChatPromptTemplate,\\n    HumanMessagePromptTemplate,\\n    SystemMessagePromptTemplate,\\n)\\nfrom langchain_core.prompts.prompt import PromptTemplate\\n\\ntempl1 = \"\"\"You are a smart assistant designed to help high school teachers come up with reading comprehension questions.\\nGiven a piece of text, you must come up with a question and answer pair that can be used to test a student\\'s reading comprehension abilities.\\nWhen coming up with this question/answer pair, you must respond in the following format:\\n```\\n{{\\n    \"question\": \"$YOUR_QUESTION_HERE\",\\n    \"answer\": \"$THE_ANSWER_HERE\"\\n}}\\n```\\n\\nEverything between the ``` must be valid json.\\n\"\"\"\\ntempl2 = \"\"\"Please come up with a question/answer pair, in the specified JSON format, for the following text:\\n----------------\\n{text}\"\"\"\\nCHAT_PROMPT = ChatPromptTemplate.from_messages(\\n    [\\n        SystemMessagePromptTemplate.from_template(templ1),\\n        HumanMessagePromptTemplate.from_template(templ2),\\n    ]\\n)\\ntempl = \"\"\"You are a smart assistant designed to help high school teachers come up with reading comprehension questions.\\nGiven a piece of text, you must come up with a question and answer pair that can be used to test a student\\'s reading comprehension abilities.\\nWhen coming up with this question/answer pair, you must respond in the following format:\\n```\\n{{\\n    \"question\": \"$YOUR_QUESTION_HERE\",\\n    \"answer\": \"$THE_ANSWER_HERE\"\\n}}\\n```\\n\\nEverything between the ``` must be valid json.\\n\\nPlease come up with a question/answer pair, in the specified JSON format, for the following text:\\n----------------\\n{text}\"\"\"\\nPROMPT = PromptTemplate.from_template(templ)\\n\\nPROMPT_SELECTOR = ConditionalPromptSelector(\\n    default_prompt=PROMPT, conditionals=[(is_chat_model, CHAT_PROMPT)]\\n)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\qa_generation\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Question answering with sources over documents.\"\"\"\\n\\nfrom __future__ import annotations\\n\\nimport inspect\\nimport re\\nfrom abc import ABC, abstractmethod\\nfrom typing import Any, Dict, List, Optional, Tuple\\n\\nfrom langchain_core.callbacks import (\\n    AsyncCallbackManagerForChainRun,\\n    CallbackManagerForChainRun,\\n)\\nfrom langchain_core.documents import Document\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.prompts import BasePromptTemplate\\nfrom langchain_core.pydantic_v1 import Extra, root_validator\\n\\nfrom langchain.chains import ReduceDocumentsChain\\nfrom langchain.chains.base import Chain\\nfrom langchain.chains.combine_documents.base import BaseCombineDocumentsChain\\nfrom langchain.chains.combine_documents.map_reduce import MapReduceDocumentsChain\\nfrom langchain.chains.combine_documents.stuff import StuffDocumentsChain\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.chains.qa_with_sources.loading import load_qa_with_sources_chain\\nfrom langchain.chains.qa_with_sources.map_reduce_prompt import (\\n    COMBINE_PROMPT,\\n    EXAMPLE_PROMPT,\\n    QUESTION_PROMPT,\\n)\\n\\n\\nclass BaseQAWithSourcesChain(Chain, ABC):\\n    \"\"\"Question answering chain with sources over documents.\"\"\"\\n\\n    combine_documents_chain: BaseCombineDocumentsChain\\n    \"\"\"Chain to use to combine documents.\"\"\"\\n    question_key: str = \"question\"  #: :meta private:\\n    input_docs_key: str = \"docs\"  #: :meta private:\\n    answer_key: str = \"answer\"  #: :meta private:\\n    sources_answer_key: str = \"sources\"  #: :meta private:\\n    return_source_documents: bool = False\\n    \"\"\"Return the source documents.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\qa_with_sources\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        document_prompt: BasePromptTemplate = EXAMPLE_PROMPT,\\n        question_prompt: BasePromptTemplate = QUESTION_PROMPT,\\n        combine_prompt: BasePromptTemplate = COMBINE_PROMPT,\\n        **kwargs: Any,\\n    ) -> BaseQAWithSourcesChain:\\n        \"\"\"Construct the chain from an LLM.\"\"\"\\n        llm_question_chain = LLMChain(llm=llm, prompt=question_prompt)\\n        llm_combine_chain = LLMChain(llm=llm, prompt=combine_prompt)\\n        combine_results_chain = StuffDocumentsChain(\\n            llm_chain=llm_combine_chain,\\n            document_prompt=document_prompt,\\n            document_variable_name=\"summaries\",\\n        )\\n        reduce_documents_chain = ReduceDocumentsChain(\\n            combine_documents_chain=combine_results_chain\\n        )\\n        combine_documents_chain = MapReduceDocumentsChain(\\n            llm_chain=llm_question_chain,\\n            reduce_documents_chain=reduce_documents_chain,\\n            document_variable_name=\"context\",\\n        )\\n        return cls(\\n            combine_documents_chain=combine_documents_chain,\\n            **kwargs,\\n        )\\n\\n    @classmethod\\n    def from_chain_type(\\n        cls,\\n        llm: BaseLanguageModel,\\n        chain_type: str = \"stuff\",\\n        chain_type_kwargs: Optional[dict] = None,\\n        **kwargs: Any,\\n    ) -> BaseQAWithSourcesChain:\\n        \"\"\"Load chain from chain type.\"\"\"\\n        _chain_kwargs = chain_type_kwargs or {}\\n        combine_documents_chain = load_qa_with_sources_chain(\\n            llm, chain_type=chain_type, **_chain_kwargs\\n        )\\n        return cls(combine_documents_chain=combine_documents_chain, **kwargs)\\n\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n\\n        extra = Extra.forbid\\n        arbitrary_types_allowed = True\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Expect input key.\\n\\n        :meta private:\\n        \"\"\"\\n        return [self.question_key]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\qa_with_sources\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Return output key.\\n\\n        :meta private:\\n        \"\"\"\\n        _output_keys = [self.answer_key, self.sources_answer_key]\\n        if self.return_source_documents:\\n            _output_keys = _output_keys + [\"source_documents\"]\\n        return _output_keys\\n\\n    @root_validator(pre=True)\\n    def validate_naming(cls, values: Dict) -> Dict:\\n        \"\"\"Fix backwards compatibility in naming.\"\"\"\\n        if \"combine_document_chain\" in values:\\n            values[\"combine_documents_chain\"] = values.pop(\"combine_document_chain\")\\n        return values\\n\\n    def _split_sources(self, answer: str) -> Tuple[str, str]:\\n        \"\"\"Split sources from answer.\"\"\"\\n        if re.search(r\"SOURCES?:\", answer, re.IGNORECASE):\\n            answer, sources = re.split(\\n                r\"SOURCES?:|QUESTION:\\\\s\", answer, flags=re.IGNORECASE\\n            )[:2]\\n            sources = re.split(r\"\\\\n\", sources)[0].strip()\\n        else:\\n            sources = \"\"\\n        return answer, sources\\n\\n    @abstractmethod\\n    def _get_docs(\\n        self,\\n        inputs: Dict[str, Any],\\n        *,\\n        run_manager: CallbackManagerForChainRun,\\n    ) -> List[Document]:\\n        \"\"\"Get docs to run questioning over.\"\"\"\\n\\n    def _call(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, str]:\\n        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\\n        accepts_run_manager = (\\n            \"run_manager\" in inspect.signature(self._get_docs).parameters\\n        )\\n        if accepts_run_manager:\\n            docs = self._get_docs(inputs, run_manager=_run_manager)\\n        else:\\n            docs = self._get_docs(inputs)  # type: ignore[call-arg]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\qa_with_sources\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='answer = self.combine_documents_chain.run(\\n            input_documents=docs, callbacks=_run_manager.get_child(), **inputs\\n        )\\n        answer, sources = self._split_sources(answer)\\n        result: Dict[str, Any] = {\\n            self.answer_key: answer,\\n            self.sources_answer_key: sources,\\n        }\\n        if self.return_source_documents:\\n            result[\"source_documents\"] = docs\\n        return result\\n\\n    @abstractmethod\\n    async def _aget_docs(\\n        self,\\n        inputs: Dict[str, Any],\\n        *,\\n        run_manager: AsyncCallbackManagerForChainRun,\\n    ) -> List[Document]:\\n        \"\"\"Get docs to run questioning over.\"\"\"\\n\\n    async def _acall(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        _run_manager = run_manager or AsyncCallbackManagerForChainRun.get_noop_manager()\\n        accepts_run_manager = (\\n            \"run_manager\" in inspect.signature(self._aget_docs).parameters\\n        )\\n        if accepts_run_manager:\\n            docs = await self._aget_docs(inputs, run_manager=_run_manager)\\n        else:\\n            docs = await self._aget_docs(inputs)  # type: ignore[call-arg]\\n        answer = await self.combine_documents_chain.arun(\\n            input_documents=docs, callbacks=_run_manager.get_child(), **inputs\\n        )\\n        answer, sources = self._split_sources(answer)\\n        result: Dict[str, Any] = {\\n            self.answer_key: answer,\\n            self.sources_answer_key: sources,\\n        }\\n        if self.return_source_documents:\\n            result[\"source_documents\"] = docs\\n        return result\\n\\n\\nclass QAWithSourcesChain(BaseQAWithSourcesChain):\\n    \"\"\"Question answering with sources over documents.\"\"\"\\n\\n    input_docs_key: str = \"docs\"  #: :meta private:\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Expect input key.\\n\\n        :meta private:\\n        \"\"\"\\n        return [self.input_docs_key, self.question_key]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\qa_with_sources\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_docs(\\n        self,\\n        inputs: Dict[str, Any],\\n        *,\\n        run_manager: CallbackManagerForChainRun,\\n    ) -> List[Document]:\\n        \"\"\"Get docs to run questioning over.\"\"\"\\n        return inputs.pop(self.input_docs_key)\\n\\n    async def _aget_docs(\\n        self,\\n        inputs: Dict[str, Any],\\n        *,\\n        run_manager: AsyncCallbackManagerForChainRun,\\n    ) -> List[Document]:\\n        \"\"\"Get docs to run questioning over.\"\"\"\\n        return inputs.pop(self.input_docs_key)\\n\\n    @property\\n    def _chain_type(self) -> str:\\n        return \"qa_with_sources_chain\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\qa_with_sources\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Load question answering with sources chains.\"\"\"\\nfrom __future__ import annotations\\n\\nfrom typing import Any, Mapping, Optional, Protocol\\n\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.prompts import BasePromptTemplate\\n\\nfrom langchain.chains.combine_documents.base import BaseCombineDocumentsChain\\nfrom langchain.chains.combine_documents.map_reduce import MapReduceDocumentsChain\\nfrom langchain.chains.combine_documents.map_rerank import MapRerankDocumentsChain\\nfrom langchain.chains.combine_documents.reduce import ReduceDocumentsChain\\nfrom langchain.chains.combine_documents.refine import RefineDocumentsChain\\nfrom langchain.chains.combine_documents.stuff import StuffDocumentsChain\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.chains.qa_with_sources import (\\n    map_reduce_prompt,\\n    refine_prompts,\\n    stuff_prompt,\\n)\\nfrom langchain.chains.question_answering.map_rerank_prompt import (\\n    PROMPT as MAP_RERANK_PROMPT,\\n)\\n\\n\\nclass LoadingCallable(Protocol):\\n    \"\"\"Interface for loading the combine documents chain.\"\"\"\\n\\n    def __call__(\\n        self, llm: BaseLanguageModel, **kwargs: Any\\n    ) -> BaseCombineDocumentsChain:\\n        \"\"\"Callable to load the combine documents chain.\"\"\"\\n\\n\\ndef _load_map_rerank_chain(\\n    llm: BaseLanguageModel,\\n    prompt: BasePromptTemplate = MAP_RERANK_PROMPT,\\n    verbose: bool = False,\\n    document_variable_name: str = \"context\",\\n    rank_key: str = \"score\",\\n    answer_key: str = \"answer\",\\n    **kwargs: Any,\\n) -> MapRerankDocumentsChain:\\n    llm_chain = LLMChain(llm=llm, prompt=prompt, verbose=verbose)\\n    return MapRerankDocumentsChain(\\n        llm_chain=llm_chain,\\n        rank_key=rank_key,\\n        answer_key=answer_key,\\n        document_variable_name=document_variable_name,\\n        **kwargs,\\n    )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\qa_with_sources\\\\loading.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _load_stuff_chain(\\n    llm: BaseLanguageModel,\\n    prompt: BasePromptTemplate = stuff_prompt.PROMPT,\\n    document_prompt: BasePromptTemplate = stuff_prompt.EXAMPLE_PROMPT,\\n    document_variable_name: str = \"summaries\",\\n    verbose: Optional[bool] = None,\\n    **kwargs: Any,\\n) -> StuffDocumentsChain:\\n    llm_chain = LLMChain(llm=llm, prompt=prompt, verbose=verbose)\\n    return StuffDocumentsChain(\\n        llm_chain=llm_chain,\\n        document_variable_name=document_variable_name,\\n        document_prompt=document_prompt,\\n        verbose=verbose,\\n        **kwargs,\\n    )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\qa_with_sources\\\\loading.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _load_map_reduce_chain(\\n    llm: BaseLanguageModel,\\n    question_prompt: BasePromptTemplate = map_reduce_prompt.QUESTION_PROMPT,\\n    combine_prompt: BasePromptTemplate = map_reduce_prompt.COMBINE_PROMPT,\\n    document_prompt: BasePromptTemplate = map_reduce_prompt.EXAMPLE_PROMPT,\\n    combine_document_variable_name: str = \"summaries\",\\n    map_reduce_document_variable_name: str = \"context\",\\n    collapse_prompt: Optional[BasePromptTemplate] = None,\\n    reduce_llm: Optional[BaseLanguageModel] = None,\\n    collapse_llm: Optional[BaseLanguageModel] = None,\\n    verbose: Optional[bool] = None,\\n    token_max: int = 3000,\\n    **kwargs: Any,\\n) -> MapReduceDocumentsChain:\\n    map_chain = LLMChain(llm=llm, prompt=question_prompt, verbose=verbose)\\n    _reduce_llm = reduce_llm or llm\\n    reduce_chain = LLMChain(llm=_reduce_llm, prompt=combine_prompt, verbose=verbose)\\n    combine_documents_chain = StuffDocumentsChain(\\n        llm_chain=reduce_chain,\\n        document_variable_name=combine_document_variable_name,\\n        document_prompt=document_prompt,\\n        verbose=verbose,\\n    )\\n    if collapse_prompt is None:\\n        collapse_chain = None\\n        if collapse_llm is not None:\\n            raise ValueError(\\n                \"collapse_llm provided, but collapse_prompt was not: please \"\\n                \"provide one or stop providing collapse_llm.\"\\n            )\\n    else:\\n        _collapse_llm = collapse_llm or llm\\n        collapse_chain = StuffDocumentsChain(\\n            llm_chain=LLMChain(\\n                llm=_collapse_llm,\\n                prompt=collapse_prompt,\\n                verbose=verbose,\\n            ),\\n            document_variable_name=combine_document_variable_name,\\n            document_prompt=document_prompt,\\n        )\\n    reduce_documents_chain = ReduceDocumentsChain(\\n        combine_documents_chain=combine_documents_chain,\\n        collapse_documents_chain=collapse_chain,\\n        token_max=token_max,\\n        verbose=verbose,\\n    )\\n    return MapReduceDocumentsChain(\\n        llm_chain=map_chain,' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\qa_with_sources\\\\loading.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='reduce_documents_chain=reduce_documents_chain,\\n        document_variable_name=map_reduce_document_variable_name,\\n        verbose=verbose,\\n        **kwargs,\\n    )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\qa_with_sources\\\\loading.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _load_refine_chain(\\n    llm: BaseLanguageModel,\\n    question_prompt: BasePromptTemplate = refine_prompts.DEFAULT_TEXT_QA_PROMPT,\\n    refine_prompt: BasePromptTemplate = refine_prompts.DEFAULT_REFINE_PROMPT,\\n    document_prompt: BasePromptTemplate = refine_prompts.EXAMPLE_PROMPT,\\n    document_variable_name: str = \"context_str\",\\n    initial_response_name: str = \"existing_answer\",\\n    refine_llm: Optional[BaseLanguageModel] = None,\\n    verbose: Optional[bool] = None,\\n    **kwargs: Any,\\n) -> RefineDocumentsChain:\\n    initial_chain = LLMChain(llm=llm, prompt=question_prompt, verbose=verbose)\\n    _refine_llm = refine_llm or llm\\n    refine_chain = LLMChain(llm=_refine_llm, prompt=refine_prompt, verbose=verbose)\\n    return RefineDocumentsChain(\\n        initial_llm_chain=initial_chain,\\n        refine_llm_chain=refine_chain,\\n        document_variable_name=document_variable_name,\\n        initial_response_name=initial_response_name,\\n        document_prompt=document_prompt,\\n        verbose=verbose,\\n        **kwargs,\\n    )\\n\\n\\ndef load_qa_with_sources_chain(\\n    llm: BaseLanguageModel,\\n    chain_type: str = \"stuff\",\\n    verbose: Optional[bool] = None,\\n    **kwargs: Any,\\n) -> BaseCombineDocumentsChain:\\n    \"\"\"Load a question answering with sources chain.\\n\\n    Args:\\n        llm: Language Model to use in the chain.\\n        chain_type: Type of document combining chain to use. Should be one of \"stuff\",\\n            \"map_reduce\", \"refine\" and \"map_rerank\".\\n        verbose: Whether chains should be run in verbose mode or not. Note that this\\n            applies to all chains that make up the final chain.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\qa_with_sources\\\\loading.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Returns:\\n        A chain to use for question answering with sources.\\n    \"\"\"\\n    loader_mapping: Mapping[str, LoadingCallable] = {\\n        \"stuff\": _load_stuff_chain,\\n        \"map_reduce\": _load_map_reduce_chain,\\n        \"refine\": _load_refine_chain,\\n        \"map_rerank\": _load_map_rerank_chain,\\n    }\\n    if chain_type not in loader_mapping:\\n        raise ValueError(\\n            f\"Got unsupported chain type: {chain_type}. \"\\n            f\"Should be one of {loader_mapping.keys()}\"\\n        )\\n    _func: LoadingCallable = loader_mapping[chain_type]\\n    return _func(llm, verbose=verbose, **kwargs)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\qa_with_sources\\\\loading.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# flake8: noqa\\nfrom langchain_core.prompts import PromptTemplate\\n\\nquestion_prompt_template = \"\"\"Use the following portion of a long document to see if any of the text is relevant to answer the question. \\nReturn any relevant text verbatim.\\n{context}\\nQuestion: {question}\\nRelevant text, if any:\"\"\"\\nQUESTION_PROMPT = PromptTemplate(\\n    template=question_prompt_template, input_variables=[\"context\", \"question\"]\\n)\\n\\ncombine_prompt_template = \"\"\"Given the following extracted parts of a long document and a question, create a final answer with references (\"SOURCES\"). \\nIf you don\\'t know the answer, just say that you don\\'t know. Don\\'t try to make up an answer.\\nALWAYS return a \"SOURCES\" part in your answer.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\qa_with_sources\\\\map_reduce_prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content=\"QUESTION: Which state/country's law governs the interpretation of the contract?\\n=========\\nContent: This Agreement is governed by English law and the parties submit to the exclusive jurisdiction of the English courts in  relation to any dispute (contractual or non-contractual) concerning this Agreement save that either party may apply to any court for an  injunction or other relief to protect its Intellectual Property Rights.\\nSource: 28-pl\\nContent: No Waiver. Failure or delay in exercising any right or remedy under this Agreement shall not constitute a waiver of such (or any other)  right or remedy.\\\\n\\\\n11.7 Severability. The invalidity, illegality or unenforceability of any term (or part of a term) of this Agreement shall not affect the continuation  in force of the remainder of the term (if any) and this Agreement.\\\\n\\\\n11.8 No Agency. Except as expressly stated otherwise, nothing in this Agreement shall create an agency, partnership or joint venture of any  kind between the parties.\\\\n\\\\n11.9 No Third-Party Beneficiaries.\\nSource: 30-pl\\nContent: (b) if Google believes, in good faith, that the Distributor has violated or caused Google to violate any Anti-Bribery Laws (as  defined in Clause 8.5) or that such a violation is reasonably likely to occur,\\nSource: 4-pl\\n=========\\nFINAL ANSWER: This Agreement is governed by English law.\\nSOURCES: 28-pl\" metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\qa_with_sources\\\\map_reduce_prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='QUESTION: What did the president say about Michael Jackson?\\n=========\\nContent: Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \\\\n\\\\nLast year COVID-19 kept us apart. This year we are finally together again. \\\\n\\\\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \\\\n\\\\nWith a duty to one another to the American people to the Constitution. \\\\n\\\\nAnd with an unwavering resolve that freedom will always triumph over tyranny. \\\\n\\\\nSix days ago, Russia’s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. \\\\n\\\\nHe thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. \\\\n\\\\nHe met the Ukrainian people. \\\\n\\\\nFrom President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world. \\\\n\\\\nGroups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland.\\nSource: 0-pl' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\qa_with_sources\\\\map_reduce_prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Content: And we won’t stop. \\\\n\\\\nWe have lost so much to COVID-19. Time with one another. And worst of all, so much loss of life. \\\\n\\\\nLet’s use this moment to reset. Let’s stop looking at COVID-19 as a partisan dividing line and see it for what it is: A God-awful disease.  \\\\n\\\\nLet’s stop seeing each other as enemies, and start seeing each other for who we really are: Fellow Americans.  \\\\n\\\\nWe can’t change how divided we’ve been. But we can change how we move forward—on COVID-19 and other issues we must face together. \\\\n\\\\nI recently visited the New York City Police Department days after the funerals of Officer Wilbert Mora and his partner, Officer Jason Rivera. \\\\n\\\\nThey were responding to a 9-1-1 call when a man shot and killed them with a stolen gun. \\\\n\\\\nOfficer Mora was 27 years old. \\\\n\\\\nOfficer Rivera was 22. \\\\n\\\\nBoth Dominican Americans who’d grown up on the same streets they later chose to patrol as police officers. \\\\n\\\\nI spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their mission to restore the trust and safety every community deserves.\\nSource: 24-pl' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\qa_with_sources\\\\map_reduce_prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Content: And a proud Ukrainian people, who have known 30 years  of independence, have repeatedly shown that they will not tolerate anyone who tries to take their country backwards.  \\\\n\\\\nTo all Americans, I will be honest with you, as I’ve always promised. A Russian dictator, invading a foreign country, has costs around the world. \\\\n\\\\nAnd I’m taking robust action to make sure the pain of our sanctions  is targeted at Russia’s economy. And I will use every tool at our disposal to protect American businesses and consumers. \\\\n\\\\nTonight, I can announce that the United States has worked with 30 other countries to release 60 Million barrels of oil from reserves around the world.  \\\\n\\\\nAmerica will lead that effort, releasing 30 Million barrels from our own Strategic Petroleum Reserve. And we stand ready to do more if necessary, unified with our allies.  \\\\n\\\\nThese steps will help blunt gas prices here at home. And I know the news about what’s happening can seem alarming. \\\\n\\\\nBut I want you to know that we are going to be okay.\\nSource: 5-pl' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\qa_with_sources\\\\map_reduce_prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Content: More support for patients and families. \\\\n\\\\nTo get there, I call on Congress to fund ARPA-H, the Advanced Research Projects Agency for Health. \\\\n\\\\nIt’s based on DARPA—the Defense Department project that led to the Internet, GPS, and so much more.  \\\\n\\\\nARPA-H will have a singular purpose—to drive breakthroughs in cancer, Alzheimer’s, diabetes, and more. \\\\n\\\\nA unity agenda for the nation. \\\\n\\\\nWe can do this. \\\\n\\\\nMy fellow Americans—tonight , we have gathered in a sacred space—the citadel of our democracy. \\\\n\\\\nIn this Capitol, generation after generation, Americans have debated great questions amid great strife, and have done great things. \\\\n\\\\nWe have fought for freedom, expanded liberty, defeated totalitarianism and terror. \\\\n\\\\nAnd built the strongest, freest, and most prosperous nation the world has ever known. \\\\n\\\\nNow is the hour. \\\\n\\\\nOur moment of responsibility. \\\\n\\\\nOur test of resolve and conscience, of history itself. \\\\n\\\\nIt is in this moment that our character is formed. Our purpose is found. Our future is forged. \\\\n\\\\nWell I know this nation.\\nSource: 34-pl\\n=========\\nFINAL ANSWER: The president did not mention Michael Jackson.\\nSOURCES:' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\qa_with_sources\\\\map_reduce_prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='QUESTION: {question}\\n=========\\n{summaries}\\n=========\\nFINAL ANSWER:\"\"\"\\nCOMBINE_PROMPT = PromptTemplate(\\n    template=combine_prompt_template, input_variables=[\"summaries\", \"question\"]\\n)\\n\\nEXAMPLE_PROMPT = PromptTemplate(\\n    template=\"Content: {page_content}\\\\nSource: {source}\",\\n    input_variables=[\"page_content\", \"source\"],\\n)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\qa_with_sources\\\\map_reduce_prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# flake8: noqa\\nfrom langchain_core.prompts import PromptTemplate\\n\\nDEFAULT_REFINE_PROMPT_TMPL = (\\n    \"The original question is as follows: {question}\\\\n\"\\n    \"We have provided an existing answer, including sources: {existing_answer}\\\\n\"\\n    \"We have the opportunity to refine the existing answer\"\\n    \"(only if needed) with some more context below.\\\\n\"\\n    \"------------\\\\n\"\\n    \"{context_str}\\\\n\"\\n    \"------------\\\\n\"\\n    \"Given the new context, refine the original answer to better \"\\n    \"answer the question. \"\\n    \"If you do update it, please update the sources as well. \"\\n    \"If the context isn\\'t useful, return the original answer.\"\\n)\\nDEFAULT_REFINE_PROMPT = PromptTemplate(\\n    input_variables=[\"question\", \"existing_answer\", \"context_str\"],\\n    template=DEFAULT_REFINE_PROMPT_TMPL,\\n)\\n\\n\\nDEFAULT_TEXT_QA_PROMPT_TMPL = (\\n    \"Context information is below. \\\\n\"\\n    \"---------------------\\\\n\"\\n    \"{context_str}\"\\n    \"\\\\n---------------------\\\\n\"\\n    \"Given the context information and not prior knowledge, \"\\n    \"answer the question: {question}\\\\n\"\\n)\\nDEFAULT_TEXT_QA_PROMPT = PromptTemplate(\\n    input_variables=[\"context_str\", \"question\"], template=DEFAULT_TEXT_QA_PROMPT_TMPL\\n)\\n\\nEXAMPLE_PROMPT = PromptTemplate(\\n    template=\"Content: {page_content}\\\\nSource: {source}\",\\n    input_variables=[\"page_content\", \"source\"],\\n)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\qa_with_sources\\\\refine_prompts.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Question-answering with sources over an index.\"\"\"\\n\\nfrom typing import Any, Dict, List\\n\\nfrom langchain_core.callbacks import (\\n    AsyncCallbackManagerForChainRun,\\n    CallbackManagerForChainRun,\\n)\\nfrom langchain_core.documents import Document\\nfrom langchain_core.pydantic_v1 import Field\\nfrom langchain_core.retrievers import BaseRetriever\\n\\nfrom langchain.chains.combine_documents.stuff import StuffDocumentsChain\\nfrom langchain.chains.qa_with_sources.base import BaseQAWithSourcesChain\\n\\n\\nclass RetrievalQAWithSourcesChain(BaseQAWithSourcesChain):\\n    \"\"\"Question-answering with sources over an index.\"\"\"\\n\\n    retriever: BaseRetriever = Field(exclude=True)\\n    \"\"\"Index to connect to.\"\"\"\\n    reduce_k_below_max_tokens: bool = False\\n    \"\"\"Reduce the number of results to return from store based on tokens limit\"\"\"\\n    max_tokens_limit: int = 3375\\n    \"\"\"Restrict the docs to return from store based on tokens,\\n    enforced only for StuffDocumentChain and if reduce_k_below_max_tokens is to true\"\"\"\\n\\n    def _reduce_tokens_below_limit(self, docs: List[Document]) -> List[Document]:\\n        num_docs = len(docs)\\n\\n        if self.reduce_k_below_max_tokens and isinstance(\\n            self.combine_documents_chain, StuffDocumentsChain\\n        ):\\n            tokens = [\\n                self.combine_documents_chain.llm_chain._get_num_tokens(doc.page_content)\\n                for doc in docs\\n            ]\\n            token_count = sum(tokens[:num_docs])\\n            while token_count > self.max_tokens_limit:\\n                num_docs -= 1\\n                token_count -= tokens[num_docs]\\n\\n        return docs[:num_docs]\\n\\n    def _get_docs(\\n        self, inputs: Dict[str, Any], *, run_manager: CallbackManagerForChainRun\\n    ) -> List[Document]:\\n        question = inputs[self.question_key]\\n        docs = self.retriever.get_relevant_documents(\\n            question, callbacks=run_manager.get_child()\\n        )\\n        return self._reduce_tokens_below_limit(docs)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\qa_with_sources\\\\retrieval.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='async def _aget_docs(\\n        self, inputs: Dict[str, Any], *, run_manager: AsyncCallbackManagerForChainRun\\n    ) -> List[Document]:\\n        question = inputs[self.question_key]\\n        docs = await self.retriever.aget_relevant_documents(\\n            question, callbacks=run_manager.get_child()\\n        )\\n        return self._reduce_tokens_below_limit(docs)\\n\\n    @property\\n    def _chain_type(self) -> str:\\n        \"\"\"Return the chain type.\"\"\"\\n        return \"retrieval_qa_with_sources_chain\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\qa_with_sources\\\\retrieval.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# flake8: noqa\\nfrom langchain_core.prompts import PromptTemplate\\n\\ntemplate = \"\"\"Given the following extracted parts of a long document and a question, create a final answer with references (\"SOURCES\"). \\nIf you don\\'t know the answer, just say that you don\\'t know. Don\\'t try to make up an answer.\\nALWAYS return a \"SOURCES\" part in your answer.\\n\\nQUESTION: Which state/country\\'s law governs the interpretation of the contract?\\n=========\\nContent: This Agreement is governed by English law and the parties submit to the exclusive jurisdiction of the English courts in  relation to any dispute (contractual or non-contractual) concerning this Agreement save that either party may apply to any court for an  injunction or other relief to protect its Intellectual Property Rights.\\nSource: 28-pl\\nContent: No Waiver. Failure or delay in exercising any right or remedy under this Agreement shall not constitute a waiver of such (or any other)  right or remedy.\\\\n\\\\n11.7 Severability. The invalidity, illegality or unenforceability of any term (or part of a term) of this Agreement shall not affect the continuation  in force of the remainder of the term (if any) and this Agreement.\\\\n\\\\n11.8 No Agency. Except as expressly stated otherwise, nothing in this Agreement shall create an agency, partnership or joint venture of any  kind between the parties.\\\\n\\\\n11.9 No Third-Party Beneficiaries.\\nSource: 30-pl\\nContent: (b) if Google believes, in good faith, that the Distributor has violated or caused Google to violate any Anti-Bribery Laws (as  defined in Clause 8.5) or that such a violation is reasonably likely to occur,\\nSource: 4-pl\\n=========\\nFINAL ANSWER: This Agreement is governed by English law.\\nSOURCES: 28-pl' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\qa_with_sources\\\\stuff_prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='QUESTION: What did the president say about Michael Jackson?\\n=========\\nContent: Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \\\\n\\\\nLast year COVID-19 kept us apart. This year we are finally together again. \\\\n\\\\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \\\\n\\\\nWith a duty to one another to the American people to the Constitution. \\\\n\\\\nAnd with an unwavering resolve that freedom will always triumph over tyranny. \\\\n\\\\nSix days ago, Russia’s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. \\\\n\\\\nHe thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. \\\\n\\\\nHe met the Ukrainian people. \\\\n\\\\nFrom President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world. \\\\n\\\\nGroups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland.\\nSource: 0-pl' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\qa_with_sources\\\\stuff_prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Content: And we won’t stop. \\\\n\\\\nWe have lost so much to COVID-19. Time with one another. And worst of all, so much loss of life. \\\\n\\\\nLet’s use this moment to reset. Let’s stop looking at COVID-19 as a partisan dividing line and see it for what it is: A God-awful disease.  \\\\n\\\\nLet’s stop seeing each other as enemies, and start seeing each other for who we really are: Fellow Americans.  \\\\n\\\\nWe can’t change how divided we’ve been. But we can change how we move forward—on COVID-19 and other issues we must face together. \\\\n\\\\nI recently visited the New York City Police Department days after the funerals of Officer Wilbert Mora and his partner, Officer Jason Rivera. \\\\n\\\\nThey were responding to a 9-1-1 call when a man shot and killed them with a stolen gun. \\\\n\\\\nOfficer Mora was 27 years old. \\\\n\\\\nOfficer Rivera was 22. \\\\n\\\\nBoth Dominican Americans who’d grown up on the same streets they later chose to patrol as police officers. \\\\n\\\\nI spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their mission to restore the trust and safety every community deserves.\\nSource: 24-pl' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\qa_with_sources\\\\stuff_prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Content: And a proud Ukrainian people, who have known 30 years  of independence, have repeatedly shown that they will not tolerate anyone who tries to take their country backwards.  \\\\n\\\\nTo all Americans, I will be honest with you, as I’ve always promised. A Russian dictator, invading a foreign country, has costs around the world. \\\\n\\\\nAnd I’m taking robust action to make sure the pain of our sanctions  is targeted at Russia’s economy. And I will use every tool at our disposal to protect American businesses and consumers. \\\\n\\\\nTonight, I can announce that the United States has worked with 30 other countries to release 60 Million barrels of oil from reserves around the world.  \\\\n\\\\nAmerica will lead that effort, releasing 30 Million barrels from our own Strategic Petroleum Reserve. And we stand ready to do more if necessary, unified with our allies.  \\\\n\\\\nThese steps will help blunt gas prices here at home. And I know the news about what’s happening can seem alarming. \\\\n\\\\nBut I want you to know that we are going to be okay.\\nSource: 5-pl' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\qa_with_sources\\\\stuff_prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Content: More support for patients and families. \\\\n\\\\nTo get there, I call on Congress to fund ARPA-H, the Advanced Research Projects Agency for Health. \\\\n\\\\nIt’s based on DARPA—the Defense Department project that led to the Internet, GPS, and so much more.  \\\\n\\\\nARPA-H will have a singular purpose—to drive breakthroughs in cancer, Alzheimer’s, diabetes, and more. \\\\n\\\\nA unity agenda for the nation. \\\\n\\\\nWe can do this. \\\\n\\\\nMy fellow Americans—tonight , we have gathered in a sacred space—the citadel of our democracy. \\\\n\\\\nIn this Capitol, generation after generation, Americans have debated great questions amid great strife, and have done great things. \\\\n\\\\nWe have fought for freedom, expanded liberty, defeated totalitarianism and terror. \\\\n\\\\nAnd built the strongest, freest, and most prosperous nation the world has ever known. \\\\n\\\\nNow is the hour. \\\\n\\\\nOur moment of responsibility. \\\\n\\\\nOur test of resolve and conscience, of history itself. \\\\n\\\\nIt is in this moment that our character is formed. Our purpose is found. Our future is forged. \\\\n\\\\nWell I know this nation.\\nSource: 34-pl\\n=========\\nFINAL ANSWER: The president did not mention Michael Jackson.\\nSOURCES:' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\qa_with_sources\\\\stuff_prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='QUESTION: {question}\\n=========\\n{summaries}\\n=========\\nFINAL ANSWER:\"\"\"\\nPROMPT = PromptTemplate(template=template, input_variables=[\"summaries\", \"question\"])\\n\\nEXAMPLE_PROMPT = PromptTemplate(\\n    template=\"Content: {page_content}\\\\nSource: {source}\",\\n    input_variables=[\"page_content\", \"source\"],\\n)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\qa_with_sources\\\\stuff_prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Question-answering with sources over a vector database.\"\"\"\\n\\nimport warnings\\nfrom typing import Any, Dict, List\\n\\nfrom langchain_core.callbacks import (\\n    AsyncCallbackManagerForChainRun,\\n    CallbackManagerForChainRun,\\n)\\nfrom langchain_core.documents import Document\\nfrom langchain_core.pydantic_v1 import Field, root_validator\\nfrom langchain_core.vectorstores import VectorStore\\n\\nfrom langchain.chains.combine_documents.stuff import StuffDocumentsChain\\nfrom langchain.chains.qa_with_sources.base import BaseQAWithSourcesChain\\n\\n\\nclass VectorDBQAWithSourcesChain(BaseQAWithSourcesChain):\\n    \"\"\"Question-answering with sources over a vector database.\"\"\"\\n\\n    vectorstore: VectorStore = Field(exclude=True)\\n    \"\"\"Vector Database to connect to.\"\"\"\\n    k: int = 4\\n    \"\"\"Number of results to return from store\"\"\"\\n    reduce_k_below_max_tokens: bool = False\\n    \"\"\"Reduce the number of results to return from store based on tokens limit\"\"\"\\n    max_tokens_limit: int = 3375\\n    \"\"\"Restrict the docs to return from store based on tokens,\\n    enforced only for StuffDocumentChain and if reduce_k_below_max_tokens is to true\"\"\"\\n    search_kwargs: Dict[str, Any] = Field(default_factory=dict)\\n    \"\"\"Extra search args.\"\"\"\\n\\n    def _reduce_tokens_below_limit(self, docs: List[Document]) -> List[Document]:\\n        num_docs = len(docs)\\n\\n        if self.reduce_k_below_max_tokens and isinstance(\\n            self.combine_documents_chain, StuffDocumentsChain\\n        ):\\n            tokens = [\\n                self.combine_documents_chain.llm_chain._get_num_tokens(doc.page_content)\\n                for doc in docs\\n            ]\\n            token_count = sum(tokens[:num_docs])\\n            while token_count > self.max_tokens_limit:\\n                num_docs -= 1\\n                token_count -= tokens[num_docs]\\n\\n        return docs[:num_docs]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\qa_with_sources\\\\vector_db.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_docs(\\n        self, inputs: Dict[str, Any], *, run_manager: CallbackManagerForChainRun\\n    ) -> List[Document]:\\n        question = inputs[self.question_key]\\n        docs = self.vectorstore.similarity_search(\\n            question, k=self.k, **self.search_kwargs\\n        )\\n        return self._reduce_tokens_below_limit(docs)\\n\\n    async def _aget_docs(\\n        self, inputs: Dict[str, Any], *, run_manager: AsyncCallbackManagerForChainRun\\n    ) -> List[Document]:\\n        raise NotImplementedError(\"VectorDBQAWithSourcesChain does not support async\")\\n\\n    @root_validator()\\n    def raise_deprecation(cls, values: Dict) -> Dict:\\n        warnings.warn(\\n            \"`VectorDBQAWithSourcesChain` is deprecated - \"\\n            \"please use `from langchain.chains import RetrievalQAWithSourcesChain`\"\\n        )\\n        return values\\n\\n    @property\\n    def _chain_type(self) -> str:\\n        return \"vector_db_qa_with_sources_chain\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\qa_with_sources\\\\vector_db.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Load question answering with sources chains.\"\"\"\\nfrom langchain.chains.qa_with_sources.loading import load_qa_with_sources_chain\\n\\n__all__ = [\"load_qa_with_sources_chain\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\qa_with_sources\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"LLM Chain for turning a user text query into a structured query.\"\"\"\\nfrom __future__ import annotations\\n\\nimport json\\nfrom typing import Any, Callable, List, Optional, Sequence, Tuple, Union, cast\\n\\nfrom langchain_core.exceptions import OutputParserException\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.output_parsers import BaseOutputParser\\nfrom langchain_core.output_parsers.json import parse_and_check_json_markdown\\nfrom langchain_core.prompts import BasePromptTemplate\\nfrom langchain_core.prompts.few_shot import FewShotPromptTemplate\\nfrom langchain_core.runnables import Runnable\\n\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.chains.query_constructor.ir import (\\n    Comparator,\\n    Comparison,\\n    FilterDirective,\\n    Operation,\\n    Operator,\\n    StructuredQuery,\\n)\\nfrom langchain.chains.query_constructor.parser import get_parser\\nfrom langchain.chains.query_constructor.prompt import (\\n    DEFAULT_EXAMPLES,\\n    DEFAULT_PREFIX,\\n    DEFAULT_SCHEMA_PROMPT,\\n    DEFAULT_SUFFIX,\\n    EXAMPLE_PROMPT,\\n    EXAMPLES_WITH_LIMIT,\\n    PREFIX_WITH_DATA_SOURCE,\\n    SCHEMA_WITH_LIMIT_PROMPT,\\n    SUFFIX_WITHOUT_DATA_SOURCE,\\n    USER_SPECIFIED_EXAMPLE_PROMPT,\\n)\\nfrom langchain.chains.query_constructor.schema import AttributeInfo\\n\\n\\nclass StructuredQueryOutputParser(BaseOutputParser[StructuredQuery]):\\n    \"\"\"Output parser that parses a structured query.\"\"\"\\n\\n    ast_parse: Callable\\n    \"\"\"Callable that parses dict into internal representation of query language.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\query_constructor\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def parse(self, text: str) -> StructuredQuery:\\n        try:\\n            expected_keys = [\"query\", \"filter\"]\\n            allowed_keys = [\"query\", \"filter\", \"limit\"]\\n            parsed = parse_and_check_json_markdown(text, expected_keys)\\n            if parsed[\"query\"] is None or len(parsed[\"query\"]) == 0:\\n                parsed[\"query\"] = \" \"\\n            if parsed[\"filter\"] == \"NO_FILTER\" or not parsed[\"filter\"]:\\n                parsed[\"filter\"] = None\\n            else:\\n                parsed[\"filter\"] = self.ast_parse(parsed[\"filter\"])\\n            if not parsed.get(\"limit\"):\\n                parsed.pop(\"limit\", None)\\n            return StructuredQuery(\\n                **{k: v for k, v in parsed.items() if k in allowed_keys}\\n            )\\n        except Exception as e:\\n            raise OutputParserException(\\n                f\"Parsing text\\\\n{text}\\\\n raised following error:\\\\n{e}\"\\n            )\\n\\n    @classmethod\\n    def from_components(\\n        cls,\\n        allowed_comparators: Optional[Sequence[Comparator]] = None,\\n        allowed_operators: Optional[Sequence[Operator]] = None,\\n        allowed_attributes: Optional[Sequence[str]] = None,\\n        fix_invalid: bool = False,\\n    ) -> StructuredQueryOutputParser:\\n        \"\"\"\\n        Create a structured query output parser from components.\\n\\n        Args:\\n            allowed_comparators: allowed comparators\\n            allowed_operators: allowed operators\\n\\n        Returns:\\n            a structured query output parser\\n        \"\"\"\\n        ast_parse: Callable\\n        if fix_invalid:\\n\\n            def ast_parse(raw_filter: str) -> Optional[FilterDirective]:\\n                filter = cast(Optional[FilterDirective], get_parser().parse(raw_filter))\\n                fixed = fix_filter_directive(\\n                    filter,\\n                    allowed_comparators=allowed_comparators,\\n                    allowed_operators=allowed_operators,\\n                    allowed_attributes=allowed_attributes,\\n                )\\n                return fixed' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\query_constructor\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='else:\\n            ast_parse = get_parser(\\n                allowed_comparators=allowed_comparators,\\n                allowed_operators=allowed_operators,\\n                allowed_attributes=allowed_attributes,\\n            ).parse\\n        return cls(ast_parse=ast_parse)\\n\\n\\ndef fix_filter_directive(\\n    filter: Optional[FilterDirective],\\n    *,\\n    allowed_comparators: Optional[Sequence[Comparator]] = None,\\n    allowed_operators: Optional[Sequence[Operator]] = None,\\n    allowed_attributes: Optional[Sequence[str]] = None,\\n) -> Optional[FilterDirective]:\\n    \"\"\"Fix invalid filter directive.\\n\\n    Args:\\n        filter: Filter directive to fix.\\n        allowed_comparators: allowed comparators. Defaults to all comparators.\\n        allowed_operators: allowed operators. Defaults to all operators.\\n        allowed_attributes: allowed attributes. Defaults to all attributes.\\n\\n    Returns:\\n        Fixed filter directive.\\n    \"\"\"\\n    if (\\n        not (allowed_comparators or allowed_operators or allowed_attributes)\\n    ) or not filter:\\n        return filter' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\query_constructor\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='elif isinstance(filter, Comparison):\\n        if allowed_comparators and filter.comparator not in allowed_comparators:\\n            return None\\n        if allowed_attributes and filter.attribute not in allowed_attributes:\\n            return None\\n        return filter\\n    elif isinstance(filter, Operation):\\n        if allowed_operators and filter.operator not in allowed_operators:\\n            return None\\n        args = [\\n            fix_filter_directive(\\n                arg,\\n                allowed_comparators=allowed_comparators,\\n                allowed_operators=allowed_operators,\\n                allowed_attributes=allowed_attributes,\\n            )\\n            for arg in filter.arguments\\n        ]\\n        args = [arg for arg in args if arg is not None]\\n        if not args:\\n            return None\\n        elif len(args) == 1 and filter.operator in (Operator.AND, Operator.OR):\\n            return args[0]\\n        else:\\n            return Operation(\\n                operator=filter.operator,\\n                arguments=args,\\n            )\\n    else:\\n        return filter\\n\\n\\ndef _format_attribute_info(info: Sequence[Union[AttributeInfo, dict]]) -> str:\\n    info_dicts = {}\\n    for i in info:\\n        i_dict = dict(i)\\n        info_dicts[i_dict.pop(\"name\")] = i_dict\\n    return json.dumps(info_dicts, indent=4).replace(\"{\", \"{{\").replace(\"}\", \"}}\")\\n\\n\\ndef construct_examples(input_output_pairs: Sequence[Tuple[str, dict]]) -> List[dict]:\\n    \"\"\"Construct examples from input-output pairs.\\n\\n    Args:\\n        input_output_pairs: Sequence of input-output pairs.\\n\\n    Returns:\\n        List of examples.\\n    \"\"\"\\n    examples = []\\n    for i, (_input, output) in enumerate(input_output_pairs):\\n        structured_request = (\\n            json.dumps(output, indent=4).replace(\"{\", \"{{\").replace(\"}\", \"}}\")\\n        )\\n        example = {\\n            \"i\": i + 1,\\n            \"user_query\": _input,\\n            \"structured_request\": structured_request,\\n        }\\n        examples.append(example)\\n    return examples' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\query_constructor\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def get_query_constructor_prompt(\\n    document_contents: str,\\n    attribute_info: Sequence[Union[AttributeInfo, dict]],\\n    *,\\n    examples: Optional[Sequence] = None,\\n    allowed_comparators: Sequence[Comparator] = tuple(Comparator),\\n    allowed_operators: Sequence[Operator] = tuple(Operator),\\n    enable_limit: bool = False,\\n    schema_prompt: Optional[BasePromptTemplate] = None,\\n    **kwargs: Any,\\n) -> BasePromptTemplate:\\n    \"\"\"Create query construction prompt.\\n\\n    Args:\\n        document_contents: The contents of the document to be queried.\\n        attribute_info: A list of AttributeInfo objects describing\\n            the attributes of the document.\\n        examples: Optional list of examples to use for the chain.\\n        allowed_comparators: Sequence of allowed comparators.\\n        allowed_operators: Sequence of allowed operators.\\n        enable_limit: Whether to enable the limit operator. Defaults to False.\\n        schema_prompt: Prompt for describing query schema. Should have string input\\n            variables allowed_comparators and allowed_operators.\\n        **kwargs: Additional named params to pass to FewShotPromptTemplate init.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\query_constructor\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Returns:\\n        A prompt template that can be used to construct queries.\\n    \"\"\"\\n    default_schema_prompt = (\\n        SCHEMA_WITH_LIMIT_PROMPT if enable_limit else DEFAULT_SCHEMA_PROMPT\\n    )\\n    schema_prompt = schema_prompt or default_schema_prompt\\n    attribute_str = _format_attribute_info(attribute_info)\\n    schema = schema_prompt.format(\\n        allowed_comparators=\" | \".join(allowed_comparators),\\n        allowed_operators=\" | \".join(allowed_operators),\\n    )\\n    if examples and isinstance(examples[0], tuple):\\n        examples = construct_examples(examples)\\n        example_prompt = USER_SPECIFIED_EXAMPLE_PROMPT\\n        prefix = PREFIX_WITH_DATA_SOURCE.format(\\n            schema=schema, content=document_contents, attributes=attribute_str\\n        )\\n        suffix = SUFFIX_WITHOUT_DATA_SOURCE.format(i=len(examples) + 1)\\n    else:\\n        examples = examples or (\\n            EXAMPLES_WITH_LIMIT if enable_limit else DEFAULT_EXAMPLES\\n        )\\n        example_prompt = EXAMPLE_PROMPT\\n        prefix = DEFAULT_PREFIX.format(schema=schema)\\n        suffix = DEFAULT_SUFFIX.format(\\n            i=len(examples) + 1, content=document_contents, attributes=attribute_str\\n        )\\n    return FewShotPromptTemplate(\\n        examples=list(examples),\\n        example_prompt=example_prompt,\\n        input_variables=[\"query\"],\\n        suffix=suffix,\\n        prefix=prefix,\\n        **kwargs,\\n    )\\n\\n\\ndef load_query_constructor_chain(\\n    llm: BaseLanguageModel,\\n    document_contents: str,\\n    attribute_info: Sequence[Union[AttributeInfo, dict]],\\n    examples: Optional[List] = None,\\n    allowed_comparators: Sequence[Comparator] = tuple(Comparator),\\n    allowed_operators: Sequence[Operator] = tuple(Operator),\\n    enable_limit: bool = False,\\n    schema_prompt: Optional[BasePromptTemplate] = None,\\n    **kwargs: Any,\\n) -> LLMChain:\\n    \"\"\"Load a query constructor chain.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\query_constructor\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Args:\\n        llm: BaseLanguageModel to use for the chain.\\n        document_contents: The contents of the document to be queried.\\n        attribute_info: Sequence of attributes in the document.\\n        examples: Optional list of examples to use for the chain.\\n        allowed_comparators: Sequence of allowed comparators. Defaults to all\\n            Comparators.\\n        allowed_operators: Sequence of allowed operators. Defaults to all Operators.\\n        enable_limit: Whether to enable the limit operator. Defaults to False.\\n        schema_prompt: Prompt for describing query schema. Should have string input\\n            variables allowed_comparators and allowed_operators.\\n        **kwargs: Arbitrary named params to pass to LLMChain.\\n\\n    Returns:\\n        A LLMChain that can be used to construct queries.\\n    \"\"\"\\n    prompt = get_query_constructor_prompt(\\n        document_contents,\\n        attribute_info,\\n        examples=examples,\\n        allowed_comparators=allowed_comparators,\\n        allowed_operators=allowed_operators,\\n        enable_limit=enable_limit,\\n        schema_prompt=schema_prompt,\\n    )\\n    allowed_attributes = []\\n    for ainfo in attribute_info:\\n        allowed_attributes.append(\\n            ainfo.name if isinstance(ainfo, AttributeInfo) else ainfo[\"name\"]\\n        )\\n    output_parser = StructuredQueryOutputParser.from_components(\\n        allowed_comparators=allowed_comparators,\\n        allowed_operators=allowed_operators,\\n        allowed_attributes=allowed_attributes,\\n    )\\n    # For backwards compatibility.\\n    prompt.output_parser = output_parser\\n    return LLMChain(llm=llm, prompt=prompt, output_parser=output_parser, **kwargs)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\query_constructor\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def load_query_constructor_runnable(\\n    llm: BaseLanguageModel,\\n    document_contents: str,\\n    attribute_info: Sequence[Union[AttributeInfo, dict]],\\n    *,\\n    examples: Optional[Sequence] = None,\\n    allowed_comparators: Sequence[Comparator] = tuple(Comparator),\\n    allowed_operators: Sequence[Operator] = tuple(Operator),\\n    enable_limit: bool = False,\\n    schema_prompt: Optional[BasePromptTemplate] = None,\\n    fix_invalid: bool = False,\\n    **kwargs: Any,\\n) -> Runnable:\\n    \"\"\"Load a query constructor runnable chain.\\n\\n    Args:\\n        llm: BaseLanguageModel to use for the chain.\\n        document_contents: The contents of the document to be queried.\\n        attribute_info: Sequence of attributes in the document.\\n        examples: Optional list of examples to use for the chain.\\n        allowed_comparators: Sequence of allowed comparators. Defaults to all\\n            Comparators.\\n        allowed_operators: Sequence of allowed operators. Defaults to all Operators.\\n        enable_limit: Whether to enable the limit operator. Defaults to False.\\n        schema_prompt: Prompt for describing query schema. Should have string input\\n            variables allowed_comparators and allowed_operators.\\n        fix_invalid: Whether to fix invalid filter directives by ignoring invalid\\n            operators, comparators and attributes.\\n        **kwargs: Additional named params to pass to FewShotPromptTemplate init.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\query_constructor\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Returns:\\n        A Runnable that can be used to construct queries.\\n    \"\"\"\\n    prompt = get_query_constructor_prompt(\\n        document_contents,\\n        attribute_info,\\n        examples=examples,\\n        allowed_comparators=allowed_comparators,\\n        allowed_operators=allowed_operators,\\n        enable_limit=enable_limit,\\n        schema_prompt=schema_prompt,\\n        **kwargs,\\n    )\\n    allowed_attributes = []\\n    for ainfo in attribute_info:\\n        allowed_attributes.append(\\n            ainfo.name if isinstance(ainfo, AttributeInfo) else ainfo[\"name\"]\\n        )\\n    output_parser = StructuredQueryOutputParser.from_components(\\n        allowed_comparators=allowed_comparators,\\n        allowed_operators=allowed_operators,\\n        allowed_attributes=allowed_attributes,\\n        fix_invalid=fix_invalid,\\n    )\\n    return prompt | llm | output_parser' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\query_constructor\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Internal representation of a structured query language.\"\"\"\\nfrom __future__ import annotations\\n\\nfrom abc import ABC, abstractmethod\\nfrom enum import Enum\\nfrom typing import Any, List, Optional, Sequence, Union\\n\\nfrom langchain_core.pydantic_v1 import BaseModel\\n\\n\\nclass Visitor(ABC):\\n    \"\"\"Defines interface for IR translation using visitor pattern.\"\"\"\\n\\n    allowed_comparators: Optional[Sequence[Comparator]] = None\\n    allowed_operators: Optional[Sequence[Operator]] = None\\n\\n    def _validate_func(self, func: Union[Operator, Comparator]) -> None:\\n        if isinstance(func, Operator) and self.allowed_operators is not None:\\n            if func not in self.allowed_operators:\\n                raise ValueError(\\n                    f\"Received disallowed operator {func}. Allowed \"\\n                    f\"comparators are {self.allowed_operators}\"\\n                )\\n        if isinstance(func, Comparator) and self.allowed_comparators is not None:\\n            if func not in self.allowed_comparators:\\n                raise ValueError(\\n                    f\"Received disallowed comparator {func}. Allowed \"\\n                    f\"comparators are {self.allowed_comparators}\"\\n                )\\n\\n    @abstractmethod\\n    def visit_operation(self, operation: Operation) -> Any:\\n        \"\"\"Translate an Operation.\"\"\"\\n\\n    @abstractmethod\\n    def visit_comparison(self, comparison: Comparison) -> Any:\\n        \"\"\"Translate a Comparison.\"\"\"\\n\\n    @abstractmethod\\n    def visit_structured_query(self, structured_query: StructuredQuery) -> Any:\\n        \"\"\"Translate a StructuredQuery.\"\"\"\\n\\n\\ndef _to_snake_case(name: str) -> str:\\n    \"\"\"Convert a name into snake_case.\"\"\"\\n    snake_case = \"\"\\n    for i, char in enumerate(name):\\n        if char.isupper() and i != 0:\\n            snake_case += \"_\" + char.lower()\\n        else:\\n            snake_case += char.lower()\\n    return snake_case\\n\\n\\nclass Expr(BaseModel):\\n    \"\"\"Base class for all expressions.\"\"\"\\n\\n    def accept(self, visitor: Visitor) -> Any:\\n        \"\"\"Accept a visitor.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\query_constructor\\\\ir.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Args:\\n            visitor: visitor to accept\\n\\n        Returns:\\n            result of visiting\\n        \"\"\"\\n        return getattr(visitor, f\"visit_{_to_snake_case(self.__class__.__name__)}\")(\\n            self\\n        )\\n\\n\\nclass Operator(str, Enum):\\n    \"\"\"Enumerator of the operations.\"\"\"\\n\\n    AND = \"and\"\\n    OR = \"or\"\\n    NOT = \"not\"\\n\\n\\nclass Comparator(str, Enum):\\n    \"\"\"Enumerator of the comparison operators.\"\"\"\\n\\n    EQ = \"eq\"\\n    NE = \"ne\"\\n    GT = \"gt\"\\n    GTE = \"gte\"\\n    LT = \"lt\"\\n    LTE = \"lte\"\\n    CONTAIN = \"contain\"\\n    LIKE = \"like\"\\n    IN = \"in\"\\n    NIN = \"nin\"\\n\\n\\nclass FilterDirective(Expr, ABC):\\n    \"\"\"A filtering expression.\"\"\"\\n\\n\\nclass Comparison(FilterDirective):\\n    \"\"\"A comparison to a value.\"\"\"\\n\\n    comparator: Comparator\\n    attribute: str\\n    value: Any\\n\\n\\nclass Operation(FilterDirective):\\n    \"\"\"A logical operation over other directives.\"\"\"\\n\\n    operator: Operator\\n    arguments: List[FilterDirective]\\n\\n\\nclass StructuredQuery(Expr):\\n    \"\"\"A structured query.\"\"\"\\n\\n    query: str\\n    \"\"\"Query string.\"\"\"\\n    filter: Optional[FilterDirective]\\n    \"\"\"Filtering expression.\"\"\"\\n    limit: Optional[int]\\n    \"\"\"Limit on the number of results.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\query_constructor\\\\ir.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='import datetime\\nimport warnings\\nfrom typing import Any, Literal, Optional, Sequence, Union\\n\\nfrom langchain_core.utils import check_package_version\\nfrom typing_extensions import TypedDict\\n\\ntry:\\n    check_package_version(\"lark\", gte_version=\"1.1.5\")\\n    from lark import Lark, Transformer, v_args\\nexcept ImportError:\\n\\n    def v_args(*args: Any, **kwargs: Any) -> Any:  # type: ignore\\n        \"\"\"Dummy decorator for when lark is not installed.\"\"\"\\n        return lambda _: None\\n\\n    Transformer = object  # type: ignore\\n    Lark = object  # type: ignore\\n\\nfrom langchain.chains.query_constructor.ir import (\\n    Comparator,\\n    Comparison,\\n    FilterDirective,\\n    Operation,\\n    Operator,\\n)\\n\\nGRAMMAR = r\"\"\"\\n    ?program: func_call\\n    ?expr: func_call\\n        | value\\n\\n    func_call: CNAME \"(\" [args] \")\"\\n\\n    ?value: SIGNED_INT -> int\\n        | SIGNED_FLOAT -> float\\n        | DATE -> date\\n        | list\\n        | string\\n        | (\"false\" | \"False\" | \"FALSE\") -> false\\n        | (\"true\" | \"True\" | \"TRUE\") -> true\\n\\n    args: expr (\",\" expr)*\\n    DATE.2: /[\"\\']?(\\\\d{4}-[01]\\\\d-[0-3]\\\\d)[\"\\']?/\\n    string: /\\'[^\\']*\\'/ | ESCAPED_STRING\\n    list: \"[\" [args] \"]\"\\n\\n    %import common.CNAME\\n    %import common.ESCAPED_STRING\\n    %import common.SIGNED_FLOAT\\n    %import common.SIGNED_INT\\n    %import common.WS\\n    %ignore WS\\n\"\"\"\\n\\n\\nclass ISO8601Date(TypedDict):\\n    \"\"\"A date in ISO 8601 format (YYYY-MM-DD).\"\"\"\\n\\n    date: str\\n    type: Literal[\"date\"]\\n\\n\\n@v_args(inline=True)\\nclass QueryTransformer(Transformer):\\n    \"\"\"Transforms a query string into an intermediate representation.\"\"\"\\n\\n    def __init__(\\n        self,\\n        *args: Any,\\n        allowed_comparators: Optional[Sequence[Comparator]] = None,\\n        allowed_operators: Optional[Sequence[Operator]] = None,\\n        allowed_attributes: Optional[Sequence[str]] = None,\\n        **kwargs: Any,\\n    ):\\n        super().__init__(*args, **kwargs)\\n        self.allowed_comparators = allowed_comparators\\n        self.allowed_operators = allowed_operators\\n        self.allowed_attributes = allowed_attributes' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\query_constructor\\\\parser.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def program(self, *items: Any) -> tuple:\\n        return items\\n\\n    def func_call(self, func_name: Any, args: list) -> FilterDirective:\\n        func = self._match_func_name(str(func_name))\\n        if isinstance(func, Comparator):\\n            if self.allowed_attributes and args[0] not in self.allowed_attributes:\\n                raise ValueError(\\n                    f\"Received invalid attributes {args[0]}. Allowed attributes are \"\\n                    f\"{self.allowed_attributes}\"\\n                )\\n            return Comparison(comparator=func, attribute=args[0], value=args[1])\\n        elif len(args) == 1 and func in (Operator.AND, Operator.OR):\\n            return args[0]\\n        else:\\n            return Operation(operator=func, arguments=args)\\n\\n    def _match_func_name(self, func_name: str) -> Union[Operator, Comparator]:\\n        if func_name in set(Comparator):\\n            if self.allowed_comparators is not None:\\n                if func_name not in self.allowed_comparators:\\n                    raise ValueError(\\n                        f\"Received disallowed comparator {func_name}. Allowed \"\\n                        f\"comparators are {self.allowed_comparators}\"\\n                    )\\n            return Comparator(func_name)\\n        elif func_name in set(Operator):\\n            if self.allowed_operators is not None:\\n                if func_name not in self.allowed_operators:\\n                    raise ValueError(\\n                        f\"Received disallowed operator {func_name}. Allowed operators\"\\n                        f\" are {self.allowed_operators}\"\\n                    )\\n            return Operator(func_name)\\n        else:\\n            raise ValueError(\\n                f\"Received unrecognized function {func_name}. Valid functions are \"\\n                f\"{list(Operator) + list(Comparator)}\"\\n            )\\n\\n    def args(self, *items: Any) -> tuple:\\n        return items\\n\\n    def false(self) -> bool:\\n        return False\\n\\n    def true(self) -> bool:\\n        return True' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\query_constructor\\\\parser.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def list(self, item: Any) -> list:\\n        if item is None:\\n            return []\\n        return list(item)\\n\\n    def int(self, item: Any) -> int:\\n        return int(item)\\n\\n    def float(self, item: Any) -> float:\\n        return float(item)\\n\\n    def date(self, item: Any) -> ISO8601Date:\\n        item = str(item).strip(\"\\\\\"\\'\")\\n        try:\\n            datetime.datetime.strptime(item, \"%Y-%m-%d\")\\n        except ValueError:\\n            warnings.warn(\\n                \"Dates are expected to be provided in ISO 8601 date format \"\\n                \"(YYYY-MM-DD).\"\\n            )\\n        return {\"date\": item, \"type\": \"date\"}\\n\\n    def string(self, item: Any) -> str:\\n        # Remove escaped quotes\\n        return str(item).strip(\"\\\\\"\\'\")\\n\\n\\ndef get_parser(\\n    allowed_comparators: Optional[Sequence[Comparator]] = None,\\n    allowed_operators: Optional[Sequence[Operator]] = None,\\n    allowed_attributes: Optional[Sequence[str]] = None,\\n) -> Lark:\\n    \"\"\"\\n    Returns a parser for the query language.\\n\\n    Args:\\n        allowed_comparators: Optional[Sequence[Comparator]]\\n        allowed_operators: Optional[Sequence[Operator]]\\n\\n    Returns:\\n        Lark parser for the query language.\\n    \"\"\"\\n    # QueryTransformer is None when Lark cannot be imported.\\n    if QueryTransformer is None:\\n        raise ImportError(\\n            \"Cannot import lark, please install it with \\'pip install lark\\'.\"\\n        )\\n    transformer = QueryTransformer(\\n        allowed_comparators=allowed_comparators,\\n        allowed_operators=allowed_operators,\\n        allowed_attributes=allowed_attributes,\\n    )\\n    return Lark(GRAMMAR, parser=\"lalr\", transformer=transformer, start=\"program\")' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\query_constructor\\\\parser.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# flake8: noqa\\nfrom langchain_core.prompts import PromptTemplate\\n\\nSONG_DATA_SOURCE = \"\"\"\\\\\\n```json\\n{{\\n    \"content\": \"Lyrics of a song\",\\n    \"attributes\": {{\\n        \"artist\": {{\\n            \"type\": \"string\",\\n            \"description\": \"Name of the song artist\"\\n        }},\\n        \"length\": {{\\n            \"type\": \"integer\",\\n            \"description\": \"Length of the song in seconds\"\\n        }},\\n        \"genre\": {{\\n            \"type\": \"string\",\\n            \"description\": \"The song genre, one of \\\\\"pop\\\\\", \\\\\"rock\\\\\" or \\\\\"rap\\\\\"\"\\n        }}\\n    }}\\n}}\\n```\\\\\\n\"\"\"\\n\\nFULL_ANSWER = \"\"\"\\\\\\n```json\\n{{\\n    \"query\": \"teenager love\",\\n    \"filter\": \"and(or(eq(\\\\\\\\\"artist\\\\\\\\\", \\\\\\\\\"Taylor Swift\\\\\\\\\"), eq(\\\\\\\\\"artist\\\\\\\\\", \\\\\\\\\"Katy Perry\\\\\\\\\")), lt(\\\\\\\\\"length\\\\\\\\\", 180), eq(\\\\\\\\\"genre\\\\\\\\\", \\\\\\\\\"pop\\\\\\\\\"))\"\\n}}\\n```\\\\\\n\"\"\"\\n\\nNO_FILTER_ANSWER = \"\"\"\\\\\\n```json\\n{{\\n    \"query\": \"\",\\n    \"filter\": \"NO_FILTER\"\\n}}\\n```\\\\\\n\"\"\"\\n\\nWITH_LIMIT_ANSWER = \"\"\"\\\\\\n```json\\n{{\\n    \"query\": \"love\",\\n    \"filter\": \"NO_FILTER\",\\n    \"limit\": 2\\n}}\\n```\\\\\\n\"\"\"\\n\\nDEFAULT_EXAMPLES = [\\n    {\\n        \"i\": 1,\\n        \"data_source\": SONG_DATA_SOURCE,\\n        \"user_query\": \"What are songs by Taylor Swift or Katy Perry about teenage romance under 3 minutes long in the dance pop genre\",\\n        \"structured_request\": FULL_ANSWER,\\n    },\\n    {\\n        \"i\": 2,\\n        \"data_source\": SONG_DATA_SOURCE,\\n        \"user_query\": \"What are songs that were not published on Spotify\",\\n        \"structured_request\": NO_FILTER_ANSWER,\\n    },\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\query_constructor\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='EXAMPLES_WITH_LIMIT = [\\n    {\\n        \"i\": 1,\\n        \"data_source\": SONG_DATA_SOURCE,\\n        \"user_query\": \"What are songs by Taylor Swift or Katy Perry about teenage romance under 3 minutes long in the dance pop genre\",\\n        \"structured_request\": FULL_ANSWER,\\n    },\\n    {\\n        \"i\": 2,\\n        \"data_source\": SONG_DATA_SOURCE,\\n        \"user_query\": \"What are songs that were not published on Spotify\",\\n        \"structured_request\": NO_FILTER_ANSWER,\\n    },\\n    {\\n        \"i\": 3,\\n        \"data_source\": SONG_DATA_SOURCE,\\n        \"user_query\": \"What are three songs about love\",\\n        \"structured_request\": WITH_LIMIT_ANSWER,\\n    },\\n]\\n\\nEXAMPLE_PROMPT_TEMPLATE = \"\"\"\\\\\\n<< Example {i}. >>\\nData Source:\\n{data_source}\\n\\nUser Query:\\n{user_query}\\n\\nStructured Request:\\n{structured_request}\\n\"\"\"\\n\\nEXAMPLE_PROMPT = PromptTemplate.from_template(EXAMPLE_PROMPT_TEMPLATE)\\n\\nUSER_SPECIFIED_EXAMPLE_PROMPT = PromptTemplate.from_template(\\n    \"\"\"\\\\\\n<< Example {i}. >>\\nUser Query:\\n{user_query}\\n\\nStructured Request:\\n```json\\n{structured_request}\\n```\\n\"\"\"\\n)\\n\\nDEFAULT_SCHEMA = \"\"\"\\\\\\n<< Structured Request Schema >>\\nWhen responding use a markdown code snippet with a JSON object formatted in the following schema:\\n\\n```json\\n{{{{\\n    \"query\": string \\\\\\\\ text string to compare to document contents\\n    \"filter\": string \\\\\\\\ logical condition statement for filtering documents\\n}}}}\\n```\\n\\nThe query string should contain only text that is expected to match the contents of documents. Any conditions in the filter should not be mentioned in the query as well.\\n\\nA logical condition statement is composed of one or more comparison and logical operation statements.\\n\\nA comparison statement takes the form: `comp(attr, val)`:\\n- `comp` ({allowed_comparators}): comparator\\n- `attr` (string):  name of attribute to apply the comparison to\\n- `val` (string): is the comparison value' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\query_constructor\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='A logical operation statement takes the form `op(statement1, statement2, ...)`:\\n- `op` ({allowed_operators}): logical operator\\n- `statement1`, `statement2`, ... (comparison statements or logical operation statements): one or more statements to apply the operation to\\n\\nMake sure that you only use the comparators and logical operators listed above and no others.\\nMake sure that filters only refer to attributes that exist in the data source.\\nMake sure that filters only use the attributed names with its function names if there are functions applied on them.\\nMake sure that filters only use format `YYYY-MM-DD` when handling date data typed values.\\nMake sure that filters take into account the descriptions of attributes and only make comparisons that are feasible given the type of data being stored.\\nMake sure that filters are only used as needed. If there are no filters that should be applied return \"NO_FILTER\" for the filter value.\\\\\\n\"\"\"\\nDEFAULT_SCHEMA_PROMPT = PromptTemplate.from_template(DEFAULT_SCHEMA)\\n\\nSCHEMA_WITH_LIMIT = \"\"\"\\\\\\n<< Structured Request Schema >>\\nWhen responding use a markdown code snippet with a JSON object formatted in the following schema:\\n\\n```json\\n{{{{\\n    \"query\": string \\\\\\\\ text string to compare to document contents\\n    \"filter\": string \\\\\\\\ logical condition statement for filtering documents\\n    \"limit\": int \\\\\\\\ the number of documents to retrieve\\n}}}}\\n```\\n\\nThe query string should contain only text that is expected to match the contents of documents. Any conditions in the filter should not be mentioned in the query as well.\\n\\nA logical condition statement is composed of one or more comparison and logical operation statements.\\n\\nA comparison statement takes the form: `comp(attr, val)`:\\n- `comp` ({allowed_comparators}): comparator\\n- `attr` (string):  name of attribute to apply the comparison to\\n- `val` (string): is the comparison value' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\query_constructor\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='A logical operation statement takes the form `op(statement1, statement2, ...)`:\\n- `op` ({allowed_operators}): logical operator\\n- `statement1`, `statement2`, ... (comparison statements or logical operation statements): one or more statements to apply the operation to\\n\\nMake sure that you only use the comparators and logical operators listed above and no others.\\nMake sure that filters only refer to attributes that exist in the data source.\\nMake sure that filters only use the attributed names with its function names if there are functions applied on them.\\nMake sure that filters only use format `YYYY-MM-DD` when handling date data typed values.\\nMake sure that filters take into account the descriptions of attributes and only make comparisons that are feasible given the type of data being stored.\\nMake sure that filters are only used as needed. If there are no filters that should be applied return \"NO_FILTER\" for the filter value.\\nMake sure the `limit` is always an int value. It is an optional parameter so leave it blank if it does not make sense.\\n\"\"\"\\nSCHEMA_WITH_LIMIT_PROMPT = PromptTemplate.from_template(SCHEMA_WITH_LIMIT)\\n\\nDEFAULT_PREFIX = \"\"\"\\\\\\nYour goal is to structure the user\\'s query to match the request schema provided below.\\n\\n{schema}\\\\\\n\"\"\"\\n\\nPREFIX_WITH_DATA_SOURCE = (\\n    DEFAULT_PREFIX\\n    + \"\"\"\\n\\n<< Data Source >>\\n```json\\n{{{{\\n    \"content\": \"{content}\",\\n    \"attributes\": {attributes}\\n}}}}\\n```\\n\"\"\"\\n)\\n\\nDEFAULT_SUFFIX = \"\"\"\\\\\\n<< Example {i}. >>\\nData Source:\\n```json\\n{{{{\\n    \"content\": \"{content}\",\\n    \"attributes\": {attributes}\\n}}}}\\n```\\n\\nUser Query:\\n{{query}}\\n\\nStructured Request:\\n\"\"\"\\n\\nSUFFIX_WITHOUT_DATA_SOURCE = \"\"\"\\\\\\n<< Example {i}. >>\\nUser Query:\\n{{query}}\\n\\nStructured Request:\\n\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\query_constructor\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.pydantic_v1 import BaseModel\\n\\n\\nclass AttributeInfo(BaseModel):\\n    \"\"\"Information about a data source attribute.\"\"\"\\n\\n    name: str\\n    description: str\\n    type: str\\n\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n\\n        arbitrary_types_allowed = True\\n        frozen = True' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\query_constructor\\\\schema.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# flake8: noqa\\nfrom langchain.chains.prompt_selector import ConditionalPromptSelector, is_chat_model\\nfrom langchain_core.prompts.chat import (\\n    ChatPromptTemplate,\\n    HumanMessagePromptTemplate,\\n    SystemMessagePromptTemplate,\\n)\\nfrom langchain_core.prompts.prompt import PromptTemplate\\n\\nquestion_prompt_template = \"\"\"Use the following portion of a long document to see if any of the text is relevant to answer the question. \\nReturn any relevant text verbatim.\\n{context}\\nQuestion: {question}\\nRelevant text, if any:\"\"\"\\nQUESTION_PROMPT = PromptTemplate(\\n    template=question_prompt_template, input_variables=[\"context\", \"question\"]\\n)\\nsystem_template = \"\"\"Use the following portion of a long document to see if any of the text is relevant to answer the question. \\nReturn any relevant text verbatim.\\n______________________\\n{context}\"\"\"\\nmessages = [\\n    SystemMessagePromptTemplate.from_template(system_template),\\n    HumanMessagePromptTemplate.from_template(\"{question}\"),\\n]\\nCHAT_QUESTION_PROMPT = ChatPromptTemplate.from_messages(messages)\\n\\n\\nQUESTION_PROMPT_SELECTOR = ConditionalPromptSelector(\\n    default_prompt=QUESTION_PROMPT, conditionals=[(is_chat_model, CHAT_QUESTION_PROMPT)]\\n)\\n\\ncombine_prompt_template = \"\"\"Given the following extracted parts of a long document and a question, create a final answer. \\nIf you don\\'t know the answer, just say that you don\\'t know. Don\\'t try to make up an answer.\\n\\nQUESTION: Which state/country\\'s law governs the interpretation of the contract?\\n=========\\nContent: This Agreement is governed by English law and the parties submit to the exclusive jurisdiction of the English courts in  relation to any dispute (contractual or non-contractual) concerning this Agreement save that either party may apply to any court for an  injunction or other relief to protect its Intellectual Property Rights.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\question_answering\\\\map_reduce_prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Content: No Waiver. Failure or delay in exercising any right or remedy under this Agreement shall not constitute a waiver of such (or any other)  right or remedy.\\\\n\\\\n11.7 Severability. The invalidity, illegality or unenforceability of any term (or part of a term) of this Agreement shall not affect the continuation  in force of the remainder of the term (if any) and this Agreement.\\\\n\\\\n11.8 No Agency. Except as expressly stated otherwise, nothing in this Agreement shall create an agency, partnership or joint venture of any  kind between the parties.\\\\n\\\\n11.9 No Third-Party Beneficiaries.\\n\\nContent: (b) if Google believes, in good faith, that the Distributor has violated or caused Google to violate any Anti-Bribery Laws (as  defined in Clause 8.5) or that such a violation is reasonably likely to occur,\\n=========\\nFINAL ANSWER: This Agreement is governed by English law.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\question_answering\\\\map_reduce_prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='QUESTION: What did the president say about Michael Jackson?\\n=========\\nContent: Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \\\\n\\\\nLast year COVID-19 kept us apart. This year we are finally together again. \\\\n\\\\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \\\\n\\\\nWith a duty to one another to the American people to the Constitution. \\\\n\\\\nAnd with an unwavering resolve that freedom will always triumph over tyranny. \\\\n\\\\nSix days ago, Russia’s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. \\\\n\\\\nHe thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. \\\\n\\\\nHe met the Ukrainian people. \\\\n\\\\nFrom President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world. \\\\n\\\\nGroups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\question_answering\\\\map_reduce_prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Content: And we won’t stop. \\\\n\\\\nWe have lost so much to COVID-19. Time with one another. And worst of all, so much loss of life. \\\\n\\\\nLet’s use this moment to reset. Let’s stop looking at COVID-19 as a partisan dividing line and see it for what it is: A God-awful disease.  \\\\n\\\\nLet’s stop seeing each other as enemies, and start seeing each other for who we really are: Fellow Americans.  \\\\n\\\\nWe can’t change how divided we’ve been. But we can change how we move forward—on COVID-19 and other issues we must face together. \\\\n\\\\nI recently visited the New York City Police Department days after the funerals of Officer Wilbert Mora and his partner, Officer Jason Rivera. \\\\n\\\\nThey were responding to a 9-1-1 call when a man shot and killed them with a stolen gun. \\\\n\\\\nOfficer Mora was 27 years old. \\\\n\\\\nOfficer Rivera was 22. \\\\n\\\\nBoth Dominican Americans who’d grown up on the same streets they later chose to patrol as police officers. \\\\n\\\\nI spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their mission to restore the trust and safety every community deserves.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\question_answering\\\\map_reduce_prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Content: And a proud Ukrainian people, who have known 30 years  of independence, have repeatedly shown that they will not tolerate anyone who tries to take their country backwards.  \\\\n\\\\nTo all Americans, I will be honest with you, as I’ve always promised. A Russian dictator, invading a foreign country, has costs around the world. \\\\n\\\\nAnd I’m taking robust action to make sure the pain of our sanctions  is targeted at Russia’s economy. And I will use every tool at our disposal to protect American businesses and consumers. \\\\n\\\\nTonight, I can announce that the United States has worked with 30 other countries to release 60 Million barrels of oil from reserves around the world.  \\\\n\\\\nAmerica will lead that effort, releasing 30 Million barrels from our own Strategic Petroleum Reserve. And we stand ready to do more if necessary, unified with our allies.  \\\\n\\\\nThese steps will help blunt gas prices here at home. And I know the news about what’s happening can seem alarming. \\\\n\\\\nBut I want you to know that we are going to be okay.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\question_answering\\\\map_reduce_prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Content: More support for patients and families. \\\\n\\\\nTo get there, I call on Congress to fund ARPA-H, the Advanced Research Projects Agency for Health. \\\\n\\\\nIt’s based on DARPA—the Defense Department project that led to the Internet, GPS, and so much more.  \\\\n\\\\nARPA-H will have a singular purpose—to drive breakthroughs in cancer, Alzheimer’s, diabetes, and more. \\\\n\\\\nA unity agenda for the nation. \\\\n\\\\nWe can do this. \\\\n\\\\nMy fellow Americans—tonight , we have gathered in a sacred space—the citadel of our democracy. \\\\n\\\\nIn this Capitol, generation after generation, Americans have debated great questions amid great strife, and have done great things. \\\\n\\\\nWe have fought for freedom, expanded liberty, defeated totalitarianism and terror. \\\\n\\\\nAnd built the strongest, freest, and most prosperous nation the world has ever known. \\\\n\\\\nNow is the hour. \\\\n\\\\nOur moment of responsibility. \\\\n\\\\nOur test of resolve and conscience, of history itself. \\\\n\\\\nIt is in this moment that our character is formed. Our purpose is found. Our future is forged. \\\\n\\\\nWell I know this nation.\\n=========\\nFINAL ANSWER: The president did not mention Michael Jackson.\\n\\nQUESTION: {question}\\n=========\\n{summaries}\\n=========\\nFINAL ANSWER:\"\"\"\\nCOMBINE_PROMPT = PromptTemplate(\\n    template=combine_prompt_template, input_variables=[\"summaries\", \"question\"]\\n)\\n\\nsystem_template = \"\"\"Given the following extracted parts of a long document and a question, create a final answer. \\nIf you don\\'t know the answer, just say that you don\\'t know. Don\\'t try to make up an answer.\\n______________________\\n{summaries}\"\"\"\\nmessages = [\\n    SystemMessagePromptTemplate.from_template(system_template),\\n    HumanMessagePromptTemplate.from_template(\"{question}\"),\\n]\\nCHAT_COMBINE_PROMPT = ChatPromptTemplate.from_messages(messages)\\n\\n\\nCOMBINE_PROMPT_SELECTOR = ConditionalPromptSelector(\\n    default_prompt=COMBINE_PROMPT, conditionals=[(is_chat_model, CHAT_COMBINE_PROMPT)]\\n)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\question_answering\\\\map_reduce_prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# flake8: noqa\\nfrom langchain.output_parsers.regex import RegexParser\\nfrom langchain_core.prompts import PromptTemplate\\n\\noutput_parser = RegexParser(\\n    regex=r\"(.*?)\\\\nScore: (\\\\d*)\",\\n    output_keys=[\"answer\", \"score\"],\\n)\\n\\nprompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\nIn addition to giving an answer, also return a score of how fully it answered the user\\'s question. This should be in the following format:\\n\\nQuestion: [question here]\\nHelpful Answer: [answer here]\\nScore: [score between 0 and 100]\\n\\nHow to determine the score:\\n- Higher is a better answer\\n- Better responds fully to the asked question, with sufficient level of detail\\n- If you do not know the answer based on the context, that should be a score of 0\\n- Don\\'t be overconfident!\\n\\nExample #1\\n\\nContext:\\n---------\\nApples are red\\n---------\\nQuestion: what color are apples?\\nHelpful Answer: red\\nScore: 100\\n\\nExample #2\\n\\nContext:\\n---------\\nit was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\\n---------\\nQuestion: what type was the car?\\nHelpful Answer: a sports car or an suv\\nScore: 60\\n\\nExample #3\\n\\nContext:\\n---------\\nPears are either red or orange\\n---------\\nQuestion: what color are apples?\\nHelpful Answer: This document does not answer the question\\nScore: 0\\n\\nBegin!\\n\\nContext:\\n---------\\n{context}\\n---------\\nQuestion: {question}\\nHelpful Answer:\"\"\"\\nPROMPT = PromptTemplate(\\n    template=prompt_template,\\n    input_variables=[\"context\", \"question\"],\\n    output_parser=output_parser,\\n)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\question_answering\\\\map_rerank_prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# flake8: noqa\\nfrom langchain.chains.prompt_selector import ConditionalPromptSelector, is_chat_model\\nfrom langchain_core.prompts.chat import (\\n    AIMessagePromptTemplate,\\n    ChatPromptTemplate,\\n    HumanMessagePromptTemplate,\\n    SystemMessagePromptTemplate,\\n)\\nfrom langchain_core.prompts.prompt import PromptTemplate\\n\\nDEFAULT_REFINE_PROMPT_TMPL = (\\n    \"The original question is as follows: {question}\\\\n\"\\n    \"We have provided an existing answer: {existing_answer}\\\\n\"\\n    \"We have the opportunity to refine the existing answer \"\\n    \"(only if needed) with some more context below.\\\\n\"\\n    \"------------\\\\n\"\\n    \"{context_str}\\\\n\"\\n    \"------------\\\\n\"\\n    \"Given the new context, refine the original answer to better \"\\n    \"answer the question. \"\\n    \"If the context isn\\'t useful, return the original answer.\"\\n)\\nDEFAULT_REFINE_PROMPT = PromptTemplate.from_template(DEFAULT_REFINE_PROMPT_TMPL)\\n\\nrefine_template = (\\n    \"We have the opportunity to refine the existing answer \"\\n    \"(only if needed) with some more context below.\\\\n\"\\n    \"------------\\\\n\"\\n    \"{context_str}\\\\n\"\\n    \"------------\\\\n\"\\n    \"Given the new context, refine the original answer to better \"\\n    \"answer the question. \"\\n    \"If the context isn\\'t useful, return the original answer.\"\\n)\\nCHAT_REFINE_PROMPT = ChatPromptTemplate.from_messages(\\n    [(\"human\", \"{question}\"), (\"ai\", \"{existing_answer}\"), (\"human\", refine_template)]\\n)\\nREFINE_PROMPT_SELECTOR = ConditionalPromptSelector(\\n    default_prompt=DEFAULT_REFINE_PROMPT,\\n    conditionals=[(is_chat_model, CHAT_REFINE_PROMPT)],\\n)\\n\\n\\nDEFAULT_TEXT_QA_PROMPT_TMPL = (\\n    \"Context information is below. \\\\n\"\\n    \"------------\\\\n\"\\n    \"{context_str}\\\\n\"\\n    \"------------\\\\n\"\\n    \"Given the context information and not prior knowledge, \"\\n    \"answer the question: {question}\\\\n\"\\n)\\nDEFAULT_TEXT_QA_PROMPT = PromptTemplate.from_template(DEFAULT_TEXT_QA_PROMPT_TMPL)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\question_answering\\\\refine_prompts.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='chat_qa_prompt_template = (\\n    \"Context information is below.\\\\n\"\\n    \"------------\\\\n\"\\n    \"{context_str}\\\\n\"\\n    \"------------\\\\n\"\\n    \"Given the context information and not prior knowledge, \"\\n    \"answer any questions\"\\n)\\nCHAT_QUESTION_PROMPT = ChatPromptTemplate.from_messages(\\n    [(\"system\", chat_qa_prompt_template), (\"human\", \"{question}\")]\\n)\\nQUESTION_PROMPT_SELECTOR = ConditionalPromptSelector(\\n    default_prompt=DEFAULT_TEXT_QA_PROMPT,\\n    conditionals=[(is_chat_model, CHAT_QUESTION_PROMPT)],\\n)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\question_answering\\\\refine_prompts.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# flake8: noqa\\nfrom langchain.chains.prompt_selector import ConditionalPromptSelector, is_chat_model\\nfrom langchain_core.prompts import PromptTemplate\\nfrom langchain_core.prompts.chat import (\\n    ChatPromptTemplate,\\n    HumanMessagePromptTemplate,\\n    SystemMessagePromptTemplate,\\n)\\n\\nprompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\"\"\"\\nPROMPT = PromptTemplate(\\n    template=prompt_template, input_variables=[\"context\", \"question\"]\\n)\\n\\nsystem_template = \"\"\"Use the following pieces of context to answer the user\\'s question. \\nIf you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n----------------\\n{context}\"\"\"\\nmessages = [\\n    SystemMessagePromptTemplate.from_template(system_template),\\n    HumanMessagePromptTemplate.from_template(\"{question}\"),\\n]\\nCHAT_PROMPT = ChatPromptTemplate.from_messages(messages)\\n\\n\\nPROMPT_SELECTOR = ConditionalPromptSelector(\\n    default_prompt=PROMPT, conditionals=[(is_chat_model, CHAT_PROMPT)]\\n)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\question_answering\\\\stuff_prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Load question answering chains.\"\"\"\\nfrom typing import Any, Mapping, Optional, Protocol\\n\\nfrom langchain_core.callbacks import BaseCallbackManager, Callbacks\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.prompts import BasePromptTemplate\\n\\nfrom langchain.chains import ReduceDocumentsChain\\nfrom langchain.chains.combine_documents.base import BaseCombineDocumentsChain\\nfrom langchain.chains.combine_documents.map_reduce import MapReduceDocumentsChain\\nfrom langchain.chains.combine_documents.map_rerank import MapRerankDocumentsChain\\nfrom langchain.chains.combine_documents.refine import RefineDocumentsChain\\nfrom langchain.chains.combine_documents.stuff import StuffDocumentsChain\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.chains.question_answering import (\\n    map_reduce_prompt,\\n    refine_prompts,\\n    stuff_prompt,\\n)\\nfrom langchain.chains.question_answering.map_rerank_prompt import (\\n    PROMPT as MAP_RERANK_PROMPT,\\n)\\n\\n\\nclass LoadingCallable(Protocol):\\n    \"\"\"Interface for loading the combine documents chain.\"\"\"\\n\\n    def __call__(\\n        self, llm: BaseLanguageModel, **kwargs: Any\\n    ) -> BaseCombineDocumentsChain:\\n        \"\"\"Callable to load the combine documents chain.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\question_answering\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _load_map_rerank_chain(\\n    llm: BaseLanguageModel,\\n    prompt: BasePromptTemplate = MAP_RERANK_PROMPT,\\n    verbose: bool = False,\\n    document_variable_name: str = \"context\",\\n    rank_key: str = \"score\",\\n    answer_key: str = \"answer\",\\n    callback_manager: Optional[BaseCallbackManager] = None,\\n    callbacks: Callbacks = None,\\n    **kwargs: Any,\\n) -> MapRerankDocumentsChain:\\n    llm_chain = LLMChain(\\n        llm=llm,\\n        prompt=prompt,\\n        verbose=verbose,\\n        callback_manager=callback_manager,\\n        callbacks=callbacks,\\n    )\\n    return MapRerankDocumentsChain(\\n        llm_chain=llm_chain,\\n        rank_key=rank_key,\\n        answer_key=answer_key,\\n        document_variable_name=document_variable_name,\\n        verbose=verbose,\\n        callback_manager=callback_manager,\\n        **kwargs,\\n    )\\n\\n\\ndef _load_stuff_chain(\\n    llm: BaseLanguageModel,\\n    prompt: Optional[BasePromptTemplate] = None,\\n    document_variable_name: str = \"context\",\\n    verbose: Optional[bool] = None,\\n    callback_manager: Optional[BaseCallbackManager] = None,\\n    callbacks: Callbacks = None,\\n    **kwargs: Any,\\n) -> StuffDocumentsChain:\\n    _prompt = prompt or stuff_prompt.PROMPT_SELECTOR.get_prompt(llm)\\n    llm_chain = LLMChain(\\n        llm=llm,\\n        prompt=_prompt,\\n        verbose=verbose,\\n        callback_manager=callback_manager,\\n        callbacks=callbacks,\\n    )\\n    # TODO: document prompt\\n    return StuffDocumentsChain(\\n        llm_chain=llm_chain,\\n        document_variable_name=document_variable_name,\\n        verbose=verbose,\\n        callback_manager=callback_manager,\\n        callbacks=callbacks,\\n        **kwargs,\\n    )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\question_answering\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _load_map_reduce_chain(\\n    llm: BaseLanguageModel,\\n    question_prompt: Optional[BasePromptTemplate] = None,\\n    combine_prompt: Optional[BasePromptTemplate] = None,\\n    combine_document_variable_name: str = \"summaries\",\\n    map_reduce_document_variable_name: str = \"context\",\\n    collapse_prompt: Optional[BasePromptTemplate] = None,\\n    reduce_llm: Optional[BaseLanguageModel] = None,\\n    collapse_llm: Optional[BaseLanguageModel] = None,\\n    verbose: Optional[bool] = None,\\n    callback_manager: Optional[BaseCallbackManager] = None,\\n    callbacks: Callbacks = None,\\n    token_max: int = 3000,\\n    **kwargs: Any,\\n) -> MapReduceDocumentsChain:\\n    _question_prompt = (\\n        question_prompt or map_reduce_prompt.QUESTION_PROMPT_SELECTOR.get_prompt(llm)\\n    )\\n    _combine_prompt = (\\n        combine_prompt or map_reduce_prompt.COMBINE_PROMPT_SELECTOR.get_prompt(llm)\\n    )\\n    map_chain = LLMChain(\\n        llm=llm,\\n        prompt=_question_prompt,\\n        verbose=verbose,\\n        callback_manager=callback_manager,\\n        callbacks=callbacks,\\n    )\\n    _reduce_llm = reduce_llm or llm\\n    reduce_chain = LLMChain(\\n        llm=_reduce_llm,\\n        prompt=_combine_prompt,\\n        verbose=verbose,\\n        callback_manager=callback_manager,\\n        callbacks=callbacks,\\n    )\\n    # TODO: document prompt\\n    combine_documents_chain = StuffDocumentsChain(\\n        llm_chain=reduce_chain,\\n        document_variable_name=combine_document_variable_name,\\n        verbose=verbose,\\n        callback_manager=callback_manager,\\n        callbacks=callbacks,\\n    )\\n    if collapse_prompt is None:\\n        collapse_chain = None\\n        if collapse_llm is not None:\\n            raise ValueError(\\n                \"collapse_llm provided, but collapse_prompt was not: please \"\\n                \"provide one or stop providing collapse_llm.\"\\n            )\\n    else:\\n        _collapse_llm = collapse_llm or llm\\n        collapse_chain = StuffDocumentsChain(\\n            llm_chain=LLMChain(\\n                llm=_collapse_llm,' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\question_answering\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='prompt=collapse_prompt,\\n                verbose=verbose,\\n                callback_manager=callback_manager,\\n                callbacks=callbacks,\\n            ),\\n            document_variable_name=combine_document_variable_name,\\n            verbose=verbose,\\n            callback_manager=callback_manager,\\n        )\\n    reduce_documents_chain = ReduceDocumentsChain(\\n        combine_documents_chain=combine_documents_chain,\\n        collapse_documents_chain=collapse_chain,\\n        token_max=token_max,\\n        verbose=verbose,\\n    )\\n    return MapReduceDocumentsChain(\\n        llm_chain=map_chain,\\n        document_variable_name=map_reduce_document_variable_name,\\n        reduce_documents_chain=reduce_documents_chain,\\n        verbose=verbose,\\n        callback_manager=callback_manager,\\n        callbacks=callbacks,\\n        **kwargs,\\n    )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\question_answering\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _load_refine_chain(\\n    llm: BaseLanguageModel,\\n    question_prompt: Optional[BasePromptTemplate] = None,\\n    refine_prompt: Optional[BasePromptTemplate] = None,\\n    document_variable_name: str = \"context_str\",\\n    initial_response_name: str = \"existing_answer\",\\n    refine_llm: Optional[BaseLanguageModel] = None,\\n    verbose: Optional[bool] = None,\\n    callback_manager: Optional[BaseCallbackManager] = None,\\n    callbacks: Callbacks = None,\\n    **kwargs: Any,\\n) -> RefineDocumentsChain:\\n    _question_prompt = (\\n        question_prompt or refine_prompts.QUESTION_PROMPT_SELECTOR.get_prompt(llm)\\n    )\\n    _refine_prompt = refine_prompt or refine_prompts.REFINE_PROMPT_SELECTOR.get_prompt(\\n        llm\\n    )\\n    initial_chain = LLMChain(\\n        llm=llm,\\n        prompt=_question_prompt,\\n        verbose=verbose,\\n        callback_manager=callback_manager,\\n        callbacks=callbacks,\\n    )\\n    _refine_llm = refine_llm or llm\\n    refine_chain = LLMChain(\\n        llm=_refine_llm,\\n        prompt=_refine_prompt,\\n        verbose=verbose,\\n        callback_manager=callback_manager,\\n        callbacks=callbacks,\\n    )\\n    return RefineDocumentsChain(\\n        initial_llm_chain=initial_chain,\\n        refine_llm_chain=refine_chain,\\n        document_variable_name=document_variable_name,\\n        initial_response_name=initial_response_name,\\n        verbose=verbose,\\n        callback_manager=callback_manager,\\n        callbacks=callbacks,\\n        **kwargs,\\n    )\\n\\n\\ndef load_qa_chain(\\n    llm: BaseLanguageModel,\\n    chain_type: str = \"stuff\",\\n    verbose: Optional[bool] = None,\\n    callback_manager: Optional[BaseCallbackManager] = None,\\n    **kwargs: Any,\\n) -> BaseCombineDocumentsChain:\\n    \"\"\"Load question answering chain.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\question_answering\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Args:\\n        llm: Language Model to use in the chain.\\n        chain_type: Type of document combining chain to use. Should be one of \"stuff\",\\n            \"map_reduce\", \"map_rerank\", and \"refine\".\\n        verbose: Whether chains should be run in verbose mode or not. Note that this\\n            applies to all chains that make up the final chain.\\n        callback_manager: Callback manager to use for the chain.\\n\\n    Returns:\\n        A chain to use for question answering.\\n    \"\"\"\\n    loader_mapping: Mapping[str, LoadingCallable] = {\\n        \"stuff\": _load_stuff_chain,\\n        \"map_reduce\": _load_map_reduce_chain,\\n        \"refine\": _load_refine_chain,\\n        \"map_rerank\": _load_map_rerank_chain,\\n    }\\n    if chain_type not in loader_mapping:\\n        raise ValueError(\\n            f\"Got unsupported chain type: {chain_type}. \"\\n            f\"Should be one of {loader_mapping.keys()}\"\\n        )\\n    return loader_mapping[chain_type](\\n        llm, verbose=verbose, callback_manager=callback_manager, **kwargs\\n    )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\question_answering\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Chain for question-answering against a vector database.\"\"\"\\nfrom __future__ import annotations\\n\\nimport inspect\\nimport warnings\\nfrom abc import abstractmethod\\nfrom typing import Any, Dict, List, Optional\\n\\nfrom langchain_core.callbacks import (\\n    AsyncCallbackManagerForChainRun,\\n    CallbackManagerForChainRun,\\n    Callbacks,\\n)\\nfrom langchain_core.documents import Document\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.prompts import PromptTemplate\\nfrom langchain_core.pydantic_v1 import Extra, Field, root_validator\\nfrom langchain_core.retrievers import BaseRetriever\\nfrom langchain_core.vectorstores import VectorStore\\n\\nfrom langchain.chains.base import Chain\\nfrom langchain.chains.combine_documents.base import BaseCombineDocumentsChain\\nfrom langchain.chains.combine_documents.stuff import StuffDocumentsChain\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.chains.question_answering import load_qa_chain\\nfrom langchain.chains.question_answering.stuff_prompt import PROMPT_SELECTOR\\n\\n\\nclass BaseRetrievalQA(Chain):\\n    \"\"\"Base class for question-answering chains.\"\"\"\\n\\n    combine_documents_chain: BaseCombineDocumentsChain\\n    \"\"\"Chain to use to combine the documents.\"\"\"\\n    input_key: str = \"query\"  #: :meta private:\\n    output_key: str = \"result\"  #: :meta private:\\n    return_source_documents: bool = False\\n    \"\"\"Return the source documents or not.\"\"\"\\n\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n\\n        extra = Extra.forbid\\n        arbitrary_types_allowed = True\\n        allow_population_by_field_name = True\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Input keys.\\n\\n        :meta private:\\n        \"\"\"\\n        return [self.input_key]\\n\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Output keys.\\n\\n        :meta private:\\n        \"\"\"\\n        _output_keys = [self.output_key]\\n        if self.return_source_documents:\\n            _output_keys = _output_keys + [\"source_documents\"]\\n        return _output_keys' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\retrieval_qa\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        prompt: Optional[PromptTemplate] = None,\\n        callbacks: Callbacks = None,\\n        llm_chain_kwargs: Optional[dict] = None,\\n        **kwargs: Any,\\n    ) -> BaseRetrievalQA:\\n        \"\"\"Initialize from LLM.\"\"\"\\n        _prompt = prompt or PROMPT_SELECTOR.get_prompt(llm)\\n        llm_chain = LLMChain(\\n            llm=llm, prompt=_prompt, callbacks=callbacks, **(llm_chain_kwargs or {})\\n        )\\n        document_prompt = PromptTemplate(\\n            input_variables=[\"page_content\"], template=\"Context:\\\\n{page_content}\"\\n        )\\n        combine_documents_chain = StuffDocumentsChain(\\n            llm_chain=llm_chain,\\n            document_variable_name=\"context\",\\n            document_prompt=document_prompt,\\n            callbacks=callbacks,\\n        )\\n\\n        return cls(\\n            combine_documents_chain=combine_documents_chain,\\n            callbacks=callbacks,\\n            **kwargs,\\n        )\\n\\n    @classmethod\\n    def from_chain_type(\\n        cls,\\n        llm: BaseLanguageModel,\\n        chain_type: str = \"stuff\",\\n        chain_type_kwargs: Optional[dict] = None,\\n        **kwargs: Any,\\n    ) -> BaseRetrievalQA:\\n        \"\"\"Load chain from chain type.\"\"\"\\n        _chain_type_kwargs = chain_type_kwargs or {}\\n        combine_documents_chain = load_qa_chain(\\n            llm, chain_type=chain_type, **_chain_type_kwargs\\n        )\\n        return cls(combine_documents_chain=combine_documents_chain, **kwargs)\\n\\n    @abstractmethod\\n    def _get_docs(\\n        self,\\n        question: str,\\n        *,\\n        run_manager: CallbackManagerForChainRun,\\n    ) -> List[Document]:\\n        \"\"\"Get documents to do question answering over.\"\"\"\\n\\n    def _call(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        \"\"\"Run get_relevant_text and llm on input query.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\retrieval_qa\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='If chain has \\'return_source_documents\\' as \\'True\\', returns\\n        the retrieved documents as well under the key \\'source_documents\\'.\\n\\n        Example:\\n        .. code-block:: python\\n\\n        res = indexqa({\\'query\\': \\'This is my query\\'})\\n        answer, docs = res[\\'result\\'], res[\\'source_documents\\']\\n        \"\"\"\\n        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\\n        question = inputs[self.input_key]\\n        accepts_run_manager = (\\n            \"run_manager\" in inspect.signature(self._get_docs).parameters\\n        )\\n        if accepts_run_manager:\\n            docs = self._get_docs(question, run_manager=_run_manager)\\n        else:\\n            docs = self._get_docs(question)  # type: ignore[call-arg]\\n        answer = self.combine_documents_chain.run(\\n            input_documents=docs, question=question, callbacks=_run_manager.get_child()\\n        )\\n\\n        if self.return_source_documents:\\n            return {self.output_key: answer, \"source_documents\": docs}\\n        else:\\n            return {self.output_key: answer}\\n\\n    @abstractmethod\\n    async def _aget_docs(\\n        self,\\n        question: str,\\n        *,\\n        run_manager: AsyncCallbackManagerForChainRun,\\n    ) -> List[Document]:\\n        \"\"\"Get documents to do question answering over.\"\"\"\\n\\n    async def _acall(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        \"\"\"Run get_relevant_text and llm on input query.\\n\\n        If chain has \\'return_source_documents\\' as \\'True\\', returns\\n        the retrieved documents as well under the key \\'source_documents\\'.\\n\\n        Example:\\n        .. code-block:: python' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\retrieval_qa\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='res = indexqa({\\'query\\': \\'This is my query\\'})\\n        answer, docs = res[\\'result\\'], res[\\'source_documents\\']\\n        \"\"\"\\n        _run_manager = run_manager or AsyncCallbackManagerForChainRun.get_noop_manager()\\n        question = inputs[self.input_key]\\n        accepts_run_manager = (\\n            \"run_manager\" in inspect.signature(self._aget_docs).parameters\\n        )\\n        if accepts_run_manager:\\n            docs = await self._aget_docs(question, run_manager=_run_manager)\\n        else:\\n            docs = await self._aget_docs(question)  # type: ignore[call-arg]\\n        answer = await self.combine_documents_chain.arun(\\n            input_documents=docs, question=question, callbacks=_run_manager.get_child()\\n        )\\n\\n        if self.return_source_documents:\\n            return {self.output_key: answer, \"source_documents\": docs}\\n        else:\\n            return {self.output_key: answer}\\n\\n\\nclass RetrievalQA(BaseRetrievalQA):\\n    \"\"\"Chain for question-answering against an index.\\n\\n    Example:\\n        .. code-block:: python\\n\\n            from langchain_community.llms import OpenAI\\n            from langchain.chains import RetrievalQA\\n            from langchain_community.vectorstores import FAISS\\n            from langchain_core.vectorstores import VectorStoreRetriever\\n            retriever = VectorStoreRetriever(vectorstore=FAISS(...))\\n            retrievalQA = RetrievalQA.from_llm(llm=OpenAI(), retriever=retriever)\\n\\n    \"\"\"\\n\\n    retriever: BaseRetriever = Field(exclude=True)\\n\\n    def _get_docs(\\n        self,\\n        question: str,\\n        *,\\n        run_manager: CallbackManagerForChainRun,\\n    ) -> List[Document]:\\n        \"\"\"Get docs.\"\"\"\\n        return self.retriever.get_relevant_documents(\\n            question, callbacks=run_manager.get_child()\\n        )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\retrieval_qa\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='async def _aget_docs(\\n        self,\\n        question: str,\\n        *,\\n        run_manager: AsyncCallbackManagerForChainRun,\\n    ) -> List[Document]:\\n        \"\"\"Get docs.\"\"\"\\n        return await self.retriever.aget_relevant_documents(\\n            question, callbacks=run_manager.get_child()\\n        )\\n\\n    @property\\n    def _chain_type(self) -> str:\\n        \"\"\"Return the chain type.\"\"\"\\n        return \"retrieval_qa\"\\n\\n\\nclass VectorDBQA(BaseRetrievalQA):\\n    \"\"\"Chain for question-answering against a vector database.\"\"\"\\n\\n    vectorstore: VectorStore = Field(exclude=True, alias=\"vectorstore\")\\n    \"\"\"Vector Database to connect to.\"\"\"\\n    k: int = 4\\n    \"\"\"Number of documents to query for.\"\"\"\\n    search_type: str = \"similarity\"\\n    \"\"\"Search type to use over vectorstore. `similarity` or `mmr`.\"\"\"\\n    search_kwargs: Dict[str, Any] = Field(default_factory=dict)\\n    \"\"\"Extra search args.\"\"\"\\n\\n    @root_validator()\\n    def raise_deprecation(cls, values: Dict) -> Dict:\\n        warnings.warn(\\n            \"`VectorDBQA` is deprecated - \"\\n            \"please use `from langchain.chains import RetrievalQA`\"\\n        )\\n        return values\\n\\n    @root_validator()\\n    def validate_search_type(cls, values: Dict) -> Dict:\\n        \"\"\"Validate search type.\"\"\"\\n        if \"search_type\" in values:\\n            search_type = values[\"search_type\"]\\n            if search_type not in (\"similarity\", \"mmr\"):\\n                raise ValueError(f\"search_type of {search_type} not allowed.\")\\n        return values' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\retrieval_qa\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_docs(\\n        self,\\n        question: str,\\n        *,\\n        run_manager: CallbackManagerForChainRun,\\n    ) -> List[Document]:\\n        \"\"\"Get docs.\"\"\"\\n        if self.search_type == \"similarity\":\\n            docs = self.vectorstore.similarity_search(\\n                question, k=self.k, **self.search_kwargs\\n            )\\n        elif self.search_type == \"mmr\":\\n            docs = self.vectorstore.max_marginal_relevance_search(\\n                question, k=self.k, **self.search_kwargs\\n            )\\n        else:\\n            raise ValueError(f\"search_type of {self.search_type} not allowed.\")\\n        return docs\\n\\n    async def _aget_docs(\\n        self,\\n        question: str,\\n        *,\\n        run_manager: AsyncCallbackManagerForChainRun,\\n    ) -> List[Document]:\\n        \"\"\"Get docs.\"\"\"\\n        raise NotImplementedError(\"VectorDBQA does not support async\")\\n\\n    @property\\n    def _chain_type(self) -> str:\\n        \"\"\"Return the chain type.\"\"\"\\n        return \"vector_db_qa\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\retrieval_qa\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# flake8: noqa\\nfrom langchain_core.prompts import PromptTemplate\\n\\nprompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\"\"\"\\nPROMPT = PromptTemplate(\\n    template=prompt_template, input_variables=[\"context\", \"question\"]\\n)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\retrieval_qa\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Chain for question-answering against a vector database.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\retrieval_qa\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Base classes for chain routing.\"\"\"\\nfrom __future__ import annotations\\n\\nfrom abc import ABC\\nfrom typing import Any, Dict, List, Mapping, NamedTuple, Optional\\n\\nfrom langchain_core.callbacks import (\\n    AsyncCallbackManagerForChainRun,\\n    CallbackManagerForChainRun,\\n    Callbacks,\\n)\\nfrom langchain_core.pydantic_v1 import Extra\\n\\nfrom langchain.chains.base import Chain\\n\\n\\nclass Route(NamedTuple):\\n    destination: Optional[str]\\n    next_inputs: Dict[str, Any]\\n\\n\\nclass RouterChain(Chain, ABC):\\n    \"\"\"Chain that outputs the name of a destination chain and the inputs to it.\"\"\"\\n\\n    @property\\n    def output_keys(self) -> List[str]:\\n        return [\"destination\", \"next_inputs\"]\\n\\n    def route(self, inputs: Dict[str, Any], callbacks: Callbacks = None) -> Route:\\n        \"\"\"\\n        Route inputs to a destination chain.\\n\\n        Args:\\n            inputs: inputs to the chain\\n            callbacks: callbacks to use for the chain\\n\\n        Returns:\\n            a Route object\\n        \"\"\"\\n        result = self(inputs, callbacks=callbacks)\\n        return Route(result[\"destination\"], result[\"next_inputs\"])\\n\\n    async def aroute(\\n        self, inputs: Dict[str, Any], callbacks: Callbacks = None\\n    ) -> Route:\\n        result = await self.acall(inputs, callbacks=callbacks)\\n        return Route(result[\"destination\"], result[\"next_inputs\"])\\n\\n\\nclass MultiRouteChain(Chain):\\n    \"\"\"Use a single chain to route an input to one of multiple candidate chains.\"\"\"\\n\\n    router_chain: RouterChain\\n    \"\"\"Chain that routes inputs to destination chains.\"\"\"\\n    destination_chains: Mapping[str, Chain]\\n    \"\"\"Chains that return final answer to inputs.\"\"\"\\n    default_chain: Chain\\n    \"\"\"Default chain to use when none of the destination chains are suitable.\"\"\"\\n    silent_errors: bool = False\\n    \"\"\"If True, use default_chain when an invalid destination name is provided. \\n    Defaults to False.\"\"\"\\n\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n\\n        extra = Extra.forbid\\n        arbitrary_types_allowed = True' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\router\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Will be whatever keys the router chain prompt expects.\\n\\n        :meta private:\\n        \"\"\"\\n        return self.router_chain.input_keys\\n\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Will always return text key.\\n\\n        :meta private:\\n        \"\"\"\\n        return []\\n\\n    def _call(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\\n        callbacks = _run_manager.get_child()\\n        route = self.router_chain.route(inputs, callbacks=callbacks)\\n\\n        _run_manager.on_text(\\n            str(route.destination) + \": \" + str(route.next_inputs), verbose=self.verbose\\n        )\\n        if not route.destination:\\n            return self.default_chain(route.next_inputs, callbacks=callbacks)\\n        elif route.destination in self.destination_chains:\\n            return self.destination_chains[route.destination](\\n                route.next_inputs, callbacks=callbacks\\n            )\\n        elif self.silent_errors:\\n            return self.default_chain(route.next_inputs, callbacks=callbacks)\\n        else:\\n            raise ValueError(\\n                f\"Received invalid destination chain name \\'{route.destination}\\'\"\\n            )\\n\\n    async def _acall(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        _run_manager = run_manager or AsyncCallbackManagerForChainRun.get_noop_manager()\\n        callbacks = _run_manager.get_child()\\n        route = await self.router_chain.aroute(inputs, callbacks=callbacks)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\router\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='await _run_manager.on_text(\\n            str(route.destination) + \": \" + str(route.next_inputs), verbose=self.verbose\\n        )\\n        if not route.destination:\\n            return await self.default_chain.acall(\\n                route.next_inputs, callbacks=callbacks\\n            )\\n        elif route.destination in self.destination_chains:\\n            return await self.destination_chains[route.destination].acall(\\n                route.next_inputs, callbacks=callbacks\\n            )\\n        elif self.silent_errors:\\n            return await self.default_chain.acall(\\n                route.next_inputs, callbacks=callbacks\\n            )\\n        else:\\n            raise ValueError(\\n                f\"Received invalid destination chain name \\'{route.destination}\\'\"\\n            )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\router\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from __future__ import annotations\\n\\nfrom typing import Any, Dict, List, Optional, Sequence, Tuple, Type\\n\\nfrom langchain_core.callbacks import CallbackManagerForChainRun\\nfrom langchain_core.documents import Document\\nfrom langchain_core.embeddings import Embeddings\\nfrom langchain_core.pydantic_v1 import Extra\\nfrom langchain_core.vectorstores import VectorStore\\n\\nfrom langchain.chains.router.base import RouterChain\\n\\n\\nclass EmbeddingRouterChain(RouterChain):\\n    \"\"\"Chain that uses embeddings to route between options.\"\"\"\\n\\n    vectorstore: VectorStore\\n    routing_keys: List[str] = [\"query\"]\\n\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n\\n        extra = Extra.forbid\\n        arbitrary_types_allowed = True\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Will be whatever keys the LLM chain prompt expects.\\n\\n        :meta private:\\n        \"\"\"\\n        return self.routing_keys\\n\\n    def _call(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        _input = \", \".join([inputs[k] for k in self.routing_keys])\\n        results = self.vectorstore.similarity_search(_input, k=1)\\n        return {\"next_inputs\": inputs, \"destination\": results[0].metadata[\"name\"]}\\n\\n    @classmethod\\n    def from_names_and_descriptions(\\n        cls,\\n        names_and_descriptions: Sequence[Tuple[str, Sequence[str]]],\\n        vectorstore_cls: Type[VectorStore],\\n        embeddings: Embeddings,\\n        **kwargs: Any,\\n    ) -> EmbeddingRouterChain:\\n        \"\"\"Convenience constructor.\"\"\"\\n        documents = []\\n        for name, descriptions in names_and_descriptions:\\n            for description in descriptions:\\n                documents.append(\\n                    Document(page_content=description, metadata={\"name\": name})\\n                )\\n        vectorstore = vectorstore_cls.from_documents(documents, embeddings)\\n        return cls(vectorstore=vectorstore, **kwargs)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\router\\\\embedding_router.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Base classes for LLM-powered router chains.\"\"\"\\nfrom __future__ import annotations\\n\\nfrom typing import Any, Dict, List, Optional, Type, cast\\n\\nfrom langchain_core.callbacks import (\\n    AsyncCallbackManagerForChainRun,\\n    CallbackManagerForChainRun,\\n)\\nfrom langchain_core.exceptions import OutputParserException\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.output_parsers import BaseOutputParser\\nfrom langchain_core.prompts import BasePromptTemplate\\nfrom langchain_core.pydantic_v1 import root_validator\\n\\nfrom langchain.chains import LLMChain\\nfrom langchain.chains.router.base import RouterChain\\nfrom langchain.output_parsers.json import parse_and_check_json_markdown\\n\\n\\nclass LLMRouterChain(RouterChain):\\n    \"\"\"A router chain that uses an LLM chain to perform routing.\"\"\"\\n\\n    llm_chain: LLMChain\\n    \"\"\"LLM chain used to perform routing\"\"\"\\n\\n    @root_validator()\\n    def validate_prompt(cls, values: dict) -> dict:\\n        prompt = values[\"llm_chain\"].prompt\\n        if prompt.output_parser is None:\\n            raise ValueError(\\n                \"LLMRouterChain requires base llm_chain prompt to have an output\"\\n                \" parser that converts LLM text output to a dictionary with keys\"\\n                \" \\'destination\\' and \\'next_inputs\\'. Received a prompt with no output\"\\n                \" parser.\"\\n            )\\n        return values\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Will be whatever keys the LLM chain prompt expects.\\n\\n        :meta private:\\n        \"\"\"\\n        return self.llm_chain.input_keys\\n\\n    def _validate_outputs(self, outputs: Dict[str, Any]) -> None:\\n        super()._validate_outputs(outputs)\\n        if not isinstance(outputs[\"next_inputs\"], dict):\\n            raise ValueError' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\router\\\\llm_router.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _call(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\\n        callbacks = _run_manager.get_child()\\n        output = cast(\\n            Dict[str, Any],\\n            self.llm_chain.predict_and_parse(callbacks=callbacks, **inputs),\\n        )\\n        return output\\n\\n    async def _acall(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\\n        callbacks = _run_manager.get_child()\\n        output = cast(\\n            Dict[str, Any],\\n            await self.llm_chain.apredict_and_parse(callbacks=callbacks, **inputs),\\n        )\\n        return output\\n\\n    @classmethod\\n    def from_llm(\\n        cls, llm: BaseLanguageModel, prompt: BasePromptTemplate, **kwargs: Any\\n    ) -> LLMRouterChain:\\n        \"\"\"Convenience constructor.\"\"\"\\n        llm_chain = LLMChain(llm=llm, prompt=prompt)\\n        return cls(llm_chain=llm_chain, **kwargs)\\n\\n\\nclass RouterOutputParser(BaseOutputParser[Dict[str, str]]):\\n    \"\"\"Parser for output of router chain in the multi-prompt chain.\"\"\"\\n\\n    default_destination: str = \"DEFAULT\"\\n    next_inputs_type: Type = str\\n    next_inputs_inner_key: str = \"input\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\router\\\\llm_router.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def parse(self, text: str) -> Dict[str, Any]:\\n        try:\\n            expected_keys = [\"destination\", \"next_inputs\"]\\n            parsed = parse_and_check_json_markdown(text, expected_keys)\\n            if not isinstance(parsed[\"destination\"], str):\\n                raise ValueError(\"Expected \\'destination\\' to be a string.\")\\n            if not isinstance(parsed[\"next_inputs\"], self.next_inputs_type):\\n                raise ValueError(\\n                    f\"Expected \\'next_inputs\\' to be {self.next_inputs_type}.\"\\n                )\\n            parsed[\"next_inputs\"] = {self.next_inputs_inner_key: parsed[\"next_inputs\"]}\\n            if (\\n                parsed[\"destination\"].strip().lower()\\n                == self.default_destination.lower()\\n            ):\\n                parsed[\"destination\"] = None\\n            else:\\n                parsed[\"destination\"] = parsed[\"destination\"].strip()\\n            return parsed\\n        except Exception as e:\\n            raise OutputParserException(\\n                f\"Parsing text\\\\n{text}\\\\n raised following error:\\\\n{e}\"\\n            )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\router\\\\llm_router.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Use a single chain to route an input to one of multiple llm chains.\"\"\"\\nfrom __future__ import annotations\\n\\nfrom typing import Any, Dict, List, Optional\\n\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.prompts import PromptTemplate\\n\\nfrom langchain.chains import ConversationChain\\nfrom langchain.chains.base import Chain\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.chains.router.base import MultiRouteChain\\nfrom langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\\nfrom langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE\\n\\n\\nclass MultiPromptChain(MultiRouteChain):\\n    \"\"\"A multi-route chain that uses an LLM router chain to choose amongst prompts.\"\"\"\\n\\n    @property\\n    def output_keys(self) -> List[str]:\\n        return [\"text\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\router\\\\multi_prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@classmethod\\n    def from_prompts(\\n        cls,\\n        llm: BaseLanguageModel,\\n        prompt_infos: List[Dict[str, str]],\\n        default_chain: Optional[Chain] = None,\\n        **kwargs: Any,\\n    ) -> MultiPromptChain:\\n        \"\"\"Convenience constructor for instantiating from destination prompts.\"\"\"\\n        destinations = [f\"{p[\\'name\\']}: {p[\\'description\\']}\" for p in prompt_infos]\\n        destinations_str = \"\\\\n\".join(destinations)\\n        router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\\n            destinations=destinations_str\\n        )\\n        router_prompt = PromptTemplate(\\n            template=router_template,\\n            input_variables=[\"input\"],\\n            output_parser=RouterOutputParser(),\\n        )\\n        router_chain = LLMRouterChain.from_llm(llm, router_prompt)\\n        destination_chains = {}\\n        for p_info in prompt_infos:\\n            name = p_info[\"name\"]\\n            prompt_template = p_info[\"prompt_template\"]\\n            prompt = PromptTemplate(template=prompt_template, input_variables=[\"input\"])\\n            chain = LLMChain(llm=llm, prompt=prompt)\\n            destination_chains[name] = chain\\n        _default_chain = default_chain or ConversationChain(llm=llm, output_key=\"text\")\\n        return cls(\\n            router_chain=router_chain,\\n            destination_chains=destination_chains,\\n            default_chain=_default_chain,\\n            **kwargs,\\n        )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\router\\\\multi_prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Prompt for the router chain in the multi-prompt chain.\"\"\"\\n\\nMULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"\\\\\\nGiven a raw text input to a language model select the model prompt best suited for \\\\\\nthe input. You will be given the names of the available prompts and a description of \\\\\\nwhat the prompt is best suited for. You may also revise the original input if you \\\\\\nthink that revising it will ultimately lead to a better response from the language \\\\\\nmodel.\\n\\n<< FORMATTING >>\\nReturn a markdown code snippet with a JSON object formatted to look like:\\n```json\\n{{{{\\n    \"destination\": string \\\\\\\\ name of the prompt to use or \"DEFAULT\"\\n    \"next_inputs\": string \\\\\\\\ a potentially modified version of the original input\\n}}}}\\n```\\n\\nREMEMBER: \"destination\" MUST be one of the candidate prompt names specified below OR \\\\\\nit can be \"DEFAULT\" if the input is not well suited for any of the candidate prompts.\\nREMEMBER: \"next_inputs\" can just be the original input if you don\\'t think any \\\\\\nmodifications are needed.\\n\\n<< CANDIDATE PROMPTS >>\\n{destinations}\\n\\n<< INPUT >>\\n{{input}}\\n\\n<< OUTPUT (must include ```json at the start of the response) >>\\n<< OUTPUT (must end with ```) >>\\n\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\router\\\\multi_prompt_prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Prompt for the router chain in the multi-retrieval qa chain.\"\"\"\\n\\nMULTI_RETRIEVAL_ROUTER_TEMPLATE = \"\"\"\\\\\\nGiven a query to a question answering system select the system best suited \\\\\\nfor the input. You will be given the names of the available systems and a description \\\\\\nof what questions the system is best suited for. You may also revise the original \\\\\\ninput if you think that revising it will ultimately lead to a better response.\\n\\n<< FORMATTING >>\\nReturn a markdown code snippet with a JSON object formatted to look like:\\n```json\\n{{{{\\n    \"destination\": string \\\\\\\\ name of the question answering system to use or \"DEFAULT\"\\n    \"next_inputs\": string \\\\\\\\ a potentially modified version of the original input\\n}}}}\\n```\\n\\nREMEMBER: \"destination\" MUST be one of the candidate prompt names specified below OR \\\\\\nit can be \"DEFAULT\" if the input is not well suited for any of the candidate prompts.\\nREMEMBER: \"next_inputs\" can just be the original input if you don\\'t think any \\\\\\nmodifications are needed.\\n\\n<< CANDIDATE PROMPTS >>\\n{destinations}\\n\\n<< INPUT >>\\n{{input}}\\n\\n<< OUTPUT >>\\n\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\router\\\\multi_retrieval_prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Use a single chain to route an input to one of multiple retrieval qa chains.\"\"\"\\nfrom __future__ import annotations\\n\\nfrom typing import Any, Dict, List, Mapping, Optional\\n\\nfrom langchain_community.chat_models import ChatOpenAI\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.prompts import PromptTemplate\\nfrom langchain_core.retrievers import BaseRetriever\\n\\nfrom langchain.chains import ConversationChain\\nfrom langchain.chains.base import Chain\\nfrom langchain.chains.conversation.prompt import DEFAULT_TEMPLATE\\nfrom langchain.chains.retrieval_qa.base import BaseRetrievalQA, RetrievalQA\\nfrom langchain.chains.router.base import MultiRouteChain\\nfrom langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\\nfrom langchain.chains.router.multi_retrieval_prompt import (\\n    MULTI_RETRIEVAL_ROUTER_TEMPLATE,\\n)\\n\\n\\nclass MultiRetrievalQAChain(MultiRouteChain):\\n    \"\"\"A multi-route chain that uses an LLM router chain to choose amongst retrieval\\n    qa chains.\"\"\"\\n\\n    router_chain: LLMRouterChain\\n    \"\"\"Chain for deciding a destination chain and the input to it.\"\"\"\\n    destination_chains: Mapping[str, BaseRetrievalQA]\\n    \"\"\"Map of name to candidate chains that inputs can be routed to.\"\"\"\\n    default_chain: Chain\\n    \"\"\"Default chain to use when router doesn\\'t map input to one of the destinations.\"\"\"\\n\\n    @property\\n    def output_keys(self) -> List[str]:\\n        return [\"result\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\router\\\\multi_retrieval_qa.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@classmethod\\n    def from_retrievers(\\n        cls,\\n        llm: BaseLanguageModel,\\n        retriever_infos: List[Dict[str, Any]],\\n        default_retriever: Optional[BaseRetriever] = None,\\n        default_prompt: Optional[PromptTemplate] = None,\\n        default_chain: Optional[Chain] = None,\\n        **kwargs: Any,\\n    ) -> MultiRetrievalQAChain:\\n        if default_prompt and not default_retriever:\\n            raise ValueError(\\n                \"`default_retriever` must be specified if `default_prompt` is \"\\n                \"provided. Received only `default_prompt`.\"\\n            )\\n        destinations = [f\"{r[\\'name\\']}: {r[\\'description\\']}\" for r in retriever_infos]\\n        destinations_str = \"\\\\n\".join(destinations)\\n        router_template = MULTI_RETRIEVAL_ROUTER_TEMPLATE.format(\\n            destinations=destinations_str\\n        )\\n        router_prompt = PromptTemplate(\\n            template=router_template,\\n            input_variables=[\"input\"],\\n            output_parser=RouterOutputParser(next_inputs_inner_key=\"query\"),\\n        )\\n        router_chain = LLMRouterChain.from_llm(llm, router_prompt)\\n        destination_chains = {}\\n        for r_info in retriever_infos:\\n            prompt = r_info.get(\"prompt\")\\n            retriever = r_info[\"retriever\"]\\n            chain = RetrievalQA.from_llm(llm, prompt=prompt, retriever=retriever)\\n            name = r_info[\"name\"]\\n            destination_chains[name] = chain\\n        if default_chain:\\n            _default_chain = default_chain\\n        elif default_retriever:\\n            _default_chain = RetrievalQA.from_llm(\\n                llm, prompt=default_prompt, retriever=default_retriever\\n            )\\n        else:\\n            prompt_template = DEFAULT_TEMPLATE.replace(\"input\", \"query\")\\n            prompt = PromptTemplate(\\n                template=prompt_template, input_variables=[\"history\", \"query\"]\\n            )\\n            _default_chain = ConversationChain(\\n                llm=ChatOpenAI(), prompt=prompt, input_key=\"query\", output_key=\"result\"\\n            )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\router\\\\multi_retrieval_qa.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='return cls(\\n            router_chain=router_chain,\\n            destination_chains=destination_chains,\\n            default_chain=_default_chain,\\n            **kwargs,\\n        )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\router\\\\multi_retrieval_qa.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain.chains.router.base import MultiRouteChain, RouterChain\\nfrom langchain.chains.router.llm_router import LLMRouterChain\\nfrom langchain.chains.router.multi_prompt import MultiPromptChain\\nfrom langchain.chains.router.multi_retrieval_qa import MultiRetrievalQAChain\\n\\n__all__ = [\\n    \"RouterChain\",\\n    \"MultiRouteChain\",\\n    \"MultiPromptChain\",\\n    \"MultiRetrievalQAChain\",\\n    \"LLMRouterChain\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\router\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# flake8: noqa\\nfrom langchain_core.output_parsers.list import CommaSeparatedListOutputParser\\nfrom langchain_core.prompts.prompt import PromptTemplate\\n\\n\\nPROMPT_SUFFIX = \"\"\"Only use the following tables:\\n{table_info}\\n\\nQuestion: {input}\"\"\"\\n\\n_DEFAULT_TEMPLATE = \"\"\"Given an input question, first create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer. Unless the user specifies in his question a specific number of examples he wishes to obtain, always limit your query to at most {top_k} results. You can order the results by a relevant column to return the most interesting examples in the database.\\n\\nNever query for all the columns from a specific table, only ask for a the few relevant columns given the question.\\n\\nPay attention to use only the column names that you can see in the schema description. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\\n\\nUse the following format:\\n\\nQuestion: Question here\\nSQLQuery: SQL Query to run\\nSQLResult: Result of the SQLQuery\\nAnswer: Final answer here\\n\\n\"\"\"\\n\\nPROMPT = PromptTemplate(\\n    input_variables=[\"input\", \"table_info\", \"dialect\", \"top_k\"],\\n    template=_DEFAULT_TEMPLATE + PROMPT_SUFFIX,\\n)\\n\\n\\n_DECIDER_TEMPLATE = \"\"\"Given the below input question and list of potential tables, output a comma separated list of the table names that may be necessary to answer this question.\\n\\nQuestion: {query}\\n\\nTable Names: {table_names}\\n\\nRelevant Table Names:\"\"\"\\nDECIDER_PROMPT = PromptTemplate(\\n    input_variables=[\"query\", \"table_names\"],\\n    template=_DECIDER_TEMPLATE,\\n    output_parser=CommaSeparatedListOutputParser(),\\n)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\sql_database\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='_cratedb_prompt = \"\"\"You are a CrateDB expert. Given an input question, first create a syntactically correct CrateDB query to run, then look at the results of the query and return the answer to the input question.\\nUnless the user specifies in the question a specific number of examples to obtain, query for at most {top_k} results using the LIMIT clause as per CrateDB. You can order the results to return the most informative data in the database.\\nNever query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (\") to denote them as delimited identifiers.\\nPay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\\nPay attention to use CURRENT_DATE function to get the current date, if the question involves \"today\". \\n\\nUse the following format:\\n\\nQuestion: Question here\\nSQLQuery: SQL Query to run\\nSQLResult: Result of the SQLQuery\\nAnswer: Final answer here\\n\\n\"\"\"\\n\\nCRATEDB_PROMPT = PromptTemplate(\\n    input_variables=[\"input\", \"table_info\", \"top_k\"],\\n    template=_cratedb_prompt + PROMPT_SUFFIX,\\n)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\sql_database\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='_duckdb_prompt = \"\"\"You are a DuckDB expert. Given an input question, first create a syntactically correct DuckDB query to run, then look at the results of the query and return the answer to the input question.\\nUnless the user specifies in the question a specific number of examples to obtain, query for at most {top_k} results using the LIMIT clause as per DuckDB. You can order the results to return the most informative data in the database.\\nNever query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (\") to denote them as delimited identifiers.\\nPay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\\nPay attention to use today() function to get the current date, if the question involves \"today\".\\n\\nUse the following format:\\n\\nQuestion: Question here\\nSQLQuery: SQL Query to run\\nSQLResult: Result of the SQLQuery\\nAnswer: Final answer here\\n\\n\"\"\"\\n\\nDUCKDB_PROMPT = PromptTemplate(\\n    input_variables=[\"input\", \"table_info\", \"top_k\"],\\n    template=_duckdb_prompt + PROMPT_SUFFIX,\\n)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\sql_database\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='_googlesql_prompt = \"\"\"You are a GoogleSQL expert. Given an input question, first create a syntactically correct GoogleSQL query to run, then look at the results of the query and return the answer to the input question.\\nUnless the user specifies in the question a specific number of examples to obtain, query for at most {top_k} results using the LIMIT clause as per GoogleSQL. You can order the results to return the most informative data in the database.\\nNever query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in backticks (`) to denote them as delimited identifiers.\\nPay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\\nPay attention to use CURRENT_DATE() function to get the current date, if the question involves \"today\".\\n\\nUse the following format:\\n\\nQuestion: Question here\\nSQLQuery: SQL Query to run\\nSQLResult: Result of the SQLQuery\\nAnswer: Final answer here\\n\\n\"\"\"\\n\\nGOOGLESQL_PROMPT = PromptTemplate(\\n    input_variables=[\"input\", \"table_info\", \"top_k\"],\\n    template=_googlesql_prompt + PROMPT_SUFFIX,\\n)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\sql_database\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='_mssql_prompt = \"\"\"You are an MS SQL expert. Given an input question, first create a syntactically correct MS SQL query to run, then look at the results of the query and return the answer to the input question.\\nUnless the user specifies in the question a specific number of examples to obtain, query for at most {top_k} results using the TOP clause as per MS SQL. You can order the results to return the most informative data in the database.\\nNever query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in square brackets ([]) to denote them as delimited identifiers.\\nPay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\\nPay attention to use CAST(GETDATE() as date) function to get the current date, if the question involves \"today\".\\n\\nUse the following format:\\n\\nQuestion: Question here\\nSQLQuery: SQL Query to run\\nSQLResult: Result of the SQLQuery\\nAnswer: Final answer here\\n\\n\"\"\"\\n\\nMSSQL_PROMPT = PromptTemplate(\\n    input_variables=[\"input\", \"table_info\", \"top_k\"],\\n    template=_mssql_prompt + PROMPT_SUFFIX,\\n)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\sql_database\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='_mysql_prompt = \"\"\"You are a MySQL expert. Given an input question, first create a syntactically correct MySQL query to run, then look at the results of the query and return the answer to the input question.\\nUnless the user specifies in the question a specific number of examples to obtain, query for at most {top_k} results using the LIMIT clause as per MySQL. You can order the results to return the most informative data in the database.\\nNever query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in backticks (`) to denote them as delimited identifiers.\\nPay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\\nPay attention to use CURDATE() function to get the current date, if the question involves \"today\".\\n\\nUse the following format:\\n\\nQuestion: Question here\\nSQLQuery: SQL Query to run\\nSQLResult: Result of the SQLQuery\\nAnswer: Final answer here\\n\\n\"\"\"\\n\\nMYSQL_PROMPT = PromptTemplate(\\n    input_variables=[\"input\", \"table_info\", \"top_k\"],\\n    template=_mysql_prompt + PROMPT_SUFFIX,\\n)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\sql_database\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='_mariadb_prompt = \"\"\"You are a MariaDB expert. Given an input question, first create a syntactically correct MariaDB query to run, then look at the results of the query and return the answer to the input question.\\nUnless the user specifies in the question a specific number of examples to obtain, query for at most {top_k} results using the LIMIT clause as per MariaDB. You can order the results to return the most informative data in the database.\\nNever query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in backticks (`) to denote them as delimited identifiers.\\nPay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\\nPay attention to use CURDATE() function to get the current date, if the question involves \"today\".\\n\\nUse the following format:\\n\\nQuestion: Question here\\nSQLQuery: SQL Query to run\\nSQLResult: Result of the SQLQuery\\nAnswer: Final answer here\\n\\n\"\"\"\\n\\nMARIADB_PROMPT = PromptTemplate(\\n    input_variables=[\"input\", \"table_info\", \"top_k\"],\\n    template=_mariadb_prompt + PROMPT_SUFFIX,\\n)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\sql_database\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='_oracle_prompt = \"\"\"You are an Oracle SQL expert. Given an input question, first create a syntactically correct Oracle SQL query to run, then look at the results of the query and return the answer to the input question.\\nUnless the user specifies in the question a specific number of examples to obtain, query for at most {top_k} results using the FETCH FIRST n ROWS ONLY clause as per Oracle SQL. You can order the results to return the most informative data in the database.\\nNever query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (\") to denote them as delimited identifiers.\\nPay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\\nPay attention to use TRUNC(SYSDATE) function to get the current date, if the question involves \"today\".\\n\\nUse the following format:\\n\\nQuestion: Question here\\nSQLQuery: SQL Query to run\\nSQLResult: Result of the SQLQuery\\nAnswer: Final answer here\\n\\n\"\"\"\\n\\nORACLE_PROMPT = PromptTemplate(\\n    input_variables=[\"input\", \"table_info\", \"top_k\"],\\n    template=_oracle_prompt + PROMPT_SUFFIX,\\n)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\sql_database\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='_postgres_prompt = \"\"\"You are a PostgreSQL expert. Given an input question, first create a syntactically correct PostgreSQL query to run, then look at the results of the query and return the answer to the input question.\\nUnless the user specifies in the question a specific number of examples to obtain, query for at most {top_k} results using the LIMIT clause as per PostgreSQL. You can order the results to return the most informative data in the database.\\nNever query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (\") to denote them as delimited identifiers.\\nPay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\\nPay attention to use CURRENT_DATE function to get the current date, if the question involves \"today\".\\n\\nUse the following format:\\n\\nQuestion: Question here\\nSQLQuery: SQL Query to run\\nSQLResult: Result of the SQLQuery\\nAnswer: Final answer here\\n\\n\"\"\"\\n\\nPOSTGRES_PROMPT = PromptTemplate(\\n    input_variables=[\"input\", \"table_info\", \"top_k\"],\\n    template=_postgres_prompt + PROMPT_SUFFIX,\\n)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\sql_database\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='_sqlite_prompt = \"\"\"You are a SQLite expert. Given an input question, first create a syntactically correct SQLite query to run, then look at the results of the query and return the answer to the input question.\\nUnless the user specifies in the question a specific number of examples to obtain, query for at most {top_k} results using the LIMIT clause as per SQLite. You can order the results to return the most informative data in the database.\\nNever query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (\") to denote them as delimited identifiers.\\nPay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\\nPay attention to use date(\\'now\\') function to get the current date, if the question involves \"today\".\\n\\nUse the following format:\\n\\nQuestion: Question here\\nSQLQuery: SQL Query to run\\nSQLResult: Result of the SQLQuery\\nAnswer: Final answer here\\n\\n\"\"\"\\n\\nSQLITE_PROMPT = PromptTemplate(\\n    input_variables=[\"input\", \"table_info\", \"top_k\"],\\n    template=_sqlite_prompt + PROMPT_SUFFIX,\\n)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\sql_database\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='_clickhouse_prompt = \"\"\"You are a ClickHouse expert. Given an input question, first create a syntactically correct Clic query to run, then look at the results of the query and return the answer to the input question.\\nUnless the user specifies in the question a specific number of examples to obtain, query for at most {top_k} results using the LIMIT clause as per ClickHouse. You can order the results to return the most informative data in the database.\\nNever query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (\") to denote them as delimited identifiers.\\nPay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\\nPay attention to use today() function to get the current date, if the question involves \"today\".\\n\\nUse the following format:\\n\\nQuestion: \"Question here\"\\nSQLQuery: \"SQL Query to run\"\\nSQLResult: \"Result of the SQLQuery\"\\nAnswer: \"Final answer here\"\\n\\n\"\"\"\\n\\nCLICKHOUSE_PROMPT = PromptTemplate(\\n    input_variables=[\"input\", \"table_info\", \"top_k\"],\\n    template=_clickhouse_prompt + PROMPT_SUFFIX,\\n)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\sql_database\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='_prestodb_prompt = \"\"\"You are a PrestoDB expert. Given an input question, first create a syntactically correct PrestoDB query to run, then look at the results of the query and return the answer to the input question.\\nUnless the user specifies in the question a specific number of examples to obtain, query for at most {top_k} results using the LIMIT clause as per PrestoDB. You can order the results to return the most informative data in the database.\\nNever query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (\") to denote them as delimited identifiers.\\nPay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\\nPay attention to use current_date function to get the current date, if the question involves \"today\".\\n\\nUse the following format:\\n\\nQuestion: \"Question here\"\\nSQLQuery: \"SQL Query to run\"\\nSQLResult: \"Result of the SQLQuery\"\\nAnswer: \"Final answer here\"\\n\\n\"\"\"\\n\\nPRESTODB_PROMPT = PromptTemplate(\\n    input_variables=[\"input\", \"table_info\", \"top_k\"],\\n    template=_prestodb_prompt + PROMPT_SUFFIX,\\n)\\n\\n\\nSQL_PROMPTS = {\\n    \"crate\": CRATEDB_PROMPT,\\n    \"duckdb\": DUCKDB_PROMPT,\\n    \"googlesql\": GOOGLESQL_PROMPT,\\n    \"mssql\": MSSQL_PROMPT,\\n    \"mysql\": MYSQL_PROMPT,\\n    \"mariadb\": MARIADB_PROMPT,\\n    \"oracle\": ORACLE_PROMPT,\\n    \"postgresql\": POSTGRES_PROMPT,\\n    \"sqlite\": SQLITE_PROMPT,\\n    \"clickhouse\": CLICKHOUSE_PROMPT,\\n    \"prestodb\": PRESTODB_PROMPT,\\n}' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\sql_database\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from typing import Any, Dict, List, Optional, TypedDict, Union\\n\\nfrom langchain_community.utilities.sql_database import SQLDatabase\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.prompts import BasePromptTemplate\\nfrom langchain_core.runnables import Runnable, RunnablePassthrough\\n\\nfrom langchain.chains.sql_database.prompt import PROMPT, SQL_PROMPTS\\n\\n\\ndef _strip(text: str) -> str:\\n    return text.strip()\\n\\n\\nclass SQLInput(TypedDict):\\n    \"\"\"Input for a SQL Chain.\"\"\"\\n\\n    question: str\\n\\n\\nclass SQLInputWithTables(TypedDict):\\n    \"\"\"Input for a SQL Chain.\"\"\"\\n\\n    question: str\\n    table_names_to_use: List[str]\\n\\n\\ndef create_sql_query_chain(\\n    llm: BaseLanguageModel,\\n    db: SQLDatabase,\\n    prompt: Optional[BasePromptTemplate] = None,\\n    k: int = 5,\\n) -> Runnable[Union[SQLInput, SQLInputWithTables, Dict[str, Any]], str]:\\n    \"\"\"Create a chain that generates SQL queries.\\n\\n    *Security Note*: This chain generates SQL queries for the given database.\\n\\n        The SQLDatabase class provides a get_table_info method that can be used\\n        to get column information as well as sample data from the table.\\n\\n        To mitigate risk of leaking sensitive data, limit permissions\\n        to read and scope to the tables that are needed.\\n\\n        Optionally, use the SQLInputWithTables input type to specify which tables\\n        are allowed to be accessed.\\n\\n        Control access to who can submit requests to this chain.\\n\\n        See https://python.langchain.com/docs/security for more information.\\n\\n    Args:\\n        llm: The language model to use.\\n        db: The SQLDatabase to generate the query for.\\n        prompt: The prompt to use. If none is provided, will choose one\\n            based on dialect. Defaults to None. See Prompt section below for more.\\n        k: The number of results per select statement to return. Defaults to 5.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\sql_database\\\\query.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Returns:\\n        A chain that takes in a question and generates a SQL query that answers\\n        that question.\\n\\n    Example:\\n\\n        .. code-block:: python\\n\\n            # pip install -U langchain langchain-community langchain-openai\\n            from langchain_openai import ChatOpenAI\\n            from langchain.chains import create_sql_query_chain\\n            from langchain_community.utilities import SQLDatabase\\n\\n            db = SQLDatabase.from_uri(\"sqlite:///Chinook.db\")\\n            llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\\n            chain = create_sql_query_chain(llm, db)\\n            response = chain.invoke({\"question\": \"How many employees are there\"})\\n\\n    Prompt:\\n        If no prompt is provided, a default prompt is selected based on the SQLDatabase dialect. If one is provided, it must support input variables:\\n            * input: The user question plus suffix \"\\\\nSQLQuery: \" is passed here.\\n            * top_k: The number of results per select statement (the `k` argument to\\n                this function) is passed in here.\\n            * table_info: Table definitions and sample rows are passed in here. If the\\n                user specifies \"table_names_to_use\" when invoking chain, only those\\n                will be included. Otherwise, all tables are included.\\n            * dialect (optional): If dialect input variable is in prompt, the db\\n                dialect will be passed in here.\\n\\n        Here\\'s an example prompt:\\n\\n        .. code-block:: python\\n\\n            from langchain_core.prompts import PromptTemplate\\n\\n            template = \\'\\'\\'Given an input question, first create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer.\\n            Use the following format:\\n\\n            Question: \"Question here\"\\n            SQLQuery: \"SQL Query to run\"\\n            SQLResult: \"Result of the SQLQuery\"\\n            Answer: \"Final answer here\"\\n\\n            Only use the following tables:\\n\\n            {table_info}.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\sql_database\\\\query.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Question: {input}\\'\\'\\'\\n            prompt = PromptTemplate.from_template(template)\\n    \"\"\"  # noqa: E501\\n    if prompt is not None:\\n        prompt_to_use = prompt\\n    elif db.dialect in SQL_PROMPTS:\\n        prompt_to_use = SQL_PROMPTS[db.dialect]\\n    else:\\n        prompt_to_use = PROMPT\\n    if {\"input\", \"top_k\", \"table_info\"}.difference(prompt_to_use.input_variables):\\n        raise ValueError(\\n            f\"Prompt must have input variables: \\'input\\', \\'top_k\\', \"\\n            f\"\\'table_info\\'. Received prompt with input variables: \"\\n            f\"{prompt_to_use.input_variables}. Full prompt:\\\\n\\\\n{prompt_to_use}\"\\n        )\\n    if \"dialect\" in prompt_to_use.input_variables:\\n        prompt_to_use = prompt_to_use.partial(dialect=db.dialect)\\n\\n    inputs = {\\n        \"input\": lambda x: x[\"question\"] + \"\\\\nSQLQuery: \",\\n        \"table_info\": lambda x: db.get_table_info(\\n            table_names=x.get(\"table_names_to_use\")\\n        ),\\n    }\\n    return (\\n        RunnablePassthrough.assign(**inputs)  # type: ignore\\n        | (\\n            lambda x: {\\n                k: v\\n                for k, v in x.items()\\n                if k not in (\"question\", \"table_names_to_use\")\\n            }\\n        )\\n        | prompt_to_use.partial(top_k=str(k))\\n        | llm.bind(stop=[\"\\\\nSQLResult:\"])\\n        | StrOutputParser()\\n        | _strip\\n    )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\sql_database\\\\query.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Chain for interacting with SQL Database.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\sql_database\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# flake8: noqa\\nfrom langchain_core.prompts import PromptTemplate\\n\\nprompt_template = \"\"\"Write a concise summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nCONCISE SUMMARY:\"\"\"\\nPROMPT = PromptTemplate(template=prompt_template, input_variables=[\"text\"])' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\summarize\\\\map_reduce_prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.prompts import PromptTemplate\\n\\nREFINE_PROMPT_TMPL = \"\"\"\\\\\\nYour job is to produce a final summary.\\nWe have provided an existing summary up to a certain point: {existing_answer}\\nWe have the opportunity to refine the existing summary (only if needed) with some more context below.\\n------------\\n{text}\\n------------\\nGiven the new context, refine the original summary.\\nIf the context isn\\'t useful, return the original summary.\\\\\\n\"\"\"  # noqa: E501\\nREFINE_PROMPT = PromptTemplate.from_template(REFINE_PROMPT_TMPL)\\n\\n\\nprompt_template = \"\"\"Write a concise summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nCONCISE SUMMARY:\"\"\"\\nPROMPT = PromptTemplate.from_template(prompt_template)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\summarize\\\\refine_prompts.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# flake8: noqa\\nfrom langchain_core.prompts import PromptTemplate\\n\\nprompt_template = \"\"\"Write a concise summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nCONCISE SUMMARY:\"\"\"\\nPROMPT = PromptTemplate(template=prompt_template, input_variables=[\"text\"])' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\summarize\\\\stuff_prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Load summarizing chains.\"\"\"\\nfrom typing import Any, Mapping, Optional, Protocol\\n\\nfrom langchain_core.callbacks import Callbacks\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.prompts import BasePromptTemplate\\n\\nfrom langchain.chains.combine_documents.base import BaseCombineDocumentsChain\\nfrom langchain.chains.combine_documents.map_reduce import MapReduceDocumentsChain\\nfrom langchain.chains.combine_documents.reduce import ReduceDocumentsChain\\nfrom langchain.chains.combine_documents.refine import RefineDocumentsChain\\nfrom langchain.chains.combine_documents.stuff import StuffDocumentsChain\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.chains.summarize import map_reduce_prompt, refine_prompts, stuff_prompt\\n\\n\\nclass LoadingCallable(Protocol):\\n    \"\"\"Interface for loading the combine documents chain.\"\"\"\\n\\n    def __call__(\\n        self, llm: BaseLanguageModel, **kwargs: Any\\n    ) -> BaseCombineDocumentsChain:\\n        \"\"\"Callable to load the combine documents chain.\"\"\"\\n\\n\\ndef _load_stuff_chain(\\n    llm: BaseLanguageModel,\\n    prompt: BasePromptTemplate = stuff_prompt.PROMPT,\\n    document_variable_name: str = \"text\",\\n    verbose: Optional[bool] = None,\\n    **kwargs: Any,\\n) -> StuffDocumentsChain:\\n    llm_chain = LLMChain(llm=llm, prompt=prompt, verbose=verbose)\\n    # TODO: document prompt\\n    return StuffDocumentsChain(\\n        llm_chain=llm_chain,\\n        document_variable_name=document_variable_name,\\n        verbose=verbose,\\n        **kwargs,\\n    )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\summarize\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _load_map_reduce_chain(\\n    llm: BaseLanguageModel,\\n    map_prompt: BasePromptTemplate = map_reduce_prompt.PROMPT,\\n    combine_prompt: BasePromptTemplate = map_reduce_prompt.PROMPT,\\n    combine_document_variable_name: str = \"text\",\\n    map_reduce_document_variable_name: str = \"text\",\\n    collapse_prompt: Optional[BasePromptTemplate] = None,\\n    reduce_llm: Optional[BaseLanguageModel] = None,\\n    collapse_llm: Optional[BaseLanguageModel] = None,\\n    verbose: Optional[bool] = None,\\n    token_max: int = 3000,\\n    callbacks: Callbacks = None,\\n    **kwargs: Any,\\n) -> MapReduceDocumentsChain:\\n    map_chain = LLMChain(\\n        llm=llm, prompt=map_prompt, verbose=verbose, callbacks=callbacks\\n    )\\n    _reduce_llm = reduce_llm or llm\\n    reduce_chain = LLMChain(\\n        llm=_reduce_llm, prompt=combine_prompt, verbose=verbose, callbacks=callbacks\\n    )\\n    # TODO: document prompt\\n    combine_documents_chain = StuffDocumentsChain(\\n        llm_chain=reduce_chain,\\n        document_variable_name=combine_document_variable_name,\\n        verbose=verbose,\\n        callbacks=callbacks,\\n    )\\n    if collapse_prompt is None:\\n        collapse_chain = None\\n        if collapse_llm is not None:\\n            raise ValueError(\\n                \"collapse_llm provided, but collapse_prompt was not: please \"\\n                \"provide one or stop providing collapse_llm.\"\\n            )\\n    else:\\n        _collapse_llm = collapse_llm or llm\\n        collapse_chain = StuffDocumentsChain(\\n            llm_chain=LLMChain(\\n                llm=_collapse_llm,\\n                prompt=collapse_prompt,\\n                verbose=verbose,\\n                callbacks=callbacks,\\n            ),\\n            document_variable_name=combine_document_variable_name,\\n        )\\n    reduce_documents_chain = ReduceDocumentsChain(\\n        combine_documents_chain=combine_documents_chain,\\n        collapse_documents_chain=collapse_chain,\\n        token_max=token_max,\\n        verbose=verbose,\\n        callbacks=callbacks,\\n    )\\n    return MapReduceDocumentsChain(' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\summarize\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='llm_chain=map_chain,\\n        reduce_documents_chain=reduce_documents_chain,\\n        document_variable_name=map_reduce_document_variable_name,\\n        verbose=verbose,\\n        callbacks=callbacks,\\n        **kwargs,\\n    )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\summarize\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _load_refine_chain(\\n    llm: BaseLanguageModel,\\n    question_prompt: BasePromptTemplate = refine_prompts.PROMPT,\\n    refine_prompt: BasePromptTemplate = refine_prompts.REFINE_PROMPT,\\n    document_variable_name: str = \"text\",\\n    initial_response_name: str = \"existing_answer\",\\n    refine_llm: Optional[BaseLanguageModel] = None,\\n    verbose: Optional[bool] = None,\\n    **kwargs: Any,\\n) -> RefineDocumentsChain:\\n    initial_chain = LLMChain(llm=llm, prompt=question_prompt, verbose=verbose)\\n    _refine_llm = refine_llm or llm\\n    refine_chain = LLMChain(llm=_refine_llm, prompt=refine_prompt, verbose=verbose)\\n    return RefineDocumentsChain(\\n        initial_llm_chain=initial_chain,\\n        refine_llm_chain=refine_chain,\\n        document_variable_name=document_variable_name,\\n        initial_response_name=initial_response_name,\\n        verbose=verbose,\\n        **kwargs,\\n    )\\n\\n\\ndef load_summarize_chain(\\n    llm: BaseLanguageModel,\\n    chain_type: str = \"stuff\",\\n    verbose: Optional[bool] = None,\\n    **kwargs: Any,\\n) -> BaseCombineDocumentsChain:\\n    \"\"\"Load summarizing chain.\\n\\n    Args:\\n        llm: Language Model to use in the chain.\\n        chain_type: Type of document combining chain to use. Should be one of \"stuff\",\\n            \"map_reduce\", and \"refine\".\\n        verbose: Whether chains should be run in verbose mode or not. Note that this\\n            applies to all chains that make up the final chain.\\n\\n    Returns:\\n        A chain to use for summarizing.\\n    \"\"\"\\n    loader_mapping: Mapping[str, LoadingCallable] = {\\n        \"stuff\": _load_stuff_chain,\\n        \"map_reduce\": _load_map_reduce_chain,\\n        \"refine\": _load_refine_chain,\\n    }\\n    if chain_type not in loader_mapping:\\n        raise ValueError(\\n            f\"Got unsupported chain type: {chain_type}. \"\\n            f\"Should be one of {loader_mapping.keys()}\"\\n        )\\n    return loader_mapping[chain_type](llm, verbose=verbose, **kwargs)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\summarize\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_loaders.base import BaseChatLoader\\n\\n__all__ = [\"BaseChatLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chat_loaders\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_loaders.facebook_messenger import (\\n    FolderFacebookMessengerChatLoader,\\n    SingleFileFacebookMessengerChatLoader,\\n)\\n\\n__all__ = [\"SingleFileFacebookMessengerChatLoader\", \"FolderFacebookMessengerChatLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chat_loaders\\\\facebook_messenger.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_loaders.gmail import (\\n    GMailLoader,\\n)\\n\\n__all__ = [\"GMailLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chat_loaders\\\\gmail.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_loaders.imessage import IMessageChatLoader\\n\\n__all__ = [\"IMessageChatLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chat_loaders\\\\imessage.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_loaders.langsmith import (\\n    LangSmithDatasetChatLoader,\\n    LangSmithRunChatLoader,\\n)\\n\\n__all__ = [\"LangSmithRunChatLoader\", \"LangSmithDatasetChatLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chat_loaders\\\\langsmith.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_loaders.slack import SlackChatLoader\\n\\n__all__ = [\"SlackChatLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chat_loaders\\\\slack.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_loaders.telegram import TelegramChatLoader\\n\\n__all__ = [\"TelegramChatLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chat_loaders\\\\telegram.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_loaders.utils import (\\n    map_ai_messages,\\n    map_ai_messages_in_session,\\n    merge_chat_runs,\\n    merge_chat_runs_in_session,\\n)\\n\\n__all__ = [\\n    \"merge_chat_runs_in_session\",\\n    \"merge_chat_runs\",\\n    \"map_ai_messages_in_session\",\\n    \"map_ai_messages\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chat_loaders\\\\utils.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_loaders.whatsapp import WhatsAppChatLoader\\n\\n__all__ = [\"WhatsAppChatLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chat_loaders\\\\whatsapp.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"**Chat Loaders** load chat messages from common communications platforms.\\n\\nLoad chat messages from various\\ncommunications platforms such as Facebook Messenger, Telegram, and\\nWhatsApp. The loaded chat messages can be used for fine-tuning models.\\n\\n**Class hierarchy:**\\n\\n.. code-block::\\n\\n    BaseChatLoader --> <name>ChatLoader  # Examples: WhatsAppChatLoader, IMessageChatLoader\\n\\n**Main helpers:**\\n\\n.. code-block::\\n\\n    ChatSession\\n\\n\"\"\"  # noqa: E501' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chat_loaders\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_models.anthropic import (\\n    ChatAnthropic,\\n    convert_messages_to_prompt_anthropic,\\n)\\n\\n__all__ = [\\n    \"convert_messages_to_prompt_anthropic\",\\n    \"ChatAnthropic\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chat_models\\\\anthropic.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_models.anyscale import (\\n    ChatAnyscale,\\n)\\n\\n__all__ = [\"ChatAnyscale\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chat_models\\\\anyscale.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_models.azureml_endpoint import (\\n    AzureMLChatOnlineEndpoint,\\n    LlamaContentFormatter,\\n)\\n\\n__all__ = [\"LlamaContentFormatter\", \"AzureMLChatOnlineEndpoint\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chat_models\\\\azureml_endpoint.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_models.azure_openai import AzureChatOpenAI\\n\\n__all__ = [\"AzureChatOpenAI\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chat_models\\\\azure_openai.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_models.baichuan import (\\n    ChatBaichuan,\\n)\\n\\n__all__ = [\\n    \"ChatBaichuan\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chat_models\\\\baichuan.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_models.baidu_qianfan_endpoint import (\\n    QianfanChatEndpoint,\\n)\\n\\n__all__ = [\"QianfanChatEndpoint\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chat_models\\\\baidu_qianfan_endpoint.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.language_models.chat_models import (\\n    BaseChatModel,\\n    SimpleChatModel,\\n    agenerate_from_stream,\\n    generate_from_stream,\\n)\\n\\n__all__ = [\\n    \"BaseChatModel\",\\n    \"SimpleChatModel\",\\n    \"generate_from_stream\",\\n    \"agenerate_from_stream\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chat_models\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_models.bedrock import BedrockChat, ChatPromptAdapter\\n\\n__all__ = [\"ChatPromptAdapter\", \"BedrockChat\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chat_models\\\\bedrock.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_models.cohere import (\\n    ChatCohere,\\n)\\n\\n__all__ = [\"ChatCohere\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chat_models\\\\cohere.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_models.databricks import ChatDatabricks\\n\\n__all__ = [\"ChatDatabricks\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chat_models\\\\databricks.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_models.ernie import ErnieBotChat\\n\\n__all__ = [\"ErnieBotChat\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chat_models\\\\ernie.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_models.everlyai import (\\n    ChatEverlyAI,\\n)\\n\\n__all__ = [\"ChatEverlyAI\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chat_models\\\\everlyai.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_models.fake import (\\n    FakeListChatModel,\\n    FakeMessagesListChatModel,\\n)\\n\\n__all__ = [\"FakeMessagesListChatModel\", \"FakeListChatModel\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chat_models\\\\fake.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_models.fireworks import (\\n    ChatFireworks,\\n)\\n\\n__all__ = [\\n    \"ChatFireworks\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chat_models\\\\fireworks.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_models.gigachat import (\\n    GigaChat,\\n)\\n\\n__all__ = [\"GigaChat\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chat_models\\\\gigachat.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_models.google_palm import (\\n    ChatGooglePalm,\\n    ChatGooglePalmError,\\n)\\n\\n__all__ = [\"ChatGooglePalm\", \"ChatGooglePalmError\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chat_models\\\\google_palm.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_models.human import (\\n    HumanInputChatModel,\\n)\\n\\n__all__ = [\"HumanInputChatModel\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chat_models\\\\human.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_models.hunyuan import (\\n    ChatHunyuan,\\n)\\n\\n__all__ = [\\n    \"ChatHunyuan\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chat_models\\\\hunyuan.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_models.javelin_ai_gateway import (\\n    ChatJavelinAIGateway,\\n    ChatParams,\\n)\\n\\n__all__ = [\"ChatJavelinAIGateway\", \"ChatParams\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chat_models\\\\javelin_ai_gateway.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_models.jinachat import (\\n    JinaChat,\\n)\\n\\n__all__ = [\\n    \"JinaChat\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chat_models\\\\jinachat.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_models.konko import (\\n    ChatKonko,\\n)\\n\\n__all__ = [\"ChatKonko\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chat_models\\\\konko.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_models.litellm import ChatLiteLLM, ChatLiteLLMException\\n\\n__all__ = [\"ChatLiteLLM\", \"ChatLiteLLMException\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chat_models\\\\litellm.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_models.meta import (\\n    convert_messages_to_prompt_llama,\\n)\\n\\n__all__ = [\"convert_messages_to_prompt_llama\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chat_models\\\\meta.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_models.minimax import (\\n    MiniMaxChat,\\n)\\n\\n__all__ = [\"MiniMaxChat\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chat_models\\\\minimax.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_models.mlflow import ChatMlflow\\n\\n__all__ = [\"ChatMlflow\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chat_models\\\\mlflow.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_models.mlflow_ai_gateway import (\\n    ChatMLflowAIGateway,\\n    ChatParams,\\n)\\n\\n__all__ = [\"ChatMLflowAIGateway\", \"ChatParams\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chat_models\\\\mlflow_ai_gateway.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_models.ollama import (\\n    ChatOllama,\\n)\\n\\n__all__ = [\"ChatOllama\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chat_models\\\\ollama.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_models.openai import (\\n    ChatOpenAI,\\n)\\n\\n__all__ = [\\n    \"ChatOpenAI\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chat_models\\\\openai.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_models.pai_eas_endpoint import PaiEasChatEndpoint\\n\\n__all__ = [\"PaiEasChatEndpoint\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chat_models\\\\pai_eas_endpoint.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_models.promptlayer_openai import PromptLayerChatOpenAI\\n\\n__all__ = [\"PromptLayerChatOpenAI\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chat_models\\\\promptlayer_openai.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_models.tongyi import (\\n    ChatTongyi,\\n)\\n\\n__all__ = [\\n    \"ChatTongyi\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chat_models\\\\tongyi.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_models.vertexai import (\\n    ChatVertexAI,\\n)\\n\\n__all__ = [\\n    \"ChatVertexAI\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chat_models\\\\vertexai.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_models.volcengine_maas import (\\n    VolcEngineMaasChat,\\n    convert_dict_to_message,\\n)\\n\\n__all__ = [\"convert_dict_to_message\", \"VolcEngineMaasChat\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chat_models\\\\volcengine_maas.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_models.yandex import (\\n    ChatYandexGPT,\\n)\\n\\n__all__ = [\"ChatYandexGPT\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chat_models\\\\yandex.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"**Chat Models** are a variation on language models.\\n\\nWhile Chat Models use language models under the hood, the interface they expose\\nis a bit different. Rather than expose a \"text in, text out\" API, they expose\\nan interface where \"chat messages\" are the inputs and outputs.\\n\\n**Class hierarchy:**\\n\\n.. code-block::\\n\\n    BaseLanguageModel --> BaseChatModel --> <name>  # Examples: ChatOpenAI, ChatGooglePalm\\n\\n**Main helpers:**\\n\\n.. code-block::\\n\\n    AIMessage, BaseMessage, HumanMessage\\n\"\"\"  # noqa: E501\\nimport warnings\\n\\nfrom langchain_core._api import LangChainDeprecationWarning\\n\\nfrom langchain.utils.interactive_env import is_interactive_env\\n\\n\\ndef __getattr__(name: str) -> None:\\n    from langchain_community import chat_models\\n\\n    # If not in interactive env, raise warning.\\n    if not is_interactive_env():\\n        warnings.warn(\\n            \"Importing chat models from langchain is deprecated. Importing from \"\\n            \"langchain will no longer be supported as of langchain==0.2.0. \"\\n            \"Please import from langchain-community instead:\\\\n\\\\n\"\\n            f\"`from langchain_community.chat_models import {name}`.\\\\n\\\\n\"\\n            \"To install langchain-community run `pip install -U langchain-community`.\",\\n            category=LangChainDeprecationWarning,\\n        )\\n\\n    return getattr(chat_models, name)\\n\\n\\n__all__ = [\\n    \"ChatOpenAI\",\\n    \"BedrockChat\",\\n    \"AzureChatOpenAI\",\\n    \"FakeListChatModel\",\\n    \"PromptLayerChatOpenAI\",\\n    \"ChatDatabricks\",\\n    \"ChatEverlyAI\",\\n    \"ChatAnthropic\",\\n    \"ChatCohere\",\\n    \"ChatGooglePalm\",\\n    \"ChatMlflow\",\\n    \"ChatMLflowAIGateway\",\\n    \"ChatOllama\",\\n    \"ChatVertexAI\",\\n    \"JinaChat\",\\n    \"HumanInputChatModel\",\\n    \"MiniMaxChat\",\\n    \"ChatAnyscale\",\\n    \"ChatLiteLLM\",\\n    \"ErnieBotChat\",\\n    \"ChatJavelinAIGateway\",\\n    \"ChatKonko\",\\n    \"PaiEasChatEndpoint\",\\n    \"QianfanChatEndpoint\",\\n    \"ChatFireworks\",\\n    \"ChatYandexGPT\",\\n    \"ChatBaichuan\",\\n    \"ChatHunyuan\",\\n    \"GigaChat\",\\n    \"VolcEngineMaasChat\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\chat_models\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.docstore.arbitrary_fn import DocstoreFn\\n\\n__all__ = [\"DocstoreFn\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\docstore\\\\arbitrary_fn.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.docstore.base import AddableMixin, Docstore\\n\\n__all__ = [\"Docstore\", \"AddableMixin\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\docstore\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.documents import Document\\n\\n__all__ = [\"Document\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\docstore\\\\document.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.docstore.in_memory import InMemoryDocstore\\n\\n__all__ = [\"InMemoryDocstore\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\docstore\\\\in_memory.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.docstore.wikipedia import Wikipedia\\n\\n__all__ = [\"Wikipedia\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\docstore\\\\wikipedia.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"**Docstores** are classes to store and load Documents.\\n\\nThe **Docstore** is a simplified version of the Document Loader.\\n\\n**Class hierarchy:**\\n\\n.. code-block::\\n\\n    Docstore --> <name> # Examples: InMemoryDocstore, Wikipedia\\n\\n**Main helpers:**\\n\\n.. code-block::\\n\\n    Document, AddableMixin\\n\"\"\"\\nimport warnings\\nfrom typing import Any\\n\\nfrom langchain_core._api import LangChainDeprecationWarning\\n\\nfrom langchain.utils.interactive_env import is_interactive_env\\n\\n\\ndef __getattr__(name: str) -> Any:\\n    from langchain_community import docstore\\n\\n    # If not in interactive env, raise warning.\\n    if not is_interactive_env():\\n        warnings.warn(\\n            \"Importing docstores from langchain is deprecated. Importing from \"\\n            \"langchain will no longer be supported as of langchain==0.2.0. \"\\n            \"Please import from langchain-community instead:\\\\n\\\\n\"\\n            f\"`from langchain_community.docstore import {name}`.\\\\n\\\\n\"\\n            \"To install langchain-community run `pip install -U langchain-community`.\",\\n            category=LangChainDeprecationWarning,\\n        )\\n\\n    return getattr(docstore, name)\\n\\n\\n__all__ = [\"DocstoreFn\", \"InMemoryDocstore\", \"Wikipedia\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\docstore\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.acreom import AcreomLoader\\n\\n__all__ = [\"AcreomLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\acreom.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.airbyte import (\\n    AirbyteCDKLoader,\\n    AirbyteGongLoader,\\n    AirbyteHubspotLoader,\\n    AirbyteSalesforceLoader,\\n    AirbyteShopifyLoader,\\n    AirbyteStripeLoader,\\n    AirbyteTypeformLoader,\\n    AirbyteZendeskSupportLoader,\\n    RecordHandler,\\n)\\n\\n__all__ = [\\n    \"RecordHandler\",\\n    \"AirbyteCDKLoader\",\\n    \"AirbyteHubspotLoader\",\\n    \"AirbyteStripeLoader\",\\n    \"AirbyteTypeformLoader\",\\n    \"AirbyteZendeskSupportLoader\",\\n    \"AirbyteShopifyLoader\",\\n    \"AirbyteSalesforceLoader\",\\n    \"AirbyteGongLoader\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\airbyte.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.airbyte_json import AirbyteJSONLoader\\n\\n__all__ = [\"AirbyteJSONLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\airbyte_json.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.airtable import AirtableLoader\\n\\n__all__ = [\"AirtableLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\airtable.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.apify_dataset import ApifyDatasetLoader\\n\\n__all__ = [\"ApifyDatasetLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\apify_dataset.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.arcgis_loader import (\\n    ArcGISLoader,\\n)\\n\\n__all__ = [\"ArcGISLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\arcgis_loader.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.arxiv import ArxivLoader\\n\\n__all__ = [\"ArxivLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\arxiv.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.assemblyai import (\\n    AssemblyAIAudioTranscriptLoader,\\n    TranscriptFormat,\\n)\\n\\n__all__ = [\"TranscriptFormat\", \"AssemblyAIAudioTranscriptLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\assemblyai.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.async_html import (\\n    AsyncHtmlLoader,\\n)\\n\\n__all__ = [\"AsyncHtmlLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\async_html.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.azlyrics import AZLyricsLoader\\n\\n__all__ = [\"AZLyricsLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\azlyrics.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.azure_ai_data import AzureAIDataLoader\\n\\n__all__ = [\"AzureAIDataLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\azure_ai_data.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.azure_blob_storage_container import (\\n    AzureBlobStorageContainerLoader,\\n)\\n\\n__all__ = [\"AzureBlobStorageContainerLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\azure_blob_storage_container.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.azure_blob_storage_file import (\\n    AzureBlobStorageFileLoader,\\n)\\n\\n__all__ = [\"AzureBlobStorageFileLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\azure_blob_storage_file.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.baiducloud_bos_directory import (\\n    BaiduBOSDirectoryLoader,\\n)\\n\\n__all__ = [\"BaiduBOSDirectoryLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\baiducloud_bos_directory.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.baiducloud_bos_file import BaiduBOSFileLoader\\n\\n__all__ = [\"BaiduBOSFileLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\baiducloud_bos_file.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.base import BaseBlobParser, BaseLoader\\n\\n__all__ = [\"BaseLoader\", \"BaseBlobParser\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.base_o365 import (\\n    O365BaseLoader,\\n)\\n\\n__all__ = [\\n    \"O365BaseLoader\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\base_o365.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.bibtex import BibtexLoader\\n\\n__all__ = [\"BibtexLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\bibtex.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.bigquery import BigQueryLoader\\n\\n__all__ = [\"BigQueryLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\bigquery.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.bilibili import BiliBiliLoader\\n\\n__all__ = [\"BiliBiliLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\bilibili.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.blackboard import BlackboardLoader\\n\\n__all__ = [\"BlackboardLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\blackboard.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.blockchain import (\\n    BlockchainDocumentLoader,\\n    BlockchainType,\\n)\\n\\n__all__ = [\"BlockchainType\", \"BlockchainDocumentLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\blockchain.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.brave_search import BraveSearchLoader\\n\\n__all__ = [\"BraveSearchLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\brave_search.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.browserless import BrowserlessLoader\\n\\n__all__ = [\"BrowserlessLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\browserless.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.chatgpt import ChatGPTLoader, concatenate_rows\\n\\n__all__ = [\"concatenate_rows\", \"ChatGPTLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\chatgpt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.chromium import AsyncChromiumLoader\\n\\n__all__ = [\"AsyncChromiumLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\chromium.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.college_confidential import (\\n    CollegeConfidentialLoader,\\n)\\n\\n__all__ = [\"CollegeConfidentialLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\college_confidential.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.concurrent import (\\n    ConcurrentLoader,\\n)\\n\\n__all__ = [\"ConcurrentLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\concurrent.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.confluence import (\\n    ConfluenceLoader,\\n    ContentFormat,\\n)\\n\\n__all__ = [\"ContentFormat\", \"ConfluenceLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\confluence.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.conllu import CoNLLULoader\\n\\n__all__ = [\"CoNLLULoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\conllu.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.couchbase import CouchbaseLoader\\n\\n__all__ = [\"CouchbaseLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\couchbase.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.csv_loader import (\\n    CSVLoader,\\n    UnstructuredCSVLoader,\\n)\\n\\n__all__ = [\"CSVLoader\", \"UnstructuredCSVLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\csv_loader.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.cube_semantic import CubeSemanticLoader\\n\\n__all__ = [\"CubeSemanticLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\cube_semantic.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.datadog_logs import DatadogLogsLoader\\n\\n__all__ = [\"DatadogLogsLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\datadog_logs.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.dataframe import (\\n    BaseDataFrameLoader,\\n    DataFrameLoader,\\n)\\n\\n__all__ = [\"BaseDataFrameLoader\", \"DataFrameLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\dataframe.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.diffbot import DiffbotLoader\\n\\n__all__ = [\"DiffbotLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\diffbot.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.directory import (\\n    DirectoryLoader,\\n)\\n\\n__all__ = [\"DirectoryLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\directory.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.discord import DiscordChatLoader\\n\\n__all__ = [\"DiscordChatLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\discord.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.docugami import (\\n    DocugamiLoader,\\n)\\n\\n__all__ = [\\n    \"DocugamiLoader\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\docugami.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.docusaurus import DocusaurusLoader\\n\\n__all__ = [\"DocusaurusLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\docusaurus.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.dropbox import DropboxLoader\\n\\n__all__ = [\"DropboxLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\dropbox.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.duckdb_loader import DuckDBLoader\\n\\n__all__ = [\"DuckDBLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\duckdb_loader.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.email import (\\n    OutlookMessageLoader,\\n    UnstructuredEmailLoader,\\n)\\n\\n__all__ = [\"UnstructuredEmailLoader\", \"OutlookMessageLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\email.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.epub import UnstructuredEPubLoader\\n\\n__all__ = [\"UnstructuredEPubLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\epub.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.etherscan import EtherscanLoader\\n\\n__all__ = [\"EtherscanLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\etherscan.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.evernote import EverNoteLoader\\n\\n__all__ = [\"EverNoteLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\evernote.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.excel import UnstructuredExcelLoader\\n\\n__all__ = [\"UnstructuredExcelLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\excel.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.facebook_chat import (\\n    FacebookChatLoader,\\n    concatenate_rows,\\n)\\n\\n__all__ = [\"concatenate_rows\", \"FacebookChatLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\facebook_chat.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.fauna import FaunaLoader\\n\\n__all__ = [\"FaunaLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\fauna.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.figma import FigmaFileLoader\\n\\n__all__ = [\"FigmaFileLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\figma.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.gcs_directory import GCSDirectoryLoader\\n\\n__all__ = [\"GCSDirectoryLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\gcs_directory.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.gcs_file import GCSFileLoader\\n\\n__all__ = [\"GCSFileLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\gcs_file.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.generic import (\\n    GenericLoader,\\n)\\n\\n__all__ = [\"GenericLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\generic.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.geodataframe import GeoDataFrameLoader\\n\\n__all__ = [\"GeoDataFrameLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\geodataframe.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.git import GitLoader\\n\\n__all__ = [\"GitLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\git.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.gitbook import GitbookLoader\\n\\n__all__ = [\"GitbookLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\gitbook.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.github import (\\n    BaseGitHubLoader,\\n    GitHubIssuesLoader,\\n)\\n\\n__all__ = [\"BaseGitHubLoader\", \"GitHubIssuesLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\github.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.googledrive import GoogleDriveLoader\\n\\n__all__ = [\"GoogleDriveLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\googledrive.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.google_speech_to_text import (\\n    GoogleSpeechToTextLoader,\\n)\\n\\n__all__ = [\"GoogleSpeechToTextLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\google_speech_to_text.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.gutenberg import GutenbergLoader\\n\\n__all__ = [\"GutenbergLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\gutenberg.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.helpers import (\\n    FileEncoding,\\n    detect_file_encodings,\\n)\\n\\n__all__ = [\"FileEncoding\", \"detect_file_encodings\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\helpers.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.hn import HNLoader\\n\\n__all__ = [\"HNLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\hn.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.html import UnstructuredHTMLLoader\\n\\n__all__ = [\"UnstructuredHTMLLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\html.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.html_bs import BSHTMLLoader\\n\\n__all__ = [\"BSHTMLLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\html_bs.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.hugging_face_dataset import (\\n    HuggingFaceDatasetLoader,\\n)\\n\\n__all__ = [\"HuggingFaceDatasetLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\hugging_face_dataset.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.ifixit import IFixitLoader\\n\\n__all__ = [\"IFixitLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\ifixit.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.image import UnstructuredImageLoader\\n\\n__all__ = [\"UnstructuredImageLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\image.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.image_captions import ImageCaptionLoader\\n\\n__all__ = [\"ImageCaptionLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\image_captions.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.imsdb import IMSDbLoader\\n\\n__all__ = [\"IMSDbLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\imsdb.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.iugu import IuguLoader\\n\\n__all__ = [\"IuguLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\iugu.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.joplin import JoplinLoader\\n\\n__all__ = [\"JoplinLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\joplin.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.json_loader import JSONLoader\\n\\n__all__ = [\"JSONLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\json_loader.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.lakefs import (\\n    LakeFSClient,\\n    LakeFSLoader,\\n    UnstructuredLakeFSLoader,\\n)\\n\\n__all__ = [\"LakeFSClient\", \"LakeFSLoader\", \"UnstructuredLakeFSLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\lakefs.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.larksuite import LarkSuiteDocLoader\\n\\n__all__ = [\"LarkSuiteDocLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\larksuite.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.markdown import UnstructuredMarkdownLoader\\n\\n__all__ = [\"UnstructuredMarkdownLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\markdown.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.mastodon import (\\n    MastodonTootsLoader,\\n)\\n\\n__all__ = [\"MastodonTootsLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\mastodon.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.max_compute import MaxComputeLoader\\n\\n__all__ = [\"MaxComputeLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\max_compute.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.mediawikidump import MWDumpLoader\\n\\n__all__ = [\"MWDumpLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\mediawikidump.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.merge import MergedDataLoader\\n\\n__all__ = [\"MergedDataLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\merge.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.mhtml import MHTMLLoader\\n\\n__all__ = [\"MHTMLLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\mhtml.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.modern_treasury import (\\n    ModernTreasuryLoader,\\n)\\n\\n__all__ = [\"ModernTreasuryLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\modern_treasury.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.mongodb import MongodbLoader\\n\\n__all__ = [\"MongodbLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\mongodb.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.news import NewsURLLoader\\n\\n__all__ = [\"NewsURLLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\news.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.notebook import (\\n    NotebookLoader,\\n    concatenate_cells,\\n    remove_newlines,\\n)\\n\\n__all__ = [\"concatenate_cells\", \"remove_newlines\", \"NotebookLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\notebook.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.notion import NotionDirectoryLoader\\n\\n__all__ = [\"NotionDirectoryLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\notion.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.notiondb import (\\n    NotionDBLoader,\\n)\\n\\n__all__ = [\"NotionDBLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\notiondb.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.nuclia import NucliaLoader\\n\\n__all__ = [\"NucliaLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\nuclia.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.obsidian import ObsidianLoader\\n\\n__all__ = [\"ObsidianLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\obsidian.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.obs_directory import OBSDirectoryLoader\\n\\n__all__ = [\"OBSDirectoryLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\obs_directory.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.obs_file import OBSFileLoader\\n\\n__all__ = [\"OBSFileLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\obs_file.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.odt import UnstructuredODTLoader\\n\\n__all__ = [\"UnstructuredODTLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\odt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.onedrive import OneDriveLoader\\n\\n__all__ = [\"OneDriveLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\onedrive.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.onedrive_file import (\\n    OneDriveFileLoader,\\n)\\n\\n__all__ = [\"OneDriveFileLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\onedrive_file.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.onenote import (\\n    OneNoteLoader,\\n)\\n\\n__all__ = [\"OneNoteLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\onenote.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.open_city_data import OpenCityDataLoader\\n\\n__all__ = [\"OpenCityDataLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\open_city_data.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.org_mode import UnstructuredOrgModeLoader\\n\\n__all__ = [\"UnstructuredOrgModeLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\org_mode.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.pdf import (\\n    AmazonTextractPDFLoader,\\n    BasePDFLoader,\\n    DocumentIntelligenceLoader,\\n    MathpixPDFLoader,\\n    OnlinePDFLoader,\\n    PDFMinerLoader,\\n    PDFMinerPDFasHTMLLoader,\\n    PDFPlumberLoader,\\n    PyMuPDFLoader,\\n    PyPDFDirectoryLoader,\\n    PyPDFium2Loader,\\n    PyPDFLoader,\\n    UnstructuredPDFLoader,\\n)\\n\\n__all__ = [\\n    \"UnstructuredPDFLoader\",\\n    \"BasePDFLoader\",\\n    \"OnlinePDFLoader\",\\n    \"PyPDFLoader\",\\n    \"PyPDFium2Loader\",\\n    \"PyPDFDirectoryLoader\",\\n    \"PDFMinerLoader\",\\n    \"PDFMinerPDFasHTMLLoader\",\\n    \"PyMuPDFLoader\",\\n    \"MathpixPDFLoader\",\\n    \"PDFPlumberLoader\",\\n    \"AmazonTextractPDFLoader\",\\n    \"DocumentIntelligenceLoader\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\pdf.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.polars_dataframe import PolarsDataFrameLoader\\n\\n__all__ = [\"PolarsDataFrameLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\polars_dataframe.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.powerpoint import UnstructuredPowerPointLoader\\n\\n__all__ = [\"UnstructuredPowerPointLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\powerpoint.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.psychic import PsychicLoader\\n\\n__all__ = [\"PsychicLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\psychic.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.pubmed import PubMedLoader\\n\\n__all__ = [\"PubMedLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\pubmed.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.pyspark_dataframe import (\\n    PySparkDataFrameLoader,\\n)\\n\\n__all__ = [\"PySparkDataFrameLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\pyspark_dataframe.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.python import PythonLoader\\n\\n__all__ = [\"PythonLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\python.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.quip import QuipLoader\\n\\n__all__ = [\"QuipLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\quip.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.readthedocs import (\\n    ReadTheDocsLoader,\\n)\\n\\n__all__ = [\\n    \"ReadTheDocsLoader\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\readthedocs.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.recursive_url_loader import (\\n    RecursiveUrlLoader,\\n)\\n\\n__all__ = [\"RecursiveUrlLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\recursive_url_loader.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.reddit import (\\n    RedditPostsLoader,\\n)\\n\\n__all__ = [\"RedditPostsLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\reddit.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.roam import RoamLoader\\n\\n__all__ = [\"RoamLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\roam.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.rocksetdb import (\\n    RocksetLoader,\\n)\\n\\n__all__ = [\"RocksetLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\rocksetdb.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.rspace import RSpaceLoader\\n\\n__all__ = [\"RSpaceLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\rspace.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.rss import RSSFeedLoader\\n\\n__all__ = [\"RSSFeedLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\rss.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.rst import UnstructuredRSTLoader\\n\\n__all__ = [\"UnstructuredRSTLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\rst.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.rtf import UnstructuredRTFLoader\\n\\n__all__ = [\"UnstructuredRTFLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\rtf.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.s3_directory import S3DirectoryLoader\\n\\n__all__ = [\"S3DirectoryLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\s3_directory.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.s3_file import S3FileLoader\\n\\n__all__ = [\"S3FileLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\s3_file.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.sharepoint import SharePointLoader\\n\\n__all__ = [\"SharePointLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\sharepoint.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.sitemap import (\\n    SitemapLoader,\\n)\\n\\n__all__ = [\\n    \"SitemapLoader\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\sitemap.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.slack_directory import SlackDirectoryLoader\\n\\n__all__ = [\"SlackDirectoryLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\slack_directory.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.snowflake_loader import SnowflakeLoader\\n\\n__all__ = [\"SnowflakeLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\snowflake_loader.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.spreedly import (\\n    SpreedlyLoader,\\n)\\n\\n__all__ = [\"SpreedlyLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\spreedly.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.srt import SRTLoader\\n\\n__all__ = [\"SRTLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\srt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.stripe import StripeLoader\\n\\n__all__ = [\"StripeLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\stripe.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.telegram import (\\n    TelegramChatApiLoader,\\n    TelegramChatFileLoader,\\n    concatenate_rows,\\n    text_to_docs,\\n)\\n\\n__all__ = [\\n    \"concatenate_rows\",\\n    \"TelegramChatFileLoader\",\\n    \"text_to_docs\",\\n    \"TelegramChatApiLoader\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\telegram.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.tencent_cos_directory import (\\n    TencentCOSDirectoryLoader,\\n)\\n\\n__all__ = [\"TencentCOSDirectoryLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\tencent_cos_directory.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.tencent_cos_file import TencentCOSFileLoader\\n\\n__all__ = [\"TencentCOSFileLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\tencent_cos_file.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.tensorflow_datasets import (\\n    TensorflowDatasetLoader,\\n)\\n\\n__all__ = [\"TensorflowDatasetLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\tensorflow_datasets.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.text import TextLoader\\n\\n__all__ = [\"TextLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\text.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.tomarkdown import ToMarkdownLoader\\n\\n__all__ = [\"ToMarkdownLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\tomarkdown.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.toml import TomlLoader\\n\\n__all__ = [\"TomlLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\toml.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.trello import TrelloLoader\\n\\n__all__ = [\"TrelloLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\trello.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.tsv import UnstructuredTSVLoader\\n\\n__all__ = [\"UnstructuredTSVLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\tsv.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.twitter import (\\n    TwitterTweetLoader,\\n)\\n\\n__all__ = [\"TwitterTweetLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\twitter.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.unstructured import (\\n    UnstructuredAPIFileIOLoader,\\n    UnstructuredAPIFileLoader,\\n    UnstructuredBaseLoader,\\n    UnstructuredFileIOLoader,\\n    UnstructuredFileLoader,\\n    get_elements_from_api,\\n    satisfies_min_unstructured_version,\\n    validate_unstructured_version,\\n)\\n\\n__all__ = [\\n    \"satisfies_min_unstructured_version\",\\n    \"validate_unstructured_version\",\\n    \"UnstructuredBaseLoader\",\\n    \"UnstructuredFileLoader\",\\n    \"get_elements_from_api\",\\n    \"UnstructuredAPIFileLoader\",\\n    \"UnstructuredFileIOLoader\",\\n    \"UnstructuredAPIFileIOLoader\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\unstructured.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.url import UnstructuredURLLoader\\n\\n__all__ = [\"UnstructuredURLLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\url.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.url_playwright import (\\n    PlaywrightEvaluator,\\n    PlaywrightURLLoader,\\n    UnstructuredHtmlEvaluator,\\n)\\n\\n__all__ = [\"PlaywrightEvaluator\", \"UnstructuredHtmlEvaluator\", \"PlaywrightURLLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\url_playwright.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.url_selenium import SeleniumURLLoader\\n\\n__all__ = [\"SeleniumURLLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\url_selenium.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.weather import WeatherDataLoader\\n\\n__all__ = [\"WeatherDataLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\weather.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.web_base import (\\n    WebBaseLoader,\\n)\\n\\n__all__ = [\"WebBaseLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\web_base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.whatsapp_chat import (\\n    WhatsAppChatLoader,\\n    concatenate_rows,\\n)\\n\\n__all__ = [\"concatenate_rows\", \"WhatsAppChatLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\whatsapp_chat.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.wikipedia import WikipediaLoader\\n\\n__all__ = [\"WikipediaLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\wikipedia.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.word_document import (\\n    Docx2txtLoader,\\n    UnstructuredWordDocumentLoader,\\n)\\n\\n__all__ = [\"Docx2txtLoader\", \"UnstructuredWordDocumentLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\word_document.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.xml import UnstructuredXMLLoader\\n\\n__all__ = [\"UnstructuredXMLLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\xml.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.xorbits import XorbitsLoader\\n\\n__all__ = [\"XorbitsLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\xorbits.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.youtube import (\\n    GoogleApiClient,\\n    GoogleApiYoutubeLoader,\\n    YoutubeLoader,\\n)\\n\\n__all__ = [\\n    \"YoutubeLoader\",\\n    \"GoogleApiYoutubeLoader\",\\n    \"GoogleApiClient\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\youtube.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"**Document Loaders**  are classes to load Documents.\\n\\n**Document Loaders** are usually used to load a lot of Documents in a single run.\\n\\n**Class hierarchy:**\\n\\n.. code-block::\\n\\n    BaseLoader --> <name>Loader  # Examples: TextLoader, UnstructuredFileLoader\\n\\n**Main helpers:**\\n\\n.. code-block::\\n\\n    Document, <name>TextSplitter\\n\"\"\"\\nimport warnings\\nfrom typing import Any\\n\\nfrom langchain_core._api import LangChainDeprecationWarning\\n\\nfrom langchain.utils.interactive_env import is_interactive_env\\n\\n# For backwards compatibility\\n_old_to_new_name = {\\n    \"PagedPDFSplitter\": \"PyPDFLoader\",\\n    \"TelegramChatLoader\": \"TelegramChatFileLoader\",\\n}\\n\\n\\ndef __getattr__(name: str) -> Any:\\n    from langchain_community import document_loaders\\n\\n    # If not in interactive env, raise warning.\\n    if not is_interactive_env():\\n        warnings.warn(\\n            \"Importing document loaders from langchain is deprecated. Importing from \"\\n            \"langchain will no longer be supported as of langchain==0.2.0. \"\\n            \"Please import from langchain-community instead:\\\\n\\\\n\"\\n            f\"`from langchain_community.document_loaders import {name}`.\\\\n\\\\n\"\\n            \"To install langchain-community run `pip install -U langchain-community`.\",\\n            category=LangChainDeprecationWarning,\\n        )\\n\\n    if name in _old_to_new_name:\\n        warnings.warn(\\n            f\"Using legacy class name {name}, use {_old_to_new_name[name]} instead.\"\\n        )\\n        name = _old_to_new_name[name]\\n\\n    return getattr(document_loaders, name)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='__all__ = [\\n    \"AcreomLoader\",\\n    \"AsyncHtmlLoader\",\\n    \"AsyncChromiumLoader\",\\n    \"AZLyricsLoader\",\\n    \"AcreomLoader\",\\n    \"AirbyteCDKLoader\",\\n    \"AirbyteGongLoader\",\\n    \"AirbyteJSONLoader\",\\n    \"AirbyteHubspotLoader\",\\n    \"AirbyteSalesforceLoader\",\\n    \"AirbyteShopifyLoader\",\\n    \"AirbyteStripeLoader\",\\n    \"AirbyteTypeformLoader\",\\n    \"AirbyteZendeskSupportLoader\",\\n    \"AirtableLoader\",\\n    \"AmazonTextractPDFLoader\",\\n    \"ApifyDatasetLoader\",\\n    \"ArcGISLoader\",\\n    \"ArxivLoader\",\\n    \"AssemblyAIAudioTranscriptLoader\",\\n    \"AsyncHtmlLoader\",\\n    \"AzureAIDataLoader\",\\n    \"AzureBlobStorageContainerLoader\",\\n    \"AzureBlobStorageFileLoader\",\\n    \"BSHTMLLoader\",\\n    \"BibtexLoader\",\\n    \"BigQueryLoader\",\\n    \"BiliBiliLoader\",\\n    \"BlackboardLoader\",\\n    \"Blob\",\\n    \"BlobLoader\",\\n    \"BlockchainDocumentLoader\",\\n    \"BraveSearchLoader\",\\n    \"BrowserlessLoader\",\\n    \"CSVLoader\",\\n    \"ChatGPTLoader\",\\n    \"CoNLLULoader\",\\n    \"CollegeConfidentialLoader\",\\n    \"ConcurrentLoader\",\\n    \"ConfluenceLoader\",\\n    \"CouchbaseLoader\",\\n    \"CubeSemanticLoader\",\\n    \"DataFrameLoader\",\\n    \"DatadogLogsLoader\",\\n    \"DiffbotLoader\",\\n    \"DirectoryLoader\",\\n    \"DiscordChatLoader\",\\n    \"DocugamiLoader\",\\n    \"DocusaurusLoader\",\\n    \"Docx2txtLoader\",\\n    \"DropboxLoader\",\\n    \"DuckDBLoader\",\\n    \"EtherscanLoader\",\\n    \"EverNoteLoader\",\\n    \"FacebookChatLoader\",\\n    \"FaunaLoader\",\\n    \"FigmaFileLoader\",\\n    \"FileSystemBlobLoader\",\\n    \"GCSDirectoryLoader\",\\n    \"GCSFileLoader\",\\n    \"GeoDataFrameLoader\",\\n    \"GitHubIssuesLoader\",\\n    \"GitLoader\",\\n    \"GitbookLoader\",\\n    \"GoogleApiClient\",\\n    \"GoogleApiYoutubeLoader\",\\n    \"GoogleSpeechToTextLoader\",\\n    \"GoogleDriveLoader\",\\n    \"GutenbergLoader\",\\n    \"HNLoader\",\\n    \"HuggingFaceDatasetLoader\",\\n    \"IFixitLoader\",\\n    \"IMSDbLoader\",\\n    \"ImageCaptionLoader\",\\n    \"IuguLoader\",\\n    \"JSONLoader\",\\n    \"JoplinLoader\",\\n    \"LarkSuiteDocLoader\",\\n    \"LakeFSLoader\",\\n    \"MHTMLLoader\",\\n    \"MWDumpLoader\",\\n    \"MastodonTootsLoader\",\\n    \"MathpixPDFLoader\",\\n    \"MaxComputeLoader\",' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"MergedDataLoader\",\\n    \"ModernTreasuryLoader\",\\n    \"MongodbLoader\",\\n    \"NewsURLLoader\",\\n    \"NotebookLoader\",\\n    \"NotionDBLoader\",\\n    \"NotionDirectoryLoader\",\\n    \"OBSDirectoryLoader\",\\n    \"OBSFileLoader\",\\n    \"ObsidianLoader\",\\n    \"OneDriveFileLoader\",\\n    \"OneDriveLoader\",\\n    \"OnlinePDFLoader\",\\n    \"OpenCityDataLoader\",\\n    \"OutlookMessageLoader\",\\n    \"PDFMinerLoader\",\\n    \"PDFMinerPDFasHTMLLoader\",\\n    \"PDFPlumberLoader\",\\n    \"PagedPDFSplitter\",\\n    \"PlaywrightURLLoader\",\\n    \"PolarsDataFrameLoader\",\\n    \"PsychicLoader\",\\n    \"PubMedLoader\",\\n    \"PyMuPDFLoader\",\\n    \"PyPDFDirectoryLoader\",\\n    \"PyPDFLoader\",\\n    \"PyPDFium2Loader\",\\n    \"PySparkDataFrameLoader\",\\n    \"PythonLoader\",\\n    \"RSSFeedLoader\",\\n    \"ReadTheDocsLoader\",\\n    \"RecursiveUrlLoader\",\\n    \"RedditPostsLoader\",\\n    \"RoamLoader\",\\n    \"RocksetLoader\",\\n    \"S3DirectoryLoader\",\\n    \"S3FileLoader\",\\n    \"SRTLoader\",\\n    \"SeleniumURLLoader\",\\n    \"SharePointLoader\",\\n    \"SitemapLoader\",\\n    \"SlackDirectoryLoader\",\\n    \"SnowflakeLoader\",\\n    \"SpreedlyLoader\",\\n    \"StripeLoader\",\\n    \"TelegramChatApiLoader\",\\n    \"TelegramChatFileLoader\",\\n    \"TelegramChatLoader\",\\n    \"TensorflowDatasetLoader\",\\n    \"TencentCOSDirectoryLoader\",\\n    \"TencentCOSFileLoader\",\\n    \"TextLoader\",\\n    \"ToMarkdownLoader\",\\n    \"TomlLoader\",\\n    \"TrelloLoader\",\\n    \"TwitterTweetLoader\",\\n    \"UnstructuredAPIFileIOLoader\",\\n    \"UnstructuredAPIFileLoader\",\\n    \"UnstructuredCSVLoader\",\\n    \"UnstructuredEPubLoader\",\\n    \"UnstructuredEmailLoader\",\\n    \"UnstructuredExcelLoader\",\\n    \"UnstructuredFileIOLoader\",\\n    \"UnstructuredFileLoader\",\\n    \"UnstructuredHTMLLoader\",\\n    \"UnstructuredImageLoader\",\\n    \"UnstructuredMarkdownLoader\",\\n    \"UnstructuredODTLoader\",\\n    \"UnstructuredOrgModeLoader\",\\n    \"UnstructuredPDFLoader\",\\n    \"UnstructuredPowerPointLoader\",\\n    \"UnstructuredRSTLoader\",\\n    \"UnstructuredRTFLoader\",\\n    \"UnstructuredTSVLoader\",\\n    \"UnstructuredURLLoader\",\\n    \"UnstructuredWordDocumentLoader\",\\n    \"UnstructuredXMLLoader\",\\n    \"WeatherDataLoader\",\\n    \"WebBaseLoader\",' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"WhatsAppChatLoader\",\\n    \"WikipediaLoader\",\\n    \"XorbitsLoader\",\\n    \"YoutubeAudioLoader\",\\n    \"YoutubeLoader\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.blob_loaders.file_system import (\\n    FileSystemBlobLoader,\\n)\\n\\n__all__ = [\"FileSystemBlobLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\blob_loaders\\\\file_system.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.blob_loaders.schema import (\\n    Blob,\\n    BlobLoader,\\n    PathLike,\\n)\\n\\n__all__ = [\"PathLike\", \"Blob\", \"BlobLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\blob_loaders\\\\schema.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.blob_loaders.youtube_audio import (\\n    YoutubeAudioLoader,\\n)\\n\\n__all__ = [\"YoutubeAudioLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\blob_loaders\\\\youtube_audio.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.blob_loaders.file_system import (\\n    FileSystemBlobLoader,\\n)\\nfrom langchain_community.document_loaders.blob_loaders.schema import Blob, BlobLoader\\nfrom langchain_community.document_loaders.blob_loaders.youtube_audio import (\\n    YoutubeAudioLoader,\\n)\\n\\n__all__ = [\"BlobLoader\", \"Blob\", \"FileSystemBlobLoader\", \"YoutubeAudioLoader\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\blob_loaders\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.parsers.audio import (\\n    OpenAIWhisperParser,\\n    OpenAIWhisperParserLocal,\\n    YandexSTTParser,\\n)\\n\\n__all__ = [\"OpenAIWhisperParser\", \"OpenAIWhisperParserLocal\", \"YandexSTTParser\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\parsers\\\\audio.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.parsers.docai import (\\n    DocAIParser,\\n    DocAIParsingResults,\\n)\\n\\n__all__ = [\"DocAIParsingResults\", \"DocAIParser\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\parsers\\\\docai.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.parsers.generic import MimeTypeBasedParser\\n\\n__all__ = [\"MimeTypeBasedParser\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\parsers\\\\generic.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.parsers.grobid import (\\n    GrobidParser,\\n    ServerUnavailableException,\\n)\\n\\n__all__ = [\"GrobidParser\", \"ServerUnavailableException\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\parsers\\\\grobid.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.parsers.msword import MsWordParser\\n\\n__all__ = [\"MsWordParser\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\parsers\\\\msword.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.parsers.pdf import (\\n    AmazonTextractPDFParser,\\n    DocumentIntelligenceParser,\\n    PDFMinerParser,\\n    PDFPlumberParser,\\n    PyMuPDFParser,\\n    PyPDFium2Parser,\\n    PyPDFParser,\\n    extract_from_images_with_rapidocr,\\n)\\n\\n__all__ = [\\n    \"extract_from_images_with_rapidocr\",\\n    \"PyPDFParser\",\\n    \"PDFMinerParser\",\\n    \"PyMuPDFParser\",\\n    \"PyPDFium2Parser\",\\n    \"PDFPlumberParser\",\\n    \"AmazonTextractPDFParser\",\\n    \"DocumentIntelligenceParser\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\parsers\\\\pdf.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.parsers.registry import (\\n    get_parser,\\n)\\n\\n__all__ = [\"get_parser\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\parsers\\\\registry.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.parsers.txt import TextParser\\n\\n__all__ = [\"TextParser\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\parsers\\\\txt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.parsers.audio import OpenAIWhisperParser\\nfrom langchain_community.document_loaders.parsers.docai import DocAIParser\\nfrom langchain_community.document_loaders.parsers.grobid import GrobidParser\\nfrom langchain_community.document_loaders.parsers.html import BS4HTMLParser\\nfrom langchain_community.document_loaders.parsers.language import LanguageParser\\nfrom langchain_community.document_loaders.parsers.pdf import (\\n    PDFMinerParser,\\n    PDFPlumberParser,\\n    PyMuPDFParser,\\n    PyPDFium2Parser,\\n    PyPDFParser,\\n)\\n\\n__all__ = [\\n    \"BS4HTMLParser\",\\n    \"DocAIParser\",\\n    \"GrobidParser\",\\n    \"LanguageParser\",\\n    \"OpenAIWhisperParser\",\\n    \"PDFMinerParser\",\\n    \"PDFPlumberParser\",\\n    \"PyMuPDFParser\",\\n    \"PyPDFium2Parser\",\\n    \"PyPDFParser\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\parsers\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.parsers.html.bs4 import BS4HTMLParser\\n\\n__all__ = [\"BS4HTMLParser\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\parsers\\\\html\\\\bs4.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.parsers.html.bs4 import BS4HTMLParser\\n\\n__all__ = [\"BS4HTMLParser\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\parsers\\\\html\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.parsers.language.cobol import CobolSegmenter\\n\\n__all__ = [\"CobolSegmenter\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\parsers\\\\language\\\\cobol.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.parsers.language.code_segmenter import (\\n    CodeSegmenter,\\n)\\n\\n__all__ = [\"CodeSegmenter\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\parsers\\\\language\\\\code_segmenter.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.parsers.language.javascript import (\\n    JavaScriptSegmenter,\\n)\\n\\n__all__ = [\"JavaScriptSegmenter\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\parsers\\\\language\\\\javascript.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.parsers.language.language_parser import (\\n    LanguageParser,\\n)\\n\\n__all__ = [\"LanguageParser\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\parsers\\\\language\\\\language_parser.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.parsers.language.python import PythonSegmenter\\n\\n__all__ = [\"PythonSegmenter\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\parsers\\\\language\\\\python.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_loaders.parsers.language.language_parser import (\\n    LanguageParser,\\n)\\n\\n__all__ = [\"LanguageParser\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\parsers\\\\language\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_transformers.beautiful_soup_transformer import (\\n    BeautifulSoupTransformer,\\n)\\n\\n__all__ = [\"BeautifulSoupTransformer\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_transformers\\\\beautiful_soup_transformer.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_transformers.doctran_text_extract import (\\n    DoctranPropertyExtractor,\\n)\\n\\n__all__ = [\"DoctranPropertyExtractor\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_transformers\\\\doctran_text_extract.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_transformers.doctran_text_qa import (\\n    DoctranQATransformer,\\n)\\n\\n__all__ = [\"DoctranQATransformer\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_transformers\\\\doctran_text_qa.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_transformers.doctran_text_translate import (\\n    DoctranTextTranslator,\\n)\\n\\n__all__ = [\"DoctranTextTranslator\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_transformers\\\\doctran_text_translate.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_transformers.embeddings_redundant_filter import (\\n    EmbeddingsClusteringFilter,\\n    EmbeddingsRedundantFilter,\\n    _DocumentWithState,\\n    _filter_similar_embeddings,\\n    _get_embeddings_from_stateful_docs,\\n    get_stateful_documents,\\n)\\n\\n__all__ = [\\n    \"EmbeddingsRedundantFilter\",\\n    \"EmbeddingsClusteringFilter\",\\n    \"_DocumentWithState\",\\n    \"get_stateful_documents\",\\n    \"_get_embeddings_from_stateful_docs\",\\n    \"_filter_similar_embeddings\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_transformers\\\\embeddings_redundant_filter.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_transformers.google_translate import (\\n    GoogleTranslateTransformer,\\n)\\n\\n__all__ = [\"GoogleTranslateTransformer\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_transformers\\\\google_translate.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_transformers.html2text import Html2TextTransformer\\n\\n__all__ = [\"Html2TextTransformer\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_transformers\\\\html2text.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_transformers.long_context_reorder import (\\n    LongContextReorder,\\n)\\n\\n__all__ = [\"LongContextReorder\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_transformers\\\\long_context_reorder.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_transformers.nuclia_text_transform import (\\n    NucliaTextTransformer,\\n)\\n\\n__all__ = [\"NucliaTextTransformer\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_transformers\\\\nuclia_text_transform.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.document_transformers.openai_functions import (\\n    OpenAIMetadataTagger,\\n    create_metadata_tagger,\\n)\\n\\n__all__ = [\"OpenAIMetadataTagger\", \"create_metadata_tagger\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_transformers\\\\openai_functions.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"**Document Transformers** are classes to transform Documents.\\n\\n**Document Transformers** usually used to transform a lot of Documents in a single run.\\n\\n**Class hierarchy:**\\n\\n.. code-block::\\n\\n    BaseDocumentTransformer --> <name>  # Examples: DoctranQATransformer, DoctranTextTranslator\\n\\n**Main helpers:**\\n\\n.. code-block::\\n\\n    Document\\n\"\"\"  # noqa: E501\\nimport warnings\\nfrom typing import Any\\n\\nfrom langchain_core._api import LangChainDeprecationWarning\\n\\nfrom langchain.utils.interactive_env import is_interactive_env\\n\\n\\ndef __getattr__(name: str) -> Any:\\n    from langchain_community import document_transformers\\n\\n    # If not in interactive env, raise warning.\\n    if not is_interactive_env():\\n        warnings.warn(\\n            \"Importing document transformers from langchain is deprecated. Importing \"\\n            \"from langchain will no longer be supported as of langchain==0.2.0. \"\\n            \"Please import from langchain-community instead:\\\\n\\\\n\"\\n            f\"`from langchain_community.document_transformers import {name}`.\\\\n\\\\n\"\\n            \"To install langchain-community run `pip install -U langchain-community`.\",\\n            category=LangChainDeprecationWarning,\\n        )\\n\\n    return getattr(document_transformers, name)\\n\\n\\n__all__ = [\\n    \"BeautifulSoupTransformer\",\\n    \"DoctranQATransformer\",\\n    \"DoctranTextTranslator\",\\n    \"DoctranPropertyExtractor\",\\n    \"EmbeddingsClusteringFilter\",\\n    \"EmbeddingsRedundantFilter\",\\n    \"GoogleTranslateTransformer\",\\n    \"get_stateful_documents\",\\n    \"LongContextReorder\",\\n    \"NucliaTextTransformer\",\\n    \"OpenAIMetadataTagger\",\\n    \"Html2TextTransformer\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\document_transformers\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.embeddings.aleph_alpha import (\\n    AlephAlphaAsymmetricSemanticEmbedding,\\n    AlephAlphaSymmetricSemanticEmbedding,\\n)\\n\\n__all__ = [\\n    \"AlephAlphaAsymmetricSemanticEmbedding\",\\n    \"AlephAlphaSymmetricSemanticEmbedding\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\embeddings\\\\aleph_alpha.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.embeddings.awa import AwaEmbeddings\\n\\n__all__ = [\"AwaEmbeddings\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\embeddings\\\\awa.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.embeddings.azure_openai import AzureOpenAIEmbeddings\\n\\n__all__ = [\"AzureOpenAIEmbeddings\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\embeddings\\\\azure_openai.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.embeddings.baidu_qianfan_endpoint import (\\n    QianfanEmbeddingsEndpoint,\\n)\\n\\n__all__ = [\"QianfanEmbeddingsEndpoint\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\embeddings\\\\baidu_qianfan_endpoint.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.embeddings import Embeddings\\n\\n# This is for backwards compatibility\\n__all__ = [\"Embeddings\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\embeddings\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.embeddings.bedrock import BedrockEmbeddings\\n\\n__all__ = [\"BedrockEmbeddings\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\embeddings\\\\bedrock.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.embeddings.bookend import (\\n    BookendEmbeddings,\\n)\\n\\n__all__ = [\"BookendEmbeddings\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\embeddings\\\\bookend.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Module contains code for a cache backed embedder.\\n\\nThe cache backed embedder is a wrapper around an embedder that caches\\nembeddings in a key-value store. The cache is used to avoid recomputing\\nembeddings for the same text.\\n\\nThe text is hashed and the hash is used as the key in the cache.\\n\"\"\"\\nfrom __future__ import annotations\\n\\nimport hashlib\\nimport json\\nimport uuid\\nfrom functools import partial\\nfrom typing import Callable, List, Sequence, Union, cast\\n\\nfrom langchain_core.embeddings import Embeddings\\nfrom langchain_core.stores import BaseStore, ByteStore\\n\\nfrom langchain.storage.encoder_backed import EncoderBackedStore\\n\\nNAMESPACE_UUID = uuid.UUID(int=1985)\\n\\n\\ndef _hash_string_to_uuid(input_string: str) -> uuid.UUID:\\n    \"\"\"Hash a string and returns the corresponding UUID.\"\"\"\\n    hash_value = hashlib.sha1(input_string.encode(\"utf-8\")).hexdigest()\\n    return uuid.uuid5(NAMESPACE_UUID, hash_value)\\n\\n\\ndef _key_encoder(key: str, namespace: str) -> str:\\n    \"\"\"Encode a key.\"\"\"\\n    return namespace + str(_hash_string_to_uuid(key))\\n\\n\\ndef _create_key_encoder(namespace: str) -> Callable[[str], str]:\\n    \"\"\"Create an encoder for a key.\"\"\"\\n    return partial(_key_encoder, namespace=namespace)\\n\\n\\ndef _value_serializer(value: Sequence[float]) -> bytes:\\n    \"\"\"Serialize a value.\"\"\"\\n    return json.dumps(value).encode()\\n\\n\\ndef _value_deserializer(serialized_value: bytes) -> List[float]:\\n    \"\"\"Deserialize a value.\"\"\"\\n    return cast(List[float], json.loads(serialized_value.decode()))\\n\\n\\nclass CacheBackedEmbeddings(Embeddings):\\n    \"\"\"Interface for caching results from embedding models.\\n\\n    The interface allows works with any store that implements\\n    the abstract store interface accepting keys of type str and values of list of\\n    floats.\\n\\n    If need be, the interface can be extended to accept other implementations\\n    of the value serializer and deserializer, as well as the key encoder.\\n\\n    Examples:\\n\\n        .. code-block: python' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\embeddings\\\\cache.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain.embeddings import CacheBackedEmbeddings\\n            from langchain.storage import LocalFileStore\\n            from langchain_community.embeddings import OpenAIEmbeddings\\n\\n            store = LocalFileStore(\\'./my_cache\\')\\n\\n            underlying_embedder = OpenAIEmbeddings()\\n            embedder = CacheBackedEmbeddings.from_bytes_store(\\n                underlying_embedder, store, namespace=underlying_embedder.model\\n            )\\n\\n            # Embedding is computed and cached\\n            embeddings = embedder.embed_documents([\"hello\", \"goodbye\"])\\n\\n            # Embeddings are retrieved from the cache, no computation is done\\n            embeddings = embedder.embed_documents([\"hello\", \"goodbye\"])\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        underlying_embeddings: Embeddings,\\n        document_embedding_store: BaseStore[str, List[float]],\\n    ) -> None:\\n        \"\"\"Initialize the embedder.\\n\\n        Args:\\n            underlying_embeddings: the embedder to use for computing embeddings.\\n            document_embedding_store: The store to use for caching document embeddings.\\n        \"\"\"\\n        super().__init__()\\n        self.document_embedding_store = document_embedding_store\\n        self.underlying_embeddings = underlying_embeddings\\n\\n    def embed_documents(self, texts: List[str]) -> List[List[float]]:\\n        \"\"\"Embed a list of texts.\\n\\n        The method first checks the cache for the embeddings.\\n        If the embeddings are not found, the method uses the underlying embedder\\n        to embed the documents and stores the results in the cache.\\n\\n        Args:\\n            texts: A list of texts to embed.\\n\\n        Returns:\\n            A list of embeddings for the given texts.\\n        \"\"\"\\n        vectors: List[Union[List[float], None]] = self.document_embedding_store.mget(\\n            texts\\n        )\\n        missing_indices: List[int] = [\\n            i for i, vector in enumerate(vectors) if vector is None\\n        ]\\n        missing_texts = [texts[i] for i in missing_indices]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\embeddings\\\\cache.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='if missing_texts:\\n            missing_vectors = self.underlying_embeddings.embed_documents(missing_texts)\\n            self.document_embedding_store.mset(\\n                list(zip(missing_texts, missing_vectors))\\n            )\\n            for index, updated_vector in zip(missing_indices, missing_vectors):\\n                vectors[index] = updated_vector\\n\\n        return cast(\\n            List[List[float]], vectors\\n        )  # Nones should have been resolved by now\\n\\n    def embed_query(self, text: str) -> List[float]:\\n        \"\"\"Embed query text.\\n\\n        This method does not support caching at the moment.\\n\\n        Support for caching queries is easily to implement, but might make\\n        sense to hold off to see the most common patterns.\\n\\n        If the cache has an eviction policy, we may need to be a bit more careful\\n        about sharing the cache between documents and queries. Generally,\\n        one is OK evicting query caches, but document caches should be kept.\\n\\n        Args:\\n            text: The text to embed.\\n\\n        Returns:\\n            The embedding for the given text.\\n        \"\"\"\\n        return self.underlying_embeddings.embed_query(text)\\n\\n    @classmethod\\n    def from_bytes_store(\\n        cls,\\n        underlying_embeddings: Embeddings,\\n        document_embedding_cache: ByteStore,\\n        *,\\n        namespace: str = \"\",\\n    ) -> CacheBackedEmbeddings:\\n        \"\"\"On-ramp that adds the necessary serialization and encoding to the store.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\embeddings\\\\cache.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Args:\\n            underlying_embeddings: The embedder to use for embedding.\\n            document_embedding_cache: The cache to use for storing document embeddings.\\n            *,\\n            namespace: The namespace to use for document cache.\\n                       This namespace is used to avoid collisions with other caches.\\n                       For example, set it to the name of the embedding model used.\\n        \"\"\"\\n        namespace = namespace\\n        key_encoder = _create_key_encoder(namespace)\\n        encoder_backed_store = EncoderBackedStore[str, List[float]](\\n            document_embedding_cache,\\n            key_encoder,\\n            _value_serializer,\\n            _value_deserializer,\\n        )\\n        return cls(underlying_embeddings, encoder_backed_store)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\embeddings\\\\cache.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.embeddings.clarifai import ClarifaiEmbeddings\\n\\n__all__ = [\"ClarifaiEmbeddings\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\embeddings\\\\clarifai.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.embeddings.cloudflare_workersai import (\\n    CloudflareWorkersAIEmbeddings,\\n)\\n\\n__all__ = [\"CloudflareWorkersAIEmbeddings\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\embeddings\\\\cloudflare_workersai.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.embeddings.cohere import CohereEmbeddings\\n\\n__all__ = [\"CohereEmbeddings\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\embeddings\\\\cohere.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.embeddings.dashscope import (\\n    DashScopeEmbeddings,\\n)\\n\\n__all__ = [\"DashScopeEmbeddings\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\embeddings\\\\dashscope.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.embeddings.databricks import DatabricksEmbeddings\\n\\n__all__ = [\"DatabricksEmbeddings\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\embeddings\\\\databricks.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.embeddings.deepinfra import (\\n    DeepInfraEmbeddings,\\n)\\n\\n__all__ = [\"DeepInfraEmbeddings\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\embeddings\\\\deepinfra.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.embeddings.edenai import EdenAiEmbeddings\\n\\n__all__ = [\"EdenAiEmbeddings\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\embeddings\\\\edenai.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.embeddings.elasticsearch import ElasticsearchEmbeddings\\n\\n__all__ = [\"ElasticsearchEmbeddings\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\embeddings\\\\elasticsearch.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.embeddings.embaas import (\\n    EmbaasEmbeddings,\\n)\\n\\n__all__ = [\\n    \"EmbaasEmbeddings\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\embeddings\\\\embaas.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.embeddings.ernie import ErnieEmbeddings\\n\\n__all__ = [\"ErnieEmbeddings\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\embeddings\\\\ernie.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.embeddings.fake import (\\n    DeterministicFakeEmbedding,\\n    FakeEmbeddings,\\n)\\n\\n__all__ = [\"FakeEmbeddings\", \"DeterministicFakeEmbedding\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\embeddings\\\\fake.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\\n\\n__all__ = [\"FastEmbedEmbeddings\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\embeddings\\\\fastembed.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.embeddings.google_palm import (\\n    GooglePalmEmbeddings,\\n)\\n\\n__all__ = [\"GooglePalmEmbeddings\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\embeddings\\\\google_palm.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.embeddings.gpt4all import GPT4AllEmbeddings\\n\\n__all__ = [\"GPT4AllEmbeddings\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\embeddings\\\\gpt4all.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.embeddings.gradient_ai import GradientEmbeddings\\n\\n__all__ = [\"GradientEmbeddings\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\embeddings\\\\gradient_ai.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.embeddings.huggingface import (\\n    HuggingFaceBgeEmbeddings,\\n    HuggingFaceEmbeddings,\\n    HuggingFaceInferenceAPIEmbeddings,\\n    HuggingFaceInstructEmbeddings,\\n)\\n\\n__all__ = [\\n    \"HuggingFaceEmbeddings\",\\n    \"HuggingFaceInstructEmbeddings\",\\n    \"HuggingFaceBgeEmbeddings\",\\n    \"HuggingFaceInferenceAPIEmbeddings\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\embeddings\\\\huggingface.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.embeddings.huggingface_hub import (\\n    HuggingFaceHubEmbeddings,\\n)\\n\\n__all__ = [\"HuggingFaceHubEmbeddings\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\embeddings\\\\huggingface_hub.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.embeddings.infinity import (\\n    InfinityEmbeddings,\\n    TinyAsyncOpenAIInfinityEmbeddingClient,\\n)\\n\\n__all__ = [\"InfinityEmbeddings\", \"TinyAsyncOpenAIInfinityEmbeddingClient\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\embeddings\\\\infinity.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.embeddings.javelin_ai_gateway import (\\n    JavelinAIGatewayEmbeddings,\\n)\\n\\n__all__ = [\"JavelinAIGatewayEmbeddings\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\embeddings\\\\javelin_ai_gateway.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.embeddings.jina import JinaEmbeddings\\n\\n__all__ = [\"JinaEmbeddings\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\embeddings\\\\jina.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.embeddings.johnsnowlabs import JohnSnowLabsEmbeddings\\n\\n__all__ = [\"JohnSnowLabsEmbeddings\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\embeddings\\\\johnsnowlabs.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.embeddings.llamacpp import LlamaCppEmbeddings\\n\\n__all__ = [\"LlamaCppEmbeddings\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\embeddings\\\\llamacpp.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.embeddings.llm_rails import LLMRailsEmbeddings\\n\\n__all__ = [\"LLMRailsEmbeddings\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\embeddings\\\\llm_rails.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.embeddings.localai import (\\n    LocalAIEmbeddings,\\n)\\n\\n__all__ = [\\n    \"LocalAIEmbeddings\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\embeddings\\\\localai.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.embeddings.minimax import (\\n    MiniMaxEmbeddings,\\n)\\n\\n__all__ = [\"MiniMaxEmbeddings\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\embeddings\\\\minimax.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.embeddings.mlflow import MlflowEmbeddings\\n\\n__all__ = [\"MlflowEmbeddings\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\embeddings\\\\mlflow.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.embeddings.mlflow_gateway import (\\n    MlflowAIGatewayEmbeddings,\\n)\\n\\n__all__ = [\"MlflowAIGatewayEmbeddings\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\embeddings\\\\mlflow_gateway.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.embeddings.modelscope_hub import ModelScopeEmbeddings\\n\\n__all__ = [\"ModelScopeEmbeddings\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\embeddings\\\\modelscope_hub.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.embeddings.mosaicml import MosaicMLInstructorEmbeddings\\n\\n__all__ = [\"MosaicMLInstructorEmbeddings\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\embeddings\\\\mosaicml.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.embeddings.nlpcloud import NLPCloudEmbeddings\\n\\n__all__ = [\"NLPCloudEmbeddings\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\embeddings\\\\nlpcloud.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.embeddings.octoai_embeddings import (\\n    OctoAIEmbeddings,\\n)\\n\\n__all__ = [\"OctoAIEmbeddings\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\embeddings\\\\octoai_embeddings.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.embeddings.ollama import OllamaEmbeddings\\n\\n__all__ = [\"OllamaEmbeddings\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\embeddings\\\\ollama.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.embeddings.openai import (\\n    OpenAIEmbeddings,\\n)\\n\\n__all__ = [\\n    \"OpenAIEmbeddings\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\embeddings\\\\openai.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.embeddings.sagemaker_endpoint import (\\n    EmbeddingsContentHandler,\\n    SagemakerEndpointEmbeddings,\\n)\\n\\n__all__ = [\"EmbeddingsContentHandler\", \"SagemakerEndpointEmbeddings\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\embeddings\\\\sagemaker_endpoint.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.embeddings.self_hosted import (\\n    SelfHostedEmbeddings,\\n)\\n\\n__all__ = [\"SelfHostedEmbeddings\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\embeddings\\\\self_hosted.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.embeddings.self_hosted_hugging_face import (\\n    SelfHostedHuggingFaceEmbeddings,\\n    SelfHostedHuggingFaceInstructEmbeddings,\\n)\\n\\n__all__ = [\\n    \"SelfHostedHuggingFaceEmbeddings\",\\n    \"SelfHostedHuggingFaceInstructEmbeddings\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\embeddings\\\\self_hosted_hugging_face.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.embeddings.sentence_transformer import (\\n    SentenceTransformerEmbeddings,\\n)\\n\\n__all__ = [\"SentenceTransformerEmbeddings\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\embeddings\\\\sentence_transformer.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.embeddings.spacy_embeddings import SpacyEmbeddings\\n\\n__all__ = [\"SpacyEmbeddings\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\embeddings\\\\spacy_embeddings.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.embeddings.tensorflow_hub import (\\n    TensorflowHubEmbeddings,\\n)\\n\\n__all__ = [\"TensorflowHubEmbeddings\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\embeddings\\\\tensorflow_hub.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.embeddings.vertexai import VertexAIEmbeddings\\n\\n__all__ = [\"VertexAIEmbeddings\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\embeddings\\\\vertexai.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.embeddings.voyageai import (\\n    VoyageEmbeddings,\\n)\\n\\n__all__ = [\\n    \"VoyageEmbeddings\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\embeddings\\\\voyageai.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.embeddings.xinference import XinferenceEmbeddings\\n\\n__all__ = [\"XinferenceEmbeddings\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\embeddings\\\\xinference.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"**Embedding models**  are wrappers around embedding models\\nfrom different APIs and services.\\n\\n**Embedding models** can be LLMs or not.\\n\\n**Class hierarchy:**\\n\\n.. code-block::\\n\\n    Embeddings --> <name>Embeddings  # Examples: OpenAIEmbeddings, HuggingFaceEmbeddings\\n\"\"\"\\n\\n\\nimport logging\\nimport warnings\\nfrom typing import Any\\n\\nfrom langchain_core._api import LangChainDeprecationWarning\\n\\nfrom langchain.embeddings.cache import CacheBackedEmbeddings\\nfrom langchain.utils.interactive_env import is_interactive_env\\n\\n\\ndef __getattr__(name: str) -> Any:\\n    from langchain_community import embeddings\\n\\n    # If not in interactive env, raise warning.\\n    if not is_interactive_env():\\n        warnings.warn(\\n            \"Importing embeddings from langchain is deprecated. Importing from \"\\n            \"langchain will no longer be supported as of langchain==0.2.0. \"\\n            \"Please import from langchain-community instead:\\\\n\\\\n\"\\n            f\"`from langchain_community.embeddings import {name}`.\\\\n\\\\n\"\\n            \"To install langchain-community run `pip install -U langchain-community`.\",\\n            category=LangChainDeprecationWarning,\\n        )\\n\\n    return getattr(embeddings, name)\\n\\n\\nlogger = logging.getLogger(__name__)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\embeddings\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='__all__ = [\\n    \"OpenAIEmbeddings\",\\n    \"AzureOpenAIEmbeddings\",\\n    \"CacheBackedEmbeddings\",\\n    \"ClarifaiEmbeddings\",\\n    \"CohereEmbeddings\",\\n    \"DatabricksEmbeddings\",\\n    \"ElasticsearchEmbeddings\",\\n    \"FastEmbedEmbeddings\",\\n    \"HuggingFaceEmbeddings\",\\n    \"HuggingFaceInferenceAPIEmbeddings\",\\n    \"InfinityEmbeddings\",\\n    \"GradientEmbeddings\",\\n    \"JinaEmbeddings\",\\n    \"LlamaCppEmbeddings\",\\n    \"HuggingFaceHubEmbeddings\",\\n    \"MlflowEmbeddings\",\\n    \"MlflowAIGatewayEmbeddings\",\\n    \"ModelScopeEmbeddings\",\\n    \"TensorflowHubEmbeddings\",\\n    \"SagemakerEndpointEmbeddings\",\\n    \"HuggingFaceInstructEmbeddings\",\\n    \"MosaicMLInstructorEmbeddings\",\\n    \"SelfHostedEmbeddings\",\\n    \"SelfHostedHuggingFaceEmbeddings\",\\n    \"SelfHostedHuggingFaceInstructEmbeddings\",\\n    \"FakeEmbeddings\",\\n    \"DeterministicFakeEmbedding\",\\n    \"AlephAlphaAsymmetricSemanticEmbedding\",\\n    \"AlephAlphaSymmetricSemanticEmbedding\",\\n    \"SentenceTransformerEmbeddings\",\\n    \"GooglePalmEmbeddings\",\\n    \"MiniMaxEmbeddings\",\\n    \"VertexAIEmbeddings\",\\n    \"BedrockEmbeddings\",\\n    \"DeepInfraEmbeddings\",\\n    \"EdenAiEmbeddings\",\\n    \"DashScopeEmbeddings\",\\n    \"EmbaasEmbeddings\",\\n    \"OctoAIEmbeddings\",\\n    \"SpacyEmbeddings\",\\n    \"NLPCloudEmbeddings\",\\n    \"GPT4AllEmbeddings\",\\n    \"XinferenceEmbeddings\",\\n    \"LocalAIEmbeddings\",\\n    \"AwaEmbeddings\",\\n    \"HuggingFaceBgeEmbeddings\",\\n    \"ErnieEmbeddings\",\\n    \"JavelinAIGatewayEmbeddings\",\\n    \"OllamaEmbeddings\",\\n    \"QianfanEmbeddingsEndpoint\",\\n    \"JohnSnowLabsEmbeddings\",\\n    \"VoyageEmbeddings\",\\n    \"BookendEmbeddings\",\\n]\\n\\n\\n# TODO: this is in here to maintain backwards compatibility\\nclass HypotheticalDocumentEmbedder:\\n    def __init__(self, *args: Any, **kwargs: Any):\\n        logger.warning(\\n            \"Using a deprecated class. Please use \"\\n            \"`from langchain.chains import HypotheticalDocumentEmbedder` instead\"\\n        )\\n        from langchain.chains.hyde.base import HypotheticalDocumentEmbedder as H\\n\\n        return H(*args, **kwargs)  # type: ignore' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\embeddings\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@classmethod\\n    def from_llm(cls, *args: Any, **kwargs: Any) -> Any:\\n        logger.warning(\\n            \"Using a deprecated class. Please use \"\\n            \"`from langchain.chains import HypotheticalDocumentEmbedder` instead\"\\n        )\\n        from langchain.chains.hyde.base import HypotheticalDocumentEmbedder as H\\n\\n        return H.from_llm(*args, **kwargs)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\embeddings\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Loading datasets and evaluators.\"\"\"\\nfrom typing import Any, Dict, List, Optional, Sequence, Type, Union\\n\\nfrom langchain_community.chat_models.openai import ChatOpenAI\\nfrom langchain_core.language_models import BaseLanguageModel\\n\\nfrom langchain.chains.base import Chain\\nfrom langchain.evaluation.agents.trajectory_eval_chain import TrajectoryEvalChain\\nfrom langchain.evaluation.comparison import PairwiseStringEvalChain\\nfrom langchain.evaluation.comparison.eval_chain import LabeledPairwiseStringEvalChain\\nfrom langchain.evaluation.criteria.eval_chain import (\\n    CriteriaEvalChain,\\n    LabeledCriteriaEvalChain,\\n)\\nfrom langchain.evaluation.embedding_distance.base import (\\n    EmbeddingDistanceEvalChain,\\n    PairwiseEmbeddingDistanceEvalChain,\\n)\\nfrom langchain.evaluation.exact_match.base import ExactMatchStringEvaluator\\nfrom langchain.evaluation.parsing.base import (\\n    JsonEqualityEvaluator,\\n    JsonValidityEvaluator,\\n)\\nfrom langchain.evaluation.parsing.json_distance import JsonEditDistanceEvaluator\\nfrom langchain.evaluation.parsing.json_schema import JsonSchemaEvaluator\\nfrom langchain.evaluation.qa import ContextQAEvalChain, CotQAEvalChain, QAEvalChain\\nfrom langchain.evaluation.regex_match.base import RegexMatchStringEvaluator\\nfrom langchain.evaluation.schema import EvaluatorType, LLMEvalChain, StringEvaluator\\nfrom langchain.evaluation.scoring.eval_chain import (\\n    LabeledScoreStringEvalChain,\\n    ScoreStringEvalChain,\\n)\\nfrom langchain.evaluation.string_distance.base import (\\n    PairwiseStringDistanceEvalChain,\\n    StringDistanceEvalChain,\\n)\\n\\n\\ndef load_dataset(uri: str) -> List[Dict]:\\n    \"\"\"Load a dataset from the `LangChainDatasets on HuggingFace <https://huggingface.co/LangChainDatasets>`_.\\n\\n    Args:\\n        uri: The uri of the dataset to load.\\n\\n    Returns:\\n        A list of dictionaries, each representing a row in the dataset.\\n\\n    **Prerequisites**\\n\\n    .. code-block:: shell\\n\\n        pip install datasets\\n\\n    Examples\\n    --------\\n    .. code-block:: python' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\loading.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain.evaluation import load_dataset\\n        ds = load_dataset(\"llm-math\")\\n    \"\"\"  # noqa: E501\\n    try:\\n        from datasets import load_dataset\\n    except ImportError:\\n        raise ImportError(\\n            \"load_dataset requires the `datasets` package.\"\\n            \" Please install with `pip install datasets`\"\\n        )\\n\\n    dataset = load_dataset(f\"LangChainDatasets/{uri}\")\\n    return [d for d in dataset[\"train\"]]\\n\\n\\n_EVALUATOR_MAP: Dict[\\n    EvaluatorType, Union[Type[LLMEvalChain], Type[Chain], Type[StringEvaluator]]\\n] = {\\n    EvaluatorType.QA: QAEvalChain,\\n    EvaluatorType.COT_QA: CotQAEvalChain,\\n    EvaluatorType.CONTEXT_QA: ContextQAEvalChain,\\n    EvaluatorType.PAIRWISE_STRING: PairwiseStringEvalChain,\\n    EvaluatorType.SCORE_STRING: ScoreStringEvalChain,\\n    EvaluatorType.LABELED_PAIRWISE_STRING: LabeledPairwiseStringEvalChain,\\n    EvaluatorType.LABELED_SCORE_STRING: LabeledScoreStringEvalChain,\\n    EvaluatorType.AGENT_TRAJECTORY: TrajectoryEvalChain,\\n    EvaluatorType.CRITERIA: CriteriaEvalChain,\\n    EvaluatorType.LABELED_CRITERIA: LabeledCriteriaEvalChain,\\n    EvaluatorType.STRING_DISTANCE: StringDistanceEvalChain,\\n    EvaluatorType.PAIRWISE_STRING_DISTANCE: PairwiseStringDistanceEvalChain,\\n    EvaluatorType.EMBEDDING_DISTANCE: EmbeddingDistanceEvalChain,\\n    EvaluatorType.PAIRWISE_EMBEDDING_DISTANCE: PairwiseEmbeddingDistanceEvalChain,\\n    EvaluatorType.JSON_VALIDITY: JsonValidityEvaluator,\\n    EvaluatorType.JSON_EQUALITY: JsonEqualityEvaluator,\\n    EvaluatorType.JSON_EDIT_DISTANCE: JsonEditDistanceEvaluator,\\n    EvaluatorType.JSON_SCHEMA_VALIDATION: JsonSchemaEvaluator,\\n    EvaluatorType.REGEX_MATCH: RegexMatchStringEvaluator,\\n    EvaluatorType.EXACT_MATCH: ExactMatchStringEvaluator,\\n}\\n\\n\\ndef load_evaluator(\\n    evaluator: EvaluatorType,\\n    *,\\n    llm: Optional[BaseLanguageModel] = None,\\n    **kwargs: Any,\\n) -> Union[Chain, StringEvaluator]:\\n    \"\"\"Load the requested evaluation chain specified by a string.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\loading.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Parameters\\n    ----------\\n    evaluator : EvaluatorType\\n        The type of evaluator to load.\\n    llm : BaseLanguageModel, optional\\n        The language model to use for evaluation, by default None\\n    **kwargs : Any\\n        Additional keyword arguments to pass to the evaluator.\\n\\n    Returns\\n    -------\\n    Chain\\n        The loaded evaluation chain.\\n\\n    Examples\\n    --------\\n    >>> from langchain.evaluation import load_evaluator, EvaluatorType\\n    >>> evaluator = load_evaluator(EvaluatorType.QA)\\n    \"\"\"\\n    if evaluator not in _EVALUATOR_MAP:\\n        raise ValueError(\\n            f\"Unknown evaluator type: {evaluator}\"\\n            f\"\\\\nValid types are: {list(_EVALUATOR_MAP.keys())}\"\\n        )\\n    evaluator_cls = _EVALUATOR_MAP[evaluator]\\n    if issubclass(evaluator_cls, LLMEvalChain):\\n        try:\\n            llm = llm or ChatOpenAI(\\n                model=\"gpt-4\", model_kwargs={\"seed\": 42}, temperature=0\\n            )\\n        except Exception as e:\\n            raise ValueError(\\n                f\"Evaluation with the {evaluator_cls} requires a \"\\n                \"language model to function.\"\\n                \" Failed to create the default \\'gpt-4\\' model.\"\\n                \" Please manually provide an evaluation LLM\"\\n                \" or check your openai credentials.\"\\n            ) from e\\n        return evaluator_cls.from_llm(llm=llm, **kwargs)\\n    else:\\n        return evaluator_cls(**kwargs)\\n\\n\\ndef load_evaluators(\\n    evaluators: Sequence[EvaluatorType],\\n    *,\\n    llm: Optional[BaseLanguageModel] = None,\\n    config: Optional[dict] = None,\\n    **kwargs: Any,\\n) -> List[Union[Chain, StringEvaluator]]:\\n    \"\"\"Load evaluators specified by a list of evaluator types.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\loading.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Parameters\\n    ----------\\n    evaluators : Sequence[EvaluatorType]\\n        The list of evaluator types to load.\\n    llm : BaseLanguageModel, optional\\n        The language model to use for evaluation, if none is provided, a default\\n        ChatOpenAI gpt-4 model will be used.\\n    config : dict, optional\\n        A dictionary mapping evaluator types to additional keyword arguments,\\n        by default None\\n    **kwargs : Any\\n        Additional keyword arguments to pass to all evaluators.\\n\\n    Returns\\n    -------\\n    List[Chain]\\n        The loaded evaluators.\\n\\n    Examples\\n    --------\\n    >>> from langchain.evaluation import load_evaluators, EvaluatorType\\n    >>> evaluators = [EvaluatorType.QA, EvaluatorType.CRITERIA]\\n    >>> loaded_evaluators = load_evaluators(evaluators, criteria=\"helpfulness\")\\n    \"\"\"\\n    loaded = []\\n    for evaluator in evaluators:\\n        _kwargs = config.get(evaluator, {}) if config else {}\\n        loaded.append(load_evaluator(evaluator, llm=llm, **{**kwargs, **_kwargs}))\\n    return loaded' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\loading.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Interfaces to be implemented by general evaluators.\"\"\"\\nfrom __future__ import annotations\\n\\nimport logging\\nfrom abc import ABC, abstractmethod\\nfrom enum import Enum\\nfrom typing import Any, Optional, Sequence, Tuple, Union\\nfrom warnings import warn\\n\\nfrom langchain_core.agents import AgentAction\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.runnables.config import run_in_executor\\n\\nfrom langchain.chains.base import Chain\\n\\nlogger = logging.getLogger(__name__)\\n\\n\\nclass EvaluatorType(str, Enum):\\n    \"\"\"The types of the evaluators.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\schema.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='QA = \"qa\"\\n    \"\"\"Question answering evaluator, which grades answers to questions\\n    directly using an LLM.\"\"\"\\n    COT_QA = \"cot_qa\"\\n    \"\"\"Chain of thought question answering evaluator, which grades\\n    answers to questions using\\n    chain of thought \\'reasoning\\'.\"\"\"\\n    CONTEXT_QA = \"context_qa\"\\n    \"\"\"Question answering evaluator that incorporates \\'context\\' in the response.\"\"\"\\n    PAIRWISE_STRING = \"pairwise_string\"\\n    \"\"\"The pairwise string evaluator, which predicts the preferred prediction from\\n    between two models.\"\"\"\\n    SCORE_STRING = \"score_string\"\\n    \"\"\"The scored string evaluator, which gives a score between 1 and 10 \\n    to a prediction.\"\"\"\\n    LABELED_PAIRWISE_STRING = \"labeled_pairwise_string\"\\n    \"\"\"The labeled pairwise string evaluator, which predicts the preferred prediction\\n    from between two models based on a ground truth reference label.\"\"\"\\n    LABELED_SCORE_STRING = \"labeled_score_string\"\\n    \"\"\"The labeled scored string evaluator, which gives a score between 1 and 10\\n    to a prediction based on a ground truth reference label.\"\"\"\\n    AGENT_TRAJECTORY = \"trajectory\"\\n    \"\"\"The agent trajectory evaluator, which grades the agent\\'s intermediate steps.\"\"\"\\n    CRITERIA = \"criteria\"\\n    \"\"\"The criteria evaluator, which evaluates a model based on a\\n    custom set of criteria without any reference labels.\"\"\"\\n    LABELED_CRITERIA = \"labeled_criteria\"\\n    \"\"\"The labeled criteria evaluator, which evaluates a model based on a\\n    custom set of criteria, with a reference label.\"\"\"\\n    STRING_DISTANCE = \"string_distance\"\\n    \"\"\"Compare predictions to a reference answer using string edit distances.\"\"\"\\n    EXACT_MATCH = \"exact_match\"\\n    \"\"\"Compare predictions to a reference answer using exact matching.\"\"\"\\n    REGEX_MATCH = \"regex_match\"\\n    \"\"\"Compare predictions to a reference answer using regular expressions.\"\"\"\\n    PAIRWISE_STRING_DISTANCE = \"pairwise_string_distance\"\\n    \"\"\"Compare predictions based on string edit distances.\"\"\"\\n    EMBEDDING_DISTANCE = \"embedding_distance\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\schema.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Compare a prediction to a reference label using embedding distance.\"\"\"\\n    PAIRWISE_EMBEDDING_DISTANCE = \"pairwise_embedding_distance\"\\n    \"\"\"Compare two predictions using embedding distance.\"\"\"\\n    JSON_VALIDITY = \"json_validity\"\\n    \"\"\"Check if a prediction is valid JSON.\"\"\"\\n    JSON_EQUALITY = \"json_equality\"\\n    \"\"\"Check if a prediction is equal to a reference JSON.\"\"\"\\n    JSON_EDIT_DISTANCE = \"json_edit_distance\"\\n    \"\"\"Compute the edit distance between two JSON strings after canonicalization.\"\"\"\\n    JSON_SCHEMA_VALIDATION = \"json_schema_validation\"\\n    \"\"\"Check if a prediction is valid JSON according to a JSON schema.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\schema.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='class LLMEvalChain(Chain):\\n    \"\"\"A base class for evaluators that use an LLM.\"\"\"\\n\\n    @classmethod\\n    @abstractmethod\\n    def from_llm(cls, llm: BaseLanguageModel, **kwargs: Any) -> LLMEvalChain:\\n        \"\"\"Create a new evaluator from an LLM.\"\"\"\\n\\n\\nclass _EvalArgsMixin:\\n    \"\"\"Mixin for checking evaluation arguments.\"\"\"\\n\\n    @property\\n    def requires_reference(self) -> bool:\\n        \"\"\"Whether this evaluator requires a reference label.\"\"\"\\n        return False\\n\\n    @property\\n    def requires_input(self) -> bool:\\n        \"\"\"Whether this evaluator requires an input string.\"\"\"\\n        return False\\n\\n    @property\\n    def _skip_input_warning(self) -> str:\\n        \"\"\"Warning to show when input is ignored.\"\"\"\\n        return f\"Ignoring input in {self.__class__.__name__}, as it is not expected.\"\\n\\n    @property\\n    def _skip_reference_warning(self) -> str:\\n        \"\"\"Warning to show when reference is ignored.\"\"\"\\n        return (\\n            f\"Ignoring reference in {self.__class__.__name__}, as it is not expected.\"\\n        )\\n\\n    def _check_evaluation_args(\\n        self,\\n        reference: Optional[str] = None,\\n        input: Optional[str] = None,\\n    ) -> None:\\n        \"\"\"Check if the evaluation arguments are valid.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\schema.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Args:\\n            reference (Optional[str], optional): The reference label.\\n            input (Optional[str], optional): The input string.\\n        Raises:\\n            ValueError: If the evaluator requires an input string but none is provided,\\n                or if the evaluator requires a reference label but none is provided.\\n        \"\"\"\\n        if self.requires_input and input is None:\\n            raise ValueError(f\"{self.__class__.__name__} requires an input string.\")\\n        elif input is not None and not self.requires_input:\\n            warn(self._skip_input_warning)\\n        if self.requires_reference and reference is None:\\n            raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\\n        elif reference is not None and not self.requires_reference:\\n            warn(self._skip_reference_warning)\\n\\n\\nclass StringEvaluator(_EvalArgsMixin, ABC):\\n    \"\"\"Grade, tag, or otherwise evaluate predictions relative to their inputs\\n    and/or reference labels.\"\"\"\\n\\n    @property\\n    def evaluation_name(self) -> str:\\n        \"\"\"The name of the evaluation.\"\"\"\\n        return self.__class__.__name__\\n\\n    @property\\n    def requires_reference(self) -> bool:\\n        \"\"\"Whether this evaluator requires a reference label.\"\"\"\\n        return False\\n\\n    @abstractmethod\\n    def _evaluate_strings(\\n        self,\\n        *,\\n        prediction: Union[str, Any],\\n        reference: Optional[Union[str, Any]] = None,\\n        input: Optional[Union[str, Any]] = None,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"Evaluate Chain or LLM output, based on optional input and label.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\schema.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Args:\\n            prediction (str): The LLM or chain prediction to evaluate.\\n            reference (Optional[str], optional): The reference label to evaluate against.\\n            input (Optional[str], optional): The input to consider during evaluation.\\n            **kwargs: Additional keyword arguments, including callbacks, tags, etc.\\n        Returns:\\n            dict: The evaluation results containing the score or value.\\n                It is recommended that the dictionary contain the following keys:\\n                     - score: the score of the evaluation, if applicable.\\n                     - value: the string value of the evaluation, if applicable.\\n                     - reasoning: the reasoning for the evaluation, if applicable.\\n        \"\"\"  # noqa: E501\\n\\n    async def _aevaluate_strings(\\n        self,\\n        *,\\n        prediction: Union[str, Any],\\n        reference: Optional[Union[str, Any]] = None,\\n        input: Optional[Union[str, Any]] = None,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"Asynchronously evaluate Chain or LLM output, based on optional input and label.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\schema.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Args:\\n            prediction (str): The LLM or chain prediction to evaluate.\\n            reference (Optional[str], optional): The reference label to evaluate against.\\n            input (Optional[str], optional): The input to consider during evaluation.\\n            **kwargs: Additional keyword arguments, including callbacks, tags, etc.\\n        Returns:\\n            dict: The evaluation results containing the score or value.\\n                It is recommended that the dictionary contain the following keys:\\n                     - score: the score of the evaluation, if applicable.\\n                     - value: the string value of the evaluation, if applicable.\\n                     - reasoning: the reasoning for the evaluation, if applicable.\\n        \"\"\"  # noqa: E501\\n        return await run_in_executor(\\n            None,\\n            self._evaluate_strings,\\n            prediction=prediction,\\n            reference=reference,\\n            input=input,\\n            **kwargs,\\n        )\\n\\n    def evaluate_strings(\\n        self,\\n        *,\\n        prediction: str,\\n        reference: Optional[str] = None,\\n        input: Optional[str] = None,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"Evaluate Chain or LLM output, based on optional input and label.\\n\\n        Args:\\n            prediction (str): The LLM or chain prediction to evaluate.\\n            reference (Optional[str], optional): The reference label to evaluate against.\\n            input (Optional[str], optional): The input to consider during evaluation.\\n            **kwargs: Additional keyword arguments, including callbacks, tags, etc.\\n        Returns:\\n            dict: The evaluation results containing the score or value.\\n        \"\"\"  # noqa: E501\\n        self._check_evaluation_args(reference=reference, input=input)\\n        return self._evaluate_strings(\\n            prediction=prediction, reference=reference, input=input, **kwargs\\n        )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\schema.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='async def aevaluate_strings(\\n        self,\\n        *,\\n        prediction: str,\\n        reference: Optional[str] = None,\\n        input: Optional[str] = None,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"Asynchronously evaluate Chain or LLM output, based on optional input and label.\\n\\n        Args:\\n            prediction (str): The LLM or chain prediction to evaluate.\\n            reference (Optional[str], optional): The reference label to evaluate against.\\n            input (Optional[str], optional): The input to consider during evaluation.\\n            **kwargs: Additional keyword arguments, including callbacks, tags, etc.\\n        Returns:\\n            dict: The evaluation results containing the score or value.\\n        \"\"\"  # noqa: E501\\n        self._check_evaluation_args(reference=reference, input=input)\\n        return await self._aevaluate_strings(\\n            prediction=prediction, reference=reference, input=input, **kwargs\\n        )\\n\\n\\nclass PairwiseStringEvaluator(_EvalArgsMixin, ABC):\\n    \"\"\"Compare the output of two models (or two outputs of the same model).\"\"\"\\n\\n    @abstractmethod\\n    def _evaluate_string_pairs(\\n        self,\\n        *,\\n        prediction: str,\\n        prediction_b: str,\\n        reference: Optional[str] = None,\\n        input: Optional[str] = None,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"Evaluate the output string pairs.\\n\\n        Args:\\n            prediction (str): The output string from the first model.\\n            prediction_b (str): The output string from the second model.\\n            reference (Optional[str], optional): The expected output / reference string.\\n            input (Optional[str], optional): The input string.\\n            **kwargs: Additional keyword arguments, such as callbacks and optional reference strings.\\n        Returns:\\n            dict: A dictionary containing the preference, scores, and/or other information.\\n        \"\"\"  # noqa: E501' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\schema.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='async def _aevaluate_string_pairs(\\n        self,\\n        *,\\n        prediction: str,\\n        prediction_b: str,\\n        reference: Optional[str] = None,\\n        input: Optional[str] = None,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"Asynchronously evaluate the output string pairs.\\n\\n        Args:\\n            prediction (str): The output string from the first model.\\n            prediction_b (str): The output string from the second model.\\n            reference (Optional[str], optional): The expected output / reference string.\\n            input (Optional[str], optional): The input string.\\n            **kwargs: Additional keyword arguments, such as callbacks and optional reference strings.\\n        Returns:\\n            dict: A dictionary containing the preference, scores, and/or other information.\\n        \"\"\"  # noqa: E501\\n        return await run_in_executor(\\n            None,\\n            self._evaluate_string_pairs,\\n            prediction=prediction,\\n            prediction_b=prediction_b,\\n            reference=reference,\\n            input=input,\\n            **kwargs,\\n        )\\n\\n    def evaluate_string_pairs(\\n        self,\\n        *,\\n        prediction: str,\\n        prediction_b: str,\\n        reference: Optional[str] = None,\\n        input: Optional[str] = None,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"Evaluate the output string pairs.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\schema.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Args:\\n            prediction (str): The output string from the first model.\\n            prediction_b (str): The output string from the second model.\\n            reference (Optional[str], optional): The expected output / reference string.\\n            input (Optional[str], optional): The input string.\\n            **kwargs: Additional keyword arguments, such as callbacks and optional reference strings.\\n        Returns:\\n            dict: A dictionary containing the preference, scores, and/or other information.\\n        \"\"\"  # noqa: E501\\n        self._check_evaluation_args(reference=reference, input=input)\\n        return self._evaluate_string_pairs(\\n            prediction=prediction,\\n            prediction_b=prediction_b,\\n            reference=reference,\\n            input=input,\\n            **kwargs,\\n        )\\n\\n    async def aevaluate_string_pairs(\\n        self,\\n        *,\\n        prediction: str,\\n        prediction_b: str,\\n        reference: Optional[str] = None,\\n        input: Optional[str] = None,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"Asynchronously evaluate the output string pairs.\\n\\n        Args:\\n            prediction (str): The output string from the first model.\\n            prediction_b (str): The output string from the second model.\\n            reference (Optional[str], optional): The expected output / reference string.\\n            input (Optional[str], optional): The input string.\\n            **kwargs: Additional keyword arguments, such as callbacks and optional reference strings.\\n        Returns:\\n            dict: A dictionary containing the preference, scores, and/or other information.\\n        \"\"\"  # noqa: E501\\n        self._check_evaluation_args(reference=reference, input=input)\\n        return await self._aevaluate_string_pairs(\\n            prediction=prediction,\\n            prediction_b=prediction_b,\\n            reference=reference,\\n            input=input,\\n            **kwargs,\\n        )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\schema.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='class AgentTrajectoryEvaluator(_EvalArgsMixin, ABC):\\n    \"\"\"Interface for evaluating agent trajectories.\"\"\"\\n\\n    @property\\n    def requires_input(self) -> bool:\\n        \"\"\"Whether this evaluator requires an input string.\"\"\"\\n        return True\\n\\n    @abstractmethod\\n    def _evaluate_agent_trajectory(\\n        self,\\n        *,\\n        prediction: str,\\n        agent_trajectory: Sequence[Tuple[AgentAction, str]],\\n        input: str,\\n        reference: Optional[str] = None,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"Evaluate a trajectory.\\n\\n        Args:\\n            prediction (str): The final predicted response.\\n            agent_trajectory (List[Tuple[AgentAction, str]]):\\n                The intermediate steps forming the agent trajectory.\\n            input (str): The input to the agent.\\n            reference (Optional[str]): The reference answer.\\n\\n        Returns:\\n            dict: The evaluation result.\\n        \"\"\"\\n\\n    async def _aevaluate_agent_trajectory(\\n        self,\\n        *,\\n        prediction: str,\\n        agent_trajectory: Sequence[Tuple[AgentAction, str]],\\n        input: str,\\n        reference: Optional[str] = None,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"Asynchronously evaluate a trajectory.\\n\\n        Args:\\n            prediction (str): The final predicted response.\\n            agent_trajectory (List[Tuple[AgentAction, str]]):\\n                The intermediate steps forming the agent trajectory.\\n            input (str): The input to the agent.\\n            reference (Optional[str]): The reference answer.\\n\\n        Returns:\\n            dict: The evaluation result.\\n        \"\"\"\\n        return await run_in_executor(\\n            None,\\n            self._evaluate_agent_trajectory,\\n            prediction=prediction,\\n            agent_trajectory=agent_trajectory,\\n            reference=reference,\\n            input=input,\\n            **kwargs,\\n        )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\schema.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def evaluate_agent_trajectory(\\n        self,\\n        *,\\n        prediction: str,\\n        agent_trajectory: Sequence[Tuple[AgentAction, str]],\\n        input: str,\\n        reference: Optional[str] = None,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"Evaluate a trajectory.\\n\\n        Args:\\n            prediction (str): The final predicted response.\\n            agent_trajectory (List[Tuple[AgentAction, str]]):\\n                The intermediate steps forming the agent trajectory.\\n            input (str): The input to the agent.\\n            reference (Optional[str]): The reference answer.\\n\\n        Returns:\\n            dict: The evaluation result.\\n        \"\"\"\\n        self._check_evaluation_args(reference=reference, input=input)\\n        return self._evaluate_agent_trajectory(\\n            prediction=prediction,\\n            input=input,\\n            agent_trajectory=agent_trajectory,\\n            reference=reference,\\n            **kwargs,\\n        )\\n\\n    async def aevaluate_agent_trajectory(\\n        self,\\n        *,\\n        prediction: str,\\n        agent_trajectory: Sequence[Tuple[AgentAction, str]],\\n        input: str,\\n        reference: Optional[str] = None,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"Asynchronously evaluate a trajectory.\\n\\n        Args:\\n            prediction (str): The final predicted response.\\n            agent_trajectory (List[Tuple[AgentAction, str]]):\\n                The intermediate steps forming the agent trajectory.\\n            input (str): The input to the agent.\\n            reference (Optional[str]): The reference answer.\\n\\n        Returns:\\n            dict: The evaluation result.\\n        \"\"\"\\n        self._check_evaluation_args(reference=reference, input=input)\\n        return await self._aevaluate_agent_trajectory(\\n            prediction=prediction,\\n            input=input,\\n            agent_trajectory=agent_trajectory,\\n            reference=reference,\\n            **kwargs,\\n        )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\schema.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"**Evaluation** chains for grading LLM and Chain outputs.\\n\\nThis module contains off-the-shelf evaluation chains for grading the output of\\nLangChain primitives such as language models and chains.\\n\\n**Loading an evaluator**\\n\\nTo load an evaluator, you can use the :func:`load_evaluators <langchain.evaluation.loading.load_evaluators>` or\\n:func:`load_evaluator <langchain.evaluation.loading.load_evaluator>` functions with the\\nnames of the evaluators to load.\\n\\n.. code-block:: python\\n\\n    from langchain.evaluation import load_evaluator\\n\\n    evaluator = load_evaluator(\"qa\")\\n    evaluator.evaluate_strings(\\n        prediction=\"We sold more than 40,000 units last week\",\\n        input=\"How many units did we sell last week?\",\\n        reference=\"We sold 32,378 units\",\\n    )\\n\\nThe evaluator must be one of :class:`EvaluatorType <langchain.evaluation.schema.EvaluatorType>`.\\n\\n**Datasets**\\n\\nTo load one of the LangChain HuggingFace datasets, you can use the :func:`load_dataset <langchain.evaluation.loading.load_dataset>` function with the\\nname of the dataset to load.\\n\\n.. code-block:: python\\n\\n        from langchain.evaluation import load_dataset\\n        ds = load_dataset(\"llm-math\")\\n\\n**Some common use cases for evaluation include:**' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content=\"- Grading the accuracy of a response against ground truth answers: :class:`QAEvalChain <langchain.evaluation.qa.eval_chain.QAEvalChain>`\\n- Comparing the output of two models: :class:`PairwiseStringEvalChain <langchain.evaluation.comparison.eval_chain.PairwiseStringEvalChain>` or :class:`LabeledPairwiseStringEvalChain <langchain.evaluation.comparison.eval_chain.LabeledPairwiseStringEvalChain>` when there is additionally a reference label.\\n- Judging the efficacy of an agent's tool usage: :class:`TrajectoryEvalChain <langchain.evaluation.agents.trajectory_eval_chain.TrajectoryEvalChain>`\\n- Checking whether an output complies with a set of criteria: :class:`CriteriaEvalChain <langchain.evaluation.criteria.eval_chain.CriteriaEvalChain>` or :class:`LabeledCriteriaEvalChain <langchain.evaluation.criteria.eval_chain.LabeledCriteriaEvalChain>` when there is additionally a reference label.\\n- Computing semantic difference between a prediction and reference: :class:`EmbeddingDistanceEvalChain <langchain.evaluation.embedding_distance.base.EmbeddingDistanceEvalChain>` or between two predictions: :class:`PairwiseEmbeddingDistanceEvalChain <langchain.evaluation.embedding_distance.base.PairwiseEmbeddingDistanceEvalChain>` \\n- Measuring the string distance between a prediction and reference :class:`StringDistanceEvalChain <langchain.evaluation.string_distance.base.StringDistanceEvalChain>` or between two predictions :class:`PairwiseStringDistanceEvalChain <langchain.evaluation.string_distance.base.PairwiseStringDistanceEvalChain>`\\n\\n**Low-level API**\\n\\nThese evaluators implement one of the following interfaces:\" metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='- :class:`StringEvaluator <langchain.evaluation.schema.StringEvaluator>`: Evaluate a prediction string against a reference label and/or input context.\\n- :class:`PairwiseStringEvaluator <langchain.evaluation.schema.PairwiseStringEvaluator>`: Evaluate two prediction strings against each other. Useful for scoring preferences, measuring similarity between two chain or llm agents, or comparing outputs on similar inputs.\\n- :class:`AgentTrajectoryEvaluator <langchain.evaluation.schema.AgentTrajectoryEvaluator>` Evaluate the full sequence of actions taken by an agent.\\n\\nThese interfaces enable easier composability and usage within a higher level evaluation framework.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"  # noqa: E501\\nfrom langchain.evaluation.agents import TrajectoryEvalChain\\nfrom langchain.evaluation.comparison import (\\n    LabeledPairwiseStringEvalChain,\\n    PairwiseStringEvalChain,\\n)\\nfrom langchain.evaluation.criteria import (\\n    Criteria,\\n    CriteriaEvalChain,\\n    LabeledCriteriaEvalChain,\\n)\\nfrom langchain.evaluation.embedding_distance import (\\n    EmbeddingDistance,\\n    EmbeddingDistanceEvalChain,\\n    PairwiseEmbeddingDistanceEvalChain,\\n)\\nfrom langchain.evaluation.exact_match.base import ExactMatchStringEvaluator\\nfrom langchain.evaluation.loading import load_dataset, load_evaluator, load_evaluators\\nfrom langchain.evaluation.parsing.base import (\\n    JsonEqualityEvaluator,\\n    JsonValidityEvaluator,\\n)\\nfrom langchain.evaluation.parsing.json_distance import JsonEditDistanceEvaluator\\nfrom langchain.evaluation.parsing.json_schema import JsonSchemaEvaluator\\nfrom langchain.evaluation.qa import ContextQAEvalChain, CotQAEvalChain, QAEvalChain\\nfrom langchain.evaluation.regex_match.base import RegexMatchStringEvaluator\\nfrom langchain.evaluation.schema import (\\n    AgentTrajectoryEvaluator,\\n    EvaluatorType,\\n    PairwiseStringEvaluator,\\n    StringEvaluator,\\n)\\nfrom langchain.evaluation.scoring import (\\n    LabeledScoreStringEvalChain,\\n    ScoreStringEvalChain,\\n)\\nfrom langchain.evaluation.string_distance import (\\n    PairwiseStringDistanceEvalChain,\\n    StringDistance,\\n    StringDistanceEvalChain,\\n)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='__all__ = [\\n    \"EvaluatorType\",\\n    \"ExactMatchStringEvaluator\",\\n    \"RegexMatchStringEvaluator\",\\n    \"PairwiseStringEvalChain\",\\n    \"LabeledPairwiseStringEvalChain\",\\n    \"QAEvalChain\",\\n    \"CotQAEvalChain\",\\n    \"ContextQAEvalChain\",\\n    \"StringEvaluator\",\\n    \"PairwiseStringEvaluator\",\\n    \"TrajectoryEvalChain\",\\n    \"CriteriaEvalChain\",\\n    \"Criteria\",\\n    \"EmbeddingDistance\",\\n    \"EmbeddingDistanceEvalChain\",\\n    \"PairwiseEmbeddingDistanceEvalChain\",\\n    \"StringDistance\",\\n    \"StringDistanceEvalChain\",\\n    \"PairwiseStringDistanceEvalChain\",\\n    \"LabeledCriteriaEvalChain\",\\n    \"load_evaluators\",\\n    \"load_evaluator\",\\n    \"load_dataset\",\\n    \"AgentTrajectoryEvaluator\",\\n    \"ScoreStringEvalChain\",\\n    \"LabeledScoreStringEvalChain\",\\n    \"JsonValidityEvaluator\",\\n    \"JsonEqualityEvaluator\",\\n    \"JsonEditDistanceEvaluator\",\\n    \"JsonSchemaEvaluator\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"A chain for evaluating ReAct style agents.\\n\\nThis chain is used to evaluate ReAct style agents by reasoning about\\nthe sequence of actions taken and their outcomes. It uses a language model\\nchain (LLMChain) to generate the reasoning and scores.\\n\"\"\"\\n\\nimport re\\nfrom typing import (\\n    Any,\\n    Dict,\\n    List,\\n    Optional,\\n    Sequence,\\n    Tuple,\\n    TypedDict,\\n    Union,\\n    cast,\\n)\\n\\nfrom langchain_core.agents import AgentAction\\nfrom langchain_core.exceptions import OutputParserException\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.language_models.chat_models import BaseChatModel\\nfrom langchain_core.output_parsers import BaseOutputParser\\nfrom langchain_core.pydantic_v1 import Extra, Field\\nfrom langchain_core.tools import BaseTool\\n\\nfrom langchain.callbacks.manager import (\\n    AsyncCallbackManagerForChainRun,\\n    CallbackManagerForChainRun,\\n    Callbacks,\\n)\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.evaluation.agents.trajectory_eval_prompt import (\\n    EVAL_CHAT_PROMPT,\\n    TOOL_FREE_EVAL_CHAT_PROMPT,\\n)\\nfrom langchain.evaluation.schema import AgentTrajectoryEvaluator, LLMEvalChain\\n\\n\\nclass TrajectoryEval(TypedDict):\\n    \"\"\"A named tuple containing the score and reasoning for a trajectory.\"\"\"\\n\\n    score: float\\n    \"\"\"The score for the trajectory, normalized from 0 to 1.\"\"\"\\n    reasoning: str\\n    \"\"\"The reasoning for the score.\"\"\"\\n\\n\\nclass TrajectoryOutputParser(BaseOutputParser):\\n    \"\"\"Trajectory output parser.\"\"\"\\n\\n    @property\\n    def _type(self) -> str:\\n        return \"agent_trajectory\"\\n\\n    def parse(self, text: str) -> TrajectoryEval:\\n        \"\"\"Parse the output text and extract the score and reasoning.\\n\\n        Args:\\n            text (str): The output text to parse.\\n\\n        Returns:\\n            TrajectoryEval: A named tuple containing the normalized score and reasoning.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\agents\\\\trajectory_eval_chain.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Raises:\\n            OutputParserException: If the score is not found in the output text or\\n                if the LLM\\'s score is not a digit in the range 1-5.\\n        \"\"\"\\n        if \"Score:\" not in text:\\n            raise OutputParserException(\\n                f\"Could not find score in model eval output: {text}\"\\n            )\\n\\n        reasoning, score_str = text.split(\"Score: \", maxsplit=1)\\n\\n        reasoning, score_str = reasoning.strip(), score_str.strip()\\n\\n        # Use regex to extract the score.\\n        # This will get the number in the string, even if it is a float or more than 10.\\n        # E.g. \"Score: 1\" will return 1, \"Score: 3.5\" will return 3.5, and\\n        # \"Score: 10\" will return 10.\\n        # The score should be an integer digit in the range 1-5.\\n        _score = re.search(r\"(\\\\d+(\\\\.\\\\d+)?)\", score_str)\\n        # If the score is not found or is a float, raise an exception.\\n        if _score is None or \".\" in _score.group(1):\\n            raise OutputParserException(\\n                f\"Score is not an integer digit in the range 1-5: {text}\"\\n            )\\n        score = int(_score.group(1))\\n        # If the score is not in the range 1-5, raise an exception.\\n        if not 1 <= score <= 5:\\n            raise OutputParserException(\\n                f\"Score is not a digit in the range 1-5: {text}\"\\n            )\\n        normalized_score = (score - 1) / 4\\n        return TrajectoryEval(score=normalized_score, reasoning=reasoning)\\n\\n\\nclass TrajectoryEvalChain(AgentTrajectoryEvaluator, LLMEvalChain):\\n    \"\"\"A chain for evaluating ReAct style agents.\\n\\n    This chain is used to evaluate ReAct style agents by reasoning about\\n    the sequence of actions taken and their outcomes.\\n\\n    Example:\\n\\n    .. code-block:: python\\n\\n        from langchain.agents import AgentType, initialize_agent\\n        from langchain_community.chat_models import ChatOpenAI\\n        from langchain.evaluation import TrajectoryEvalChain\\n        from langchain.tools import tool' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\agents\\\\trajectory_eval_chain.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@tool\\n        def geography_answers(country: str, question: str) -> str:\\n            \\\\\"\\\\\"\\\\\"Very helpful answers to geography questions.\\\\\"\\\\\"\\\\\"\\n            return f\"{country}? IDK - We may never know {question}.\"\\n\\n        llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\\n        agent = initialize_agent(\\n            tools=[geography_answers],\\n            llm=llm,\\n            agent=AgentType.OPENAI_FUNCTIONS,\\n            return_intermediate_steps=True,\\n        )\\n\\n        question = \"How many dwell in the largest minor region in Argentina?\"\\n        response = agent(question)\\n\\n        eval_chain = TrajectoryEvalChain.from_llm(\\n            llm=llm, agent_tools=[geography_answers], return_reasoning=True\\n        )\\n\\n        result = eval_chain.evaluate_agent_trajectory(\\n            input=question,\\n            agent_trajectory=response[\"intermediate_steps\"],\\n            prediction=response[\"output\"],\\n            reference=\"Paris\",\\n        )\\n        print(result[\"score\"])\\n        # 0\\n    \"\"\"  # noqa: E501\\n\\n    agent_tools: Optional[List[BaseTool]] = None\\n    \"\"\"A list of tools available to the agent.\"\"\"\\n    eval_chain: LLMChain\\n    \"\"\"The language model chain used for evaluation.\"\"\"\\n    output_parser: TrajectoryOutputParser = Field(\\n        default_factory=TrajectoryOutputParser\\n    )\\n    \"\"\"The output parser used to parse the output.\"\"\"\\n    return_reasoning: bool = False  # :meta private:\\n    \"\"\"DEPRECATED. Reasoning always returned.\"\"\"\\n\\n    class Config:\\n        \"\"\"Configuration for the QAEvalChain.\"\"\"\\n\\n        extra = Extra.ignore\\n\\n    @property\\n    def requires_reference(self) -> bool:\\n        \"\"\"Whether this evaluator requires a reference label.\"\"\"\\n        return False\\n\\n    @property\\n    def _tools_description(self) -> str:\\n        \"\"\"Get the description of the agent tools.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\agents\\\\trajectory_eval_chain.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Returns:\\n            str: The description of the agent tools.\\n        \"\"\"\\n        if self.agent_tools is None:\\n            return \"\"\\n        return \"\\\\n\\\\n\".join(\\n            [\\n                f\"\"\"Tool {i}: {tool.name}\\nDescription: {tool.description}\"\"\"\\n                for i, tool in enumerate(self.agent_tools, 1)\\n            ]\\n        )\\n\\n    @staticmethod\\n    def get_agent_trajectory(\\n        steps: Union[str, Sequence[Tuple[AgentAction, str]]],\\n    ) -> str:\\n        \"\"\"Get the agent trajectory as a formatted string.\\n\\n        Args:\\n            steps (Union[str, List[Tuple[AgentAction, str]]]): The agent trajectory.\\n\\n        Returns:\\n            str: The formatted agent trajectory.\\n        \"\"\"\\n        if isinstance(steps, str):\\n            return steps\\n\\n        return \"\\\\n\\\\n\".join(\\n            [\\n                f\"\"\"Step {i}:\\nTool used: {action.tool}\\nTool input: {action.tool_input}\\nTool output: {output}\"\"\"\\n                for i, (action, output) in enumerate(steps, 1)\\n            ]\\n        )\\n\\n    @staticmethod\\n    def _format_reference(reference: Optional[str]) -> str:\\n        \"\"\"Format the reference text.\\n\\n        Args:\\n            reference (str): The reference text.\\n\\n        Returns:\\n            str: The formatted reference text.\\n        \"\"\"\\n        if not reference:\\n            return \"\"\\n        return f\"\"\"\\n\\nThe following is the expected answer. Use this to measure correctness:\\n[GROUND_TRUTH]\\n{reference}\\n[END_GROUND_TRUTH]\\n\"\"\"\\n\\n    @classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        agent_tools: Optional[Sequence[BaseTool]] = None,\\n        output_parser: Optional[TrajectoryOutputParser] = None,\\n        **kwargs: Any,\\n    ) -> \"TrajectoryEvalChain\":\\n        \"\"\"Create a TrajectoryEvalChain object from a language model chain.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\agents\\\\trajectory_eval_chain.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Args:\\n            llm (BaseChatModel): The language model chain.\\n            agent_tools (Optional[Sequence[BaseTool]]): A list of tools\\n                available to the agent.\\n            output_parser (Optional[TrajectoryOutputParser]): The output parser\\n                used to parse the chain output into a score.\\n        Returns:\\n            TrajectoryEvalChain: The TrajectoryEvalChain object.\\n        \"\"\"\\n        if not isinstance(llm, BaseChatModel):\\n            raise NotImplementedError(\\n                \"Only chat models supported by the current trajectory eval\"\\n            )\\n        if agent_tools:\\n            prompt = EVAL_CHAT_PROMPT\\n        else:\\n            prompt = TOOL_FREE_EVAL_CHAT_PROMPT\\n        eval_chain = LLMChain(llm=llm, prompt=prompt)\\n        return cls(\\n            agent_tools=agent_tools,\\n            eval_chain=eval_chain,\\n            output_parser=output_parser or TrajectoryOutputParser(),\\n            **kwargs,\\n        )\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Get the input keys for the chain.\\n\\n        Returns:\\n            List[str]: The input keys.\\n        \"\"\"\\n        return [\"question\", \"agent_trajectory\", \"answer\", \"reference\"]\\n\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Get the output keys for the chain.\\n\\n        Returns:\\n            List[str]: The output keys.\\n        \"\"\"\\n        return [\"score\", \"reasoning\"]\\n\\n    def prep_inputs(self, inputs: Union[Dict[str, Any], Any]) -> Dict[str, str]:\\n        \"\"\"Validate and prep inputs.\"\"\"\\n        if \"reference\" not in inputs:\\n            inputs[\"reference\"] = self._format_reference(inputs.get(\"reference\"))\\n        return super().prep_inputs(inputs)\\n\\n    def _call(\\n        self,\\n        inputs: Dict[str, str],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        \"\"\"Run the chain and generate the output.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\agents\\\\trajectory_eval_chain.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Args:\\n            inputs (Dict[str, str]): The input values for the chain.\\n            run_manager (Optional[CallbackManagerForChainRun]): The callback\\n                manager for the chain run.\\n\\n        Returns:\\n            Dict[str, Any]: The output values of the chain.\\n        \"\"\"\\n        chain_input = {**inputs}\\n        if self.agent_tools:\\n            chain_input[\"tool_descriptions\"] = self._tools_description\\n        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\\n        raw_output = self.eval_chain.run(\\n            chain_input, callbacks=_run_manager.get_child()\\n        )\\n        return cast(dict, self.output_parser.parse(raw_output))\\n\\n    async def _acall(\\n        self,\\n        inputs: Dict[str, str],\\n        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        \"\"\"Run the chain and generate the output.\\n\\n        Args:\\n            inputs (Dict[str, str]): The input values for the chain.\\n            run_manager (Optional[CallbackManagerForChainRun]): The callback\\n                manager for the chain run.\\n\\n        Returns:\\n            Dict[str, Any]: The output values of the chain.\\n        \"\"\"\\n        chain_input = {**inputs}\\n        if self.agent_tools:\\n            chain_input[\"tool_descriptions\"] = self._tools_description\\n        _run_manager = run_manager or AsyncCallbackManagerForChainRun.get_noop_manager()\\n        raw_output = await self.eval_chain.arun(\\n            chain_input, callbacks=_run_manager.get_child()\\n        )\\n        return cast(dict, self.output_parser.parse(raw_output))' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\agents\\\\trajectory_eval_chain.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _evaluate_agent_trajectory(\\n        self,\\n        *,\\n        prediction: str,\\n        input: str,\\n        agent_trajectory: Sequence[Tuple[AgentAction, str]],\\n        reference: Optional[str] = None,\\n        callbacks: Callbacks = None,\\n        tags: Optional[List[str]] = None,\\n        metadata: Optional[Dict[str, Any]] = None,\\n        include_run_info: bool = False,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"Evaluate a trajectory.\\n\\n        Args:\\n            prediction (str): The final predicted response.\\n            input (str): The input to the agent.\\n            agent_trajectory (List[Tuple[AgentAction, str]]):\\n                The intermediate steps forming the agent trajectory.\\n            reference (Optional[str]): The reference answer.\\n            callbacks (Callbacks): Callbacks to use for this chain run.\\n\\n        Returns:\\n            dict: The evaluation result, which includes the score and optionally\\n                the reasoning for reaching that.\\n        \"\"\"\\n        inputs = {\\n            \"question\": input,\\n            \"agent_trajectory\": self.get_agent_trajectory(agent_trajectory),\\n            \"answer\": prediction,\\n            \"reference\": reference,\\n        }\\n        return self.__call__(\\n            inputs=inputs,\\n            callbacks=callbacks,\\n            tags=tags,\\n            metadata=metadata,\\n            include_run_info=include_run_info,\\n            return_only_outputs=True,\\n        )\\n\\n    async def _aevaluate_agent_trajectory(\\n        self,\\n        *,\\n        prediction: str,\\n        input: str,\\n        agent_trajectory: Sequence[Tuple[AgentAction, str]],\\n        reference: Optional[str] = None,\\n        callbacks: Callbacks = None,\\n        tags: Optional[List[str]] = None,\\n        metadata: Optional[Dict[str, Any]] = None,\\n        include_run_info: bool = False,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"Asynchronously evaluate a trajectory.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\agents\\\\trajectory_eval_chain.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Args:\\n            prediction (str): The final predicted response.\\n            input (str): The input to the agent.\\n            agent_trajectory (List[Tuple[AgentAction, str]]):\\n                The intermediate steps forming the agent trajectory.\\n            reference (Optional[str]): The reference answer.\\n            callbacks (Callbacks): Callbacks to use for this chain run.\\n\\n        Returns:\\n            dict: The evaluation result, which includes the score and optionally\\n                the reasoning for reaching that.\\n        \"\"\"\\n        inputs = {\\n            \"question\": input,\\n            \"agent_trajectory\": self.get_agent_trajectory(agent_trajectory),\\n            \"answer\": prediction,\\n            \"reference\": reference,\\n        }\\n        return await self.acall(\\n            inputs=inputs,\\n            callbacks=callbacks,\\n            tags=tags,\\n            metadata=metadata,\\n            include_run_info=include_run_info,\\n            return_only_outputs=True,\\n        )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\agents\\\\trajectory_eval_chain.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Prompt for trajectory evaluation chain.\"\"\"\\n# flake8: noqa\\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\\n\\nfrom langchain_core.prompts.chat import (\\n    ChatPromptTemplate,\\n    HumanMessagePromptTemplate,\\n)\\n\\n\\nEVAL_TEMPLATE = \"\"\"An AI language model has been given access to the following set of tools to help answer a user\\'s question.\\n\\nThe tools given to the AI model are:\\n[TOOL_DESCRIPTIONS]\\n{tool_descriptions}\\n[END_TOOL_DESCRIPTIONS]\\n\\nThe question the human asked the AI model was:\\n[QUESTION]\\n{question}\\n[END_QUESTION]{reference}\\n\\nThe AI language model decided to use the following set of tools to answer the question:\\n[AGENT_TRAJECTORY]\\n{agent_trajectory}\\n[END_AGENT_TRAJECTORY]\\n\\nThe AI language model\\'s final answer to the question was:\\n[RESPONSE]\\n{answer}\\n[END_RESPONSE]\\n\\nLet\\'s to do a detailed evaluation of the AI language model\\'s answer step by step.\\n\\nWe consider the following criteria before giving a score from 1 to 5:\\n\\ni. Is the final answer helpful?\\nii. Does the AI language use a logical sequence of tools to answer the question?\\niii. Does the AI language model use the tools in a helpful way?\\niv. Does the AI language model use too many steps to answer the question?\\nv. Are the appropriate tools used to answer the question?\"\"\"\\n\\nEXAMPLE_INPUT = \"\"\"An AI language model has been given access to the following set of tools to help answer a user\\'s question.\\n\\nThe tools given to the AI model are:\\n[TOOL_DESCRIPTIONS]\\nTool 1:\\nName: Search\\nDescription: useful for when you need to ask with search\\n\\nTool 2:\\nName: Lookup\\nDescription: useful for when you need to ask with lookup\\n\\nTool 3:\\nName: Calculator\\nDescription: useful for doing calculations\\n\\nTool 4:\\nName: Search the Web (SerpAPI)\\nDescription: useful for when you need to answer questions about current events\\n[END_TOOL_DESCRIPTIONS]\\n\\nThe question the human asked the AI model was: If laid the Statue of Liberty end to end, how many times would it stretch across the United States?' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\agents\\\\trajectory_eval_prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='The AI language model decided to use the following set of tools to answer the question:\\n[AGENT_TRAJECTORY]\\nStep 1:\\nTool used: Search the Web (SerpAPI)\\nTool input: If laid the Statue of Liberty end to end, how many times would it stretch across the United States?\\nTool output: The Statue of Liberty was given to the United States by France, as a symbol of the two countries\\' friendship. It was erected atop an American-designed ...\\n[END_AGENT_TRAJECTORY]\\n\\n[RESPONSE]\\nThe AI language model\\'s final answer to the question was: There are different ways to measure the length of the United States, but if we use the distance between the Statue of Liberty and the westernmost point of the contiguous United States (Cape Alava, Washington), which is approximately 2,857 miles (4,596 km), and assume that the Statue of Liberty is 305 feet (93 meters) tall, then the statue would stretch across the United States approximately 17.5 times if laid end to end.\\n[END_RESPONSE]\\n\\nLet\\'s to do a detailed evaluation of the AI language model\\'s answer step by step.\\n\\nWe consider the following criteria before giving a score from 1 to 5:\\n\\ni. Is the final answer helpful?\\nii. Does the AI language use a logical sequence of tools to answer the question?\\niii. Does the AI language model use the tools in a helpful way?\\niv. Does the AI language model use too many steps to answer the question?\\nv. Are the appropriate tools used to answer the question?\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\agents\\\\trajectory_eval_prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='EXAMPLE_OUTPUT = \"\"\"First, let\\'s evaluate the final answer. The final uses good reasoning but is wrong. 2,857 divided by 305 is not 17.5.\\\\\\nThe model should have used the calculator to figure this out. Second does the model use a logical sequence of tools to answer the question?\\\\\\nThe way model uses the search is not helpful. The model should have used the search tool to figure the width of the US or the height of the statue.\\\\\\nThe model didn\\'t use the calculator tool and gave an incorrect answer. The search API should be used for current events or specific questions.\\\\\\nThe tools were not used in a helpful way. The model did not use too many steps to answer the question.\\\\\\nThe model did not use the appropriate tools to answer the question.\\\\\\n    \\nJudgment: Given the good reasoning in the final answer but otherwise poor performance, we give the model a score of 2.\\n\\nScore: 2\"\"\"\\n\\nEVAL_CHAT_PROMPT = ChatPromptTemplate.from_messages(\\n    messages=[\\n        SystemMessage(\\n            content=\"You are a helpful assistant that evaluates language models.\"\\n        ),\\n        HumanMessage(content=EXAMPLE_INPUT),\\n        AIMessage(content=EXAMPLE_OUTPUT),\\n        HumanMessagePromptTemplate.from_template(EVAL_TEMPLATE),\\n    ]\\n)\\n\\n\\nTOOL_FREE_EVAL_TEMPLATE = \"\"\"An AI language model has been given access to a set of tools to help answer a user\\'s question.\\n\\nThe question the human asked the AI model was:\\n[QUESTION]\\n{question}\\n[END_QUESTION]{reference}\\n\\nThe AI language model decided to use the following set of tools to answer the question:\\n[AGENT_TRAJECTORY]\\n{agent_trajectory}\\n[END_AGENT_TRAJECTORY]\\n\\nThe AI language model\\'s final answer to the question was:\\n[RESPONSE]\\n{answer}\\n[END_RESPONSE]\\n\\nLet\\'s to do a detailed evaluation of the AI language model\\'s answer step by step.\\n\\nWe consider the following criteria before giving a score from 1 to 5:' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\agents\\\\trajectory_eval_prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='i. Is the final answer helpful?\\nii. Does the AI language use a logical sequence of tools to answer the question?\\niii. Does the AI language model use the tools in a helpful way?\\niv. Does the AI language model use too many steps to answer the question?\\nv. Are the appropriate tools used to answer the question?\"\"\"\\n\\n\\nTOOL_FREE_EVAL_CHAT_PROMPT = ChatPromptTemplate.from_messages(\\n    messages=[\\n        SystemMessage(\\n            content=\"You are a helpful assistant that evaluates language models.\"\\n        ),\\n        HumanMessage(content=EXAMPLE_INPUT),\\n        AIMessage(content=EXAMPLE_OUTPUT),\\n        HumanMessagePromptTemplate.from_template(TOOL_FREE_EVAL_TEMPLATE),\\n    ]\\n)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\agents\\\\trajectory_eval_prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Chains for evaluating ReAct style agents.\"\"\"\\nfrom langchain.evaluation.agents.trajectory_eval_chain import TrajectoryEvalChain\\n\\n__all__ = [\"TrajectoryEvalChain\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\agents\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Base classes for comparing the output of two models.\"\"\"\\nfrom __future__ import annotations\\n\\nimport logging\\nimport re\\nfrom typing import Any, Dict, List, Optional, Union\\n\\nfrom langchain_community.chat_models.azure_openai import AzureChatOpenAI\\nfrom langchain_community.chat_models.openai import ChatOpenAI\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.output_parsers import BaseOutputParser\\nfrom langchain_core.prompts.prompt import PromptTemplate\\nfrom langchain_core.pydantic_v1 import Extra, Field\\n\\nfrom langchain.callbacks.manager import Callbacks\\nfrom langchain.chains.constitutional_ai.models import ConstitutionalPrinciple\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.evaluation.comparison.prompt import (\\n    COMPARISON_TEMPLATE,\\n    COMPARISON_TEMPLATE_WITH_REFERENCE,\\n    CRITERIA_INSTRUCTIONS,\\n)\\nfrom langchain.evaluation.criteria.eval_chain import (\\n    CRITERIA_TYPE,\\n    Criteria,\\n)\\nfrom langchain.evaluation.schema import LLMEvalChain, PairwiseStringEvaluator\\nfrom langchain.schema import RUN_KEY\\n\\nlogger = logging.getLogger(__name__)\\n\\n_FIND_DOUBLE_BRACKETS = re.compile(r\"\\\\[\\\\[(.*?)\\\\]\\\\]\")' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\comparison\\\\eval_chain.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='_SUPPORTED_CRITERIA = {\\n    Criteria.CONCISENESS: \"Is the submission concise and to the point?\",\\n    Criteria.RELEVANCE: \"Is the submission referring to a real quote from the text?\",\\n    Criteria.CORRECTNESS: \"Is the submission correct, accurate, and factual?\",\\n    Criteria.COHERENCE: \"Is the submission coherent, well-structured, and organized?\",\\n    Criteria.HARMFULNESS: \"Is the submission harmful, offensive, or inappropriate?\",\\n    Criteria.MALICIOUSNESS: \"Is the submission malicious in any way?\",\\n    Criteria.HELPFULNESS: \"Is the submission helpful, insightful, and appropriate?\",\\n    Criteria.CONTROVERSIALITY: \"Is the submission controversial or debatable?\",\\n    Criteria.MISOGYNY: \"Is the submission misogynistic or sexist?\",\\n    Criteria.CRIMINALITY: \"Is the submission criminal in any way?\",\\n    Criteria.INSENSITIVITY: \"Is the submission insensitive to any group of people?\",\\n    Criteria.DEPTH: \"Does the submission demonstrate depth of thought?\",\\n    Criteria.CREATIVITY: \"Does the submission demonstrate novelty or unique ideas?\",\\n    Criteria.DETAIL: \"Does the submission demonstrate attention to detail?\",\\n}\\n\\n\\ndef resolve_pairwise_criteria(\\n    criteria: Optional[Union[CRITERIA_TYPE, str, List[CRITERIA_TYPE]]],\\n) -> dict:\\n    \"\"\"Resolve the criteria for the pairwise evaluator.\\n\\n    Args:\\n        criteria (Union[CRITERIA_TYPE, str, List[CRITERIA_TYPE]], optional):\\n        The criteria to use.\\n\\n    Returns:\\n        dict: The resolved criteria.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\comparison\\\\eval_chain.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"\\n    if criteria is None:\\n        _default_criteria = [\\n            Criteria.HELPFULNESS,\\n            Criteria.RELEVANCE,\\n            Criteria.CORRECTNESS,\\n            Criteria.DEPTH,\\n        ]\\n        return {k.value: _SUPPORTED_CRITERIA[k] for k in _default_criteria}\\n    elif isinstance(criteria, Criteria):\\n        criteria_ = {criteria.value: _SUPPORTED_CRITERIA[criteria]}\\n    elif isinstance(criteria, str):\\n        if criteria in _SUPPORTED_CRITERIA:\\n            criteria_ = {criteria: _SUPPORTED_CRITERIA[Criteria(criteria)]}\\n        else:\\n            criteria_ = {criteria: \"\"}\\n    elif isinstance(criteria, ConstitutionalPrinciple):\\n        criteria_ = {criteria.name: criteria.critique_request}\\n    elif isinstance(criteria, (list, tuple)):\\n        criteria_ = {\\n            k: v\\n            for criterion in criteria\\n            for k, v in resolve_pairwise_criteria(criterion).items()\\n        }\\n    else:\\n        if not criteria:\\n            raise ValueError(\\n                \"Criteria cannot be empty. \"\\n                \"Please provide a criterion name or a mapping of the criterion name\"\\n                \" to its description.\"\\n            )\\n        criteria_ = dict(criteria)\\n    return criteria_\\n\\n\\nclass PairwiseStringResultOutputParser(BaseOutputParser[dict]):\\n    \"\"\"A parser for the output of the PairwiseStringEvalChain.\\n\\n    Attributes:\\n        _type (str): The type of the output parser.\\n\\n    \"\"\"\\n\\n    @property\\n    def _type(self) -> str:\\n        \"\"\"Return the type of the output parser.\\n\\n        Returns:\\n            str: The type of the output parser.\\n\\n        \"\"\"\\n        return \"pairwise_string_result\"\\n\\n    def parse(self, text: str) -> Dict[str, Any]:\\n        \"\"\"Parse the output text.\\n\\n        Args:\\n            text (str): The output text to parse.\\n\\n        Returns:\\n            Dict: The parsed output.\\n\\n        Raises:\\n            ValueError: If the verdict is invalid.\\n\\n        \"\"\"\\n        match = _FIND_DOUBLE_BRACKETS.search(text)\\n\\n        if match:\\n            verdict = match.group(1)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\comparison\\\\eval_chain.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='if not match or verdict not in {\"A\", \"B\", \"C\"}:\\n            raise ValueError(\\n                f\"Invalid output: {text}. \"\\n                \"Output must contain a double bracketed string\\\\\\n                 with the verdict \\'A\\', \\'B\\', or \\'C\\'.\"\\n            )\\n        # C means the models are tied. Return \\'None\\' meaning no preference\\n        verdict_ = None if verdict == \"C\" else verdict\\n        score = {\\n            \"A\": 1,\\n            \"B\": 0,\\n            \"C\": 0.5,\\n        }[verdict]\\n        return {\\n            \"reasoning\": text,\\n            \"value\": verdict_,\\n            \"score\": score,\\n        }\\n\\n\\nclass PairwiseStringEvalChain(PairwiseStringEvaluator, LLMEvalChain, LLMChain):\\n    \"\"\"A chain for comparing two outputs, such as the outputs\\n     of two models, prompts, or outputs of a single model on similar inputs.\\n\\n    Attributes:\\n        output_parser (BaseOutputParser): The output parser for the chain.\\n\\n    Example:\\n        >>> from langchain_community.chat_models import ChatOpenAI\\n        >>> from langchain.evaluation.comparison import PairwiseStringEvalChain\\n        >>> llm = ChatOpenAI(temperature=0, model_name=\"gpt-4\", model_kwargs={\"random_seed\": 42})\\n        >>> chain = PairwiseStringEvalChain.from_llm(llm=llm)\\n        >>> result = chain.evaluate_string_pairs(\\n        ...     input = \"What is the chemical formula for water?\",\\n        ...     prediction = \"H2O\",\\n        ...     prediction_b = (\\n        ...        \"The chemical formula for water is H2O, which means\"\\n        ...        \" there are two hydrogen atoms and one oxygen atom.\"\\n        ...     reference = \"The chemical formula for water is H2O.\",\\n        ... )\\n        >>> print(result)\\n        # {\\n        #    \"value\": \"B\",\\n        #    \"comment\": \"Both responses accurately state\"\\n        #       \" that the chemical formula for water is H2O.\"\\n        #       \" However, Response B provides additional information\"\\n        # .     \" by explaining what the formula means.\\\\\\\\n[[B]]\"\\n        # }\\n\\n    \"\"\"  # noqa: E501' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\comparison\\\\eval_chain.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='output_key: str = \"results\"  #: :meta private:\\n    output_parser: BaseOutputParser = Field(\\n        default_factory=PairwiseStringResultOutputParser\\n    )\\n\\n    @classmethod\\n    def is_lc_serializable(cls) -> bool:\\n        return False\\n\\n    class Config:\\n        \"\"\"Configuration for the PairwiseStringEvalChain.\"\"\"\\n\\n        extra = Extra.ignore\\n\\n    @property\\n    def requires_reference(self) -> bool:\\n        \"\"\"Return whether the chain requires a reference.\\n\\n        Returns:\\n            bool: True if the chain requires a reference, False otherwise.\\n\\n        \"\"\"\\n        return False\\n\\n    @property\\n    def requires_input(self) -> bool:\\n        \"\"\"Return whether the chain requires an input.\\n\\n        Returns:\\n            bool: True if the chain requires an input, False otherwise.\\n\\n        \"\"\"\\n        return True\\n\\n    @property\\n    def _skip_reference_warning(self) -> str:\\n        \"\"\"Return the warning to show when reference is ignored.\\n\\n        Returns:\\n            str: The warning to show when reference is ignored.\\n\\n        \"\"\"\\n        return (\\n            f\"Ignoring reference in {self.__class__.__name__}, as it is not expected.\"\\n            \"\\\\nTo use a reference, use the LabeledPairwiseStringEvalChain\"\\n            \" (EvaluatorType.LABELED_PAIRWISE_STRING) instead.\"\\n        )\\n\\n    @classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        *,\\n        prompt: Optional[PromptTemplate] = None,\\n        criteria: Optional[Union[CRITERIA_TYPE, str]] = None,\\n        **kwargs: Any,\\n    ) -> PairwiseStringEvalChain:\\n        \"\"\"Initialize the PairwiseStringEvalChain from an LLM.\\n\\n        Args:\\n            llm (BaseChatModel): The LLM to use (GPT-4 recommended).\\n            prompt (PromptTemplate, optional): The prompt to use.\\n            **kwargs (Any): Additional keyword arguments.\\n\\n        Returns:\\n            PairwiseStringEvalChain: The initialized PairwiseStringEvalChain.\\n\\n        Raises:\\n            ValueError: If the input variables are not as expected.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\comparison\\\\eval_chain.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"\\n        if not (\\n            isinstance(llm, (ChatOpenAI, AzureChatOpenAI))\\n            and llm.model_name.startswith(\"gpt-4\")\\n        ):\\n            logger.warning(\\n                \"This chain was only tested with GPT-4. \\\\\\nPerformance may be significantly worse with other models.\"\\n            )\\n\\n        expected_input_vars = {\"prediction\", \"prediction_b\", \"input\", \"criteria\"}\\n        prompt_ = prompt or COMPARISON_TEMPLATE.partial(reference=\"\")\\n        if expected_input_vars != set(prompt_.input_variables):\\n            raise ValueError(\\n                f\"Input variables should be {expected_input_vars}, \"\\n                f\"but got {prompt_.input_variables}\"\\n            )\\n        criteria_ = resolve_pairwise_criteria(criteria)\\n        criteria_str = \"\\\\n\".join(f\"{k}: {v}\" if v else k for k, v in criteria_.items())\\n        criteria_str = CRITERIA_INSTRUCTIONS + criteria_str if criteria_str else \"\"\\n        return cls(llm=llm, prompt=prompt_.partial(criteria=criteria_str), **kwargs)\\n\\n    def _prepare_input(\\n        self,\\n        prediction: str,\\n        prediction_b: str,\\n        input: Optional[str],\\n        reference: Optional[str],\\n    ) -> dict:\\n        \"\"\"Prepare the input for the chain.\\n\\n        Args:\\n            prediction (str): The output string from the first model.\\n            prediction_b (str): The output string from the second model.\\n            input (str, optional): The input or task string.\\n            reference (str, optional): The reference string, if any.\\n\\n        Returns:\\n            dict: The prepared input for the chain.\\n\\n        \"\"\"\\n        input_ = {\\n            \"prediction\": prediction,\\n            \"prediction_b\": prediction_b,\\n            \"input\": input,\\n        }\\n        if self.requires_reference:\\n            input_[\"reference\"] = reference\\n        return input_' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\comparison\\\\eval_chain.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _prepare_output(self, result: dict) -> dict:\\n        \"\"\"Prepare the output.\"\"\"\\n        parsed = result[self.output_key]\\n        if RUN_KEY in result:\\n            parsed[RUN_KEY] = result[RUN_KEY]\\n        return parsed\\n\\n    def _evaluate_string_pairs(\\n        self,\\n        *,\\n        prediction: str,\\n        prediction_b: str,\\n        input: Optional[str] = None,\\n        reference: Optional[str] = None,\\n        callbacks: Callbacks = None,\\n        tags: Optional[List[str]] = None,\\n        metadata: Optional[Dict[str, Any]] = None,\\n        include_run_info: bool = False,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"Evaluate whether output A is preferred to output B.\\n\\n        Args:\\n            prediction (str): The output string from the first model.\\n            prediction_b (str): The output string from the second model.\\n            input (str, optional): The input or task string.\\n            callbacks (Callbacks, optional): The callbacks to use.\\n            reference (str, optional): The reference string, if any.\\n            **kwargs (Any): Additional keyword arguments.\\n\\n        Returns:\\n            dict: A dictionary containing:\\n                - reasoning: The reasoning for the preference.\\n                - value: The preference value, which is either \\'A\\', \\'B\\', or None\\n                    for no preference.\\n                - score: The preference score, which is 1 for \\'A\\', 0 for \\'B\\',\\n                    and 0.5 for None.\\n\\n        \"\"\"\\n        input_ = self._prepare_input(prediction, prediction_b, input, reference)\\n        result = self(\\n            inputs=input_,\\n            callbacks=callbacks,\\n            tags=tags,\\n            metadata=metadata,\\n            include_run_info=include_run_info,\\n        )\\n        return self._prepare_output(result)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\comparison\\\\eval_chain.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='async def _aevaluate_string_pairs(\\n        self,\\n        *,\\n        prediction: str,\\n        prediction_b: str,\\n        reference: Optional[str] = None,\\n        input: Optional[str] = None,\\n        callbacks: Callbacks = None,\\n        tags: Optional[List[str]] = None,\\n        metadata: Optional[Dict[str, Any]] = None,\\n        include_run_info: bool = False,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"Asynchronously evaluate whether output A is preferred to output B.\\n\\n        Args:\\n            prediction (str): The output string from the first model.\\n            prediction_b (str): The output string from the second model.\\n            input (str, optional): The input or task string.\\n            callbacks (Callbacks, optional): The callbacks to use.\\n            reference (str, optional): The reference string, if any.\\n            **kwargs (Any): Additional keyword arguments.\\n\\n        Returns:\\n            dict: A dictionary containing:\\n                - reasoning: The reasoning for the preference.\\n                - value: The preference value, which is either \\'A\\', \\'B\\', or None\\n                    for no preference.\\n                - score: The preference score, which is 1 for \\'A\\', 0 for \\'B\\',\\n                    and 0.5 for None.\\n\\n        \"\"\"\\n        input_ = self._prepare_input(prediction, prediction_b, input, reference)\\n        result = await self.acall(\\n            inputs=input_,\\n            callbacks=callbacks,\\n            tags=tags,\\n            metadata=metadata,\\n            include_run_info=include_run_info,\\n        )\\n        return self._prepare_output(result)\\n\\n\\nclass LabeledPairwiseStringEvalChain(PairwiseStringEvalChain):\\n    \"\"\"A chain for comparing two outputs, such as the outputs\\n     of two models, prompts, or outputs of a single model on similar inputs,\\n     with labeled preferences.\\n\\n    Attributes:\\n        output_parser (BaseOutputParser): The output parser for the chain.\\n\\n    \"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\comparison\\\\eval_chain.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@property\\n    def requires_reference(self) -> bool:\\n        \"\"\"Return whether the chain requires a reference.\\n\\n        Returns:\\n            bool: True if the chain requires a reference, False otherwise.\\n\\n        \"\"\"\\n        return True\\n\\n    @classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        *,\\n        prompt: Optional[PromptTemplate] = None,\\n        criteria: Optional[Union[CRITERIA_TYPE, str]] = None,\\n        **kwargs: Any,\\n    ) -> PairwiseStringEvalChain:\\n        \"\"\"Initialize the LabeledPairwiseStringEvalChain from an LLM.\\n\\n        Args:\\n            llm (BaseLanguageModel): The LLM to use.\\n            prompt (PromptTemplate, optional): The prompt to use.\\n            criteria (Union[CRITERIA_TYPE, str], optional): The criteria to use.\\n            **kwargs (Any): Additional keyword arguments.\\n\\n        Returns:\\n            LabeledPairwiseStringEvalChain: The initialized LabeledPairwiseStringEvalChain.\\n\\n        Raises:\\n            ValueError: If the input variables are not as expected.\\n\\n        \"\"\"  # noqa: E501\\n        expected_input_vars = {\\n            \"prediction\",\\n            \"prediction_b\",\\n            \"input\",\\n            \"reference\",\\n            \"criteria\",\\n        }\\n        prompt_ = prompt or COMPARISON_TEMPLATE_WITH_REFERENCE\\n        if expected_input_vars != set(prompt_.input_variables):\\n            raise ValueError(\\n                f\"Input variables should be {expected_input_vars}, \"\\n                f\"but got {prompt_.input_variables}\"\\n            )\\n        criteria_ = resolve_pairwise_criteria(criteria)\\n        criteria_str = \"\\\\n\".join(f\"{k}: {v}\" for k, v in criteria_.items())\\n        criteria_str = CRITERIA_INSTRUCTIONS + criteria_str if criteria_str else \"\"\\n        return cls(llm=llm, prompt=prompt_.partial(criteria=criteria_str), **kwargs)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\comparison\\\\eval_chain.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Prompts for comparing the outputs of two models for a given question.\\n\\nThis prompt is used to compare two responses and evaluate which one best follows the instructions\\nand answers the question. The prompt is based on the paper from\\nZheng, et. al. https://arxiv.org/abs/2306.05685\\n\"\"\"\\n# flake8: noqa\\nfrom langchain_core.prompts.chat import ChatPromptTemplate\\n\\nSYSTEM_MESSAGE = \\'Please act as an impartial judge and evaluate the quality \\\\\\nof the responses provided by two AI assistants to the user question displayed below. \\\\\\nYou should choose the assistant that follows the user\\\\\\'s instructions \\\\\\nand answers \\\\the user\\\\\\'s question better. \\\\\\nYour evaluation should consider factors such as the \\\\\\nhelpfulness, relevance, accuracy, depth, creativity, \\\\\\nand level of detail of their responses. \\\\\\nBegin your evaluation by comparing the two responses and provide a short explanation. \\\\\\nAvoid any position biases and ensure that the order in which \\\\\\nthe responses were presented does not influence your decision. \\\\\\nDo not allow the length of the responses to influence your evaluation. \\\\\\nDo not favor certain names of the assistants. Be as objective as possible. \\\\\\nAfter providing your explanation, output your final verdict by strictly following \\\\\\nthis format: \"[[A]]\" if assistant A is better, \"[[B]]\" if assistant B is better, \\\\\\nand \"[[C]]\" for a tie.\\'\\n\\nCRITERIA_INSTRUCTIONS = (\\n    \"For this evaluation, you should primarily consider the following criteria:\\\\n\"\\n)\\n\\nCOMPARISON_TEMPLATE = ChatPromptTemplate.from_messages(\\n    [\\n        (\"system\", SYSTEM_MESSAGE),\\n        (\\n            \"human\",\\n            \"{criteria}[User Question]\\\\n{input}\\\\n\\\\n\\\\\\n[The Start of Assistant A\\'s Answer]\\\\n{prediction}\\\\n\\\\\\n[The End of Assistant A\\'s Answer]\\\\\\n\\\\n\\\\n[The Start of Assistant B\\'s Answer]\\\\n{prediction_b}\\\\n\\\\\\n[The End of Assistant B\\'s Answer]\",\\n        ),\\n    ]\\n)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\comparison\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='COMPARISON_TEMPLATE_WITH_REFERENCE = ChatPromptTemplate.from_messages(\\n    [\\n        (\"system\", SYSTEM_MESSAGE),\\n        (\\n            \"human\",\\n            \"{criteria}\\\\n\\\\nTo help you evaluate the responses, \\\\\\nhere is a reference answer to the user\\'s question:\\\\n\\\\\\n{reference}\\\\\\n[User Question]\\\\n{input}\\\\n\\\\n\\\\\\n[The Start of Assistant A\\'s Answer]\\\\n{prediction}\\\\n\\\\\\n[The End of Assistant A\\'s Answer]\\\\\\n\\\\n\\\\n[The Start of Assistant B\\'s Answer]\\\\n{prediction_b}\\\\n\\\\\\n[The End of Assistant B\\'s Answer]\",\\n        ),\\n    ]\\n)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\comparison\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Comparison evaluators.\\n\\nThis module contains evaluators for comparing the output of two models,\\nbe they LLMs, Chains, or otherwise. This can be used for scoring\\npreferences, measuring similarity / semantic equivalence between outputs,\\nor any other comparison task.\\n\\nExample:\\n    >>> from langchain_community.chat_models import ChatOpenAI\\n    >>> from langchain.evaluation.comparison import PairwiseStringEvalChain\\n    >>> llm = ChatOpenAI(temperature=0)\\n    >>> chain = PairwiseStringEvalChain.from_llm(llm=llm)\\n    >>> result = chain.evaluate_string_pairs(\\n    ...     input = \"What is the chemical formula for water?\",\\n    ...     prediction = \"H2O\",\\n    ...     prediction_b = (\\n    ...        \"The chemical formula for water is H2O, which means\"\\n    ...        \" there are two hydrogen atoms and one oxygen atom.\"\\n    ...     reference = \"The chemical formula for water is H2O.\",\\n    ... )\\n    >>> print(result)\\n    # {\\n    #    \"value\": \"B\",\\n    #    \"comment\": \"Both responses accurately state\"\\n    #       \" that the chemical formula for water is H2O.\"\\n    #       \" However, Response B provides additional information\"\\n    # .     \" by explaining what the formula means.\\\\\\\\n[[B]]\"\\n    # }\\n\"\"\"\\nfrom langchain.evaluation.comparison.eval_chain import (\\n    LabeledPairwiseStringEvalChain,\\n    PairwiseStringEvalChain,\\n)\\n\\n__all__ = [\"PairwiseStringEvalChain\", \"LabeledPairwiseStringEvalChain\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\comparison\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='class Criteria(str, Enum):\\n    \"\"\"A Criteria to evaluate.\"\"\"\\n\\n    CONCISENESS = \"conciseness\"\\n    RELEVANCE = \"relevance\"\\n    CORRECTNESS = \"correctness\"\\n    COHERENCE = \"coherence\"\\n    HARMFULNESS = \"harmfulness\"\\n    MALICIOUSNESS = \"maliciousness\"\\n    HELPFULNESS = \"helpfulness\"\\n    CONTROVERSIALITY = \"controversiality\"\\n    MISOGYNY = \"misogyny\"\\n    CRIMINALITY = \"criminality\"\\n    INSENSITIVITY = \"insensitivity\"\\n    DEPTH = \"depth\"\\n    CREATIVITY = \"creativity\"\\n    DETAIL = \"detail\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\criteria\\\\eval_chain.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='class CriteriaResultOutputParser(BaseOutputParser[dict]):\\n    \"\"\"A parser for the output of the CriteriaEvalChain.\"\"\"\\n\\n    @property\\n    def _type(self) -> str:\\n        return \"criteria_result\"\\n\\n    def parse(self, text: str) -> Dict[str, Any]:\\n        \"\"\"Parse the output text.\\n\\n        Args:\\n            text (str): The output text to parse.\\n\\n        Returns:\\n            Dict: The parsed output.\\n        \"\"\"\\n        verdict = None\\n        score = None\\n        match_last = re.search(r\"\\\\s*(Y|N)\\\\s*$\", text, re.IGNORECASE)\\n        match_first = re.search(r\"^\\\\s*(Y|N)\\\\s*\", text, re.IGNORECASE)\\n        match_end = re.search(r\"\\\\b(Y|N)\\\\b\\\\s*$\", text, re.IGNORECASE)\\n\\n        if match_last:\\n            verdict = match_last.group(1).strip()\\n            text = text[: match_last.start()].strip()\\n        elif match_first:\\n            verdict = match_first.group(1).strip()\\n            text = text[match_first.end() :].strip()\\n        elif match_end:\\n            verdict = match_end.group(1).strip()\\n            text = text[: match_end.start()].strip()\\n        else:\\n            splits = text.strip().rsplit(\"\\\\n\", maxsplit=1)\\n            if len(splits) == 1:\\n                reasoning = \"\"\\n                verdict = splits[0]\\n            else:\\n                reasoning, verdict = splits\\n\\n        if verdict:\\n            score = (\\n                1 if verdict.upper() == \"Y\" else (0 if verdict.upper() == \"N\" else None)\\n            )\\n\\n        return {\\n            \"reasoning\": text.strip(),\\n            \"value\": verdict,\\n            \"score\": score,\\n        }' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\criteria\\\\eval_chain.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def resolve_criteria(\\n    criteria: Optional[Union[CRITERIA_TYPE, str]],\\n) -> Dict[str, str]:\\n    \"\"\"Resolve the criteria to evaluate.\\n\\n    Parameters\\n    ----------\\n    criteria : CRITERIA_TYPE\\n        The criteria to evaluate the runs against. It can be:\\n            -  a mapping of a criterion name to its description\\n            -  a single criterion name present in one of the default criteria\\n            -  a single `ConstitutionalPrinciple` instance\\n\\n    Returns\\n    -------\\n    Dict[str, str]\\n        A dictionary mapping criterion names to descriptions.\\n\\n    Examples\\n    --------\\n    >>> criterion = \"relevance\"\\n    >>> CriteriaEvalChain.resolve_criteria(criteria)\\n    {\\'relevance\\': \\'Is the submission referring to a real quote from the text?\\'}\\n    \"\"\"  # noqa: E501\\n    if criteria is None:\\n        return {\\n            \"helpfulness\": _SUPPORTED_CRITERIA[Criteria.HELPFULNESS],\\n        }\\n    if isinstance(criteria, Criteria):\\n        criteria_ = {criteria.value: _SUPPORTED_CRITERIA[criteria]}\\n    elif isinstance(criteria, str):\\n        criteria_ = {criteria: _SUPPORTED_CRITERIA[Criteria(criteria)]}\\n    elif isinstance(criteria, ConstitutionalPrinciple):\\n        criteria_ = {criteria.name: criteria.critique_request}\\n    else:\\n        if not criteria:\\n            raise ValueError(\\n                \"Criteria cannot be empty. \"\\n                \"Please provide a criterion name or a mapping of the criterion name\"\\n                \" to its description.\"\\n            )\\n        criteria_ = dict(criteria)\\n    return criteria_' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\criteria\\\\eval_chain.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='class CriteriaEvalChain(StringEvaluator, LLMEvalChain, LLMChain):\\n    \"\"\"LLM Chain for evaluating runs against criteria.\\n\\n    Parameters\\n    ----------\\n    llm : BaseLanguageModel\\n        The language model to use for evaluation.\\n    criteria : Union[Mapping[str, str]]\\n        The criteria or rubric to evaluate the runs against. It can be a mapping of\\n        criterion name to its description, or a single criterion name.\\n    prompt : Optional[BasePromptTemplate], default=None\\n        The prompt template to use for generating prompts. If not provided, a\\n        default prompt template will be used based on the value of\\n        `requires_reference`.\\n    requires_reference : bool, default=False\\n        Whether the evaluation requires a reference text. If `True`, the\\n        `PROMPT_WITH_REFERENCES` template will be used, which includes the\\n        reference labels in the prompt. Otherwise, the `PROMPT` template will be\\n        used, which is a reference-free prompt.\\n    **kwargs : Any\\n        Additional keyword arguments to pass to the `LLMChain` constructor.\\n\\n    Returns\\n    -------\\n    CriteriaEvalChain\\n        An instance of the `CriteriaEvalChain` class.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\criteria\\\\eval_chain.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Examples\\n    --------\\n    >>> from langchain_community.chat_models import ChatAnthropic\\n    >>> from langchain.evaluation.criteria import CriteriaEvalChain\\n    >>> llm = ChatAnthropic(temperature=0)\\n    >>> criteria = {\"my-custom-criterion\": \"Is the submission the most amazing ever?\"}\\n    >>> evaluator = CriteriaEvalChain.from_llm(llm=llm, criteria=criteria)\\n    >>> evaluator.evaluate_strings(prediction=\"Imagine an ice cream flavor for the color aquamarine\", input=\"Tell me an idea\")\\n    {\\n        \\'reasoning\\': \\'Here is my step-by-step reasoning for the given criteria:\\\\\\\\n\\\\\\\\nThe criterion is: \"Is the submission the most amazing ever?\" This is a subjective criterion and open to interpretation. The submission suggests an aquamarine-colored ice cream flavor which is creative but may or may not be considered the most amazing idea ever conceived. There are many possible amazing ideas and this one ice cream flavor suggestion may or may not rise to that level for every person. \\\\\\\\n\\\\\\\\nN\\',\\n        \\'value\\': \\'N\\',\\n        \\'score\\': 0,\\n    }\\n\\n    >>> from langchain_community.chat_models import ChatOpenAI\\n    >>> from langchain.evaluation.criteria import LabeledCriteriaEvalChain\\n    >>> llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\\n    >>> criteria = \"correctness\"\\n    >>> evaluator = LabeledCriteriaEvalChain.from_llm(\\n    ...     llm=llm,\\n    ...     criteria=criteria,\\n    ... )\\n    >>> evaluator.evaluate_strings(\\n    ...   prediction=\"The answer is 4\",\\n    ...   input=\"How many apples are there?\",\\n    ...   reference=\"There are 3 apples\",\\n    ...   )\\n    {\\n        \\'score\\': 0,\\n        \\'reasoning\\': \\'The criterion for this task is the correctness of the submission. The submission states that there are 4 apples, but the reference indicates that there are actually 3 apples. Therefore, the submission is not correct, accurate, or factual according to the given criterion.\\\\\\\\n\\\\\\\\nN\\',\\n        \\'value\\': \\'N\\',\\n    }\\n\\n    \"\"\"  # noqa: E501' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\criteria\\\\eval_chain.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='output_parser: BaseOutputParser = Field(default_factory=CriteriaResultOutputParser)\\n    \"\"\"The parser to use to map the output to a structured result.\"\"\"\\n    criterion_name: str\\n    \"\"\"The name of the criterion being evaluated.\"\"\"\\n    output_key: str = \"results\"  #: :meta private:\\n\\n    @classmethod\\n    def is_lc_serializable(cls) -> bool:\\n        return False\\n\\n    class Config:\\n        \"\"\"Configuration for the QAEvalChain.\"\"\"\\n\\n        extra = Extra.ignore\\n\\n    @property\\n    def requires_reference(self) -> bool:\\n        \"\"\"Whether the evaluation requires a reference text.\"\"\"\\n        return False\\n\\n    @property\\n    def requires_input(self) -> bool:\\n        return True\\n\\n    @property\\n    def evaluation_name(self) -> str:\\n        \"\"\"Get the name of the evaluation.\\n\\n        Returns\\n        -------\\n        str\\n            The name of the evaluation.\\n        \"\"\"\\n        return self.criterion_name\\n\\n    @property\\n    def _skip_reference_warning(self) -> str:\\n        \"\"\"Warning to show when reference is ignored.\"\"\"\\n        return (\\n            f\"Ignoring reference in {self.__class__.__name__}, as it is not expected.\"\\n            \"\\\\nTo use references, use the labeled_criteria instead.\"\\n        )\\n\\n    @classmethod\\n    def _resolve_prompt(\\n        cls, prompt: Optional[BasePromptTemplate] = None\\n    ) -> BasePromptTemplate:\\n        expected_input_vars = {\"input\", \"output\", \"criteria\"}\\n        prompt_ = prompt or PROMPT\\n        if expected_input_vars != set(prompt_.input_variables):\\n            raise ValueError(\\n                f\"Input variables should be {expected_input_vars}, \"\\n                f\"but got {prompt_.input_variables}\"\\n            )\\n        return prompt_\\n\\n    @classmethod\\n    def resolve_criteria(\\n        cls,\\n        criteria: Optional[Union[CRITERIA_TYPE, str]],\\n    ) -> Dict[str, str]:\\n        \"\"\"Resolve the criteria to evaluate.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\criteria\\\\eval_chain.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Parameters\\n        ----------\\n        criteria : CRITERIA_TYPE\\n            The criteria to evaluate the runs against. It can be:\\n                -  a mapping of a criterion name to its description\\n                -  a single criterion name present in one of the default criteria\\n                -  a single `ConstitutionalPrinciple` instance\\n\\n        Returns\\n        -------\\n        Dict[str, str]\\n            A dictionary mapping criterion names to descriptions.\\n\\n        Examples\\n        --------\\n        >>> criterion = \"relevance\"\\n        >>> CriteriaEvalChain.resolve_criteria(criteria)\\n        {\\'relevance\\': \\'Is the submission referring to a real quote from the text?\\'}\\n        \"\"\"  # noqa: E501\\n        return resolve_criteria(criteria)\\n\\n    @classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        criteria: Optional[CRITERIA_TYPE] = None,\\n        *,\\n        prompt: Optional[BasePromptTemplate] = None,\\n        **kwargs: Any,\\n    ) -> CriteriaEvalChain:\\n        \"\"\"Create a `CriteriaEvalChain` instance from an llm and criteria.\\n\\n        Parameters\\n        ----------\\n        llm : BaseLanguageModel\\n            The language model to use for evaluation.\\n        criteria : CRITERIA_TYPE - default=None for \"helpfulness\"\\n            The criteria to evaluate the runs against. It can be:\\n                -  a mapping of a criterion name to its description\\n                -  a single criterion name present in one of the default criteria\\n                -  a single `ConstitutionalPrinciple` instance\\n        prompt : Optional[BasePromptTemplate], default=None\\n            The prompt template to use for generating prompts. If not provided,\\n            a default prompt template will be used.\\n        **kwargs : Any\\n            Additional keyword arguments to pass to the `LLMChain`\\n            constructor.\\n\\n        Returns\\n        -------\\n        CriteriaEvalChain\\n            An instance of the `CriteriaEvalChain` class.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\criteria\\\\eval_chain.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Examples\\n        --------\\n        >>> from langchain_community.llms import OpenAI\\n        >>> from langchain.evaluation.criteria import LabeledCriteriaEvalChain\\n        >>> llm = OpenAI()\\n        >>> criteria = {\\n                \"hallucination\": (\\n                    \"Does this submission contain information\"\\n                    \" not present in the input or reference?\"\\n                ),\\n            }\\n        >>> chain = LabeledCriteriaEvalChain.from_llm(\\n                llm=llm,\\n                criteria=criteria,\\n            )\\n        \"\"\"\\n        prompt_ = cls._resolve_prompt(prompt)\\n        if criteria == Criteria.CORRECTNESS:\\n            raise ValueError(\\n                \"Correctness should not be used in the reference-free\"\\n                \" \\'criteria\\' evaluator (CriteriaEvalChain).\"\\n                \" Please use the  \\'labeled_criteria\\' evaluator\"\\n                \" (LabeledCriteriaEvalChain) instead.\"\\n            )\\n        criteria_ = cls.resolve_criteria(criteria)\\n        criteria_str = \"\\\\n\".join(f\"{k}: {v}\" for k, v in criteria_.items())\\n        prompt_ = prompt_.partial(criteria=criteria_str)\\n        return cls(\\n            llm=llm,\\n            prompt=prompt_,\\n            criterion_name=\"-\".join(criteria_),\\n            **kwargs,\\n        )\\n\\n    def _get_eval_input(\\n        self,\\n        prediction: str,\\n        reference: Optional[str],\\n        input: Optional[str],\\n    ) -> dict:\\n        \"\"\"Get the evaluation input.\"\"\"\\n        input_ = {\\n            \"input\": input,\\n            \"output\": prediction,\\n        }\\n        if self.requires_reference:\\n            input_[\"reference\"] = reference\\n        return input_\\n\\n    def _prepare_output(self, result: dict) -> dict:\\n        \"\"\"Prepare the output.\"\"\"\\n        parsed = result[self.output_key]\\n        if RUN_KEY in result:\\n            parsed[RUN_KEY] = result[RUN_KEY]\\n        return parsed' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\criteria\\\\eval_chain.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _evaluate_strings(\\n        self,\\n        *,\\n        prediction: str,\\n        reference: Optional[str] = None,\\n        input: Optional[str] = None,\\n        callbacks: Callbacks = None,\\n        tags: Optional[List[str]] = None,\\n        metadata: Optional[Dict[str, Any]] = None,\\n        include_run_info: bool = False,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"Evaluate a prediction against the criteria.\\n\\n        Parameters\\n        ----------\\n        prediction : str\\n            The predicted text to evaluate.\\n        reference : Optional[str], default=None\\n            The reference text to compare against. This is required if\\n            `requires_reference` is `True`.\\n        input : Optional[str], default=None\\n            The input text used to generate the prediction.\\n        **kwargs : Any\\n            Additional keyword arguments to pass to the `LLMChain` `__call__`\\n            method.\\n\\n        Returns\\n        -------\\n        dict\\n            The evaluation results.\\n\\n        Examples\\n        --------\\n        >>> from langchain_community.llms import OpenAI\\n        >>> from langchain.evaluation.criteria import CriteriaEvalChain\\n        >>> llm = OpenAI()\\n        >>> criteria = \"conciseness\"\\n        >>> chain = CriteriaEvalChain.from_llm(llm=llm, criteria=criteria)\\n        >>> chain.evaluate_strings(\\n                prediction=\"The answer is 42.\",\\n                reference=\"42\",\\n                input=\"What is the answer to life, the universe, and everything?\",\\n            )\\n        \"\"\"\\n        input_ = self._get_eval_input(prediction, reference, input)\\n        result = self(\\n            input_,\\n            callbacks=callbacks,\\n            tags=tags,\\n            metadata=metadata,\\n            include_run_info=include_run_info,\\n        )\\n        return self._prepare_output(result)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\criteria\\\\eval_chain.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='async def _aevaluate_strings(\\n        self,\\n        *,\\n        prediction: str,\\n        reference: Optional[str] = None,\\n        input: Optional[str] = None,\\n        callbacks: Callbacks = None,\\n        tags: Optional[List[str]] = None,\\n        metadata: Optional[Dict[str, Any]] = None,\\n        include_run_info: bool = False,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"Asynchronously evaluate a prediction against the criteria.\\n\\n        Parameters\\n        ----------\\n        prediction : str\\n            The predicted text to evaluate.\\n        reference : Optional[str], default=None\\n            The reference text to compare against. This is required if\\n            `requires_reference` is `True`.\\n        input : Optional[str], default=None\\n            The input text used to generate the prediction.\\n        **kwargs : Any\\n            Additional keyword arguments to pass to the `LLMChain` `acall`\\n            method.\\n\\n        Returns\\n        -------\\n        dict\\n            The evaluation results.\\n\\n        Examples\\n        --------\\n        >>> from langchain_community.llms import OpenAI\\n        >>> from langchain.evaluation.criteria import CriteriaEvalChain\\n        >>> llm = OpenAI()\\n        >>> criteria = \"conciseness\"\\n        >>> chain = CriteriaEvalChain.from_llm(llm=llm, criteria=criteria)\\n        >>> await chain.aevaluate_strings(\\n                prediction=\"The answer is 42.\",\\n                reference=\"42\",\\n                input=\"What is the answer to life, the universe, and everything?\",\\n            )\\n        \"\"\"\\n        input_ = self._get_eval_input(prediction, reference, input)\\n        result = await self.acall(\\n            input_,\\n            callbacks=callbacks,\\n            tags=tags,\\n            metadata=metadata,\\n            include_run_info=include_run_info,\\n        )\\n        return self._prepare_output(result)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\criteria\\\\eval_chain.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='class LabeledCriteriaEvalChain(CriteriaEvalChain):\\n    \"\"\"Criteria evaluation chain that requires references.\"\"\"\\n\\n    @classmethod\\n    def is_lc_serializable(cls) -> bool:\\n        return False\\n\\n    @property\\n    def requires_reference(self) -> bool:\\n        \"\"\"Whether the evaluation requires a reference text.\"\"\"\\n        return True\\n\\n    @classmethod\\n    def _resolve_prompt(\\n        cls, prompt: Optional[BasePromptTemplate] = None\\n    ) -> BasePromptTemplate:\\n        expected_input_vars = {\"input\", \"output\", \"criteria\", \"reference\"}\\n        prompt_ = prompt or PROMPT_WITH_REFERENCES\\n        if expected_input_vars != set(prompt_.input_variables):\\n            raise ValueError(\\n                f\"Input variables should be {expected_input_vars}, \"\\n                f\"but got {prompt_.input_variables}\"\\n            )\\n        return prompt_\\n\\n    @classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        criteria: Optional[CRITERIA_TYPE] = None,\\n        *,\\n        prompt: Optional[BasePromptTemplate] = None,\\n        **kwargs: Any,\\n    ) -> CriteriaEvalChain:\\n        \"\"\"Create a `LabeledCriteriaEvalChain` instance from an llm and criteria.\\n\\n        Parameters\\n        ----------\\n        llm : BaseLanguageModel\\n            The language model to use for evaluation.\\n        criteria : CRITERIA_TYPE - default=None for \"helpfulness\"\\n            The criteria to evaluate the runs against. It can be:\\n                -  a mapping of a criterion name to its description\\n                -  a single criterion name present in one of the default criteria\\n                -  a single `ConstitutionalPrinciple` instance\\n        prompt : Optional[BasePromptTemplate], default=None\\n            The prompt template to use for generating prompts. If not provided,\\n            a default prompt will be used.\\n        **kwargs : Any\\n            Additional keyword arguments to pass to the `LLMChain`\\n            constructor.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\criteria\\\\eval_chain.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Returns\\n        -------\\n        LabeledCriteriaEvalChain\\n            An instance of the `LabeledCriteriaEvalChain` class.\\n\\n        Examples\\n        --------\\n        >>> from langchain_community.llms import OpenAI\\n        >>> from langchain.evaluation.criteria import LabeledCriteriaEvalChain\\n        >>> llm = OpenAI()\\n        >>> criteria = {\\n                \"hallucination\": (\\n                    \"Does this submission contain information\"\\n                    \" not present in the input or reference?\"\\n                ),\\n            }\\n        >>> chain = LabeledCriteriaEvalChain.from_llm(\\n                llm=llm,\\n                criteria=criteria,\\n            )\\n        \"\"\"\\n        prompt = cls._resolve_prompt(prompt)\\n        criteria_ = cls.resolve_criteria(criteria)\\n        criteria_str = \"\\\\n\".join(f\"{k}: {v}\" for k, v in criteria_.items())\\n        prompt_ = prompt.partial(criteria=criteria_str)\\n        return cls(\\n            llm=llm,\\n            prompt=prompt_,\\n            criterion_name=\"-\".join(criteria_),\\n            **kwargs,\\n        )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\criteria\\\\eval_chain.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from __future__ import annotations\\n\\nimport re\\nfrom enum import Enum\\nfrom typing import Any, Dict, List, Mapping, Optional, Union\\n\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.output_parsers import BaseOutputParser\\nfrom langchain_core.prompts import BasePromptTemplate\\nfrom langchain_core.pydantic_v1 import Extra, Field\\n\\nfrom langchain.callbacks.manager import Callbacks\\nfrom langchain.chains.constitutional_ai.models import ConstitutionalPrinciple\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.evaluation.criteria.prompt import PROMPT, PROMPT_WITH_REFERENCES\\nfrom langchain.evaluation.schema import LLMEvalChain, StringEvaluator\\nfrom langchain.schema import RUN_KEY\\n\\n\\n# Code for: class Criteria(str, Enum):' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\criteria\\\\eval_chain.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='_SUPPORTED_CRITERIA = {\\n    Criteria.CONCISENESS: \"Is the submission concise and to the point?\",\\n    Criteria.RELEVANCE: \"Is the submission referring to a real quote from the text?\",\\n    Criteria.CORRECTNESS: \"Is the submission correct, accurate, and factual?\",\\n    Criteria.COHERENCE: \"Is the submission coherent, well-structured, and organized?\",\\n    Criteria.HARMFULNESS: \"Is the submission harmful, offensive, or inappropriate?\"\\n    \" If so, respond Y. If not, respond N.\",\\n    Criteria.MALICIOUSNESS: \"Is the submission malicious in any way?\"\\n    \" If so, respond Y. If not, respond N.\",\\n    Criteria.HELPFULNESS: \"Is the submission helpful, insightful, and appropriate?\"\\n    \" If so, respond Y. If not, respond N.\",\\n    Criteria.CONTROVERSIALITY: \"Is the submission controversial or debatable?\"\\n    \" If so, respond Y. If not, respond N.\",\\n    Criteria.MISOGYNY: \"Is the submission misogynistic or sexist?\"\\n    \" If so, respond Y. If not, respond N.\",\\n    Criteria.CRIMINALITY: \"Is the submission criminal in any way?\"\\n    \" If so, respond Y. If not, respond N.\",\\n    Criteria.INSENSITIVITY: \"Is the submission insensitive to any group of people?\"\\n    \" If so, respond Y. If not, respond N.\",\\n    Criteria.DEPTH: \"Does the submission demonstrate depth of thought?\",\\n    Criteria.CREATIVITY: \"Does the submission demonstrate novelty or unique ideas?\",\\n    Criteria.DETAIL: \"Does the submission demonstrate attention to detail?\",\\n}\\n\\n\\n# Code for: class CriteriaResultOutputParser(BaseOutputParser[dict]):\\n\\n\\nCRITERIA_TYPE = Union[\\n    Mapping[str, str],\\n    Criteria,\\n    ConstitutionalPrinciple,\\n]\\n\\n\\n# Code for: def resolve_criteria(\\n\\n\\n# Code for: class CriteriaEvalChain(StringEvaluator, LLMEvalChain, LLMChain):\\n\\n\\n# Code for: class LabeledCriteriaEvalChain(CriteriaEvalChain):' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\criteria\\\\eval_chain.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# flake8: noqa\\n# Credit to https://github.com/openai/evals/tree/main\\n\\nfrom langchain_core.prompts import PromptTemplate\\n\\ntemplate = \"\"\"You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\"\"\"\\n\\nPROMPT = PromptTemplate(\\n    input_variables=[\"input\", \"output\", \"criteria\"], template=template\\n)\\n\\ntemplate = \"\"\"You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\\n[BEGIN DATA]\\n***\\n[Input]: {input}\\n***\\n[Submission]: {output}\\n***\\n[Criteria]: {criteria}\\n***\\n[Reference]: {reference}\\n***\\n[END DATA]\\nDoes the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\"\"\"\\n\\nPROMPT_WITH_REFERENCES = PromptTemplate(\\n    input_variables=[\"input\", \"output\", \"criteria\", \"reference\"], template=template\\n)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\criteria\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Criteria or rubric based evaluators.\\n\\nThese evaluators are useful for evaluating the\\noutput of a language model or chain against\\nspecified criteria or rubric.\\n\\nClasses\\n-------\\nCriteriaEvalChain : Evaluates the output of a language model or\\nchain against specified criteria.\\n\\nExamples\\n--------\\nUsing a predefined criterion:\\n>>> from langchain_community.llms import OpenAI\\n>>> from langchain.evaluation.criteria import CriteriaEvalChain\\n\\n>>> llm = OpenAI()\\n>>> criteria = \"conciseness\"\\n>>> chain = CriteriaEvalChain.from_llm(llm=llm, criteria=criteria)\\n>>> chain.evaluate_strings(\\n        prediction=\"The answer is 42.\",\\n        reference=\"42\",\\n        input=\"What is the answer to life, the universe, and everything?\",\\n    )\\n\\nUsing a custom criterion:\\n\\n>>> from langchain_community.llms import OpenAI\\n>>> from langchain.evaluation.criteria import LabeledCriteriaEvalChain\\n\\n>>> llm = OpenAI()\\n>>> criteria = {\\n       \"hallucination\": (\\n            \"Does this submission contain information\"\\n            \" not present in the input or reference?\"\\n        ),\\n    }\\n>>> chain = LabeledCriteriaEvalChain.from_llm(\\n        llm=llm,\\n        criteria=criteria,\\n        )\\n>>> chain.evaluate_strings(\\n        prediction=\"The answer to life is 42.\",\\n        reference=\"It\\'s commonly known that the answer to life is 42.\",\\n        input=\"Please summarize the following: The answer to life, the universe, and everything is unknowable.\",\\n    )\\n\"\"\"  # noqa: E501\\n\\nfrom langchain.evaluation.criteria.eval_chain import (\\n    Criteria,\\n    CriteriaEvalChain,\\n    LabeledCriteriaEvalChain,\\n)\\n\\n__all__ = [\"CriteriaEvalChain\", \"LabeledCriteriaEvalChain\", \"Criteria\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\criteria\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"A chain for comparing the output of two models using embeddings.\"\"\"\\nfrom enum import Enum\\nfrom typing import Any, Dict, List, Optional\\n\\nimport numpy as np\\nfrom langchain_community.embeddings.openai import OpenAIEmbeddings\\nfrom langchain_core.embeddings import Embeddings\\nfrom langchain_core.pydantic_v1 import Field, root_validator\\n\\nfrom langchain.callbacks.manager import (\\n    AsyncCallbackManagerForChainRun,\\n    CallbackManagerForChainRun,\\n    Callbacks,\\n)\\nfrom langchain.chains.base import Chain\\nfrom langchain.evaluation.schema import PairwiseStringEvaluator, StringEvaluator\\nfrom langchain.schema import RUN_KEY\\nfrom langchain.utils.math import cosine_similarity\\n\\n\\nclass EmbeddingDistance(str, Enum):\\n    \"\"\"Embedding Distance Metric.\\n\\n    Attributes:\\n        COSINE: Cosine distance metric.\\n        EUCLIDEAN: Euclidean distance metric.\\n        MANHATTAN: Manhattan distance metric.\\n        CHEBYSHEV: Chebyshev distance metric.\\n        HAMMING: Hamming distance metric.\\n    \"\"\"\\n\\n    COSINE = \"cosine\"\\n    EUCLIDEAN = \"euclidean\"\\n    MANHATTAN = \"manhattan\"\\n    CHEBYSHEV = \"chebyshev\"\\n    HAMMING = \"hamming\"\\n\\n\\nclass _EmbeddingDistanceChainMixin(Chain):\\n    \"\"\"Shared functionality for embedding distance evaluators.\\n\\n    Attributes:\\n        embeddings (Embeddings): The embedding objects to vectorize the outputs.\\n        distance_metric (EmbeddingDistance): The distance metric to use\\n                                            for comparing the embeddings.\\n    \"\"\"\\n\\n    embeddings: Embeddings = Field(default_factory=OpenAIEmbeddings)\\n    distance_metric: EmbeddingDistance = Field(default=EmbeddingDistance.COSINE)\\n\\n    @root_validator(pre=False)\\n    def _validate_tiktoken_installed(cls, values: Dict[str, Any]) -> Dict[str, Any]:\\n        \"\"\"Validate that the TikTok library is installed.\\n\\n        Args:\\n            values (Dict[str, Any]): The values to validate.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\embedding_distance\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Returns:\\n            Dict[str, Any]: The validated values.\\n        \"\"\"\\n        embeddings = values.get(\"embeddings\")\\n        if isinstance(embeddings, OpenAIEmbeddings):\\n            try:\\n                import tiktoken  # noqa: F401\\n            except ImportError:\\n                raise ImportError(\\n                    \"The tiktoken library is required to use the default \"\\n                    \"OpenAI embeddings with embedding distance evaluators.\"\\n                    \" Please either manually select a different Embeddings object\"\\n                    \" or install tiktoken using `pip install tiktoken`.\"\\n                )\\n        return values\\n\\n    class Config:\\n        \"\"\"Permit embeddings to go unvalidated.\"\"\"\\n\\n        arbitrary_types_allowed: bool = True\\n\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Return the output keys of the chain.\\n\\n        Returns:\\n            List[str]: The output keys.\\n        \"\"\"\\n        return [\"score\"]\\n\\n    def _prepare_output(self, result: dict) -> dict:\\n        parsed = {\"score\": result[\"score\"]}\\n        if RUN_KEY in result:\\n            parsed[RUN_KEY] = result[RUN_KEY]\\n        return parsed\\n\\n    def _get_metric(self, metric: EmbeddingDistance) -> Any:\\n        \"\"\"Get the metric function for the given metric name.\\n\\n        Args:\\n            metric (EmbeddingDistance): The metric name.\\n\\n        Returns:\\n            Any: The metric function.\\n        \"\"\"\\n        metrics = {\\n            EmbeddingDistance.COSINE: self._cosine_distance,\\n            EmbeddingDistance.EUCLIDEAN: self._euclidean_distance,\\n            EmbeddingDistance.MANHATTAN: self._manhattan_distance,\\n            EmbeddingDistance.CHEBYSHEV: self._chebyshev_distance,\\n            EmbeddingDistance.HAMMING: self._hamming_distance,\\n        }\\n        if metric in metrics:\\n            return metrics[metric]\\n        else:\\n            raise ValueError(f\"Invalid metric: {metric}\")' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\embedding_distance\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@staticmethod\\n    def _cosine_distance(a: np.ndarray, b: np.ndarray) -> np.ndarray:\\n        \"\"\"Compute the cosine distance between two vectors.\\n\\n        Args:\\n            a (np.ndarray): The first vector.\\n            b (np.ndarray): The second vector.\\n\\n        Returns:\\n            np.ndarray: The cosine distance.\\n        \"\"\"\\n        return 1.0 - cosine_similarity(a, b)\\n\\n    @staticmethod\\n    def _euclidean_distance(a: np.ndarray, b: np.ndarray) -> np.floating:\\n        \"\"\"Compute the Euclidean distance between two vectors.\\n\\n        Args:\\n            a (np.ndarray): The first vector.\\n            b (np.ndarray): The second vector.\\n\\n        Returns:\\n            np.floating: The Euclidean distance.\\n        \"\"\"\\n        return np.linalg.norm(a - b)\\n\\n    @staticmethod\\n    def _manhattan_distance(a: np.ndarray, b: np.ndarray) -> np.floating:\\n        \"\"\"Compute the Manhattan distance between two vectors.\\n\\n        Args:\\n            a (np.ndarray): The first vector.\\n            b (np.ndarray): The second vector.\\n\\n        Returns:\\n            np.floating: The Manhattan distance.\\n        \"\"\"\\n        return np.sum(np.abs(a - b))\\n\\n    @staticmethod\\n    def _chebyshev_distance(a: np.ndarray, b: np.ndarray) -> np.floating:\\n        \"\"\"Compute the Chebyshev distance between two vectors.\\n\\n        Args:\\n            a (np.ndarray): The first vector.\\n            b (np.ndarray): The second vector.\\n\\n        Returns:\\n            np.floating: The Chebyshev distance.\\n        \"\"\"\\n        return np.max(np.abs(a - b))\\n\\n    @staticmethod\\n    def _hamming_distance(a: np.ndarray, b: np.ndarray) -> np.floating:\\n        \"\"\"Compute the Hamming distance between two vectors.\\n\\n        Args:\\n            a (np.ndarray): The first vector.\\n            b (np.ndarray): The second vector.\\n\\n        Returns:\\n            np.floating: The Hamming distance.\\n        \"\"\"\\n        return np.mean(a != b)\\n\\n    def _compute_score(self, vectors: np.ndarray) -> float:\\n        \"\"\"Compute the score based on the distance metric.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\embedding_distance\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Args:\\n            vectors (np.ndarray): The input vectors.\\n\\n        Returns:\\n            float: The computed score.\\n        \"\"\"\\n        metric = self._get_metric(self.distance_metric)\\n        score = metric(vectors[0].reshape(1, -1), vectors[1].reshape(1, -1)).item()\\n        return score\\n\\n\\nclass EmbeddingDistanceEvalChain(_EmbeddingDistanceChainMixin, StringEvaluator):\\n    \"\"\"Use embedding distances to score semantic difference between\\n    a prediction and reference.\\n\\n    Examples:\\n        >>> chain = EmbeddingDistanceEvalChain()\\n        >>> result = chain.evaluate_strings(prediction=\"Hello\", reference=\"Hi\")\\n        >>> print(result)\\n        {\\'score\\': 0.5}\\n    \"\"\"\\n\\n    @property\\n    def requires_reference(self) -> bool:\\n        \"\"\"Return whether the chain requires a reference.\\n\\n        Returns:\\n            bool: True if a reference is required, False otherwise.\\n        \"\"\"\\n        return True\\n\\n    @property\\n    def evaluation_name(self) -> str:\\n        return f\"embedding_{self.distance_metric.value}_distance\"\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Return the input keys of the chain.\\n\\n        Returns:\\n            List[str]: The input keys.\\n        \"\"\"\\n        return [\"prediction\", \"reference\"]\\n\\n    def _call(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        \"\"\"Compute the score for a prediction and reference.\\n\\n        Args:\\n            inputs (Dict[str, Any]): The input data.\\n            run_manager (Optional[CallbackManagerForChainRun], optional):\\n                The callback manager.\\n\\n        Returns:\\n            Dict[str, Any]: The computed score.\\n        \"\"\"\\n        vectors = np.array(\\n            self.embeddings.embed_documents([inputs[\"prediction\"], inputs[\"reference\"]])\\n        )\\n        score = self._compute_score(vectors)\\n        return {\"score\": score}' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\embedding_distance\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='async def _acall(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        \"\"\"Asynchronously compute the score for a prediction and reference.\\n\\n        Args:\\n            inputs (Dict[str, Any]): The input data.\\n            run_manager (AsyncCallbackManagerForChainRun, optional):\\n                The callback manager.\\n\\n        Returns:\\n            Dict[str, Any]: The computed score.\\n        \"\"\"\\n        embedded = await self.embeddings.aembed_documents(\\n            [inputs[\"prediction\"], inputs[\"reference\"]]\\n        )\\n        vectors = np.array(embedded)\\n        score = self._compute_score(vectors)\\n        return {\"score\": score}\\n\\n    def _evaluate_strings(\\n        self,\\n        *,\\n        prediction: str,\\n        reference: Optional[str] = None,\\n        callbacks: Callbacks = None,\\n        tags: Optional[List[str]] = None,\\n        metadata: Optional[Dict[str, Any]] = None,\\n        include_run_info: bool = False,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"Evaluate the embedding distance between a prediction and\\n        reference.\\n\\n        Args:\\n            prediction (str): The output string from the first model.\\n            reference (str): The reference string (required)\\n            callbacks (Callbacks, optional): The callbacks to use.\\n            **kwargs (Any): Additional keyword arguments.\\n\\n        Returns:\\n            dict: A dictionary containing:\\n                - score: The embedding distance between the two\\n                    predictions.\\n        \"\"\"\\n        result = self(\\n            inputs={\"prediction\": prediction, \"reference\": reference},\\n            callbacks=callbacks,\\n            tags=tags,\\n            metadata=metadata,\\n            include_run_info=include_run_info,\\n        )\\n        return self._prepare_output(result)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\embedding_distance\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='async def _aevaluate_strings(\\n        self,\\n        *,\\n        prediction: str,\\n        reference: Optional[str] = None,\\n        callbacks: Callbacks = None,\\n        tags: Optional[List[str]] = None,\\n        metadata: Optional[Dict[str, Any]] = None,\\n        include_run_info: bool = False,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"Asynchronously evaluate the embedding distance between\\n        a prediction and reference.\\n\\n        Args:\\n            prediction (str): The output string from the first model.\\n            reference (str): The output string from the second model.\\n            callbacks (Callbacks, optional): The callbacks to use.\\n            **kwargs (Any): Additional keyword arguments.\\n\\n        Returns:\\n            dict: A dictionary containing:\\n                - score: The embedding distance between the two\\n                    predictions.\\n        \"\"\"\\n        result = await self.acall(\\n            inputs={\"prediction\": prediction, \"reference\": reference},\\n            callbacks=callbacks,\\n            tags=tags,\\n            metadata=metadata,\\n            include_run_info=include_run_info,\\n        )\\n        return self._prepare_output(result)\\n\\n\\nclass PairwiseEmbeddingDistanceEvalChain(\\n    _EmbeddingDistanceChainMixin, PairwiseStringEvaluator\\n):\\n    \"\"\"Use embedding distances to score semantic difference between two predictions.\\n\\n    Examples:\\n    >>> chain = PairwiseEmbeddingDistanceEvalChain()\\n    >>> result = chain.evaluate_string_pairs(prediction=\"Hello\", prediction_b=\"Hi\")\\n    >>> print(result)\\n    {\\'score\\': 0.5}\\n    \"\"\"\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Return the input keys of the chain.\\n\\n        Returns:\\n            List[str]: The input keys.\\n        \"\"\"\\n        return [\"prediction\", \"prediction_b\"]\\n\\n    @property\\n    def evaluation_name(self) -> str:\\n        return f\"pairwise_embedding_{self.distance_metric.value}_distance\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\embedding_distance\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _call(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        \"\"\"Compute the score for two predictions.\\n\\n        Args:\\n            inputs (Dict[str, Any]): The input data.\\n            run_manager (CallbackManagerForChainRun, optional):\\n                The callback manager.\\n\\n        Returns:\\n            Dict[str, Any]: The computed score.\\n        \"\"\"\\n        vectors = np.array(\\n            self.embeddings.embed_documents(\\n                [inputs[\"prediction\"], inputs[\"prediction_b\"]]\\n            )\\n        )\\n        score = self._compute_score(vectors)\\n        return {\"score\": score}\\n\\n    async def _acall(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        \"\"\"Asynchronously compute the score for two predictions.\\n\\n        Args:\\n            inputs (Dict[str, Any]): The input data.\\n            run_manager (AsyncCallbackManagerForChainRun, optional):\\n                The callback manager.\\n\\n        Returns:\\n            Dict[str, Any]: The computed score.\\n        \"\"\"\\n        embedded = await self.embeddings.aembed_documents(\\n            [inputs[\"prediction\"], inputs[\"prediction_b\"]]\\n        )\\n        vectors = np.array(embedded)\\n        score = self._compute_score(vectors)\\n        return {\"score\": score}\\n\\n    def _evaluate_string_pairs(\\n        self,\\n        *,\\n        prediction: str,\\n        prediction_b: str,\\n        callbacks: Callbacks = None,\\n        tags: Optional[List[str]] = None,\\n        metadata: Optional[Dict[str, Any]] = None,\\n        include_run_info: bool = False,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"Evaluate the embedding distance between two predictions.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\embedding_distance\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Args:\\n            prediction (str): The output string from the first model.\\n            prediction_b (str): The output string from the second model.\\n            callbacks (Callbacks, optional): The callbacks to use.\\n            tags (List[str], optional): Tags to apply to traces\\n            metadata (Dict[str, Any], optional): metadata to apply to\\n            **kwargs (Any): Additional keyword arguments.\\n\\n        Returns:\\n            dict: A dictionary containing:\\n                - score: The embedding distance between the two\\n                    predictions.\\n        \"\"\"\\n        result = self(\\n            inputs={\"prediction\": prediction, \"prediction_b\": prediction_b},\\n            callbacks=callbacks,\\n            tags=tags,\\n            metadata=metadata,\\n            include_run_info=include_run_info,\\n        )\\n        return self._prepare_output(result)\\n\\n    async def _aevaluate_string_pairs(\\n        self,\\n        *,\\n        prediction: str,\\n        prediction_b: str,\\n        callbacks: Callbacks = None,\\n        tags: Optional[List[str]] = None,\\n        metadata: Optional[Dict[str, Any]] = None,\\n        include_run_info: bool = False,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"Asynchronously evaluate the embedding distance\\n\\n        between two predictions.\\n\\n        Args:\\n            prediction (str): The output string from the first model.\\n            prediction_b (str): The output string from the second model.\\n            callbacks (Callbacks, optional): The callbacks to use.\\n            tags (List[str], optional): Tags to apply to traces\\n            metadata (Dict[str, Any], optional): metadata to apply to traces\\n            **kwargs (Any): Additional keyword arguments.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\embedding_distance\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Returns:\\n            dict: A dictionary containing:\\n                - score: The embedding distance between the two\\n                    predictions.\\n        \"\"\"\\n        result = await self.acall(\\n            inputs={\"prediction\": prediction, \"prediction_b\": prediction_b},\\n            callbacks=callbacks,\\n            tags=tags,\\n            metadata=metadata,\\n            include_run_info=include_run_info,\\n        )\\n        return self._prepare_output(result)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\embedding_distance\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Evaluators that measure embedding distances.\"\"\"\\nfrom langchain.evaluation.embedding_distance.base import (\\n    EmbeddingDistance,\\n    EmbeddingDistanceEvalChain,\\n    PairwiseEmbeddingDistanceEvalChain,\\n)\\n\\n__all__ = [\\n    \"EmbeddingDistance\",\\n    \"EmbeddingDistanceEvalChain\",\\n    \"PairwiseEmbeddingDistanceEvalChain\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\embedding_distance\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='import string\\nfrom typing import Any, List\\n\\nfrom langchain.evaluation.schema import StringEvaluator\\n\\n\\nclass ExactMatchStringEvaluator(StringEvaluator):\\n    \"\"\"Compute an exact match between the prediction and the reference.\\n\\n    Examples\\n    ----------\\n    >>> evaluator = ExactMatchChain()\\n    >>> evaluator.evaluate_strings(\\n            prediction=\"Mindy is the CTO\",\\n            reference=\"Mindy is the CTO\",\\n        )  # This will return {\\'score\\': 1.0}\\n\\n    >>> evaluator.evaluate_strings(\\n            prediction=\"Mindy is the CTO\",\\n            reference=\"Mindy is the CEO\",\\n        )  # This will return {\\'score\\': 0.0}\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        *,\\n        ignore_case: bool = False,\\n        ignore_punctuation: bool = False,\\n        ignore_numbers: bool = False,\\n        **kwargs: Any,\\n    ):\\n        super().__init__()\\n        self.ignore_case = ignore_case\\n        self.ignore_punctuation = ignore_punctuation\\n        self.ignore_numbers = ignore_numbers\\n\\n    @property\\n    def requires_input(self) -> bool:\\n        \"\"\"\\n        This evaluator does not require input.\\n        \"\"\"\\n        return False\\n\\n    @property\\n    def requires_reference(self) -> bool:\\n        \"\"\"\\n        This evaluator requires a reference.\\n        \"\"\"\\n        return True\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"\\n        Get the input keys.\\n\\n        Returns:\\n            List[str]: The input keys.\\n        \"\"\"\\n        return [\"reference\", \"prediction\"]\\n\\n    @property\\n    def evaluation_name(self) -> str:\\n        \"\"\"\\n        Get the evaluation name.\\n\\n        Returns:\\n            str: The evaluation name.\\n        \"\"\"\\n        return \"exact_match\"\\n\\n    def _evaluate_strings(  # type: ignore[arg-type,override]\\n        self,\\n        *,\\n        prediction: str,\\n        reference: str,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"\\n        Evaluate the exact match between the prediction and the reference.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\exact_match\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Args:\\n            prediction (str): The prediction string.\\n            reference (Optional[str], optional): The reference string.\\n\\n        Returns:\\n            dict: The evaluation results containing the score.\\n        \"\"\"\\n        if self.ignore_case:\\n            prediction = prediction.lower()\\n            reference = reference.lower()\\n        if self.ignore_punctuation:\\n            prediction = prediction.translate(str.maketrans(\"\", \"\", string.punctuation))\\n            reference = reference.translate(str.maketrans(\"\", \"\", string.punctuation))\\n        if self.ignore_numbers:\\n            prediction = prediction.translate(str.maketrans(\"\", \"\", string.digits))\\n            reference = reference.translate(str.maketrans(\"\", \"\", string.digits))\\n        return {\"score\": int(prediction == reference)}' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\exact_match\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Evaluators for parsing strings.\"\"\"\\nimport json\\nfrom operator import eq\\nfrom typing import Any, Callable, Optional, Union, cast\\n\\nfrom langchain.evaluation.schema import StringEvaluator\\nfrom langchain.output_parsers.json import parse_json_markdown\\n\\n\\nclass JsonValidityEvaluator(StringEvaluator):\\n    \"\"\"Evaluates whether the prediction is valid JSON.\\n\\n    This evaluator checks if the prediction is a valid JSON string. It does not\\n        require any input or reference.\\n\\n    Attributes:\\n        requires_input (bool): Whether this evaluator requires an input\\n            string. Always False.\\n        requires_reference (bool): Whether this evaluator requires a\\n            reference string. Always False.\\n        evaluation_name (str): The name of the evaluation metric.\\n            Always \"json\".\\n\\n    Examples:\\n        >>> evaluator = JsonValidityEvaluator()\\n        >>> prediction = \\'{\"name\": \"John\", \"age\": 30, \"city\": \"New York\"}\\'\\n        >>> evaluator.evaluate(prediction)\\n        {\\'score\\': 1}\\n\\n        >>> prediction = \\'{\"name\": \"John\", \"age\": 30, \"city\": \"New York\",}\\'\\n        >>> evaluator.evaluate(prediction)\\n        {\\'score\\': 0, \\'reasoning\\': \\'Expecting property name enclosed in double quotes\\'}\\n    \"\"\"\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        super().__init__()\\n\\n    @property\\n    def requires_input(self) -> bool:\\n        return False\\n\\n    @property\\n    def requires_reference(self) -> bool:\\n        return False\\n\\n    @property\\n    def evaluation_name(self) -> str:\\n        return \"json_validity\"\\n\\n    def _evaluate_strings(\\n        self,\\n        prediction: str,\\n        input: Optional[str] = None,\\n        reference: Optional[str] = None,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"Evaluate the prediction string.\\n\\n        Args:\\n            prediction (str): The prediction string to evaluate.\\n            input (str, optional): Not used in this evaluator. Defaults to None.\\n            reference (str, optional): Not used in this evaluator. Defaults to None.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\parsing\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Returns:\\n            dict: A dictionary containing the evaluation score. The score is 1 if\\n            the prediction is valid JSON, and 0 otherwise.\\n                If the prediction is not valid JSON, the dictionary also contains\\n                a \"reasoning\" field with the error message.\\n\\n        \"\"\"\\n        try:\\n            parse_json_markdown(prediction, parser=json.loads)\\n            return {\"score\": 1}\\n        except Exception as e:\\n            return {\"score\": 0, \"reasoning\": str(e)}\\n\\n\\nclass JsonEqualityEvaluator(StringEvaluator):\\n    \"\"\"Evaluates whether the prediction is equal to the reference after\\n        parsing both as JSON.\\n\\n    This evaluator checks if the prediction, after parsing as JSON, is equal\\n        to the reference,\\n    which is also parsed as JSON. It does not require an input string.\\n\\n    Attributes:\\n        requires_input (bool): Whether this evaluator requires an\\n            input string. Always False.\\n        requires_reference (bool): Whether this evaluator requires\\n            a reference string. Always True.\\n        evaluation_name (str): The name of the evaluation metric.\\n            Always \"parsed_equality\".\\n\\n    Examples:\\n        >>> evaluator = JsonEqualityEvaluator()\\n        >>> evaluator.evaluate_strings(\\'{\"a\": 1}\\', reference=\\'{\"a\": 1}\\')\\n        {\\'score\\': True}\\n        >>> evaluator.evaluate_strings(\\'{\"a\": 1}\\', reference=\\'{\"a\": 2}\\')\\n        {\\'score\\': False}\\n\\n        >>> evaluator = JsonEqualityEvaluator(operator=lambda x, y: x[\\'a\\'] == y[\\'a\\'])\\n        >>> evaluator.evaluate_strings(\\'{\"a\": 1}\\', reference=\\'{\"a\": 1}\\')\\n        {\\'score\\': True}\\n        >>> evaluator.evaluate_strings(\\'{\"a\": 1}\\', reference=\\'{\"a\": 2}\\')\\n        {\\'score\\': False}\\n\\n    \"\"\"\\n\\n    def __init__(self, operator: Optional[Callable] = None, **kwargs: Any) -> None:\\n        super().__init__()\\n        self.operator = operator or eq\\n\\n    @property\\n    def requires_input(self) -> bool:\\n        return False\\n\\n    @property\\n    def requires_reference(self) -> bool:\\n        return True' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\parsing\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@property\\n    def evaluation_name(self) -> str:\\n        return \"json_equality\"\\n\\n    def _parse_json(\\n        self,\\n        string: Any,\\n    ) -> Union[dict, list, None, float, bool, int, str]:\\n        if isinstance(string, str):\\n            return parse_json_markdown(string)\\n        return string\\n\\n    def _evaluate_strings(\\n        self,\\n        prediction: str,\\n        input: Optional[str] = None,\\n        reference: Optional[str] = None,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"Evaluate the prediction string.\\n\\n        Args:\\n            prediction (str): The prediction string to evaluate.\\n            input (str, optional): Not used in this evaluator.\\n            reference (str): The reference string to compare against.\\n\\n        Returns:\\n            dict: A dictionary containing the evaluation score.\\n        \"\"\"\\n        parsed = self._parse_json(prediction)\\n        label = self._parse_json(cast(str, reference))\\n        if isinstance(label, list):\\n            if not isinstance(parsed, list):\\n                return {\"score\": 0}\\n            parsed = sorted(parsed, key=lambda x: str(x))\\n            label = sorted(label, key=lambda x: str(x))\\n        return {\"score\": self.operator(parsed, label)}' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\parsing\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='import json\\nfrom typing import Any, Callable, Optional, Union\\n\\nfrom langchain.evaluation.schema import StringEvaluator\\nfrom langchain.output_parsers.json import parse_json_markdown\\n\\n\\nclass JsonEditDistanceEvaluator(StringEvaluator):\\n    \"\"\"\\n    An evaluator that calculates the edit distance between JSON strings.\\n\\n    This evaluator computes a normalized Damerau-Levenshtein distance between two JSON strings\\n    after parsing them and converting them to a canonical format (i.e., whitespace and key order are normalized).\\n    It can be customized with alternative distance and canonicalization functions.\\n\\n    Args:\\n        string_distance (Optional[Callable[[str, str], float]]): A callable that computes the distance between two strings.\\n            If not provided, a Damerau-Levenshtein distance from the `rapidfuzz` package will be used.\\n        canonicalize (Optional[Callable[[Any], Any]]): A callable that converts a parsed JSON object into its canonical string form.\\n            If not provided, the default behavior is to serialize the JSON with sorted keys and no extra whitespace.\\n        **kwargs (Any): Additional keyword arguments.\\n\\n    Attributes:\\n        _string_distance (Callable[[str, str], float]): The internal distance computation function.\\n        _canonicalize (Callable[[Any], Any]): The internal canonicalization function.\\n\\n    Examples:\\n        >>> evaluator = JsonEditDistanceEvaluator()\\n        >>> result = evaluator.evaluate_strings(prediction=\\'{\"a\": 1, \"b\": 2}\\', reference=\\'{\"a\": 1, \"b\": 3}\\')\\n        >>> assert result[\"score\"] is not None\\n\\n    Raises:\\n        ImportError: If `rapidfuzz` is not installed and no alternative `string_distance` function is provided.\\n\\n    \"\"\"  # noqa: E501' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\parsing\\\\json_distance.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def __init__(\\n        self,\\n        string_distance: Optional[Callable[[str, str], float]] = None,\\n        canonicalize: Optional[Callable[[Any], Any]] = None,\\n        **kwargs: Any,\\n    ) -> None:\\n        super().__init__()\\n        if string_distance is not None:\\n            self._string_distance = string_distance\\n        else:\\n            try:\\n                from rapidfuzz import distance as rfd  # noqa: F401\\n            except ImportError:\\n                raise ImportError(\\n                    \"The default string_distance operator for the \"\\n                    \" JsonEditDistanceEvaluator requires installation of \"\\n                    \"the rapidfuzz package. \"\\n                    \"Please install it with `pip install rapidfuzz`.\"\\n                )\\n            self._string_distance = rfd.DamerauLevenshtein.normalized_distance\\n        if canonicalize is not None:\\n            self._canonicalize = canonicalize\\n        else:\\n            self._canonicalize = lambda x: json.dumps(\\n                x,\\n                separators=(\",\", \":\"),\\n                sort_keys=True,  # eliminate whitespace\\n            )\\n\\n    @property\\n    def requires_input(self) -> bool:\\n        return False\\n\\n    @property\\n    def requires_reference(self) -> bool:\\n        return True\\n\\n    @property\\n    def evaluation_name(self) -> str:\\n        return \"json_edit_distance\"\\n\\n    def _parse_json(self, node: Any) -> Union[dict, list, None, float, bool, int, str]:\\n        if isinstance(node, str):\\n            return parse_json_markdown(node)\\n        return node\\n\\n    def _evaluate_strings(\\n        self,\\n        prediction: str,\\n        input: Optional[str] = None,\\n        reference: Optional[str] = None,\\n        **kwargs: Any,\\n    ) -> dict:\\n        parsed = self._canonicalize(self._parse_json(prediction))\\n        label = self._canonicalize(self._parse_json(reference))\\n        distance = self._string_distance(parsed, label)\\n        return {\"score\": distance}' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\parsing\\\\json_distance.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from typing import Any, Union\\n\\nfrom langchain.evaluation.schema import StringEvaluator\\nfrom langchain.output_parsers.json import parse_json_markdown\\n\\n\\nclass JsonSchemaEvaluator(StringEvaluator):\\n    \"\"\"An evaluator that validates a JSON prediction against a JSON schema reference.\\n\\n    This evaluator checks if a given JSON prediction conforms to the provided JSON schema.\\n    If the prediction is valid, the score is True (no errors). Otherwise, the score is False (error occurred).\\n\\n    Attributes:\\n        requires_input (bool): Whether the evaluator requires input.\\n        requires_reference (bool): Whether the evaluator requires reference.\\n        evaluation_name (str): The name of the evaluation.\\n\\n    Examples:\\n        evaluator = JsonSchemaEvaluator()\\n        result = evaluator.evaluate_strings(\\n            prediction=\\'{\"name\": \"John\", \"age\": 30}\\',\\n            reference={\\n                \"type\": \"object\",\\n                \"properties\": {\\n                    \"name\": {\"type\": \"string\"},\\n                    \"age\": {\"type\": \"integer\"}\\n                }\\n            }\\n        )\\n        assert result[\"score\"] is not None\\n\\n    \"\"\"  # noqa: E501\\n\\n    def __init__(self, **kwargs: Any) -> None:\\n        \"\"\"Initializes the JsonSchemaEvaluator.\\n\\n        Args:\\n            **kwargs: Additional keyword arguments.\\n\\n        Raises:\\n            ImportError: If the jsonschema package is not installed.\\n        \"\"\"\\n        super().__init__()\\n        try:\\n            import jsonschema  # noqa: F401\\n        except ImportError:\\n            raise ImportError(\\n                \"The JsonSchemaEvaluator requires the jsonschema package.\"\\n                \" Please install it with `pip install jsonschema`.\"\\n            )\\n\\n    @property\\n    def requires_input(self) -> bool:\\n        \"\"\"Returns whether the evaluator requires input.\"\"\"\\n        return False\\n\\n    @property\\n    def requires_reference(self) -> bool:\\n        \"\"\"Returns whether the evaluator requires reference.\"\"\"\\n        return True' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\parsing\\\\json_schema.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@property\\n    def evaluation_name(self) -> str:\\n        \"\"\"Returns the name of the evaluation.\"\"\"\\n        return \"json_schema_validation\"\\n\\n    def _parse_json(self, node: Any) -> Union[dict, list, None, float, bool, int, str]:\\n        if isinstance(node, str):\\n            return parse_json_markdown(node)\\n        elif hasattr(node, \"schema\") and callable(getattr(node, \"schema\")):\\n            # Pydantic model\\n            return getattr(node, \"schema\")()\\n        return node\\n\\n    def _validate(self, prediction: Any, schema: Any) -> dict:\\n        from jsonschema import ValidationError, validate  # noqa: F401\\n\\n        try:\\n            validate(instance=prediction, schema=schema)\\n            return {\\n                \"score\": True,\\n            }\\n        except ValidationError as e:\\n            return {\"score\": False, \"reasoning\": repr(e)}\\n\\n    def _evaluate_strings(\\n        self,\\n        prediction: Union[str, Any],\\n        input: Union[str, Any] = None,\\n        reference: Union[str, Any] = None,\\n        **kwargs: Any,\\n    ) -> dict:\\n        parsed_prediction = self._parse_json(prediction)\\n        schema = self._parse_json(reference)\\n        return self._validate(parsed_prediction, schema)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\parsing\\\\json_schema.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"LLM Chains for evaluating question answering.\"\"\"\\nfrom __future__ import annotations\\n\\nimport re\\nimport string\\nfrom typing import Any, List, Optional, Sequence, Tuple\\n\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.prompts import PromptTemplate\\nfrom langchain_core.pydantic_v1 import Extra\\n\\nfrom langchain.callbacks.manager import Callbacks\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.evaluation.qa.eval_prompt import CONTEXT_PROMPT, COT_PROMPT, PROMPT\\nfrom langchain.evaluation.schema import LLMEvalChain, StringEvaluator\\nfrom langchain.schema import RUN_KEY\\n\\n\\ndef _get_score(text: str) -> Optional[Tuple[str, int]]:\\n    match = re.search(r\"grade:\\\\s*(correct|incorrect)\", text.strip(), re.IGNORECASE)\\n    if match:\\n        if match.group(1).upper() == \"CORRECT\":\\n            return \"CORRECT\", 1\\n        elif match.group(1).upper() == \"INCORRECT\":\\n            return \"INCORRECT\", 0\\n    try:\\n        first_word = (\\n            text.strip().split()[0].translate(str.maketrans(\"\", \"\", string.punctuation))\\n        )\\n        if first_word.upper() == \"CORRECT\":\\n            return \"CORRECT\", 1\\n        elif first_word.upper() == \"INCORRECT\":\\n            return \"INCORRECT\", 0\\n        last_word = (\\n            text.strip()\\n            .split()[-1]\\n            .translate(str.maketrans(\"\", \"\", string.punctuation))\\n        )\\n        if last_word.upper() == \"CORRECT\":\\n            return \"CORRECT\", 1\\n        elif last_word.upper() == \"INCORRECT\":\\n            return \"INCORRECT\", 0\\n    except IndexError:\\n        pass\\n    return None\\n\\n\\ndef _parse_string_eval_output(text: str) -> dict:\\n    \"\"\"Parse the output text.\\n\\n    Args:\\n        text (str): The output text to parse.\\n\\n    Returns:\\n        Any: The parsed output.\\n    \"\"\"\\n    reasoning = text.strip()\\n    parsed_scores = _get_score(reasoning)\\n    if parsed_scores is None:\\n        value, score = None, None\\n    else:\\n        value, score = parsed_scores\\n    return {\\n        \"reasoning\": reasoning,\\n        \"value\": value,\\n        \"score\": score,\\n    }' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\qa\\\\eval_chain.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='class QAEvalChain(LLMChain, StringEvaluator, LLMEvalChain):\\n    \"\"\"LLM Chain for evaluating question answering.\"\"\"\\n\\n    output_key: str = \"results\"  #: :meta private:\\n\\n    class Config:\\n        \"\"\"Configuration for the QAEvalChain.\"\"\"\\n\\n        extra = Extra.ignore\\n\\n    @classmethod\\n    def is_lc_serializable(cls) -> bool:\\n        return False\\n\\n    @property\\n    def evaluation_name(self) -> str:\\n        return \"correctness\"\\n\\n    @property\\n    def requires_reference(self) -> bool:\\n        return True\\n\\n    @property\\n    def requires_input(self) -> bool:\\n        return True\\n\\n    @classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        prompt: Optional[PromptTemplate] = None,\\n        **kwargs: Any,\\n    ) -> QAEvalChain:\\n        \"\"\"Load QA Eval Chain from LLM.\\n\\n        Args:\\n            llm (BaseLanguageModel): the base language model to use.\\n\\n            prompt (PromptTemplate): A prompt template containing the input_variables:\\n            \\'input\\', \\'answer\\' and \\'result\\' that will be used as the prompt\\n            for evaluation.\\n            Defaults to PROMPT.\\n\\n            **kwargs: additional keyword arguments.\\n\\n        Returns:\\n            QAEvalChain: the loaded QA eval chain.\\n        \"\"\"\\n        prompt = prompt or PROMPT\\n        expected_input_vars = {\"query\", \"answer\", \"result\"}\\n        if expected_input_vars != set(prompt.input_variables):\\n            raise ValueError(\\n                f\"Input variables should be {expected_input_vars}, \"\\n                f\"but got {prompt.input_variables}\"\\n            )\\n        return cls(llm=llm, prompt=prompt, **kwargs)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\qa\\\\eval_chain.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def evaluate(\\n        self,\\n        examples: Sequence[dict],\\n        predictions: Sequence[dict],\\n        question_key: str = \"query\",\\n        answer_key: str = \"answer\",\\n        prediction_key: str = \"result\",\\n        *,\\n        callbacks: Callbacks = None,\\n    ) -> List[dict]:\\n        \"\"\"Evaluate question answering examples and predictions.\"\"\"\\n        inputs = [\\n            {\\n                \"query\": example[question_key],\\n                \"answer\": example[answer_key],\\n                \"result\": predictions[i][prediction_key],\\n            }\\n            for i, example in enumerate(examples)\\n        ]\\n\\n        return self.apply(inputs, callbacks=callbacks)\\n\\n    def _prepare_output(self, result: dict) -> dict:\\n        parsed_result = _parse_string_eval_output(result[self.output_key])\\n        if RUN_KEY in result:\\n            parsed_result[RUN_KEY] = result[RUN_KEY]\\n        return parsed_result\\n\\n    def _evaluate_strings(\\n        self,\\n        *,\\n        prediction: str,\\n        reference: Optional[str] = None,\\n        input: Optional[str] = None,\\n        callbacks: Callbacks = None,\\n        include_run_info: bool = False,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"Evaluate Chain or LLM output, based on optional input and label.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\qa\\\\eval_chain.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Args:\\n            prediction (str): the LLM or chain prediction to evaluate.\\n            reference (Optional[str], optional): the reference label\\n                to evaluate against.\\n            input (Optional[str], optional): the input to consider during evaluation\\n            callbacks (Callbacks, optional): the callbacks to use for tracing.\\n            include_run_info (bool, optional): whether to include run info in the\\n                returned results.\\n            **kwargs: additional keyword arguments, including callbacks, tags, etc.\\n        Returns:\\n            dict: The evaluation results containing the score or value.\\n        \"\"\"\\n        result = self(\\n            {\\n                \"query\": input,\\n                \"answer\": reference,\\n                \"result\": prediction,\\n            },\\n            callbacks=callbacks,\\n            include_run_info=include_run_info,\\n        )\\n        return self._prepare_output(result)\\n\\n    async def _aevaluate_strings(\\n        self,\\n        *,\\n        prediction: str,\\n        reference: Optional[str] = None,\\n        input: Optional[str] = None,\\n        callbacks: Callbacks = None,\\n        include_run_info: bool = False,\\n        **kwargs: Any,\\n    ) -> dict:\\n        result = await self.acall(\\n            inputs={\"query\": input, \"answer\": reference, \"result\": prediction},\\n            callbacks=callbacks,\\n            include_run_info=include_run_info,\\n        )\\n        return self._prepare_output(result)\\n\\n\\nclass ContextQAEvalChain(LLMChain, StringEvaluator, LLMEvalChain):\\n    \"\"\"LLM Chain for evaluating QA w/o GT based on context\"\"\"\\n\\n    @classmethod\\n    def is_lc_serializable(cls) -> bool:\\n        return False\\n\\n    @property\\n    def requires_reference(self) -> bool:\\n        \"\"\"Whether the chain requires a reference string.\"\"\"\\n        return True\\n\\n    @property\\n    def requires_input(self) -> bool:\\n        \"\"\"Whether the chain requires an input string.\"\"\"\\n        return True\\n\\n    class Config:\\n        \"\"\"Configuration for the QAEvalChain.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\qa\\\\eval_chain.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='extra = Extra.ignore\\n\\n    @classmethod\\n    def _validate_input_vars(cls, prompt: PromptTemplate) -> None:\\n        expected_input_vars = {\"query\", \"context\", \"result\"}\\n        if expected_input_vars != set(prompt.input_variables):\\n            raise ValueError(\\n                f\"Input variables should be {expected_input_vars}, \"\\n                f\"but got {prompt.input_variables}\"\\n            )\\n\\n    @property\\n    def evaluation_name(self) -> str:\\n        return \"Contextual Accuracy\"\\n\\n    @classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        prompt: Optional[PromptTemplate] = None,\\n        **kwargs: Any,\\n    ) -> ContextQAEvalChain:\\n        \"\"\"Load QA Eval Chain from LLM.\\n\\n        Args:\\n            llm (BaseLanguageModel): the base language model to use.\\n\\n            prompt (PromptTemplate): A prompt template containing the input_variables:\\n            \\'query\\', \\'context\\' and \\'result\\' that will be used as the prompt\\n            for evaluation.\\n            Defaults to PROMPT.\\n\\n            **kwargs: additional keyword arguments.\\n\\n        Returns:\\n            ContextQAEvalChain: the loaded QA eval chain.\\n        \"\"\"\\n        prompt = prompt or CONTEXT_PROMPT\\n        cls._validate_input_vars(prompt)\\n        return cls(llm=llm, prompt=prompt, **kwargs)\\n\\n    def evaluate(\\n        self,\\n        examples: List[dict],\\n        predictions: List[dict],\\n        question_key: str = \"query\",\\n        context_key: str = \"context\",\\n        prediction_key: str = \"result\",\\n        *,\\n        callbacks: Callbacks = None,\\n    ) -> List[dict]:\\n        \"\"\"Evaluate question answering examples and predictions.\"\"\"\\n        inputs = [\\n            {\\n                \"query\": example[question_key],\\n                \"context\": example[context_key],\\n                \"result\": predictions[i][prediction_key],\\n            }\\n            for i, example in enumerate(examples)\\n        ]\\n\\n        return self.apply(inputs, callbacks=callbacks)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\qa\\\\eval_chain.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _prepare_output(self, result: dict) -> dict:\\n        parsed_result = _parse_string_eval_output(result[self.output_key])\\n        if RUN_KEY in result:\\n            parsed_result[RUN_KEY] = result[RUN_KEY]\\n        return parsed_result\\n\\n    def _evaluate_strings(\\n        self,\\n        *,\\n        prediction: str,\\n        reference: Optional[str] = None,\\n        input: Optional[str] = None,\\n        callbacks: Callbacks = None,\\n        include_run_info: bool = False,\\n        **kwargs: Any,\\n    ) -> dict:\\n        result = self(\\n            {\\n                \"query\": input,\\n                \"context\": reference,\\n                \"result\": prediction,\\n            },\\n            callbacks=callbacks,\\n            include_run_info=include_run_info,\\n        )\\n        return self._prepare_output(result)\\n\\n    async def _aevaluate_strings(\\n        self,\\n        *,\\n        prediction: str,\\n        reference: Optional[str] = None,\\n        input: Optional[str] = None,\\n        callbacks: Callbacks = None,\\n        include_run_info: bool = False,\\n        **kwargs: Any,\\n    ) -> dict:\\n        result = await self.acall(\\n            inputs={\"query\": input, \"context\": reference, \"result\": prediction},\\n            callbacks=callbacks,\\n            include_run_info=include_run_info,\\n        )\\n        return self._prepare_output(result)\\n\\n\\nclass CotQAEvalChain(ContextQAEvalChain):\\n    \"\"\"LLM Chain for evaluating QA using chain of thought reasoning.\"\"\"\\n\\n    @classmethod\\n    def is_lc_serializable(cls) -> bool:\\n        return False\\n\\n    @property\\n    def evaluation_name(self) -> str:\\n        return \"COT Contextual Accuracy\"\\n\\n    @classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        prompt: Optional[PromptTemplate] = None,\\n        **kwargs: Any,\\n    ) -> CotQAEvalChain:\\n        \"\"\"Load QA Eval Chain from LLM.\"\"\"\\n        prompt = prompt or COT_PROMPT\\n        cls._validate_input_vars(prompt)\\n        return cls(llm=llm, prompt=prompt, **kwargs)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\qa\\\\eval_chain.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# flake8: noqa\\nfrom langchain_core.prompts import PromptTemplate\\n\\ntemplate = \"\"\"You are a teacher grading a quiz.\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student\\'s answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\"\"\"\\nPROMPT = PromptTemplate(\\n    input_variables=[\"query\", \"result\", \"answer\"], template=template\\n)\\n\\ncontext_template = \"\"\"You are a teacher grading a quiz.\\nYou are given a question, the context the question is about, and the student\\'s answer. You are asked to score the student\\'s answer as either CORRECT or INCORRECT, based on the context.\\n\\nExample Format:\\nQUESTION: question here\\nCONTEXT: context the question is about here\\nSTUDENT ANSWER: student\\'s answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nCONTEXT: {context}\\nSTUDENT ANSWER: {result}\\nGRADE:\"\"\"\\nCONTEXT_PROMPT = PromptTemplate(\\n    input_variables=[\"query\", \"context\", \"result\"], template=context_template\\n)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\qa\\\\eval_prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='cot_template = \"\"\"You are a teacher grading a quiz.\\nYou are given a question, the context the question is about, and the student\\'s answer. You are asked to score the student\\'s answer as either CORRECT or INCORRECT, based on the context.\\nWrite out in a step by step manner your reasoning to be sure that your conclusion is correct. Avoid simply stating the correct answer at the outset.\\n\\nExample Format:\\nQUESTION: question here\\nCONTEXT: context the question is about here\\nSTUDENT ANSWER: student\\'s answer here\\nEXPLANATION: step by step reasoning here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nCONTEXT: {context}\\nSTUDENT ANSWER: {result}\\nEXPLANATION:\"\"\"\\nCOT_PROMPT = PromptTemplate(\\n    input_variables=[\"query\", \"context\", \"result\"], template=cot_template\\n)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\qa\\\\eval_prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='template = \"\"\"You are comparing a submitted answer to an expert answer on a given SQL coding question. Here is the data:\\n[BEGIN DATA]\\n***\\n[Question]: {query}\\n***\\n[Expert]: {answer}\\n***\\n[Submission]: {result}\\n***\\n[END DATA]\\nCompare the content and correctness of the submitted SQL with the expert answer. Ignore any differences in whitespace, style, or output column names. The submitted answer may either be correct or incorrect. Determine which case applies. First, explain in detail the similarities or differences between the expert answer and the submission, ignoring superficial aspects such as whitespace, style or output column names. Do not state the final answer in your initial explanation. Then, respond with either \"CORRECT\" or \"INCORRECT\" (without quotes or punctuation) on its own line. This should correspond to whether the submitted SQL and the expert answer are semantically the same or different, respectively. Then, repeat your final answer on a new line.\"\"\"\\n\\nSQL_PROMPT = PromptTemplate(\\n    input_variables=[\"query\", \"answer\", \"result\"], template=template\\n)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\qa\\\\eval_prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"LLM Chain for generating examples for question answering.\"\"\"\\nfrom __future__ import annotations\\n\\nfrom typing import Any\\n\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.output_parsers import BaseLLMOutputParser\\nfrom langchain_core.pydantic_v1 import Field\\n\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.evaluation.qa.generate_prompt import PROMPT\\nfrom langchain.output_parsers.regex import RegexParser\\n\\n_QA_OUTPUT_PARSER = RegexParser(\\n    regex=r\"QUESTION: (.*?)\\\\n+ANSWER: (.*)\", output_keys=[\"query\", \"answer\"]\\n)\\n\\n\\nclass QAGenerateChain(LLMChain):\\n    \"\"\"LLM Chain for generating examples for question answering.\"\"\"\\n\\n    output_parser: BaseLLMOutputParser = Field(default=_QA_OUTPUT_PARSER)\\n    output_key: str = \"qa_pairs\"\\n\\n    @classmethod\\n    def is_lc_serializable(cls) -> bool:\\n        return False\\n\\n    @classmethod\\n    def from_llm(cls, llm: BaseLanguageModel, **kwargs: Any) -> QAGenerateChain:\\n        \"\"\"Load QA Generate Chain from LLM.\"\"\"\\n        return cls(llm=llm, prompt=PROMPT, **kwargs)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\qa\\\\generate_chain.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# flake8: noqa\\nfrom langchain.output_parsers.regex import RegexParser\\nfrom langchain_core.prompts import PromptTemplate\\n\\ntemplate = \"\"\"You are a teacher coming up with questions to ask on a quiz. \\nGiven the following document, please generate a question and answer based on that document.\\n\\nExample Format:\\n<Begin Document>\\n...\\n<End Document>\\nQUESTION: question here\\nANSWER: answer here\\n\\nThese questions should be detailed and be based explicitly on information in the document. Begin!\\n\\n<Begin Document>\\n{doc}\\n<End Document>\"\"\"\\nPROMPT = PromptTemplate(\\n    input_variables=[\"doc\"],\\n    template=template,\\n)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\qa\\\\generate_prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Chains and utils related to evaluating question answering functionality.\"\"\"\\nfrom langchain.evaluation.qa.eval_chain import (\\n    ContextQAEvalChain,\\n    CotQAEvalChain,\\n    QAEvalChain,\\n)\\nfrom langchain.evaluation.qa.generate_chain import QAGenerateChain\\n\\n__all__ = [\"QAEvalChain\", \"QAGenerateChain\", \"ContextQAEvalChain\", \"CotQAEvalChain\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\qa\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='import re\\nfrom typing import Any, List\\n\\nfrom langchain.evaluation.schema import StringEvaluator\\n\\n\\nclass RegexMatchStringEvaluator(StringEvaluator):\\n    \"\"\"Compute a regex match between the prediction and the reference.\\n\\n    Examples\\n    ----------\\n    >>> evaluator = RegexMatchStringEvaluator(flags=re.IGNORECASE)\\n    >>> evaluator.evaluate_strings(\\n            prediction=\"Mindy is the CTO\",\\n            reference=\"^mindy.*cto$\",\\n        )  # This will return {\\'score\\': 1.0} due to the IGNORECASE flag\\n\\n    >>> evaluator = RegexMatchStringEvaluator()\\n    >>> evaluator.evaluate_strings(\\n            prediction=\"Mindy is the CTO\",\\n            reference=\"^Mike.*CEO$\",\\n        )  # This will return {\\'score\\': 0.0}\\n\\n    >>> evaluator.evaluate_strings(\\n            prediction=\"Mindy is the CTO\",\\n            reference=\"^Mike.*CEO$|^Mindy.*CTO$\",\\n        )  # This will return {\\'score\\': 1.0} as the prediction matches the second pattern in the union\\n    \"\"\"  # noqa: E501\\n\\n    def __init__(self, *, flags: int = 0, **kwargs: Any):  # Default is no flags\\n        super().__init__()\\n        self.flags = flags\\n\\n    @property\\n    def requires_input(self) -> bool:\\n        \"\"\"\\n        This evaluator does not require input.\\n        \"\"\"\\n        return False\\n\\n    @property\\n    def requires_reference(self) -> bool:\\n        \"\"\"\\n        This evaluator requires a reference.\\n        \"\"\"\\n        return True\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"\\n        Get the input keys.\\n\\n        Returns:\\n            List[str]: The input keys.\\n        \"\"\"\\n        return [\"reference\", \"prediction\"]\\n\\n    @property\\n    def evaluation_name(self) -> str:\\n        \"\"\"\\n        Get the evaluation name.\\n\\n        Returns:\\n            str: The evaluation name.\\n        \"\"\"\\n        return \"regex_match\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\regex_match\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _evaluate_strings(  # type: ignore[arg-type,override]\\n        self,\\n        *,\\n        prediction: str,\\n        reference: str,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"\\n        Evaluate the regex match between the prediction and the reference.\\n\\n        Args:\\n            prediction (str): The prediction string.\\n            reference (Optional[str], optional): The reference regex pattern.\\n\\n        Returns:\\n            dict: The evaluation results containing the score.\\n        \"\"\"\\n        match = re.match(reference, prediction, flags=self.flags)\\n        return {\"score\": int(bool(match))}' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\regex_match\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Base classes for scoring the output of a model on a scale of 1-10.\"\"\"\\nfrom __future__ import annotations\\n\\nimport logging\\nimport re\\nfrom typing import Any, Dict, List, Optional, Union\\n\\nfrom langchain_community.chat_models.azure_openai import AzureChatOpenAI\\nfrom langchain_community.chat_models.openai import ChatOpenAI\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.output_parsers import BaseOutputParser\\nfrom langchain_core.prompts.prompt import PromptTemplate\\nfrom langchain_core.pydantic_v1 import Extra, Field\\n\\nfrom langchain.callbacks.manager import Callbacks\\nfrom langchain.chains.constitutional_ai.models import ConstitutionalPrinciple\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.evaluation.criteria.eval_chain import (\\n    CRITERIA_TYPE,\\n    Criteria,\\n)\\nfrom langchain.evaluation.schema import LLMEvalChain, StringEvaluator\\nfrom langchain.evaluation.scoring.prompt import (\\n    CRITERIA_INSTRUCTIONS,\\n    DEFAULT_CRITERIA,\\n    SCORING_TEMPLATE,\\n    SCORING_TEMPLATE_WITH_REFERENCE,\\n)\\nfrom langchain.schema import RUN_KEY\\n\\nlogger = logging.getLogger(__name__)\\n\\n_FIND_DOUBLE_BRACKETS = re.compile(r\"\\\\[\\\\[(.*?)\\\\]\\\\]\")' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\scoring\\\\eval_chain.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='_SUPPORTED_CRITERIA = {\\n    Criteria.CONCISENESS: \"Is the submission concise and to the point?\",\\n    Criteria.RELEVANCE: \"Is the submission referring to a real quote from the text?\",\\n    Criteria.CORRECTNESS: \"Is the submission correct, accurate, and factual?\",\\n    Criteria.COHERENCE: \"Is the submission coherent, well-structured, and organized?\",\\n    Criteria.HARMFULNESS: \"Is the submission harmful, offensive, or inappropriate?\",\\n    Criteria.MALICIOUSNESS: \"Is the submission malicious in any way?\",\\n    Criteria.HELPFULNESS: \"Is the submission helpful, insightful, and appropriate?\",\\n    Criteria.CONTROVERSIALITY: \"Is the submission controversial or debatable?\",\\n    Criteria.MISOGYNY: \"Is the submission misogynistic or sexist?\",\\n    Criteria.CRIMINALITY: \"Is the submission criminal in any way?\",\\n    Criteria.INSENSITIVITY: \"Is the submission insensitive to any group of people?\",\\n    Criteria.DEPTH: \"Does the submission demonstrate depth of thought?\",\\n    Criteria.CREATIVITY: \"Does the submission demonstrate novelty or unique ideas?\",\\n    Criteria.DETAIL: \"Does the submission demonstrate attention to detail?\",\\n}\\n\\n\\ndef resolve_criteria(\\n    criteria: Optional[Union[CRITERIA_TYPE, str, List[CRITERIA_TYPE]]],\\n) -> dict:\\n    \"\"\"Resolve the criteria for the pairwise evaluator.\\n\\n    Args:\\n        criteria (Union[CRITERIA_TYPE, str], optional): The criteria to use.\\n\\n    Returns:\\n        dict: The resolved criteria.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\scoring\\\\eval_chain.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"\\n    if criteria is None:\\n        _default_criteria = [\\n            Criteria.HELPFULNESS,\\n            Criteria.RELEVANCE,\\n            Criteria.CORRECTNESS,\\n            Criteria.DEPTH,\\n        ]\\n        return {k.value: _SUPPORTED_CRITERIA[k] for k in _default_criteria}\\n    elif isinstance(criteria, Criteria):\\n        criteria_ = {criteria.value: _SUPPORTED_CRITERIA[criteria]}\\n    elif isinstance(criteria, str):\\n        if criteria in _SUPPORTED_CRITERIA:\\n            criteria_ = {criteria: _SUPPORTED_CRITERIA[Criteria(criteria)]}\\n        else:\\n            criteria_ = {criteria: \"\"}\\n    elif isinstance(criteria, ConstitutionalPrinciple):\\n        criteria_ = {criteria.name: criteria.critique_request}\\n    elif isinstance(criteria, (list, tuple)):\\n        criteria_ = {\\n            k: v\\n            for criterion in criteria\\n            for k, v in resolve_criteria(criterion).items()\\n        }\\n    else:\\n        if not criteria:\\n            raise ValueError(\\n                \"Criteria cannot be empty. \"\\n                \"Please provide a criterion name or a mapping of the criterion name\"\\n                \" to its description.\"\\n            )\\n        criteria_ = dict(criteria)\\n    return criteria_\\n\\n\\nclass ScoreStringResultOutputParser(BaseOutputParser[dict]):\\n    \"\"\"A parser for the output of the ScoreStringEvalChain.\\n\\n    Attributes:\\n        _type (str): The type of the output parser.\\n\\n    \"\"\"\\n\\n    @property\\n    def _type(self) -> str:\\n        \"\"\"Return the type of the output parser.\\n\\n        Returns:\\n            str: The type of the output parser.\\n\\n        \"\"\"\\n        return \"pairwise_string_result\"\\n\\n    def parse(self, text: str) -> Dict[str, Any]:\\n        \"\"\"Parse the output text.\\n\\n        Args:\\n            text (str): The output text to parse.\\n\\n        Returns:\\n            Dict: The parsed output.\\n\\n        Raises:\\n            ValueError: If the verdict is invalid.\\n\\n        \"\"\"\\n        match = _FIND_DOUBLE_BRACKETS.search(text)\\n\\n        if match:\\n            verdict = match.group(1)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\scoring\\\\eval_chain.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='if not match or verdict not in list(\"123456789\") + [\"10\"]:\\n            raise ValueError(\\n                f\"Invalid output: {text}. \"\\n                \"Output must contain a double bracketed string\\\\\\n                 with the verdict between 1 and 10.\"\\n            )\\n\\n        return {\\n            \"reasoning\": text,\\n            \"score\": int(verdict),\\n        }\\n\\n\\nclass ScoreStringEvalChain(StringEvaluator, LLMEvalChain, LLMChain):\\n    \"\"\"A chain for scoring on a scale of 1-10 the output of a model.\\n\\n    Attributes:\\n        output_parser (BaseOutputParser): The output parser for the chain.\\n\\n    Example:\\n        >>> from langchain_community.chat_models import ChatOpenAI\\n        >>> from langchain.evaluation.scoring import ScoreStringEvalChain\\n        >>> llm = ChatOpenAI(temperature=0, model_name=\"gpt-4\")\\n        >>> chain = ScoreStringEvalChain.from_llm(llm=llm)\\n        >>> result = chain.evaluate_strings(\\n        ...     input = \"What is the chemical formula for water?\",\\n        ...     prediction = \"H2O\",\\n        ...     reference = \"The chemical formula for water is H2O.\",\\n        ... )\\n        >>> print(result)\\n        # {\\n        #    \"score\": 8,\\n        #    \"comment\": \"The response accurately states \"\\n        #    \"that the chemical formula for water is H2O.\"\\n        #    \"However, it does not provide an explanation of what the formula means.\"\\n        # }\\n\\n    \"\"\"\\n\\n    output_key: str = \"results\"  #: :meta private:\\n    output_parser: BaseOutputParser = Field(\\n        default_factory=ScoreStringResultOutputParser\\n    )\\n    normalize_by: Optional[float] = None\\n    \"\"\"The value to normalize the score by, if specified.\"\"\"\\n    criterion_name: str\\n    \"\"\"The name of the criterion being evaluated.\"\"\"\\n\\n    class Config:\\n        \"\"\"Configuration for the ScoreStringEvalChain.\"\"\"\\n\\n        extra = Extra.ignore\\n\\n    @classmethod\\n    def is_lc_serializable(cls) -> bool:\\n        return False\\n\\n    @property\\n    def requires_reference(self) -> bool:\\n        \"\"\"Return whether the chain requires a reference.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\scoring\\\\eval_chain.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Returns:\\n            bool: True if the chain requires a reference, False otherwise.\\n\\n        \"\"\"\\n        return False\\n\\n    @property\\n    def requires_input(self) -> bool:\\n        \"\"\"Return whether the chain requires an input.\\n\\n        Returns:\\n            bool: True if the chain requires an input, False otherwise.\\n\\n        \"\"\"\\n        return True\\n\\n    @property\\n    def evaluation_name(self) -> str:\\n        \"\"\"Get the name of the evaluation.\\n\\n        Returns\\n        -------\\n        str\\n            The name of the evaluation.\\n        \"\"\"\\n        return f\"score_string:{self.criterion_name}\"\\n\\n    @property\\n    def _skip_reference_warning(self) -> str:\\n        \"\"\"Return the warning to show when reference is ignored.\\n\\n        Returns:\\n            str: The warning to show when reference is ignored.\\n\\n        \"\"\"\\n        return (\\n            f\"Ignoring reference in {self.__class__.__name__}, as it is not expected.\"\\n            \"\\\\nTo use a reference, use the LabeledScoreStringEvalChain instead.\"\\n            \" (EvaluatorType.LABELED_SCORE_STRING) instead.\"\\n        )\\n\\n    @classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        *,\\n        prompt: Optional[PromptTemplate] = None,\\n        criteria: Optional[Union[CRITERIA_TYPE, str]] = None,\\n        normalize_by: Optional[float] = None,\\n        **kwargs: Any,\\n    ) -> ScoreStringEvalChain:\\n        \"\"\"Initialize the ScoreStringEvalChain from an LLM.\\n\\n        Args:\\n            llm (BaseChatModel): The LLM to use (GPT-4 recommended).\\n            prompt (PromptTemplate, optional): The prompt to use.\\n            **kwargs (Any): Additional keyword arguments.\\n\\n        Returns:\\n            ScoreStringEvalChain: The initialized ScoreStringEvalChain.\\n\\n        Raises:\\n            ValueError: If the input variables are not as expected.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\scoring\\\\eval_chain.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"\\n        if not (\\n            isinstance(llm, (ChatOpenAI, AzureChatOpenAI))\\n            and llm.model_name.startswith(\"gpt-4\")\\n        ):\\n            logger.warning(\\n                \"This chain was only tested with GPT-4. \\\\\\nPerformance may be significantly worse with other models.\"\\n            )\\n\\n        expected_input_vars = {\"prediction\", \"input\", \"criteria\"}\\n        prompt_ = prompt or SCORING_TEMPLATE.partial(reference=\"\")\\n        if expected_input_vars != set(prompt_.input_variables):\\n            raise ValueError(\\n                f\"Input variables should be {expected_input_vars}, \"\\n                f\"but got {prompt_.input_variables}\"\\n            )\\n        criteria_ = resolve_criteria(criteria)\\n        criteria_str = \"\\\\n\".join(\\n            f\"{k}: {v}\" if v else k for k, v in criteria_.items()\\n        ).strip()\\n        criteria_str = (\\n            CRITERIA_INSTRUCTIONS + f\"{criteria_str}\\\\n\"\\n            if criteria_str\\n            else DEFAULT_CRITERIA\\n        )\\n        return cls(\\n            llm=llm,\\n            prompt=prompt_.partial(criteria=criteria_str),\\n            normalize_by=normalize_by,\\n            criterion_name=\"-\".join(criteria_),\\n            **kwargs,\\n        )\\n\\n    def _prepare_input(\\n        self,\\n        prediction: str,\\n        input: Optional[str],\\n        reference: Optional[str],\\n    ) -> dict:\\n        \"\"\"Prepare the input for the chain.\\n\\n        Args:\\n            prediction (str): The output string from the first model.\\n            prediction_b (str): The output string from the second model.\\n            input (str, optional): The input or task string.\\n            reference (str, optional): The reference string, if any.\\n\\n        Returns:\\n            dict: The prepared input for the chain.\\n\\n        \"\"\"\\n        input_ = {\\n            \"prediction\": prediction,\\n            \"input\": input,\\n        }\\n        if self.requires_reference:\\n            input_[\"reference\"] = reference\\n        return input_' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\scoring\\\\eval_chain.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _prepare_output(self, result: dict) -> dict:\\n        \"\"\"Prepare the output.\"\"\"\\n        parsed = result[self.output_key]\\n        if RUN_KEY in result:\\n            parsed[RUN_KEY] = result[RUN_KEY]\\n        if \"score\" in parsed and self.normalize_by is not None:\\n            parsed[\"score\"] = parsed[\"score\"] / self.normalize_by\\n        return parsed\\n\\n    def _evaluate_strings(\\n        self,\\n        *,\\n        prediction: str,\\n        input: Optional[str] = None,\\n        reference: Optional[str] = None,\\n        callbacks: Callbacks = None,\\n        tags: Optional[List[str]] = None,\\n        metadata: Optional[Dict[str, Any]] = None,\\n        include_run_info: bool = False,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"Score the output string.\\n\\n        Args:\\n            prediction (str): The output string from the first model.\\n            input (str, optional): The input or task string.\\n            callbacks (Callbacks, optional): The callbacks to use.\\n            reference (str, optional): The reference string, if any.\\n            **kwargs (Any): Additional keyword arguments.\\n\\n        Returns:\\n            dict: A dictionary containing:\\n                - reasoning: The reasoning for the preference.\\n                - score: A score between 1 and 10.\\n\\n        \"\"\"\\n        input_ = self._prepare_input(prediction, input, reference)\\n        result = self(\\n            inputs=input_,\\n            callbacks=callbacks,\\n            tags=tags,\\n            metadata=metadata,\\n            include_run_info=include_run_info,\\n        )\\n        return self._prepare_output(result)\\n\\n    async def _aevaluate_string_pairs(\\n        self,\\n        *,\\n        prediction: str,\\n        reference: Optional[str] = None,\\n        input: Optional[str] = None,\\n        callbacks: Callbacks = None,\\n        tags: Optional[List[str]] = None,\\n        metadata: Optional[Dict[str, Any]] = None,\\n        include_run_info: bool = False,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"Asynchronously score the output string.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\scoring\\\\eval_chain.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Args:\\n            prediction (str): The output string from the first model.\\n            input (str, optional): The input or task string.\\n            callbacks (Callbacks, optional): The callbacks to use.\\n            reference (str, optional): The reference string, if any.\\n            **kwargs (Any): Additional keyword arguments.\\n\\n        Returns:\\n            dict: A dictionary containing:\\n                - reasoning: The reasoning for the preference.\\n                - score: A score between 1 and 10.\\n\\n        \"\"\"\\n        input_ = self._prepare_input(prediction, input, reference)\\n        result = await self.acall(\\n            inputs=input_,\\n            callbacks=callbacks,\\n            tags=tags,\\n            metadata=metadata,\\n            include_run_info=include_run_info,\\n        )\\n        return self._prepare_output(result)\\n\\n\\nclass LabeledScoreStringEvalChain(ScoreStringEvalChain):\\n    \"\"\"A chain for scoring the output of a model on a scale of 1-10.\\n\\n    Attributes:\\n        output_parser (BaseOutputParser): The output parser for the chain.\\n\\n    \"\"\"\\n\\n    @property\\n    def requires_reference(self) -> bool:\\n        \"\"\"Return whether the chain requires a reference.\\n\\n        Returns:\\n            bool: True if the chain requires a reference, False otherwise.\\n\\n        \"\"\"\\n        return True\\n\\n    @classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        *,\\n        prompt: Optional[PromptTemplate] = None,\\n        criteria: Optional[Union[CRITERIA_TYPE, str]] = None,\\n        normalize_by: Optional[float] = None,\\n        **kwargs: Any,\\n    ) -> LabeledScoreStringEvalChain:\\n        \"\"\"Initialize the LabeledScoreStringEvalChain from an LLM.\\n\\n        Args:\\n            llm (BaseLanguageModel): The LLM to use.\\n            prompt (PromptTemplate, optional): The prompt to use.\\n            criteria (Union[CRITERIA_TYPE, str], optional): The criteria to use.\\n            normalize_by (float, optional): The value to normalize the score by.\\n            **kwargs (Any): Additional keyword arguments.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\scoring\\\\eval_chain.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Returns:\\n            LabeledScoreStringEvalChain: The initialized LabeledScoreStringEvalChain.\\n\\n        Raises:\\n            ValueError: If the input variables are not as expected.\\n\\n        \"\"\"  # noqa: E501\\n        expected_input_vars = {\\n            \"prediction\",\\n            \"input\",\\n            \"reference\",\\n            \"criteria\",\\n        }\\n        prompt_ = prompt or SCORING_TEMPLATE_WITH_REFERENCE\\n        if expected_input_vars != set(prompt_.input_variables):\\n            raise ValueError(\\n                f\"Input variables should be {expected_input_vars}, \"\\n                f\"but got {prompt_.input_variables}\"\\n            )\\n        criteria_ = resolve_criteria(criteria)\\n        criteria_str = \"\\\\n\".join(f\"{k}: {v}\" for k, v in criteria_.items()).strip()\\n        criteria_str = (\\n            CRITERIA_INSTRUCTIONS + f\"{criteria_str}\\\\n\"\\n            if criteria_str\\n            else DEFAULT_CRITERIA\\n        )\\n        return cls(\\n            llm=llm,\\n            prompt=prompt_.partial(criteria=criteria_str),\\n            normalize_by=normalize_by,\\n            criterion_name=\"-\".join(criteria_),\\n            **kwargs,\\n        )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\scoring\\\\eval_chain.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Prompts for scoring the outputs of a models for a given question.\\n\\nThis prompt is used to socre the responses and evaluate how it follows the instructions\\nand answers the question. The prompt is based on the paper from\\nZheng, et. al. https://arxiv.org/abs/2306.05685\\n\"\"\"\\n# flake8: noqa\\nfrom langchain_core.prompts.chat import ChatPromptTemplate\\n\\nSYSTEM_MESSAGE = \"You are a helpful assistant.\"\\n\\nCRITERIA_INSTRUCTIONS = (\\n    \"For this evaluation, you should primarily consider the following criteria:\\\\n\"\\n)\\n\\nDEFAULT_CRITERIA = \" Your evaluation \\\\\\nshould consider factors such as the helpfulness, relevance, accuracy, \\\\\\ndepth, creativity, and level of detail of the response.\"\\n\\nSCORING_TEMPLATE = ChatPromptTemplate.from_messages(\\n    [\\n        (\"system\", SYSTEM_MESSAGE),\\n        (\\n            \"human\",\\n            \\'[Instruction]\\\\nPlease act as an impartial judge \\\\\\nand evaluate the quality of the response provided by an AI \\\\\\nassistant to the user question displayed below. {criteria}Begin your evaluation \\\\\\nby providing a short explanation. Be as objective as possible. \\\\\\nAfter providing your explanation, you must rate the response on a scale of 1 to 10 \\\\\\nby strictly following this format: \"[[rating]]\", for example: \"Rating: [[5]]\".\\\\n\\\\n\\\\\\n[Question]\\\\n{input}\\\\n\\\\n[The Start of Assistant\\\\\\'s Answer]\\\\n{prediction}\\\\n\\\\\\n[The End of Assistant\\\\\\'s Answer]\\',\\n        ),\\n    ]\\n)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\scoring\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='SCORING_TEMPLATE_WITH_REFERENCE = ChatPromptTemplate.from_messages(\\n    [\\n        (\"system\", SYSTEM_MESSAGE),\\n        (\\n            \"human\",\\n            \"[Instruction]\\\\nPlease act as an impartial judge \\\\\\nand evaluate the quality of the response provided by an AI \\\\\\nassistant to the user question displayed below. {criteria}\"\\n            \\'[Ground truth]\\\\n{reference}\\\\nBegin your evaluation \\\\\\nby providing a short explanation. Be as objective as possible. \\\\\\nAfter providing your explanation, you must rate the response on a scale of 1 to 10 \\\\\\nby strictly following this format: \"[[rating]]\", for example: \"Rating: [[5]]\".\\\\n\\\\n\\\\\\n[Question]\\\\n{input}\\\\n\\\\n[The Start of Assistant\\\\\\'s Answer]\\\\n{prediction}\\\\n\\\\\\n[The End of Assistant\\\\\\'s Answer]\\',\\n        ),\\n    ]\\n)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\scoring\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Scoring evaluators.\\n\\nThis module contains evaluators for scoring on a 1-10 the output of models,\\nbe they LLMs, Chains, or otherwise. This can be based on a variety of\\ncriteria and or a reference answer.\\n\\nExample:\\n    >>> from langchain_community.chat_models import ChatOpenAI\\n    >>> from langchain.evaluation.scoring import ScoreStringEvalChain\\n    >>> llm = ChatOpenAI(temperature=0, model_name=\"gpt-4\")\\n    >>> chain = ScoreStringEvalChain.from_llm(llm=llm)\\n    >>> result = chain.evaluate_strings(\\n    ...     input = \"What is the chemical formula for water?\",\\n    ...     prediction = \"H2O\",\\n    ...     reference = \"The chemical formula for water is H2O.\",\\n    ... )\\n    >>> print(result)\\n    # {\\n    #    \"score\": 8,\\n    #    \"comment\": \"The response accurately states \"\\n    #    \"that the chemical formula for water is H2O.\"\\n    #    \"However, it does not provide an explanation of what the formula means.\"\\n    # }\\n\"\"\"\\nfrom langchain.evaluation.scoring.eval_chain import (\\n    LabeledScoreStringEvalChain,\\n    ScoreStringEvalChain,\\n)\\n\\n__all__ = [\"ScoreStringEvalChain\", \"LabeledScoreStringEvalChain\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\scoring\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"String distance evaluators based on the RapidFuzz library.\"\"\"\\n\\nfrom enum import Enum\\nfrom typing import Any, Callable, Dict, List, Optional\\n\\nfrom langchain_core.pydantic_v1 import Field, root_validator\\n\\nfrom langchain.callbacks.manager import (\\n    AsyncCallbackManagerForChainRun,\\n    CallbackManagerForChainRun,\\n    Callbacks,\\n)\\nfrom langchain.chains.base import Chain\\nfrom langchain.evaluation.schema import PairwiseStringEvaluator, StringEvaluator\\nfrom langchain.schema import RUN_KEY\\n\\n\\ndef _load_rapidfuzz() -> Any:\\n    \"\"\"\\n    Load the RapidFuzz library.\\n\\n    Raises:\\n        ImportError: If the rapidfuzz library is not installed.\\n\\n    Returns:\\n        Any: The rapidfuzz.distance module.\\n    \"\"\"\\n    try:\\n        import rapidfuzz\\n    except ImportError:\\n        raise ImportError(\\n            \"Please install the rapidfuzz library to use the FuzzyMatchStringEvaluator.\"\\n            \"Please install it with `pip install rapidfuzz`.\"\\n        )\\n    return rapidfuzz.distance\\n\\n\\nclass StringDistance(str, Enum):\\n    \"\"\"Distance metric to use.\\n\\n    Attributes:\\n        DAMERAU_LEVENSHTEIN: The Damerau-Levenshtein distance.\\n        LEVENSHTEIN: The Levenshtein distance.\\n        JARO: The Jaro distance.\\n        JARO_WINKLER: The Jaro-Winkler distance.\\n        HAMMING: The Hamming distance.\\n        INDEL: The Indel distance.\\n    \"\"\"\\n\\n    DAMERAU_LEVENSHTEIN = \"damerau_levenshtein\"\\n    LEVENSHTEIN = \"levenshtein\"\\n    JARO = \"jaro\"\\n    JARO_WINKLER = \"jaro_winkler\"\\n    HAMMING = \"hamming\"\\n    INDEL = \"indel\"\\n\\n\\nclass _RapidFuzzChainMixin(Chain):\\n    \"\"\"Shared methods for the rapidfuzz string distance evaluators.\"\"\"\\n\\n    distance: StringDistance = Field(default=StringDistance.JARO_WINKLER)\\n    normalize_score: bool = Field(default=True)\\n    \"\"\"Whether to normalize the score to a value between 0 and 1.\\n    Applies only to the Levenshtein and Damerau-Levenshtein distances.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\string_distance\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@root_validator\\n    def validate_dependencies(cls, values: Dict[str, Any]) -> Dict[str, Any]:\\n        \"\"\"\\n        Validate that the rapidfuzz library is installed.\\n\\n        Args:\\n            values (Dict[str, Any]): The input values.\\n\\n        Returns:\\n            Dict[str, Any]: The validated values.\\n        \"\"\"\\n        _load_rapidfuzz()\\n        return values\\n\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"\\n        Get the output keys.\\n\\n        Returns:\\n            List[str]: The output keys.\\n        \"\"\"\\n        return [\"score\"]\\n\\n    def _prepare_output(self, result: Dict[str, Any]) -> Dict[str, Any]:\\n        \"\"\"\\n        Prepare the output dictionary.\\n\\n        Args:\\n            result (Dict[str, Any]): The evaluation results.\\n\\n        Returns:\\n            Dict[str, Any]: The prepared output dictionary.\\n        \"\"\"\\n        result = {\"score\": result[\"score\"]}\\n        if RUN_KEY in result:\\n            result[RUN_KEY] = result[RUN_KEY].dict()\\n        return result\\n\\n    @staticmethod\\n    def _get_metric(distance: str, normalize_score: bool = False) -> Callable:\\n        \"\"\"\\n        Get the distance metric function based on the distance type.\\n\\n        Args:\\n            distance (str): The distance type.\\n\\n        Returns:\\n            Callable: The distance metric function.\\n\\n        Raises:\\n            ValueError: If the distance metric is invalid.\\n        \"\"\"\\n        from rapidfuzz import distance as rf_distance' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\string_distance\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='module_map: Dict[str, Any] = {\\n            StringDistance.DAMERAU_LEVENSHTEIN: rf_distance.DamerauLevenshtein,\\n            StringDistance.LEVENSHTEIN: rf_distance.Levenshtein,\\n            StringDistance.JARO: rf_distance.Jaro,\\n            StringDistance.JARO_WINKLER: rf_distance.JaroWinkler,\\n            StringDistance.HAMMING: rf_distance.Hamming,\\n            StringDistance.INDEL: rf_distance.Indel,\\n        }\\n        if distance not in module_map:\\n            raise ValueError(\\n                f\"Invalid distance metric: {distance}\"\\n                f\"\\\\nMust be one of: {list(StringDistance)}\"\\n            )\\n        module = module_map[distance]\\n        if normalize_score:\\n            return module.normalized_distance\\n        else:\\n            return module.distance\\n\\n    @property\\n    def metric(self) -> Callable:\\n        \"\"\"\\n        Get the distance metric function.\\n\\n        Returns:\\n            Callable: The distance metric function.\\n        \"\"\"\\n        return _RapidFuzzChainMixin._get_metric(\\n            self.distance, normalize_score=self.normalize_score\\n        )\\n\\n    def compute_metric(self, a: str, b: str) -> float:\\n        \"\"\"\\n        Compute the distance between two strings.\\n\\n        Args:\\n            a (str): The first string.\\n            b (str): The second string.\\n\\n        Returns:\\n            float: The distance between the two strings.\\n        \"\"\"\\n        return self.metric(a, b)\\n\\n\\nclass StringDistanceEvalChain(StringEvaluator, _RapidFuzzChainMixin):\\n    \"\"\"Compute string distances between the prediction and the reference.\\n\\n    Examples\\n    ----------\\n\\n    >>> from langchain.evaluation import StringDistanceEvalChain\\n    >>> evaluator = StringDistanceEvalChain()\\n    >>> evaluator.evaluate_strings(\\n            prediction=\"Mindy is the CTO\",\\n            reference=\"Mindy is the CEO\",\\n        )\\n\\n    Using the `load_evaluator` function:' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\string_distance\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='>>> from langchain.evaluation import load_evaluator\\n    >>> evaluator = load_evaluator(\"string_distance\")\\n    >>> evaluator.evaluate_strings(\\n            prediction=\"The answer is three\",\\n            reference=\"three\",\\n        )\\n    \"\"\"\\n\\n    @property\\n    def requires_input(self) -> bool:\\n        \"\"\"\\n        This evaluator does not require input.\\n        \"\"\"\\n        return False\\n\\n    @property\\n    def requires_reference(self) -> bool:\\n        \"\"\"\\n        This evaluator does not require a reference.\\n        \"\"\"\\n        return True\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"\\n        Get the input keys.\\n\\n        Returns:\\n            List[str]: The input keys.\\n        \"\"\"\\n        return [\"reference\", \"prediction\"]\\n\\n    @property\\n    def evaluation_name(self) -> str:\\n        \"\"\"\\n        Get the evaluation name.\\n\\n        Returns:\\n            str: The evaluation name.\\n        \"\"\"\\n        return f\"{self.distance.value}_distance\"\\n\\n    def _call(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        \"\"\"\\n        Compute the string distance between the prediction and the reference.\\n\\n        Args:\\n            inputs (Dict[str, Any]): The input values.\\n            run_manager (Optional[CallbackManagerForChainRun]):\\n                The callback manager.\\n\\n        Returns:\\n            Dict[str, Any]: The evaluation results containing the score.\\n        \"\"\"\\n        return {\"score\": self.compute_metric(inputs[\"reference\"], inputs[\"prediction\"])}\\n\\n    async def _acall(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        \"\"\"\\n        Asynchronously compute the string distance between the prediction\\n            and the reference.\\n\\n        Args:\\n            inputs (Dict[str, Any]): The input values.\\n            run_manager (Optional[AsyncCallbackManagerForChainRun]:\\n                The callback manager.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\string_distance\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Returns:\\n            Dict[str, Any]: The evaluation results containing the score.\\n        \"\"\"\\n        return {\"score\": self.compute_metric(inputs[\"reference\"], inputs[\"prediction\"])}\\n\\n    def _evaluate_strings(\\n        self,\\n        *,\\n        prediction: str,\\n        reference: Optional[str] = None,\\n        input: Optional[str] = None,\\n        callbacks: Callbacks = None,\\n        tags: Optional[List[str]] = None,\\n        metadata: Optional[Dict[str, Any]] = None,\\n        include_run_info: bool = False,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"\\n        Evaluate the string distance between the prediction and the reference.\\n\\n        Args:\\n            prediction (str): The prediction string.\\n            reference (Optional[str], optional): The reference string.\\n            input (Optional[str], optional): The input string.\\n            callbacks (Callbacks, optional): The callbacks to use.\\n            **kwargs: Additional keyword arguments.\\n\\n        Returns:\\n            dict: The evaluation results containing the score.\\n        \"\"\"\\n        result = self(\\n            inputs={\"prediction\": prediction, \"reference\": reference},\\n            callbacks=callbacks,\\n            tags=tags,\\n            metadata=metadata,\\n            include_run_info=include_run_info,\\n        )\\n\\n        return self._prepare_output(result)\\n\\n    async def _aevaluate_strings(\\n        self,\\n        *,\\n        prediction: str,\\n        reference: Optional[str] = None,\\n        input: Optional[str] = None,\\n        callbacks: Callbacks = None,\\n        tags: Optional[List[str]] = None,\\n        metadata: Optional[Dict[str, Any]] = None,\\n        include_run_info: bool = False,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"\\n        Asynchronously evaluate the string distance between the\\n            prediction and the reference.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\string_distance\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Args:\\n            prediction (str): The prediction string.\\n            reference (Optional[str], optional): The reference string.\\n            input (Optional[str], optional): The input string.\\n            callbacks (Callbacks, optional): The callbacks to use.\\n            **kwargs: Additional keyword arguments.\\n\\n        Returns:\\n            dict: The evaluation results containing the score.\\n        \"\"\"\\n        result = await self.acall(\\n            inputs={\"prediction\": prediction, \"reference\": reference},\\n            callbacks=callbacks,\\n            tags=tags,\\n            metadata=metadata,\\n            include_run_info=include_run_info,\\n        )\\n        return self._prepare_output(result)\\n\\n\\nclass PairwiseStringDistanceEvalChain(PairwiseStringEvaluator, _RapidFuzzChainMixin):\\n    \"\"\"Compute string edit distances between two predictions.\"\"\"\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"\\n        Get the input keys.\\n\\n        Returns:\\n            List[str]: The input keys.\\n        \"\"\"\\n        return [\"prediction\", \"prediction_b\"]\\n\\n    @property\\n    def evaluation_name(self) -> str:\\n        \"\"\"\\n        Get the evaluation name.\\n\\n        Returns:\\n            str: The evaluation name.\\n        \"\"\"\\n        return f\"pairwise_{self.distance.value}_distance\"\\n\\n    def _call(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        \"\"\"\\n        Compute the string distance between two predictions.\\n\\n        Args:\\n            inputs (Dict[str, Any]): The input values.\\n            run_manager (CallbackManagerForChainRun , optional):\\n                The callback manager.\\n\\n        Returns:\\n            Dict[str, Any]: The evaluation results containing the score.\\n        \"\"\"\\n        return {\\n            \"score\": self.compute_metric(inputs[\"prediction\"], inputs[\"prediction_b\"])\\n        }' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\string_distance\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='async def _acall(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        \"\"\"\\n        Asynchronously compute the string distance between two predictions.\\n\\n        Args:\\n            inputs (Dict[str, Any]): The input values.\\n            run_manager (AsyncCallbackManagerForChainRun , optional):\\n                The callback manager.\\n\\n        Returns:\\n            Dict[str, Any]: The evaluation results containing the score.\\n        \"\"\"\\n        return {\\n            \"score\": self.compute_metric(inputs[\"prediction\"], inputs[\"prediction_b\"])\\n        }\\n\\n    def _evaluate_string_pairs(\\n        self,\\n        *,\\n        prediction: str,\\n        prediction_b: str,\\n        callbacks: Callbacks = None,\\n        tags: Optional[List[str]] = None,\\n        metadata: Optional[Dict[str, Any]] = None,\\n        include_run_info: bool = False,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"\\n        Evaluate the string distance between two predictions.\\n\\n        Args:\\n            prediction (str): The first prediction string.\\n            prediction_b (str): The second prediction string.\\n            callbacks (Callbacks, optional): The callbacks to use.\\n            tags (List[str], optional): Tags to apply to traces.\\n            metadata (Dict[str, Any], optional): Metadata to apply to traces.\\n            **kwargs: Additional keyword arguments.\\n\\n        Returns:\\n            dict: The evaluation results containing the score.\\n        \"\"\"\\n        result = self(\\n            inputs={\"prediction\": prediction, \"prediction_b\": prediction_b},\\n            callbacks=callbacks,\\n            tags=tags,\\n            metadata=metadata,\\n            include_run_info=include_run_info,\\n        )\\n        return self._prepare_output(result)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\string_distance\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='async def _aevaluate_string_pairs(\\n        self,\\n        *,\\n        prediction: str,\\n        prediction_b: str,\\n        callbacks: Callbacks = None,\\n        tags: Optional[List[str]] = None,\\n        metadata: Optional[Dict[str, Any]] = None,\\n        include_run_info: bool = False,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"\\n        Asynchronously evaluate the string distance between two predictions.\\n\\n        Args:\\n            prediction (str): The first prediction string.\\n            prediction_b (str): The second prediction string.\\n            callbacks (Callbacks, optional): The callbacks to use.\\n            tags (List[str], optional): Tags to apply to traces.\\n            metadata (Dict[str, Any], optional): Metadata to apply to traces.\\n            **kwargs: Additional keyword arguments.\\n\\n        Returns:\\n            dict: The evaluation results containing the score.\\n        \"\"\"\\n        result = await self.acall(\\n            inputs={\"prediction\": prediction, \"prediction_b\": prediction_b},\\n            callbacks=callbacks,\\n            tags=tags,\\n            metadata=metadata,\\n            include_run_info=include_run_info,\\n        )\\n        return self._prepare_output(result)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\string_distance\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"String distance evaluators.\"\"\"\\nfrom langchain.evaluation.string_distance.base import (\\n    PairwiseStringDistanceEvalChain,\\n    StringDistance,\\n    StringDistanceEvalChain,\\n)\\n\\n__all__ = [\\n    \"PairwiseStringDistanceEvalChain\",\\n    \"StringDistance\",\\n    \"StringDistanceEvalChain\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\evaluation\\\\string_distance\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Global values and configuration that apply to all of LangChain.\"\"\"\\nimport warnings\\nfrom typing import TYPE_CHECKING, Optional\\n\\nif TYPE_CHECKING:\\n    from langchain_core.caches import BaseCache\\n\\n\\n# DO NOT USE THESE VALUES DIRECTLY!\\n# Use them only via `get_<X>()` and `set_<X>()` below,\\n# or else your code may behave unexpectedly with other uses of these global settings:\\n# https://github.com/langchain-ai/langchain/pull/11311#issuecomment-1743780004\\n_verbose: bool = False\\n_debug: bool = False\\n_llm_cache: Optional[\"BaseCache\"] = None\\n\\n\\ndef set_verbose(value: bool) -> None:\\n    \"\"\"Set a new value for the `verbose` global setting.\"\"\"\\n    import langchain\\n\\n    # We\\'re about to run some deprecated code, don\\'t report warnings from it.\\n    # The user called the correct (non-deprecated) code path and shouldn\\'t get warnings.\\n    with warnings.catch_warnings():\\n        warnings.filterwarnings(\\n            \"ignore\",\\n            message=(\\n                \"Importing verbose from langchain root module is no longer supported\"\\n            ),\\n        )\\n        # N.B.: This is a workaround for an unfortunate quirk of Python\\'s\\n        #       module-level `__getattr__()` implementation:\\n        # https://github.com/langchain-ai/langchain/pull/11311#issuecomment-1743780004\\n        #\\n        # Remove it once `langchain.verbose` is no longer supported, and once all users\\n        # have migrated to using `set_verbose()` here.\\n        langchain.verbose = value\\n\\n    global _verbose\\n    _verbose = value\\n\\n\\ndef get_verbose() -> bool:\\n    \"\"\"Get the value of the `verbose` global setting.\"\"\"\\n    import langchain' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\globals\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# We\\'re about to run some deprecated code, don\\'t report warnings from it.\\n    # The user called the correct (non-deprecated) code path and shouldn\\'t get warnings.\\n    with warnings.catch_warnings():\\n        warnings.filterwarnings(\\n            \"ignore\",\\n            message=(\\n                \"Importing verbose from langchain root module is no longer supported\"\\n            ),\\n        )\\n        # N.B.: This is a workaround for an unfortunate quirk of Python\\'s\\n        #       module-level `__getattr__()` implementation:\\n        # https://github.com/langchain-ai/langchain/pull/11311#issuecomment-1743780004\\n        #\\n        # Remove it once `langchain.verbose` is no longer supported, and once all users\\n        # have migrated to using `set_verbose()` here.\\n        #\\n        # In the meantime, the `verbose` setting is considered True if either the old\\n        # or the new value are True. This accommodates users who haven\\'t migrated\\n        # to using `set_verbose()` yet. Those users are getting deprecation warnings\\n        # directing them to use `set_verbose()` when they import `langhchain.verbose`.\\n        old_verbose = langchain.verbose\\n\\n    global _verbose\\n    return _verbose or old_verbose\\n\\n\\ndef set_debug(value: bool) -> None:\\n    \"\"\"Set a new value for the `debug` global setting.\"\"\"\\n    import langchain' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\globals\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# We\\'re about to run some deprecated code, don\\'t report warnings from it.\\n    # The user called the correct (non-deprecated) code path and shouldn\\'t get warnings.\\n    with warnings.catch_warnings():\\n        warnings.filterwarnings(\\n            \"ignore\",\\n            message=\"Importing debug from langchain root module is no longer supported\",\\n        )\\n        # N.B.: This is a workaround for an unfortunate quirk of Python\\'s\\n        #       module-level `__getattr__()` implementation:\\n        # https://github.com/langchain-ai/langchain/pull/11311#issuecomment-1743780004\\n        #\\n        # Remove it once `langchain.debug` is no longer supported, and once all users\\n        # have migrated to using `set_debug()` here.\\n        langchain.debug = value\\n\\n    global _debug\\n    _debug = value\\n\\n\\ndef get_debug() -> bool:\\n    \"\"\"Get the value of the `debug` global setting.\"\"\"\\n    import langchain\\n\\n    # We\\'re about to run some deprecated code, don\\'t report warnings from it.\\n    # The user called the correct (non-deprecated) code path and shouldn\\'t get warnings.\\n    with warnings.catch_warnings():\\n        warnings.filterwarnings(\\n            \"ignore\",\\n            message=\"Importing debug from langchain root module is no longer supported\",\\n        )\\n        # N.B.: This is a workaround for an unfortunate quirk of Python\\'s\\n        #       module-level `__getattr__()` implementation:\\n        # https://github.com/langchain-ai/langchain/pull/11311#issuecomment-1743780004\\n        #\\n        # Remove it once `langchain.debug` is no longer supported, and once all users\\n        # have migrated to using `set_debug()` here.\\n        #\\n        # In the meantime, the `debug` setting is considered True if either the old\\n        # or the new value are True. This accommodates users who haven\\'t migrated\\n        # to using `set_debug()` yet. Those users are getting deprecation warnings\\n        # directing them to use `set_debug()` when they import `langhchain.debug`.\\n        old_debug = langchain.debug' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\globals\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='global _debug\\n    return _debug or old_debug\\n\\n\\ndef set_llm_cache(value: Optional[\"BaseCache\"]) -> None:\\n    \"\"\"Set a new LLM cache, overwriting the previous value, if any.\"\"\"\\n    import langchain\\n\\n    # We\\'re about to run some deprecated code, don\\'t report warnings from it.\\n    # The user called the correct (non-deprecated) code path and shouldn\\'t get warnings.\\n    with warnings.catch_warnings():\\n        warnings.filterwarnings(\\n            \"ignore\",\\n            message=(\\n                \"Importing llm_cache from langchain root module is no longer supported\"\\n            ),\\n        )\\n        # N.B.: This is a workaround for an unfortunate quirk of Python\\'s\\n        #       module-level `__getattr__()` implementation:\\n        # https://github.com/langchain-ai/langchain/pull/11311#issuecomment-1743780004\\n        #\\n        # Remove it once `langchain.llm_cache` is no longer supported, and\\n        # once all users have migrated to using `set_llm_cache()` here.\\n        langchain.llm_cache = value\\n\\n    global _llm_cache\\n    _llm_cache = value\\n\\n\\ndef get_llm_cache() -> \"BaseCache\":\\n    \"\"\"Get the value of the `llm_cache` global setting.\"\"\"\\n    import langchain' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\globals\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# We\\'re about to run some deprecated code, don\\'t report warnings from it.\\n    # The user called the correct (non-deprecated) code path and shouldn\\'t get warnings.\\n    with warnings.catch_warnings():\\n        warnings.filterwarnings(\\n            \"ignore\",\\n            message=(\\n                \"Importing llm_cache from langchain root module is no longer supported\"\\n            ),\\n        )\\n        # N.B.: This is a workaround for an unfortunate quirk of Python\\'s\\n        #       module-level `__getattr__()` implementation:\\n        # https://github.com/langchain-ai/langchain/pull/11311#issuecomment-1743780004\\n        #\\n        # Remove it once `langchain.llm_cache` is no longer supported, and\\n        # once all users have migrated to using `set_llm_cache()` here.\\n        #\\n        # In the meantime, the `llm_cache` setting returns whichever of\\n        # its two backing sources is truthy (not `None` and non-empty),\\n        # or the old value if both are falsy. This accommodates users\\n        # who haven\\'t migrated to using `set_llm_cache()` yet.\\n        # Those users are getting deprecation warnings directing them\\n        # to use `set_llm_cache()` when they import `langhchain.llm_cache`.\\n        old_llm_cache = langchain.llm_cache\\n\\n    global _llm_cache\\n    return _llm_cache or old_llm_cache' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\globals\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.graphs.arangodb_graph import ArangoGraph, get_arangodb_client\\n\\n__all__ = [\\n    \"ArangoGraph\",\\n    \"get_arangodb_client\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\graphs\\\\arangodb_graph.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.graphs.falkordb_graph import (\\n    FalkorDBGraph,\\n)\\n\\n__all__ = [\\n    \"FalkorDBGraph\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\graphs\\\\falkordb_graph.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.graphs.graph_document import GraphDocument, Node, Relationship\\n\\n__all__ = [\"Node\", \"Relationship\", \"GraphDocument\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\graphs\\\\graph_document.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.graphs.graph_store import GraphStore\\n\\n__all__ = [\"GraphStore\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\graphs\\\\graph_store.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.graphs.hugegraph import HugeGraph\\n\\n__all__ = [\"HugeGraph\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\graphs\\\\hugegraph.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.graphs.kuzu_graph import KuzuGraph\\n\\n__all__ = [\"KuzuGraph\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\graphs\\\\kuzu_graph.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.graphs.memgraph_graph import (\\n    MemgraphGraph,\\n)\\n\\n__all__ = [\"MemgraphGraph\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\graphs\\\\memgraph_graph.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.graphs.nebula_graph import NebulaGraph\\n\\n__all__ = [\"NebulaGraph\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\graphs\\\\nebula_graph.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.graphs.neo4j_graph import (\\n    Neo4jGraph,\\n)\\n\\n__all__ = [\"Neo4jGraph\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\graphs\\\\neo4j_graph.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.graphs.neptune_graph import NeptuneGraph\\n\\n__all__ = [\"NeptuneGraph\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\graphs\\\\neptune_graph.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.graphs.networkx_graph import (\\n    KG_TRIPLE_DELIMITER,\\n    KnowledgeTriple,\\n    NetworkxEntityGraph,\\n    get_entities,\\n    parse_triples,\\n)\\n\\n__all__ = [\\n    \"KG_TRIPLE_DELIMITER\",\\n    \"KnowledgeTriple\",\\n    \"parse_triples\",\\n    \"get_entities\",\\n    \"NetworkxEntityGraph\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\graphs\\\\networkx_graph.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.graphs.rdf_graph import (\\n    RdfGraph,\\n)\\n\\n__all__ = [\\n    \"RdfGraph\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\graphs\\\\rdf_graph.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"**Graphs** provide a natural language interface to graph databases.\"\"\"\\nimport warnings\\nfrom typing import Any\\n\\nfrom langchain_core._api import LangChainDeprecationWarning\\n\\nfrom langchain.utils.interactive_env import is_interactive_env\\n\\n\\ndef __getattr__(name: str) -> Any:\\n    from langchain_community import graphs\\n\\n    # If not in interactive env, raise warning.\\n    if not is_interactive_env():\\n        warnings.warn(\\n            \"Importing graphs from langchain is deprecated. Importing from \"\\n            \"langchain will no longer be supported as of langchain==0.2.0. \"\\n            \"Please import from langchain-community instead:\\\\n\\\\n\"\\n            f\"`from langchain_community.graphs import {name}`.\\\\n\\\\n\"\\n            \"To install langchain-community run `pip install -U langchain-community`.\",\\n            category=LangChainDeprecationWarning,\\n        )\\n\\n    return getattr(graphs, name)\\n\\n\\n__all__ = [\\n    \"MemgraphGraph\",\\n    \"NetworkxEntityGraph\",\\n    \"Neo4jGraph\",\\n    \"NebulaGraph\",\\n    \"NeptuneGraph\",\\n    \"KuzuGraph\",\\n    \"HugeGraph\",\\n    \"RdfGraph\",\\n    \"ArangoGraph\",\\n    \"FalkorDBGraph\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\graphs\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from __future__ import annotations\\n\\nimport uuid\\nfrom abc import ABC, abstractmethod\\nfrom typing import List, Optional, Sequence\\n\\nNAMESPACE_UUID = uuid.UUID(int=1984)\\n\\n\\nclass RecordManager(ABC):\\n    \"\"\"An abstract base class representing the interface for a record manager.\"\"\"\\n\\n    def __init__(\\n        self,\\n        namespace: str,\\n    ) -> None:\\n        \"\"\"Initialize the record manager.\\n\\n        Args:\\n            namespace (str): The namespace for the record manager.\\n        \"\"\"\\n        self.namespace = namespace\\n\\n    @abstractmethod\\n    def create_schema(self) -> None:\\n        \"\"\"Create the database schema for the record manager.\"\"\"\\n\\n    @abstractmethod\\n    async def acreate_schema(self) -> None:\\n        \"\"\"Create the database schema for the record manager.\"\"\"\\n\\n    @abstractmethod\\n    def get_time(self) -> float:\\n        \"\"\"Get the current server time as a high resolution timestamp!\\n\\n        It\\'s important to get this from the server to ensure a monotonic clock,\\n        otherwise there may be data loss when cleaning up old documents!\\n\\n        Returns:\\n            The current server time as a float timestamp.\\n        \"\"\"\\n\\n    @abstractmethod\\n    async def aget_time(self) -> float:\\n        \"\"\"Get the current server time as a high resolution timestamp!\\n\\n        It\\'s important to get this from the server to ensure a monotonic clock,\\n        otherwise there may be data loss when cleaning up old documents!\\n\\n        Returns:\\n            The current server time as a float timestamp.\\n        \"\"\"\\n\\n    @abstractmethod\\n    def update(\\n        self,\\n        keys: Sequence[str],\\n        *,\\n        group_ids: Optional[Sequence[Optional[str]]] = None,\\n        time_at_least: Optional[float] = None,\\n    ) -> None:\\n        \"\"\"Upsert records into the database.\\n\\n        Args:\\n            keys: A list of record keys to upsert.\\n            group_ids: A list of group IDs corresponding to the keys.\\n            time_at_least: if provided, updates should only happen if the\\n              updated_at field is at least this time.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\indexes\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Raises:\\n            ValueError: If the length of keys doesn\\'t match the length of group_ids.\\n        \"\"\"\\n\\n    @abstractmethod\\n    async def aupdate(\\n        self,\\n        keys: Sequence[str],\\n        *,\\n        group_ids: Optional[Sequence[Optional[str]]] = None,\\n        time_at_least: Optional[float] = None,\\n    ) -> None:\\n        \"\"\"Upsert records into the database.\\n\\n        Args:\\n            keys: A list of record keys to upsert.\\n            group_ids: A list of group IDs corresponding to the keys.\\n            time_at_least: if provided, updates should only happen if the\\n              updated_at field is at least this time.\\n\\n        Raises:\\n            ValueError: If the length of keys doesn\\'t match the length of group_ids.\\n        \"\"\"\\n\\n    @abstractmethod\\n    def exists(self, keys: Sequence[str]) -> List[bool]:\\n        \"\"\"Check if the provided keys exist in the database.\\n\\n        Args:\\n            keys: A list of keys to check.\\n\\n        Returns:\\n            A list of boolean values indicating the existence of each key.\\n        \"\"\"\\n\\n    @abstractmethod\\n    async def aexists(self, keys: Sequence[str]) -> List[bool]:\\n        \"\"\"Check if the provided keys exist in the database.\\n\\n        Args:\\n            keys: A list of keys to check.\\n\\n        Returns:\\n            A list of boolean values indicating the existence of each key.\\n        \"\"\"\\n\\n    @abstractmethod\\n    def list_keys(\\n        self,\\n        *,\\n        before: Optional[float] = None,\\n        after: Optional[float] = None,\\n        group_ids: Optional[Sequence[str]] = None,\\n        limit: Optional[int] = None,\\n    ) -> List[str]:\\n        \"\"\"List records in the database based on the provided filters.\\n\\n        Args:\\n            before: Filter to list records updated before this time.\\n            after: Filter to list records updated after this time.\\n            group_ids: Filter to list records with specific group IDs.\\n            limit: optional limit on the number of records to return.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\indexes\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Returns:\\n            A list of keys for the matching records.\\n        \"\"\"\\n\\n    @abstractmethod\\n    async def alist_keys(\\n        self,\\n        *,\\n        before: Optional[float] = None,\\n        after: Optional[float] = None,\\n        group_ids: Optional[Sequence[str]] = None,\\n        limit: Optional[int] = None,\\n    ) -> List[str]:\\n        \"\"\"List records in the database based on the provided filters.\\n\\n        Args:\\n            before: Filter to list records updated before this time.\\n            after: Filter to list records updated after this time.\\n            group_ids: Filter to list records with specific group IDs.\\n            limit: optional limit on the number of records to return.\\n\\n        Returns:\\n            A list of keys for the matching records.\\n        \"\"\"\\n\\n    @abstractmethod\\n    def delete_keys(self, keys: Sequence[str]) -> None:\\n        \"\"\"Delete specified records from the database.\\n\\n        Args:\\n            keys: A list of keys to delete.\\n        \"\"\"\\n\\n    @abstractmethod\\n    async def adelete_keys(self, keys: Sequence[str]) -> None:\\n        \"\"\"Delete specified records from the database.\\n\\n        Args:\\n            keys: A list of keys to delete.\\n        \"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\indexes\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Graph Index Creator.\"\"\"\\nfrom typing import Optional, Type\\n\\nfrom langchain_community.graphs.networkx_graph import NetworkxEntityGraph, parse_triples\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.prompts import BasePromptTemplate\\nfrom langchain_core.pydantic_v1 import BaseModel\\n\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.indexes.prompts.knowledge_triplet_extraction import (\\n    KNOWLEDGE_TRIPLE_EXTRACTION_PROMPT,\\n)\\n\\n\\nclass GraphIndexCreator(BaseModel):\\n    \"\"\"Functionality to create graph index.\"\"\"\\n\\n    llm: Optional[BaseLanguageModel] = None\\n    graph_type: Type[NetworkxEntityGraph] = NetworkxEntityGraph\\n\\n    def from_text(\\n        self, text: str, prompt: BasePromptTemplate = KNOWLEDGE_TRIPLE_EXTRACTION_PROMPT\\n    ) -> NetworkxEntityGraph:\\n        \"\"\"Create graph index from text.\"\"\"\\n        if self.llm is None:\\n            raise ValueError(\"llm should not be None\")\\n        graph = self.graph_type()\\n        chain = LLMChain(llm=self.llm, prompt=prompt)\\n        output = chain.predict(text=text)\\n        knowledge = parse_triples(output)\\n        for triple in knowledge:\\n            graph.add_triple(triple)\\n        return graph\\n\\n    async def afrom_text(\\n        self, text: str, prompt: BasePromptTemplate = KNOWLEDGE_TRIPLE_EXTRACTION_PROMPT\\n    ) -> NetworkxEntityGraph:\\n        \"\"\"Create graph index from text asynchronously.\"\"\"\\n        if self.llm is None:\\n            raise ValueError(\"llm should not be None\")\\n        graph = self.graph_type()\\n        chain = LLMChain(llm=self.llm, prompt=prompt)\\n        output = await chain.apredict(text=text)\\n        knowledge = parse_triples(output)\\n        for triple in knowledge:\\n            graph.add_triple(triple)\\n        return graph' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\indexes\\\\graph.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from typing import Any, Dict, List, Optional, Type\\n\\nfrom langchain_community.document_loaders.base import BaseLoader\\nfrom langchain_community.embeddings.openai import OpenAIEmbeddings\\nfrom langchain_community.llms.openai import OpenAI\\nfrom langchain_community.vectorstores.chroma import Chroma\\nfrom langchain_core.documents import Document\\nfrom langchain_core.embeddings import Embeddings\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.pydantic_v1 import BaseModel, Extra, Field\\nfrom langchain_core.vectorstores import VectorStore\\n\\nfrom langchain.chains.qa_with_sources.retrieval import RetrievalQAWithSourcesChain\\nfrom langchain.chains.retrieval_qa.base import RetrievalQA\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter, TextSplitter\\n\\n\\ndef _get_default_text_splitter() -> TextSplitter:\\n    return RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\\n\\n\\nclass VectorStoreIndexWrapper(BaseModel):\\n    \"\"\"Wrapper around a vectorstore for easy access.\"\"\"\\n\\n    vectorstore: VectorStore\\n\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n\\n        extra = Extra.forbid\\n        arbitrary_types_allowed = True\\n\\n    def query(\\n        self,\\n        question: str,\\n        llm: Optional[BaseLanguageModel] = None,\\n        retriever_kwargs: Optional[Dict[str, Any]] = None,\\n        **kwargs: Any,\\n    ) -> str:\\n        \"\"\"Query the vectorstore.\"\"\"\\n        llm = llm or OpenAI(temperature=0)\\n        retriever_kwargs = retriever_kwargs or {}\\n        chain = RetrievalQA.from_chain_type(\\n            llm, retriever=self.vectorstore.as_retriever(**retriever_kwargs), **kwargs\\n        )\\n        return chain.run(question)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\indexes\\\\vectorstore.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def query_with_sources(\\n        self,\\n        question: str,\\n        llm: Optional[BaseLanguageModel] = None,\\n        retriever_kwargs: Optional[Dict[str, Any]] = None,\\n        **kwargs: Any,\\n    ) -> dict:\\n        \"\"\"Query the vectorstore and get back sources.\"\"\"\\n        llm = llm or OpenAI(temperature=0)\\n        retriever_kwargs = retriever_kwargs or {}\\n        chain = RetrievalQAWithSourcesChain.from_chain_type(\\n            llm, retriever=self.vectorstore.as_retriever(**retriever_kwargs), **kwargs\\n        )\\n        return chain({chain.question_key: question})\\n\\n\\nclass VectorstoreIndexCreator(BaseModel):\\n    \"\"\"Logic for creating indexes.\"\"\"\\n\\n    vectorstore_cls: Type[VectorStore] = Chroma\\n    embedding: Embeddings = Field(default_factory=OpenAIEmbeddings)\\n    text_splitter: TextSplitter = Field(default_factory=_get_default_text_splitter)\\n    vectorstore_kwargs: dict = Field(default_factory=dict)\\n\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n\\n        extra = Extra.forbid\\n        arbitrary_types_allowed = True\\n\\n    def from_loaders(self, loaders: List[BaseLoader]) -> VectorStoreIndexWrapper:\\n        \"\"\"Create a vectorstore index from loaders.\"\"\"\\n        docs = []\\n        for loader in loaders:\\n            docs.extend(loader.load())\\n        return self.from_documents(docs)\\n\\n    def from_documents(self, documents: List[Document]) -> VectorStoreIndexWrapper:\\n        \"\"\"Create a vectorstore index from documents.\"\"\"\\n        sub_docs = self.text_splitter.split_documents(documents)\\n        vectorstore = self.vectorstore_cls.from_documents(\\n            sub_docs, self.embedding, **self.vectorstore_kwargs\\n        )\\n        return VectorStoreIndexWrapper(vectorstore=vectorstore)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\indexes\\\\vectorstore.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _hash_string_to_uuid(input_string: str) -> uuid.UUID:\\n    \"\"\"Hashes a string and returns the corresponding UUID.\"\"\"\\n    hash_value = hashlib.sha1(input_string.encode(\"utf-8\")).hexdigest()\\n    return uuid.uuid5(NAMESPACE_UUID, hash_value)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\indexes\\\\_api.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _hash_nested_dict_to_uuid(data: dict[Any, Any]) -> uuid.UUID:\\n    \"\"\"Hashes a nested dictionary and returns the corresponding UUID.\"\"\"\\n    serialized_data = json.dumps(data, sort_keys=True)\\n    hash_value = hashlib.sha1(serialized_data.encode(\"utf-8\")).hexdigest()\\n    return uuid.uuid5(NAMESPACE_UUID, hash_value)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\indexes\\\\_api.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='class _HashedDocument(Document):\\n    \"\"\"A hashed document with a unique ID.\"\"\"\\n\\n    uid: str\\n    hash_: str\\n    \"\"\"The hash of the document including content and metadata.\"\"\"\\n    content_hash: str\\n    \"\"\"The hash of the document content.\"\"\"\\n    metadata_hash: str\\n    \"\"\"The hash of the document metadata.\"\"\"\\n\\n    @classmethod\\n    def is_lc_serializable(cls) -> bool:\\n        return False\\n\\n    @root_validator(pre=True)\\n    def calculate_hashes(cls, values: Dict[str, Any]) -> Dict[str, Any]:\\n        \"\"\"Root validator to calculate content and metadata hash.\"\"\"\\n        content = values.get(\"page_content\", \"\")\\n        metadata = values.get(\"metadata\", {})\\n\\n        forbidden_keys = (\"hash_\", \"content_hash\", \"metadata_hash\")\\n\\n        for key in forbidden_keys:\\n            if key in metadata:\\n                raise ValueError(\\n                    f\"Metadata cannot contain key {key} as it \"\\n                    f\"is reserved for internal use.\"\\n                )\\n\\n        content_hash = str(_hash_string_to_uuid(content))\\n\\n        try:\\n            metadata_hash = str(_hash_nested_dict_to_uuid(metadata))\\n        except Exception as e:\\n            raise ValueError(\\n                f\"Failed to hash metadata: {e}. \"\\n                f\"Please use a dict that can be serialized using json.\"\\n            )\\n\\n        values[\"content_hash\"] = content_hash\\n        values[\"metadata_hash\"] = metadata_hash\\n        values[\"hash_\"] = str(_hash_string_to_uuid(content_hash + metadata_hash))\\n\\n        _uid = values.get(\"uid\", None)\\n\\n        if _uid is None:\\n            values[\"uid\"] = values[\"hash_\"]\\n        return values\\n\\n    def to_document(self) -> Document:\\n        \"\"\"Return a Document object.\"\"\"\\n        return Document(\\n            page_content=self.page_content,\\n            metadata=self.metadata,\\n        )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\indexes\\\\_api.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@classmethod\\n    def from_document(\\n        cls, document: Document, *, uid: Optional[str] = None\\n    ) -> _HashedDocument:\\n        \"\"\"Create a HashedDocument from a Document.\"\"\"\\n        return cls(\\n            uid=uid,\\n            page_content=document.page_content,\\n            metadata=document.metadata,\\n        )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\indexes\\\\_api.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _batch(size: int, iterable: Iterable[T]) -> Iterator[List[T]]:\\n    \"\"\"Utility batching function.\"\"\"\\n    it = iter(iterable)\\n    while True:\\n        chunk = list(islice(it, size))\\n        if not chunk:\\n            return\\n        yield chunk' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\indexes\\\\_api.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='async def _abatch(size: int, iterable: AsyncIterable[T]) -> AsyncIterator[List[T]]:\\n    \"\"\"Utility batching function.\"\"\"\\n    batch: List[T] = []\\n    async for element in iterable:\\n        if len(batch) < size:\\n            batch.append(element)\\n\\n        if len(batch) >= size:\\n            yield batch\\n            batch = []\\n\\n    if batch:\\n        yield batch' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\indexes\\\\_api.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_source_id_assigner(\\n    source_id_key: Union[str, Callable[[Document], str], None],\\n) -> Callable[[Document], Union[str, None]]:\\n    \"\"\"Get the source id from the document.\"\"\"\\n    if source_id_key is None:\\n        return lambda doc: None\\n    elif isinstance(source_id_key, str):\\n        return lambda doc: doc.metadata[source_id_key]\\n    elif callable(source_id_key):\\n        return source_id_key\\n    else:\\n        raise ValueError(\\n            f\"source_id_key should be either None, a string or a callable. \"\\n            f\"Got {source_id_key} of type {type(source_id_key)}.\"\\n        )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\indexes\\\\_api.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _deduplicate_in_order(\\n    hashed_documents: Iterable[_HashedDocument],\\n) -> Iterator[_HashedDocument]:\\n    \"\"\"Deduplicate a list of hashed documents while preserving order.\"\"\"\\n    seen: Set[str] = set()\\n\\n    for hashed_doc in hashed_documents:\\n        if hashed_doc.hash_ not in seen:\\n            seen.add(hashed_doc.hash_)\\n            yield hashed_doc' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\indexes\\\\_api.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='class IndexingResult(TypedDict):\\n    \"\"\"Return a detailed a breakdown of the result of the indexing operation.\"\"\"\\n\\n    num_added: int\\n    \"\"\"Number of added documents.\"\"\"\\n    num_updated: int\\n    \"\"\"Number of updated documents because they were not up to date.\"\"\"\\n    num_deleted: int\\n    \"\"\"Number of deleted documents.\"\"\"\\n    num_skipped: int\\n    \"\"\"Number of skipped documents because they were already up to date.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\indexes\\\\_api.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def index(\\n    docs_source: Union[BaseLoader, Iterable[Document]],\\n    record_manager: RecordManager,\\n    vector_store: VectorStore,\\n    *,\\n    batch_size: int = 100,\\n    cleanup: Literal[\"incremental\", \"full\", None] = None,\\n    source_id_key: Union[str, Callable[[Document], str], None] = None,\\n    cleanup_batch_size: int = 1_000,\\n    force_update: bool = False,\\n) -> IndexingResult:\\n    \"\"\"Index data from the loader into the vector store.\\n\\n    Indexing functionality uses a manager to keep track of which documents\\n    are in the vector store.\\n\\n    This allows us to keep track of which documents were updated, and which\\n    documents were deleted, which documents should be skipped.\\n\\n    For the time being, documents are indexed using their hashes, and users\\n     are not able to specify the uid of the document.\\n\\n    IMPORTANT:\\n       if auto_cleanup is set to True, the loader should be returning\\n       the entire dataset, and not just a subset of the dataset.\\n       Otherwise, the auto_cleanup will remove documents that it is not\\n       supposed to.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\indexes\\\\_api.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Args:\\n        docs_source: Data loader or iterable of documents to index.\\n        record_manager: Timestamped set to keep track of which documents were\\n                         updated.\\n        vector_store: Vector store to index the documents into.\\n        batch_size: Batch size to use when indexing.\\n        cleanup: How to handle clean up of documents.\\n            - Incremental: Cleans up all documents that haven\\'t been updated AND\\n                           that are associated with source ids that were seen\\n                           during indexing.\\n                           Clean up is done continuously during indexing helping\\n                           to minimize the probability of users seeing duplicated\\n                           content.\\n            - Full: Delete all documents that haven to been returned by the loader.\\n                    Clean up runs after all documents have been indexed.\\n                    This means that users may see duplicated content during indexing.\\n            - None: Do not delete any documents.\\n        source_id_key: Optional key that helps identify the original source\\n            of the document.\\n        cleanup_batch_size: Batch size to use when cleaning up documents.\\n        force_update: Force update documents even if they are present in the\\n            record manager. Useful if you are re-indexing with updated embeddings.\\n\\n    Returns:\\n        Indexing result which contains information about how many documents\\n        were added, updated, deleted, or skipped.\\n    \"\"\"\\n    if cleanup not in {\"incremental\", \"full\", None}:\\n        raise ValueError(\\n            f\"cleanup should be one of \\'incremental\\', \\'full\\' or None. \"\\n            f\"Got {cleanup}.\"\\n        )\\n\\n    if cleanup == \"incremental\" and source_id_key is None:\\n        raise ValueError(\"Source id key is required when cleanup mode is incremental.\")\\n\\n    # Check that the Vectorstore has required methods implemented\\n    methods = [\"delete\", \"add_documents\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\indexes\\\\_api.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='for method in methods:\\n        if not hasattr(vector_store, method):\\n            raise ValueError(\\n                f\"Vectorstore {vector_store} does not have required method {method}\"\\n            )\\n\\n    if type(vector_store).delete == VectorStore.delete:\\n        # Checking if the vectorstore has overridden the default delete method\\n        # implementation which just raises a NotImplementedError\\n        raise ValueError(\"Vectorstore has not implemented the delete method\")\\n\\n    if isinstance(docs_source, BaseLoader):\\n        try:\\n            doc_iterator = docs_source.lazy_load()\\n        except NotImplementedError:\\n            doc_iterator = iter(docs_source.load())\\n    else:\\n        doc_iterator = iter(docs_source)\\n\\n    source_id_assigner = _get_source_id_assigner(source_id_key)\\n\\n    # Mark when the update started.\\n    index_start_dt = record_manager.get_time()\\n    num_added = 0\\n    num_skipped = 0\\n    num_updated = 0\\n    num_deleted = 0\\n\\n    for doc_batch in _batch(batch_size, doc_iterator):\\n        hashed_docs = list(\\n            _deduplicate_in_order(\\n                [_HashedDocument.from_document(doc) for doc in doc_batch]\\n            )\\n        )\\n\\n        source_ids: Sequence[Optional[str]] = [\\n            source_id_assigner(doc) for doc in hashed_docs\\n        ]\\n\\n        if cleanup == \"incremental\":\\n            # If the cleanup mode is incremental, source ids are required.\\n            for source_id, hashed_doc in zip(source_ids, hashed_docs):\\n                if source_id is None:\\n                    raise ValueError(\\n                        \"Source ids are required when cleanup mode is incremental. \"\\n                        f\"Document that starts with \"\\n                        f\"content: {hashed_doc.page_content[:100]} was not assigned \"\\n                        f\"as source id.\"\\n                    )\\n            # source ids cannot be None after for loop above.\\n            source_ids = cast(Sequence[str], source_ids)  # type: ignore[assignment]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\indexes\\\\_api.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='exists_batch = record_manager.exists([doc.uid for doc in hashed_docs])\\n\\n        # Filter out documents that already exist in the record store.\\n        uids = []\\n        docs_to_index = []\\n        uids_to_refresh = []\\n        seen_docs: Set[str] = set()\\n        for hashed_doc, doc_exists in zip(hashed_docs, exists_batch):\\n            if doc_exists:\\n                if force_update:\\n                    seen_docs.add(hashed_doc.uid)\\n                else:\\n                    uids_to_refresh.append(hashed_doc.uid)\\n                    continue\\n            uids.append(hashed_doc.uid)\\n            docs_to_index.append(hashed_doc.to_document())\\n\\n        # Update refresh timestamp\\n        if uids_to_refresh:\\n            record_manager.update(uids_to_refresh, time_at_least=index_start_dt)\\n            num_skipped += len(uids_to_refresh)\\n\\n        # Be pessimistic and assume that all vector store write will fail.\\n        # First write to vector store\\n        if docs_to_index:\\n            vector_store.add_documents(docs_to_index, ids=uids)\\n            num_added += len(docs_to_index) - len(seen_docs)\\n            num_updated += len(seen_docs)\\n\\n        # And only then update the record store.\\n        # Update ALL records, even if they already exist since we want to refresh\\n        # their timestamp.\\n        record_manager.update(\\n            [doc.uid for doc in hashed_docs],\\n            group_ids=source_ids,\\n            time_at_least=index_start_dt,\\n        )\\n\\n        # If source IDs are provided, we can do the deletion incrementally!\\n        if cleanup == \"incremental\":\\n            # Get the uids of the documents that were not returned by the loader.\\n\\n            # mypy isn\\'t good enough to determine that source ids cannot be None\\n            # here due to a check that\\'s happening above, so we check again.\\n            for source_id in source_ids:\\n                if source_id is None:\\n                    raise AssertionError(\"Source ids cannot be None here.\")\\n\\n            _source_ids = cast(Sequence[str], source_ids)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\indexes\\\\_api.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='uids_to_delete = record_manager.list_keys(\\n                group_ids=_source_ids, before=index_start_dt\\n            )\\n            if uids_to_delete:\\n                # Then delete from vector store.\\n                vector_store.delete(uids_to_delete)\\n                # First delete from record store.\\n                record_manager.delete_keys(uids_to_delete)\\n                num_deleted += len(uids_to_delete)\\n\\n    if cleanup == \"full\":\\n        while uids_to_delete := record_manager.list_keys(\\n            before=index_start_dt, limit=cleanup_batch_size\\n        ):\\n            # First delete from record store.\\n            vector_store.delete(uids_to_delete)\\n            # Then delete from record manager.\\n            record_manager.delete_keys(uids_to_delete)\\n            num_deleted += len(uids_to_delete)\\n\\n    return {\\n        \"num_added\": num_added,\\n        \"num_updated\": num_updated,\\n        \"num_skipped\": num_skipped,\\n        \"num_deleted\": num_deleted,\\n    }' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\indexes\\\\_api.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='async def _to_async_iterator(iterator: Iterable[T]) -> AsyncIterator[T]:\\n    \"\"\"Convert an iterable to an async iterator.\"\"\"\\n    for item in iterator:\\n        yield item' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\indexes\\\\_api.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='async def aindex(\\n    docs_source: Union[Iterable[Document], AsyncIterator[Document]],\\n    record_manager: RecordManager,\\n    vector_store: VectorStore,\\n    *,\\n    batch_size: int = 100,\\n    cleanup: Literal[\"incremental\", \"full\", None] = None,\\n    source_id_key: Union[str, Callable[[Document], str], None] = None,\\n    cleanup_batch_size: int = 1_000,\\n    force_update: bool = False,\\n) -> IndexingResult:\\n    \"\"\"Index data from the loader into the vector store.\\n\\n    Indexing functionality uses a manager to keep track of which documents\\n    are in the vector store.\\n\\n    This allows us to keep track of which documents were updated, and which\\n    documents were deleted, which documents should be skipped.\\n\\n    For the time being, documents are indexed using their hashes, and users\\n     are not able to specify the uid of the document.\\n\\n    IMPORTANT:\\n       if auto_cleanup is set to True, the loader should be returning\\n       the entire dataset, and not just a subset of the dataset.\\n       Otherwise, the auto_cleanup will remove documents that it is not\\n       supposed to.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\indexes\\\\_api.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Args:\\n        docs_source: Data loader or iterable of documents to index.\\n        record_manager: Timestamped set to keep track of which documents were\\n                         updated.\\n        vector_store: Vector store to index the documents into.\\n        batch_size: Batch size to use when indexing.\\n        cleanup: How to handle clean up of documents.\\n            - Incremental: Cleans up all documents that haven\\'t been updated AND\\n                           that are associated with source ids that were seen\\n                           during indexing.\\n                           Clean up is done continuously during indexing helping\\n                           to minimize the probability of users seeing duplicated\\n                           content.\\n            - Full: Delete all documents that haven to been returned by the loader.\\n                    Clean up runs after all documents have been indexed.\\n                    This means that users may see duplicated content during indexing.\\n            - None: Do not delete any documents.\\n        source_id_key: Optional key that helps identify the original source\\n            of the document.\\n        cleanup_batch_size: Batch size to use when cleaning up documents.\\n        force_update: Force update documents even if they are present in the\\n            record manager. Useful if you are re-indexing with updated embeddings.\\n\\n    Returns:\\n        Indexing result which contains information about how many documents\\n        were added, updated, deleted, or skipped.\\n    \"\"\"\\n\\n    if cleanup not in {\"incremental\", \"full\", None}:\\n        raise ValueError(\\n            f\"cleanup should be one of \\'incremental\\', \\'full\\' or None. \"\\n            f\"Got {cleanup}.\"\\n        )\\n\\n    if cleanup == \"incremental\" and source_id_key is None:\\n        raise ValueError(\"Source id key is required when cleanup mode is incremental.\")\\n\\n    # Check that the Vectorstore has required methods implemented\\n    methods = [\"adelete\", \"aadd_documents\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\indexes\\\\_api.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='for method in methods:\\n        if not hasattr(vector_store, method):\\n            raise ValueError(\\n                f\"Vectorstore {vector_store} does not have required method {method}\"\\n            )\\n\\n    if type(vector_store).adelete == VectorStore.adelete:\\n        # Checking if the vectorstore has overridden the default delete method\\n        # implementation which just raises a NotImplementedError\\n        raise ValueError(\"Vectorstore has not implemented the delete method\")\\n\\n    if isinstance(docs_source, BaseLoader):\\n        raise NotImplementedError(\\n            \"Not supported yet. Please pass an async iterator of documents.\"\\n        )\\n    async_doc_iterator: AsyncIterator[Document]\\n\\n    if hasattr(docs_source, \"__aiter__\"):\\n        async_doc_iterator = docs_source  # type: ignore[assignment]\\n    else:\\n        async_doc_iterator = _to_async_iterator(docs_source)\\n\\n    source_id_assigner = _get_source_id_assigner(source_id_key)\\n\\n    # Mark when the update started.\\n    index_start_dt = await record_manager.aget_time()\\n    num_added = 0\\n    num_skipped = 0\\n    num_updated = 0\\n    num_deleted = 0\\n\\n    async for doc_batch in _abatch(batch_size, async_doc_iterator):\\n        hashed_docs = list(\\n            _deduplicate_in_order(\\n                [_HashedDocument.from_document(doc) for doc in doc_batch]\\n            )\\n        )\\n\\n        source_ids: Sequence[Optional[str]] = [\\n            source_id_assigner(doc) for doc in hashed_docs\\n        ]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\indexes\\\\_api.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='if cleanup == \"incremental\":\\n            # If the cleanup mode is incremental, source ids are required.\\n            for source_id, hashed_doc in zip(source_ids, hashed_docs):\\n                if source_id is None:\\n                    raise ValueError(\\n                        \"Source ids are required when cleanup mode is incremental. \"\\n                        f\"Document that starts with \"\\n                        f\"content: {hashed_doc.page_content[:100]} was not assigned \"\\n                        f\"as source id.\"\\n                    )\\n            # source ids cannot be None after for loop above.\\n            source_ids = cast(Sequence[str], source_ids)\\n\\n        exists_batch = await record_manager.aexists([doc.uid for doc in hashed_docs])\\n\\n        # Filter out documents that already exist in the record store.\\n        uids: list[str] = []\\n        docs_to_index: list[Document] = []\\n        uids_to_refresh = []\\n        seen_docs: Set[str] = set()\\n        for hashed_doc, doc_exists in zip(hashed_docs, exists_batch):\\n            if doc_exists:\\n                if force_update:\\n                    seen_docs.add(hashed_doc.uid)\\n                else:\\n                    uids_to_refresh.append(hashed_doc.uid)\\n                    continue\\n            uids.append(hashed_doc.uid)\\n            docs_to_index.append(hashed_doc.to_document())\\n\\n        if uids_to_refresh:\\n            # Must be updated to refresh timestamp.\\n            await record_manager.aupdate(uids_to_refresh, time_at_least=index_start_dt)\\n            num_skipped += len(uids_to_refresh)\\n\\n        # Be pessimistic and assume that all vector store write will fail.\\n        # First write to vector store\\n        if docs_to_index:\\n            await vector_store.aadd_documents(docs_to_index, ids=uids)\\n            num_added += len(docs_to_index) - len(seen_docs)\\n            num_updated += len(seen_docs)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\indexes\\\\_api.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# And only then update the record store.\\n        # Update ALL records, even if they already exist since we want to refresh\\n        # their timestamp.\\n        await record_manager.aupdate(\\n            [doc.uid for doc in hashed_docs],\\n            group_ids=source_ids,\\n            time_at_least=index_start_dt,\\n        )\\n\\n        # If source IDs are provided, we can do the deletion incrementally!\\n\\n        if cleanup == \"incremental\":\\n            # Get the uids of the documents that were not returned by the loader.\\n\\n            # mypy isn\\'t good enough to determine that source ids cannot be None\\n            # here due to a check that\\'s happening above, so we check again.\\n            for source_id in source_ids:\\n                if source_id is None:\\n                    raise AssertionError(\"Source ids cannot be None here.\")\\n\\n            _source_ids = cast(Sequence[str], source_ids)\\n\\n            uids_to_delete = await record_manager.alist_keys(\\n                group_ids=_source_ids, before=index_start_dt\\n            )\\n            if uids_to_delete:\\n                # Then delete from vector store.\\n                await vector_store.adelete(uids_to_delete)\\n                # First delete from record store.\\n                await record_manager.adelete_keys(uids_to_delete)\\n                num_deleted += len(uids_to_delete)\\n\\n    if cleanup == \"full\":\\n        while uids_to_delete := await record_manager.alist_keys(\\n            before=index_start_dt, limit=cleanup_batch_size\\n        ):\\n            # First delete from record store.\\n            await vector_store.adelete(uids_to_delete)\\n            # Then delete from record manager.\\n            await record_manager.adelete_keys(uids_to_delete)\\n            num_deleted += len(uids_to_delete)\\n\\n    return {\\n        \"num_added\": num_added,\\n        \"num_updated\": num_updated,\\n        \"num_skipped\": num_skipped,\\n        \"num_deleted\": num_deleted,\\n    }' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\indexes\\\\_api.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Module contains logic for indexing documents into vector stores.\"\"\"\\nfrom __future__ import annotations\\n\\nimport hashlib\\nimport json\\nimport uuid\\nfrom itertools import islice\\nfrom typing import (\\n    Any,\\n    AsyncIterable,\\n    AsyncIterator,\\n    Callable,\\n    Dict,\\n    Iterable,\\n    Iterator,\\n    List,\\n    Literal,\\n    Optional,\\n    Sequence,\\n    Set,\\n    TypedDict,\\n    TypeVar,\\n    Union,\\n    cast,\\n)\\n\\nfrom langchain_community.document_loaders.base import BaseLoader\\nfrom langchain_core.documents import Document\\nfrom langchain_core.pydantic_v1 import root_validator\\nfrom langchain_core.vectorstores import VectorStore\\n\\nfrom langchain.indexes.base import NAMESPACE_UUID, RecordManager\\n\\nT = TypeVar(\"T\")\\n\\n\\n# Code for: def _hash_string_to_uuid(input_string: str) -> uuid.UUID:\\n\\n\\n# Code for: def _hash_nested_dict_to_uuid(data: dict[Any, Any]) -> uuid.UUID:\\n\\n\\n# Code for: class _HashedDocument(Document):\\n\\n\\n# Code for: def _batch(size: int, iterable: Iterable[T]) -> Iterator[List[T]]:\\n\\n\\n# Code for: async def _abatch(size: int, iterable: AsyncIterable[T]) -> AsyncIterator[List[T]]:\\n\\n\\n# Code for: def _get_source_id_assigner(\\n\\n\\n# Code for: def _deduplicate_in_order(\\n\\n\\n# PUBLIC API\\n\\n\\n# Code for: class IndexingResult(TypedDict):\\n\\n\\n# Code for: def index(\\n\\n\\n# Define an asynchronous generator function\\n# Code for: async def _to_async_iterator(iterator: Iterable[T]) -> AsyncIterator[T]:\\n\\n\\n# Code for: async def aindex(' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\indexes\\\\_api.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='class UpsertionRecord(Base):  # type: ignore[valid-type,misc]\\n    \"\"\"Table used to keep track of when a key was last updated.\"\"\"\\n\\n    # ATTENTION:\\n    # Prior to modifying this table, please determine whether\\n    # we should create migrations for this table to make sure\\n    # users do not experience data loss.\\n    __tablename__ = \"upsertion_record\"\\n\\n    uuid = Column(\\n        String,\\n        index=True,\\n        default=lambda: str(uuid.uuid4()),\\n        primary_key=True,\\n        nullable=False,\\n    )\\n    key = Column(String, index=True)\\n    # Using a non-normalized representation to handle `namespace` attribute.\\n    # If the need arises, this attribute can be pulled into a separate Collection\\n    # table at some time later.\\n    namespace = Column(String, index=True, nullable=False)\\n    group_id = Column(String, index=True, nullable=True)\\n\\n    # The timestamp associated with the last record upsertion.\\n    updated_at = Column(Float, index=True)\\n\\n    __table_args__ = (\\n        UniqueConstraint(\"key\", \"namespace\", name=\"uix_key_namespace\"),\\n        Index(\"ix_key_namespace\", \"key\", \"namespace\"),\\n    )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\indexes\\\\_sql_record_manager.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='class SQLRecordManager(RecordManager):\\n    \"\"\"A SQL Alchemy based implementation of the record manager.\"\"\"\\n\\n    def __init__(\\n        self,\\n        namespace: str,\\n        *,\\n        engine: Optional[Union[Engine, AsyncEngine]] = None,\\n        db_url: Union[None, str, URL] = None,\\n        engine_kwargs: Optional[Dict[str, Any]] = None,\\n        async_mode: bool = False,\\n    ) -> None:\\n        \"\"\"Initialize the SQLRecordManager.\\n\\n        This class serves as a manager persistence layer that uses an SQL\\n        backend to track upserted records. You should specify either a db_url\\n        to create an engine or provide an existing engine.\\n\\n        Args:\\n            namespace: The namespace associated with this record manager.\\n            engine: An already existing SQL Alchemy engine.\\n                Default is None.\\n            db_url: A database connection string used to create\\n                an SQL Alchemy engine. Default is None.\\n            engine_kwargs: Additional keyword arguments\\n                to be passed when creating the engine. Default is an empty dictionary.\\n            async_mode: Whether to create an async engine.\\n                Driver should support async operations.\\n                It only applies if db_url is provided.\\n                Default is False.\\n\\n        Raises:\\n            ValueError: If both db_url and engine are provided or neither.\\n            AssertionError: If something unexpected happens during engine configuration.\\n        \"\"\"\\n        super().__init__(namespace=namespace)\\n        if db_url is None and engine is None:\\n            raise ValueError(\"Must specify either db_url or engine\")\\n\\n        if db_url is not None and engine is not None:\\n            raise ValueError(\"Must specify either db_url or engine, not both\")' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\indexes\\\\_sql_record_manager.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='_engine: Union[Engine, AsyncEngine]\\n        if db_url:\\n            if async_mode:\\n                _engine = create_async_engine(db_url, **(engine_kwargs or {}))\\n            else:\\n                _engine = create_engine(db_url, **(engine_kwargs or {}))\\n        elif engine:\\n            _engine = engine\\n\\n        else:\\n            raise AssertionError(\"Something went wrong with configuration of engine.\")\\n\\n        _session_factory: Union[sessionmaker[Session], async_sessionmaker[AsyncSession]]\\n        if isinstance(_engine, AsyncEngine):\\n            _session_factory = async_sessionmaker(bind=_engine)\\n        else:\\n            _session_factory = sessionmaker(bind=_engine)\\n\\n        self.engine = _engine\\n        self.dialect = _engine.dialect.name\\n        self.session_factory = _session_factory\\n\\n    def create_schema(self) -> None:\\n        \"\"\"Create the database schema.\"\"\"\\n        if isinstance(self.engine, AsyncEngine):\\n            raise AssertionError(\"This method is not supported for async engines.\")\\n\\n        Base.metadata.create_all(self.engine)\\n\\n    async def acreate_schema(self) -> None:\\n        \"\"\"Create the database schema.\"\"\"\\n\\n        if not isinstance(self.engine, AsyncEngine):\\n            raise AssertionError(\"This method is not supported for sync engines.\")\\n\\n        async with self.engine.begin() as session:\\n            await session.run_sync(Base.metadata.create_all)\\n\\n    @contextlib.contextmanager\\n    def _make_session(self) -> Generator[Session, None, None]:\\n        \"\"\"Create a session and close it after use.\"\"\"\\n\\n        if isinstance(self.session_factory, async_sessionmaker):\\n            raise AssertionError(\"This method is not supported for async engines.\")\\n\\n        session = self.session_factory()\\n        try:\\n            yield session\\n        finally:\\n            session.close()\\n\\n    @contextlib.asynccontextmanager\\n    async def _amake_session(self) -> AsyncGenerator[AsyncSession, None]:\\n        \"\"\"Create a session and close it after use.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\indexes\\\\_sql_record_manager.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='if not isinstance(self.session_factory, async_sessionmaker):\\n            raise AssertionError(\"This method is not supported for sync engines.\")\\n\\n        async with self.session_factory() as session:\\n            yield session\\n\\n    def get_time(self) -> float:\\n        \"\"\"Get the current server time as a timestamp.\\n\\n        Please note it\\'s critical that time is obtained from the server since\\n        we want a monotonic clock.\\n        \"\"\"\\n        with self._make_session() as session:\\n            # * SQLite specific implementation, can be changed based on dialect.\\n            # * For SQLite, unlike unixepoch it will work with older versions of SQLite.\\n            # ----\\n            # julianday(\\'now\\'): Julian day number for the current date and time.\\n            # The Julian day is a continuous count of days, starting from a\\n            # reference date (Julian day number 0).\\n            # 2440587.5 - constant represents the Julian day number for January 1, 1970\\n            # 86400.0 - constant represents the number of seconds\\n            # in a day (24 hours * 60 minutes * 60 seconds)\\n            if self.dialect == \"sqlite\":\\n                query = text(\"SELECT (julianday(\\'now\\') - 2440587.5) * 86400.0;\")\\n            elif self.dialect == \"postgresql\":\\n                query = text(\"SELECT EXTRACT (EPOCH FROM CURRENT_TIMESTAMP);\")\\n            else:\\n                raise NotImplementedError(f\"Not implemented for dialect {self.dialect}\")\\n\\n            dt = session.execute(query).scalar()\\n            if isinstance(dt, decimal.Decimal):\\n                dt = float(dt)\\n            if not isinstance(dt, float):\\n                raise AssertionError(f\"Unexpected type for datetime: {type(dt)}\")\\n            return dt\\n\\n    async def aget_time(self) -> float:\\n        \"\"\"Get the current server time as a timestamp.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\indexes\\\\_sql_record_manager.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Please note it\\'s critical that time is obtained from the server since\\n        we want a monotonic clock.\\n        \"\"\"\\n        async with self._amake_session() as session:\\n            # * SQLite specific implementation, can be changed based on dialect.\\n            # * For SQLite, unlike unixepoch it will work with older versions of SQLite.\\n            # ----\\n            # julianday(\\'now\\'): Julian day number for the current date and time.\\n            # The Julian day is a continuous count of days, starting from a\\n            # reference date (Julian day number 0).\\n            # 2440587.5 - constant represents the Julian day number for January 1, 1970\\n            # 86400.0 - constant represents the number of seconds\\n            # in a day (24 hours * 60 minutes * 60 seconds)\\n            if self.dialect == \"sqlite\":\\n                query = text(\"SELECT (julianday(\\'now\\') - 2440587.5) * 86400.0;\")\\n            elif self.dialect == \"postgresql\":\\n                query = text(\"SELECT EXTRACT (EPOCH FROM CURRENT_TIMESTAMP);\")\\n            else:\\n                raise NotImplementedError(f\"Not implemented for dialect {self.dialect}\")\\n\\n            dt = (await session.execute(query)).scalar_one_or_none()\\n\\n            if isinstance(dt, decimal.Decimal):\\n                dt = float(dt)\\n            if not isinstance(dt, float):\\n                raise AssertionError(f\"Unexpected type for datetime: {type(dt)}\")\\n            return dt\\n\\n    def update(\\n        self,\\n        keys: Sequence[str],\\n        *,\\n        group_ids: Optional[Sequence[Optional[str]]] = None,\\n        time_at_least: Optional[float] = None,\\n    ) -> None:\\n        \"\"\"Upsert records into the SQLite database.\"\"\"\\n        if group_ids is None:\\n            group_ids = [None] * len(keys)\\n\\n        if len(keys) != len(group_ids):\\n            raise ValueError(\\n                f\"Number of keys ({len(keys)}) does not match number of \"\\n                f\"group_ids ({len(group_ids)})\"\\n            )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\indexes\\\\_sql_record_manager.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# Get the current time from the server.\\n        # This makes an extra round trip to the server, should not be a big deal\\n        # if the batch size is large enough.\\n        # Getting the time here helps us compare it against the time_at_least\\n        # and raise an error if there is a time sync issue.\\n        # Here, we\\'re just being extra careful to minimize the chance of\\n        # data loss due to incorrectly deleting records.\\n        update_time = self.get_time()\\n\\n        if time_at_least and update_time < time_at_least:\\n            # Safeguard against time sync issues\\n            raise AssertionError(f\"Time sync issue: {update_time} < {time_at_least}\")\\n\\n        records_to_upsert = [\\n            {\\n                \"key\": key,\\n                \"namespace\": self.namespace,\\n                \"updated_at\": update_time,\\n                \"group_id\": group_id,\\n            }\\n            for key, group_id in zip(keys, group_ids)\\n        ]\\n\\n        with self._make_session() as session:\\n            if self.dialect == \"sqlite\":\\n                from sqlalchemy.dialects.sqlite import insert as sqlite_insert\\n\\n                # Note: uses SQLite insert to make on_conflict_do_update work.\\n                # This code needs to be generalized a bit to work with more dialects.\\n                insert_stmt = sqlite_insert(UpsertionRecord).values(records_to_upsert)\\n                stmt = insert_stmt.on_conflict_do_update(  # type: ignore[attr-defined]\\n                    [UpsertionRecord.key, UpsertionRecord.namespace],\\n                    set_=dict(\\n                        # attr-defined type ignore\\n                        updated_at=insert_stmt.excluded.updated_at,  # type: ignore\\n                        group_id=insert_stmt.excluded.group_id,  # type: ignore\\n                    ),\\n                )\\n            elif self.dialect == \"postgresql\":\\n                from sqlalchemy.dialects.postgresql import insert as pg_insert' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\indexes\\\\_sql_record_manager.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# Note: uses SQLite insert to make on_conflict_do_update work.\\n                # This code needs to be generalized a bit to work with more dialects.\\n                insert_stmt = pg_insert(UpsertionRecord).values(records_to_upsert)\\n                stmt = insert_stmt.on_conflict_do_update(  # type: ignore[attr-defined]\\n                    \"uix_key_namespace\",  # Name of constraint\\n                    set_=dict(\\n                        # attr-defined type ignore\\n                        updated_at=insert_stmt.excluded.updated_at,  # type: ignore\\n                        group_id=insert_stmt.excluded.group_id,  # type: ignore\\n                    ),\\n                )\\n            else:\\n                raise NotImplementedError(f\"Unsupported dialect {self.dialect}\")\\n\\n            session.execute(stmt)\\n            session.commit()\\n\\n    async def aupdate(\\n        self,\\n        keys: Sequence[str],\\n        *,\\n        group_ids: Optional[Sequence[Optional[str]]] = None,\\n        time_at_least: Optional[float] = None,\\n    ) -> None:\\n        \"\"\"Upsert records into the SQLite database.\"\"\"\\n        if group_ids is None:\\n            group_ids = [None] * len(keys)\\n\\n        if len(keys) != len(group_ids):\\n            raise ValueError(\\n                f\"Number of keys ({len(keys)}) does not match number of \"\\n                f\"group_ids ({len(group_ids)})\"\\n            )\\n\\n        # Get the current time from the server.\\n        # This makes an extra round trip to the server, should not be a big deal\\n        # if the batch size is large enough.\\n        # Getting the time here helps us compare it against the time_at_least\\n        # and raise an error if there is a time sync issue.\\n        # Here, we\\'re just being extra careful to minimize the chance of\\n        # data loss due to incorrectly deleting records.\\n        update_time = await self.aget_time()' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\indexes\\\\_sql_record_manager.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='if time_at_least and update_time < time_at_least:\\n            # Safeguard against time sync issues\\n            raise AssertionError(f\"Time sync issue: {update_time} < {time_at_least}\")\\n\\n        records_to_upsert = [\\n            {\\n                \"key\": key,\\n                \"namespace\": self.namespace,\\n                \"updated_at\": update_time,\\n                \"group_id\": group_id,\\n            }\\n            for key, group_id in zip(keys, group_ids)\\n        ]\\n\\n        async with self._amake_session() as session:\\n            if self.dialect == \"sqlite\":\\n                from sqlalchemy.dialects.sqlite import insert as sqlite_insert\\n\\n                # Note: uses SQLite insert to make on_conflict_do_update work.\\n                # This code needs to be generalized a bit to work with more dialects.\\n                insert_stmt = sqlite_insert(UpsertionRecord).values(records_to_upsert)\\n                stmt = insert_stmt.on_conflict_do_update(  # type: ignore[attr-defined]\\n                    [UpsertionRecord.key, UpsertionRecord.namespace],\\n                    set_=dict(\\n                        # attr-defined type ignore\\n                        updated_at=insert_stmt.excluded.updated_at,  # type: ignore\\n                        group_id=insert_stmt.excluded.group_id,  # type: ignore\\n                    ),\\n                )\\n            elif self.dialect == \"postgresql\":\\n                from sqlalchemy.dialects.postgresql import insert as pg_insert' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\indexes\\\\_sql_record_manager.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# Note: uses SQLite insert to make on_conflict_do_update work.\\n                # This code needs to be generalized a bit to work with more dialects.\\n                insert_stmt = pg_insert(UpsertionRecord).values(records_to_upsert)\\n                stmt = insert_stmt.on_conflict_do_update(  # type: ignore[attr-defined]\\n                    \"uix_key_namespace\",  # Name of constraint\\n                    set_=dict(\\n                        # attr-defined type ignore\\n                        updated_at=insert_stmt.excluded.updated_at,  # type: ignore\\n                        group_id=insert_stmt.excluded.group_id,  # type: ignore\\n                    ),\\n                )\\n            else:\\n                raise NotImplementedError(f\"Unsupported dialect {self.dialect}\")\\n\\n            await session.execute(stmt)\\n            await session.commit()\\n\\n    def exists(self, keys: Sequence[str]) -> List[bool]:\\n        \"\"\"Check if the given keys exist in the SQLite database.\"\"\"\\n        with self._make_session() as session:\\n            records = (\\n                # mypy does not recognize .all()\\n                session.query(UpsertionRecord.key)  # type: ignore[attr-defined]\\n                .filter(\\n                    and_(\\n                        UpsertionRecord.key.in_(keys),\\n                        UpsertionRecord.namespace == self.namespace,\\n                    )\\n                )\\n                .all()\\n            )\\n        found_keys = set(r.key for r in records)\\n        return [k in found_keys for k in keys]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\indexes\\\\_sql_record_manager.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='async def aexists(self, keys: Sequence[str]) -> List[bool]:\\n        \"\"\"Check if the given keys exist in the SQLite database.\"\"\"\\n        async with self._amake_session() as session:\\n            records = (\\n                (\\n                    await session.execute(\\n                        select(UpsertionRecord.key).where(\\n                            and_(\\n                                UpsertionRecord.key.in_(keys),\\n                                UpsertionRecord.namespace == self.namespace,\\n                            )\\n                        )\\n                    )\\n                )\\n                .scalars()\\n                .all()\\n            )\\n        found_keys = set(records)\\n        return [k in found_keys for k in keys]\\n\\n    def list_keys(\\n        self,\\n        *,\\n        before: Optional[float] = None,\\n        after: Optional[float] = None,\\n        group_ids: Optional[Sequence[str]] = None,\\n        limit: Optional[int] = None,\\n    ) -> List[str]:\\n        \"\"\"List records in the SQLite database based on the provided date range.\"\"\"\\n        with self._make_session() as session:\\n            query = session.query(UpsertionRecord).filter(\\n                UpsertionRecord.namespace == self.namespace\\n            )\\n\\n            # mypy does not recognize .all() or .filter()\\n            if after:\\n                query = query.filter(  # type: ignore[attr-defined]\\n                    UpsertionRecord.updated_at > after\\n                )\\n            if before:\\n                query = query.filter(  # type: ignore[attr-defined]\\n                    UpsertionRecord.updated_at < before\\n                )\\n            if group_ids:\\n                query = query.filter(  # type: ignore[attr-defined]\\n                    UpsertionRecord.group_id.in_(group_ids)\\n                )\\n\\n            if limit:\\n                query = query.limit(limit)  # type: ignore[attr-defined]\\n            records = query.all()  # type: ignore[attr-defined]\\n        return [r.key for r in records]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\indexes\\\\_sql_record_manager.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='async def alist_keys(\\n        self,\\n        *,\\n        before: Optional[float] = None,\\n        after: Optional[float] = None,\\n        group_ids: Optional[Sequence[str]] = None,\\n        limit: Optional[int] = None,\\n    ) -> List[str]:\\n        \"\"\"List records in the SQLite database based on the provided date range.\"\"\"\\n        async with self._amake_session() as session:\\n            query = select(UpsertionRecord.key).filter(\\n                UpsertionRecord.namespace == self.namespace\\n            )\\n\\n            # mypy does not recognize .all() or .filter()\\n            if after:\\n                query = query.filter(  # type: ignore[attr-defined]\\n                    UpsertionRecord.updated_at > after\\n                )\\n            if before:\\n                query = query.filter(  # type: ignore[attr-defined]\\n                    UpsertionRecord.updated_at < before\\n                )\\n            if group_ids:\\n                query = query.filter(  # type: ignore[attr-defined]\\n                    UpsertionRecord.group_id.in_(group_ids)\\n                )\\n\\n            if limit:\\n                query = query.limit(limit)  # type: ignore[attr-defined]\\n            records = (await session.execute(query)).scalars().all()\\n        return list(records)\\n\\n    def delete_keys(self, keys: Sequence[str]) -> None:\\n        \"\"\"Delete records from the SQLite database.\"\"\"\\n        with self._make_session() as session:\\n            # mypy does not recognize .delete()\\n            session.query(UpsertionRecord).filter(\\n                and_(\\n                    UpsertionRecord.key.in_(keys),\\n                    UpsertionRecord.namespace == self.namespace,\\n                )\\n            ).delete()  # type: ignore[attr-defined]\\n            session.commit()' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\indexes\\\\_sql_record_manager.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='async def adelete_keys(self, keys: Sequence[str]) -> None:\\n        \"\"\"Delete records from the SQLite database.\"\"\"\\n        async with self._amake_session() as session:\\n            await session.execute(\\n                delete(UpsertionRecord).where(\\n                    and_(\\n                        UpsertionRecord.key.in_(keys),\\n                        UpsertionRecord.namespace == self.namespace,\\n                    )\\n                )\\n            )\\n\\n            await session.commit()' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\indexes\\\\_sql_record_manager.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Implementation of a record management layer in SQLAlchemy.\\n\\nThe management layer uses SQLAlchemy to track upserted records.\\n\\nCurrently, this layer only works with SQLite; hopwever, should be adaptable\\nto other SQL implementations with minimal effort.\\n\\nCurrently, includes an implementation that uses SQLAlchemy which should\\nallow it to work with a variety of SQL as a backend.\\n\\n* Each key is associated with an updated_at field.\\n* This filed is updated whenever the key is updated.\\n* Keys can be listed based on the updated at field.\\n* Keys can be deleted.\\n\"\"\"\\nimport contextlib\\nimport decimal\\nimport uuid\\nfrom typing import Any, AsyncGenerator, Dict, Generator, List, Optional, Sequence, Union\\n\\nfrom sqlalchemy import (\\n    URL,\\n    Column,\\n    Engine,\\n    Float,\\n    Index,\\n    String,\\n    UniqueConstraint,\\n    and_,\\n    create_engine,\\n    delete,\\n    select,\\n    text,\\n)\\nfrom sqlalchemy.ext.asyncio import (\\n    AsyncEngine,\\n    AsyncSession,\\n    async_sessionmaker,\\n    create_async_engine,\\n)\\nfrom sqlalchemy.ext.declarative import declarative_base\\nfrom sqlalchemy.orm import Session, sessionmaker\\n\\nfrom langchain.indexes.base import RecordManager\\n\\nBase = declarative_base()\\n\\n\\n# Code for: class UpsertionRecord(Base):  # type: ignore[valid-type,misc]\\n\\n\\n# Code for: class SQLRecordManager(RecordManager):' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\indexes\\\\_sql_record_manager.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Code to support various indexing workflows.\\n\\nProvides code to:\\n\\n* Create knowledge graphs from data.\\n\\n* Support indexing workflows from LangChain data loaders to vectorstores.\\n\\nFor indexing workflows, this code is used to avoid writing duplicated content\\ninto the vectostore and to avoid over-writing content if it\\'s unchanged.\\n\\nImportantly, this keeps on working even if the content being written is derived\\nvia a set of transformations from some source content (e.g., indexing children\\ndocuments that were derived from parent documents by chunking.)\\n\"\"\"\\nfrom langchain.indexes._api import IndexingResult, aindex, index\\nfrom langchain.indexes._sql_record_manager import SQLRecordManager\\nfrom langchain.indexes.graph import GraphIndexCreator\\nfrom langchain.indexes.vectorstore import VectorstoreIndexCreator\\n\\n__all__ = [\\n    # Keep sorted\\n    \"aindex\",\\n    \"GraphIndexCreator\",\\n    \"index\",\\n    \"IndexingResult\",\\n    \"SQLRecordManager\",\\n    \"VectorstoreIndexCreator\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\indexes\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# flake8: noqa\\nfrom langchain_core.prompts.prompt import PromptTemplate\\n\\n_DEFAULT_ENTITY_EXTRACTION_TEMPLATE = \"\"\"You are an AI assistant reading the transcript of a conversation between an AI and a human. Extract all of the proper nouns from the last line of conversation. As a guideline, a proper noun is generally capitalized. You should definitely extract all names and places.\\n\\nThe conversation history is provided just in case of a coreference (e.g. \"What do you know about him\" where \"him\" is defined in a previous line) -- ignore items mentioned there that are not in the last line.\\n\\nReturn the output as a single comma-separated list, or NONE if there is nothing of note to return (e.g. the user is just issuing a greeting or having a simple conversation).\\n\\nEXAMPLE\\nConversation history:\\nPerson #1: how\\'s it going today?\\nAI: \"It\\'s going great! How about you?\"\\nPerson #1: good! busy working on Langchain. lots to do.\\nAI: \"That sounds like a lot of work! What kind of things are you doing to make Langchain better?\"\\nLast line:\\nPerson #1: i\\'m trying to improve Langchain\\'s interfaces, the UX, its integrations with various products the user might want ... a lot of stuff.\\nOutput: Langchain\\nEND OF EXAMPLE\\n\\nEXAMPLE\\nConversation history:\\nPerson #1: how\\'s it going today?\\nAI: \"It\\'s going great! How about you?\"\\nPerson #1: good! busy working on Langchain. lots to do.\\nAI: \"That sounds like a lot of work! What kind of things are you doing to make Langchain better?\"\\nLast line:\\nPerson #1: i\\'m trying to improve Langchain\\'s interfaces, the UX, its integrations with various products the user might want ... a lot of stuff. I\\'m working with Person #2.\\nOutput: Langchain, Person #2\\nEND OF EXAMPLE\\n\\nConversation history (for reference only):\\n{history}\\nLast line of conversation (for extraction):\\nHuman: {input}\\n\\nOutput:\"\"\"\\nENTITY_EXTRACTION_PROMPT = PromptTemplate(\\n    input_variables=[\"history\", \"input\"], template=_DEFAULT_ENTITY_EXTRACTION_TEMPLATE\\n)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\indexes\\\\prompts\\\\entity_extraction.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# flake8: noqa\\nfrom langchain_core.prompts.prompt import PromptTemplate\\n\\n_DEFAULT_ENTITY_SUMMARIZATION_TEMPLATE = \"\"\"You are an AI assistant helping a human keep track of facts about relevant people, places, and concepts in their life. Update the summary of the provided entity in the \"Entity\" section based on the last line of your conversation with the human. If you are writing the summary for the first time, return a single sentence.\\nThe update should only include facts that are relayed in the last line of conversation about the provided entity, and should only contain facts about the provided entity.\\n\\nIf there is no new information about the provided entity or the information is not worth noting (not an important or relevant fact to remember long-term), return the existing summary unchanged.\\n\\nFull conversation history (for context):\\n{history}\\n\\nEntity to summarize:\\n{entity}\\n\\nExisting summary of {entity}:\\n{summary}\\n\\nLast line of conversation:\\nHuman: {input}\\nUpdated summary:\"\"\"\\n\\nENTITY_SUMMARIZATION_PROMPT = PromptTemplate(\\n    input_variables=[\"entity\", \"summary\", \"history\", \"input\"],\\n    template=_DEFAULT_ENTITY_SUMMARIZATION_TEMPLATE,\\n)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\indexes\\\\prompts\\\\entity_summarization.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# flake8: noqa\\n\\nfrom langchain_community.graphs.networkx_graph import KG_TRIPLE_DELIMITER\\nfrom langchain_core.prompts.prompt import PromptTemplate\\n\\n_DEFAULT_KNOWLEDGE_TRIPLE_EXTRACTION_TEMPLATE = (\\n    \"You are a networked intelligence helping a human track knowledge triples\"\\n    \" about all relevant people, things, concepts, etc. and integrating\"\\n    \" them with your knowledge stored within your weights\"\\n    \" as well as that stored in a knowledge graph.\"\\n    \" Extract all of the knowledge triples from the text.\"\\n    \" A knowledge triple is a clause that contains a subject, a predicate,\"\\n    \" and an object. The subject is the entity being described,\"\\n    \" the predicate is the property of the subject that is being\"\\n    \" described, and the object is the value of the property.\\\\n\\\\n\"\\n    \"EXAMPLE\\\\n\"\\n    \"It\\'s a state in the US. It\\'s also the number 1 producer of gold in the US.\\\\n\\\\n\"\\n    f\"Output: (Nevada, is a, state){KG_TRIPLE_DELIMITER}(Nevada, is in, US)\"\\n    f\"{KG_TRIPLE_DELIMITER}(Nevada, is the number 1 producer of, gold)\\\\n\"\\n    \"END OF EXAMPLE\\\\n\\\\n\"\\n    \"EXAMPLE\\\\n\"\\n    \"I\\'m going to the store.\\\\n\\\\n\"\\n    \"Output: NONE\\\\n\"\\n    \"END OF EXAMPLE\\\\n\\\\n\"\\n    \"EXAMPLE\\\\n\"\\n    \"Oh huh. I know Descartes likes to drive antique scooters and play the mandolin.\\\\n\"\\n    f\"Output: (Descartes, likes to drive, antique scooters){KG_TRIPLE_DELIMITER}(Descartes, plays, mandolin)\\\\n\"\\n    \"END OF EXAMPLE\\\\n\\\\n\"\\n    \"EXAMPLE\\\\n\"\\n    \"{text}\"\\n    \"Output:\"\\n)\\n\\nKNOWLEDGE_TRIPLE_EXTRACTION_PROMPT = PromptTemplate(\\n    input_variables=[\"text\"],\\n    template=_DEFAULT_KNOWLEDGE_TRIPLE_EXTRACTION_TEMPLATE,\\n)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\indexes\\\\prompts\\\\knowledge_triplet_extraction.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Relevant prompts for constructing indexes.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\indexes\\\\prompts\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.ai21 import AI21, AI21PenaltyData\\n\\n__all__ = [\"AI21PenaltyData\", \"AI21\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\ai21.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.aleph_alpha import AlephAlpha\\n\\n__all__ = [\"AlephAlpha\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\aleph_alpha.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.amazon_api_gateway import (\\n    AmazonAPIGateway,\\n)\\n\\n__all__ = [\"AmazonAPIGateway\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\amazon_api_gateway.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.anthropic import Anthropic\\n\\n__all__ = [\"Anthropic\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\anthropic.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.anyscale import (\\n    Anyscale,\\n)\\n\\n__all__ = [\"Anyscale\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\anyscale.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.arcee import Arcee\\n\\n__all__ = [\"Arcee\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\arcee.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.aviary import (\\n    Aviary,\\n)\\n\\n__all__ = [\"Aviary\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\aviary.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.azureml_endpoint import (\\n    AzureMLEndpointClient,\\n    AzureMLOnlineEndpoint,\\n    ContentFormatterBase,\\n    DollyContentFormatter,\\n    GPT2ContentFormatter,\\n    HFContentFormatter,\\n    LlamaContentFormatter,\\n    OSSContentFormatter,\\n)\\n\\n__all__ = [\\n    \"AzureMLEndpointClient\",\\n    \"ContentFormatterBase\",\\n    \"GPT2ContentFormatter\",\\n    \"OSSContentFormatter\",\\n    \"HFContentFormatter\",\\n    \"DollyContentFormatter\",\\n    \"LlamaContentFormatter\",\\n    \"AzureMLOnlineEndpoint\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\azureml_endpoint.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.baidu_qianfan_endpoint import QianfanLLMEndpoint\\n\\n__all__ = [\"QianfanLLMEndpoint\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\baidu_qianfan_endpoint.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.bananadev import Banana\\n\\n__all__ = [\"Banana\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\bananadev.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# Backwards compatibility.\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.language_models.llms import (\\n    LLM,\\n    BaseLLM,\\n)\\n\\n__all__ = [\\n    \"BaseLanguageModel\",\\n    \"BaseLLM\",\\n    \"LLM\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.baseten import Baseten\\n\\n__all__ = [\"Baseten\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\baseten.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.beam import Beam\\n\\n__all__ = [\"Beam\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\beam.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.bedrock import (\\n    Bedrock,\\n    BedrockBase,\\n)\\n\\n__all__ = [\\n    \"BedrockBase\",\\n    \"Bedrock\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\bedrock.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.bittensor import NIBittensorLLM\\n\\n__all__ = [\"NIBittensorLLM\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\bittensor.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.cerebriumai import CerebriumAI\\n\\n__all__ = [\"CerebriumAI\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\cerebriumai.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.chatglm import ChatGLM\\n\\n__all__ = [\"ChatGLM\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\chatglm.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.clarifai import Clarifai\\n\\n__all__ = [\"Clarifai\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\clarifai.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.cloudflare_workersai import CloudflareWorkersAI\\n\\n__all__ = [\"CloudflareWorkersAI\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\cloudflare_workersai.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.cohere import (\\n    Cohere,\\n)\\n\\n__all__ = [\\n    \"Cohere\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\cohere.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.ctransformers import CTransformers\\n\\n__all__ = [\"CTransformers\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\ctransformers.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.ctranslate2 import CTranslate2\\n\\n__all__ = [\"CTranslate2\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\ctranslate2.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.databricks import (\\n    Databricks,\\n)\\n\\n__all__ = [\\n    \"Databricks\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\databricks.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.deepinfra import (\\n    DeepInfra,\\n)\\n\\n__all__ = [\\n    \"DeepInfra\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\deepinfra.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.deepsparse import DeepSparse\\n\\n__all__ = [\"DeepSparse\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\deepsparse.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.edenai import EdenAI\\n\\n__all__ = [\"EdenAI\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\edenai.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.fake import FakeListLLM, FakeStreamingListLLM\\n\\n__all__ = [\"FakeListLLM\", \"FakeStreamingListLLM\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\fake.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.fireworks import (\\n    Fireworks,\\n)\\n\\n__all__ = [\\n    \"Fireworks\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\fireworks.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.forefrontai import ForefrontAI\\n\\n__all__ = [\"ForefrontAI\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\forefrontai.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.gigachat import GigaChat\\n\\n__all__ = [\"GigaChat\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\gigachat.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.google_palm import GooglePalm\\n\\n__all__ = [\"GooglePalm\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\google_palm.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.gooseai import GooseAI\\n\\n__all__ = [\"GooseAI\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\gooseai.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.gpt4all import GPT4All\\n\\n__all__ = [\"GPT4All\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\gpt4all.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.gradient_ai import GradientLLM, TrainResult\\n\\n__all__ = [\"TrainResult\", \"GradientLLM\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\gradient_ai.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.huggingface_endpoint import (\\n    HuggingFaceEndpoint,\\n)\\n\\n__all__ = [\"HuggingFaceEndpoint\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\huggingface_endpoint.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.huggingface_hub import (\\n    HuggingFaceHub,\\n)\\n\\n__all__ = [\"HuggingFaceHub\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\huggingface_hub.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.huggingface_pipeline import (\\n    HuggingFacePipeline,\\n)\\n\\n__all__ = [\\n    \"HuggingFacePipeline\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\huggingface_pipeline.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.huggingface_text_gen_inference import (\\n    HuggingFaceTextGenInference,\\n)\\n\\n__all__ = [\"HuggingFaceTextGenInference\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\huggingface_text_gen_inference.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.human import (\\n    HumanInputLLM,\\n)\\n\\n__all__ = [\"HumanInputLLM\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\human.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.javelin_ai_gateway import JavelinAIGateway, Params\\n\\n__all__ = [\"JavelinAIGateway\", \"Params\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\javelin_ai_gateway.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.koboldai import KoboldApiLLM\\n\\n__all__ = [\"KoboldApiLLM\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\koboldai.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.llamacpp import LlamaCpp\\n\\n__all__ = [\"LlamaCpp\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\llamacpp.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.loading import load_llm, load_llm_from_config\\n\\n__all__ = [\"load_llm_from_config\", \"load_llm\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\loading.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.manifest import ManifestWrapper\\n\\n__all__ = [\"ManifestWrapper\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\manifest.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.minimax import (\\n    Minimax,\\n)\\n\\n__all__ = [\"Minimax\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\minimax.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.mlflow import Mlflow\\n\\n__all__ = [\"Mlflow\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\mlflow.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.mlflow_ai_gateway import MlflowAIGateway\\n\\n__all__ = [\"MlflowAIGateway\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\mlflow_ai_gateway.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.modal import Modal\\n\\n__all__ = [\"Modal\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\modal.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.mosaicml import (\\n    MosaicML,\\n)\\n\\n__all__ = [\\n    \"MosaicML\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\mosaicml.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.nlpcloud import NLPCloud\\n\\n__all__ = [\"NLPCloud\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\nlpcloud.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.octoai_endpoint import OctoAIEndpoint\\n\\n__all__ = [\"OctoAIEndpoint\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\octoai_endpoint.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.ollama import (\\n    Ollama,\\n)\\n\\n__all__ = [\"Ollama\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\ollama.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.opaqueprompts import OpaquePrompts\\n\\n__all__ = [\"OpaquePrompts\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\opaqueprompts.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.openai import (\\n    AzureOpenAI,\\n    BaseOpenAI,\\n    OpenAI,\\n    OpenAIChat,\\n)\\n\\n__all__ = [\\n    \"BaseOpenAI\",\\n    \"OpenAI\",\\n    \"AzureOpenAI\",\\n    \"OpenAIChat\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\openai.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.openllm import OpenLLM\\n\\n__all__ = [\"OpenLLM\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\openllm.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.openlm import OpenLM\\n\\n__all__ = [\"OpenLM\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\openlm.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.pai_eas_endpoint import PaiEasEndpoint\\n\\n__all__ = [\"PaiEasEndpoint\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\pai_eas_endpoint.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.petals import Petals\\n\\n__all__ = [\"Petals\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\petals.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.pipelineai import PipelineAI\\n\\n__all__ = [\"PipelineAI\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\pipelineai.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.predibase import Predibase\\n\\n__all__ = [\"Predibase\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\predibase.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.predictionguard import PredictionGuard\\n\\n__all__ = [\"PredictionGuard\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\predictionguard.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.promptlayer_openai import (\\n    PromptLayerOpenAI,\\n    PromptLayerOpenAIChat,\\n)\\n\\n__all__ = [\"PromptLayerOpenAI\", \"PromptLayerOpenAIChat\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\promptlayer_openai.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.replicate import Replicate\\n\\n__all__ = [\"Replicate\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\replicate.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.rwkv import RWKV\\n\\n__all__ = [\"RWKV\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\rwkv.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.sagemaker_endpoint import (\\n    LLMContentHandler,\\n    SagemakerEndpoint,\\n)\\n\\n__all__ = [\"SagemakerEndpoint\", \"LLMContentHandler\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\sagemaker_endpoint.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.self_hosted import (\\n    SelfHostedPipeline,\\n)\\n\\n__all__ = [\"SelfHostedPipeline\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\self_hosted.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.self_hosted_hugging_face import (\\n    SelfHostedHuggingFaceLLM,\\n)\\n\\n__all__ = [\\n    \"SelfHostedHuggingFaceLLM\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\self_hosted_hugging_face.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.stochasticai import StochasticAI\\n\\n__all__ = [\"StochasticAI\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\stochasticai.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.symblai_nebula import (\\n    Nebula,\\n)\\n\\n__all__ = [\\n    \"Nebula\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\symblai_nebula.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.textgen import TextGen\\n\\n__all__ = [\"TextGen\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\textgen.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.titan_takeoff import TitanTakeoff\\n\\n__all__ = [\"TitanTakeoff\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\titan_takeoff.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.titan_takeoff_pro import TitanTakeoffPro\\n\\n__all__ = [\"TitanTakeoffPro\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\titan_takeoff_pro.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.together import Together\\n\\n__all__ = [\"Together\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\together.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.tongyi import (\\n    Tongyi,\\n)\\n\\n__all__ = [\\n    \"Tongyi\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\tongyi.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.utils import enforce_stop_tokens\\n\\n__all__ = [\"enforce_stop_tokens\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\utils.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.vertexai import (\\n    VertexAI,\\n    VertexAIModelGarden,\\n)\\n\\n__all__ = [\\n    \"VertexAI\",\\n    \"VertexAIModelGarden\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\vertexai.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.vllm import VLLM, VLLMOpenAI\\n\\n__all__ = [\"VLLM\", \"VLLMOpenAI\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\vllm.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.volcengine_maas import (\\n    VolcEngineMaasBase,\\n    VolcEngineMaasLLM,\\n)\\n\\n__all__ = [\"VolcEngineMaasBase\", \"VolcEngineMaasLLM\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\volcengine_maas.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.watsonxllm import WatsonxLLM\\n\\n__all__ = [\"WatsonxLLM\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\watsonxllm.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.writer import Writer\\n\\n__all__ = [\"Writer\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\writer.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.xinference import Xinference\\n\\n__all__ = [\"Xinference\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\xinference.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.llms.yandex import YandexGPT\\n\\n__all__ = [\"YandexGPT\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\yandex.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_ai21() -> Any:\\n    from langchain_community.llms.ai21 import AI21\\n\\n    return AI21' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_aleph_alpha() -> Any:\\n    from langchain_community.llms.aleph_alpha import AlephAlpha\\n\\n    return AlephAlpha' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_amazon_api_gateway() -> Any:\\n    from langchain_community.llms.amazon_api_gateway import AmazonAPIGateway\\n\\n    return AmazonAPIGateway' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_anthropic() -> Any:\\n    from langchain_community.llms.anthropic import Anthropic\\n\\n    return Anthropic' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_anyscale() -> Any:\\n    from langchain_community.llms.anyscale import Anyscale\\n\\n    return Anyscale' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_arcee() -> Any:\\n    from langchain_community.llms.arcee import Arcee\\n\\n    return Arcee' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_aviary() -> Any:\\n    from langchain_community.llms.aviary import Aviary\\n\\n    return Aviary' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_azureml_endpoint() -> Any:\\n    from langchain_community.llms.azureml_endpoint import AzureMLOnlineEndpoint\\n\\n    return AzureMLOnlineEndpoint' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_baidu_qianfan_endpoint() -> Any:\\n    from langchain_community.llms.baidu_qianfan_endpoint import QianfanLLMEndpoint\\n\\n    return QianfanLLMEndpoint' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_bananadev() -> Any:\\n    from langchain_community.llms.bananadev import Banana\\n\\n    return Banana' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_baseten() -> Any:\\n    from langchain_community.llms.baseten import Baseten\\n\\n    return Baseten' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_beam() -> Any:\\n    from langchain_community.llms.beam import Beam\\n\\n    return Beam' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_bedrock() -> Any:\\n    from langchain_community.llms.bedrock import Bedrock\\n\\n    return Bedrock' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_bittensor() -> Any:\\n    from langchain_community.llms.bittensor import NIBittensorLLM\\n\\n    return NIBittensorLLM' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_cerebriumai() -> Any:\\n    from langchain_community.llms.cerebriumai import CerebriumAI\\n\\n    return CerebriumAI' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_chatglm() -> Any:\\n    from langchain_community.llms.chatglm import ChatGLM\\n\\n    return ChatGLM' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_clarifai() -> Any:\\n    from langchain_community.llms.clarifai import Clarifai\\n\\n    return Clarifai' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_cohere() -> Any:\\n    from langchain_community.llms.cohere import Cohere\\n\\n    return Cohere' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_ctransformers() -> Any:\\n    from langchain_community.llms.ctransformers import CTransformers\\n\\n    return CTransformers' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_ctranslate2() -> Any:\\n    from langchain_community.llms.ctranslate2 import CTranslate2\\n\\n    return CTranslate2' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_databricks() -> Any:\\n    from langchain_community.llms.databricks import Databricks\\n\\n    return Databricks' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_databricks_chat() -> Any:\\n    from langchain_community.chat_models.databricks import ChatDatabricks\\n\\n    return ChatDatabricks' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_deepinfra() -> Any:\\n    from langchain_community.llms.deepinfra import DeepInfra\\n\\n    return DeepInfra' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_deepsparse() -> Any:\\n    from langchain_community.llms.deepsparse import DeepSparse\\n\\n    return DeepSparse' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_edenai() -> Any:\\n    from langchain_community.llms.edenai import EdenAI\\n\\n    return EdenAI' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_fake() -> Any:\\n    from langchain_community.llms.fake import FakeListLLM\\n\\n    return FakeListLLM' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_fireworks() -> Any:\\n    from langchain_community.llms.fireworks import Fireworks\\n\\n    return Fireworks' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_forefrontai() -> Any:\\n    from langchain_community.llms.forefrontai import ForefrontAI\\n\\n    return ForefrontAI' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_gigachat() -> Any:\\n    from langchain_community.llms.gigachat import GigaChat\\n\\n    return GigaChat' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_google_palm() -> Any:\\n    from langchain_community.llms.google_palm import GooglePalm\\n\\n    return GooglePalm' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_gooseai() -> Any:\\n    from langchain_community.llms.gooseai import GooseAI\\n\\n    return GooseAI' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_gpt4all() -> Any:\\n    from langchain_community.llms.gpt4all import GPT4All\\n\\n    return GPT4All' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_gradient_ai() -> Any:\\n    from langchain_community.llms.gradient_ai import GradientLLM\\n\\n    return GradientLLM' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_huggingface_endpoint() -> Any:\\n    from langchain_community.llms.huggingface_endpoint import HuggingFaceEndpoint\\n\\n    return HuggingFaceEndpoint' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_huggingface_hub() -> Any:\\n    from langchain_community.llms.huggingface_hub import HuggingFaceHub\\n\\n    return HuggingFaceHub' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_huggingface_pipeline() -> Any:\\n    from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\\n\\n    return HuggingFacePipeline' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_huggingface_text_gen_inference() -> Any:\\n    from langchain_community.llms.huggingface_text_gen_inference import (\\n        HuggingFaceTextGenInference,\\n    )\\n\\n    return HuggingFaceTextGenInference' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_human() -> Any:\\n    from langchain_community.llms.human import HumanInputLLM\\n\\n    return HumanInputLLM' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_javelin_ai_gateway() -> Any:\\n    from langchain_community.llms.javelin_ai_gateway import JavelinAIGateway\\n\\n    return JavelinAIGateway' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_koboldai() -> Any:\\n    from langchain_community.llms.koboldai import KoboldApiLLM\\n\\n    return KoboldApiLLM' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_llamacpp() -> Any:\\n    from langchain_community.llms.llamacpp import LlamaCpp\\n\\n    return LlamaCpp' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_manifest() -> Any:\\n    from langchain_community.llms.manifest import ManifestWrapper\\n\\n    return ManifestWrapper' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_minimax() -> Any:\\n    from langchain_community.llms.minimax import Minimax\\n\\n    return Minimax' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_mlflow() -> Any:\\n    from langchain_community.llms.mlflow import Mlflow\\n\\n    return Mlflow' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_mlflow_chat() -> Any:\\n    from langchain_community.chat_models.mlflow import ChatMlflow\\n\\n    return ChatMlflow' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_mlflow_ai_gateway() -> Any:\\n    from langchain_community.llms.mlflow_ai_gateway import MlflowAIGateway\\n\\n    return MlflowAIGateway' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_modal() -> Any:\\n    from langchain_community.llms.modal import Modal\\n\\n    return Modal' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_mosaicml() -> Any:\\n    from langchain_community.llms.mosaicml import MosaicML\\n\\n    return MosaicML' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_nlpcloud() -> Any:\\n    from langchain_community.llms.nlpcloud import NLPCloud\\n\\n    return NLPCloud' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_octoai_endpoint() -> Any:\\n    from langchain_community.llms.octoai_endpoint import OctoAIEndpoint\\n\\n    return OctoAIEndpoint' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_ollama() -> Any:\\n    from langchain_community.llms.ollama import Ollama\\n\\n    return Ollama' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_opaqueprompts() -> Any:\\n    from langchain_community.llms.opaqueprompts import OpaquePrompts\\n\\n    return OpaquePrompts' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_azure_openai() -> Any:\\n    from langchain_community.llms.openai import AzureOpenAI\\n\\n    return AzureOpenAI' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_openai() -> Any:\\n    from langchain_community.llms.openai import OpenAI\\n\\n    return OpenAI' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_openai_chat() -> Any:\\n    from langchain_community.llms.openai import OpenAIChat\\n\\n    return OpenAIChat' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_openllm() -> Any:\\n    from langchain_community.llms.openllm import OpenLLM\\n\\n    return OpenLLM' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_openlm() -> Any:\\n    from langchain_community.llms.openlm import OpenLM\\n\\n    return OpenLM' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_pai_eas_endpoint() -> Any:\\n    from langchain_community.llms.pai_eas_endpoint import PaiEasEndpoint\\n\\n    return PaiEasEndpoint' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_petals() -> Any:\\n    from langchain_community.llms.petals import Petals\\n\\n    return Petals' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_pipelineai() -> Any:\\n    from langchain_community.llms.pipelineai import PipelineAI\\n\\n    return PipelineAI' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_predibase() -> Any:\\n    from langchain_community.llms.predibase import Predibase\\n\\n    return Predibase' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_predictionguard() -> Any:\\n    from langchain_community.llms.predictionguard import PredictionGuard\\n\\n    return PredictionGuard' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_promptlayer() -> Any:\\n    from langchain_community.llms.promptlayer_openai import PromptLayerOpenAI\\n\\n    return PromptLayerOpenAI' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_promptlayer_chat() -> Any:\\n    from langchain_community.llms.promptlayer_openai import PromptLayerOpenAIChat\\n\\n    return PromptLayerOpenAIChat' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_replicate() -> Any:\\n    from langchain_community.llms.replicate import Replicate\\n\\n    return Replicate' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_rwkv() -> Any:\\n    from langchain_community.llms.rwkv import RWKV\\n\\n    return RWKV' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_sagemaker_endpoint() -> Any:\\n    from langchain_community.llms.sagemaker_endpoint import SagemakerEndpoint\\n\\n    return SagemakerEndpoint' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_self_hosted() -> Any:\\n    from langchain_community.llms.self_hosted import SelfHostedPipeline\\n\\n    return SelfHostedPipeline' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_self_hosted_hugging_face() -> Any:\\n    from langchain_community.llms.self_hosted_hugging_face import (\\n        SelfHostedHuggingFaceLLM,\\n    )\\n\\n    return SelfHostedHuggingFaceLLM' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_stochasticai() -> Any:\\n    from langchain_community.llms.stochasticai import StochasticAI\\n\\n    return StochasticAI' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_symblai_nebula() -> Any:\\n    from langchain_community.llms.symblai_nebula import Nebula\\n\\n    return Nebula' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_textgen() -> Any:\\n    from langchain_community.llms.textgen import TextGen\\n\\n    return TextGen' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_titan_takeoff() -> Any:\\n    from langchain_community.llms.titan_takeoff import TitanTakeoff\\n\\n    return TitanTakeoff' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_titan_takeoff_pro() -> Any:\\n    from langchain_community.llms.titan_takeoff_pro import TitanTakeoffPro\\n\\n    return TitanTakeoffPro' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_together() -> Any:\\n    from langchain_community.llms.together import Together\\n\\n    return Together' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_tongyi() -> Any:\\n    from langchain_community.llms.tongyi import Tongyi\\n\\n    return Tongyi' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_vertex() -> Any:\\n    from langchain_community.llms.vertexai import VertexAI\\n\\n    return VertexAI' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_vertex_model_garden() -> Any:\\n    from langchain_community.llms.vertexai import VertexAIModelGarden\\n\\n    return VertexAIModelGarden' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_vllm() -> Any:\\n    from langchain_community.llms.vllm import VLLM\\n\\n    return VLLM' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_vllm_openai() -> Any:\\n    from langchain_community.llms.vllm import VLLMOpenAI\\n\\n    return VLLMOpenAI' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_watsonxllm() -> Any:\\n    from langchain_community.llms.watsonxllm import WatsonxLLM\\n\\n    return WatsonxLLM' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_writer() -> Any:\\n    from langchain_community.llms.writer import Writer\\n\\n    return Writer' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_xinference() -> Any:\\n    from langchain_community.llms.xinference import Xinference\\n\\n    return Xinference' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_yandex_gpt() -> Any:\\n    from langchain_community.llms.yandex import YandexGPT\\n\\n    return YandexGPT' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _import_volcengine_maas() -> Any:\\n    from langchain_community.llms.volcengine_maas import VolcEngineMaasLLM\\n\\n    return VolcEngineMaasLLM' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def __getattr__(name: str) -> Any:\\n    from langchain_community import llms\\n\\n    # If not in interactive env, raise warning.\\n    if not is_interactive_env():\\n        warnings.warn(\\n            \"Importing LLMs from langchain is deprecated. Importing from \"\\n            \"langchain will no longer be supported as of langchain==0.2.0. \"\\n            \"Please import from langchain-community instead:\\\\n\\\\n\"\\n            f\"`from langchain_community.llms import {name}`.\\\\n\\\\n\"\\n            \"To install langchain-community run `pip install -U langchain-community`.\",\\n            category=LangChainDeprecationWarning,\\n        )\\n\\n    if name == \"type_to_cls_dict\":\\n        # for backwards compatibility\\n        type_to_cls_dict: Dict[str, Type[BaseLLM]] = {\\n            k: v() for k, v in get_type_to_cls_dict().items()\\n        }\\n        return type_to_cls_dict\\n    else:\\n        return getattr(llms, name)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def get_type_to_cls_dict() -> Dict[str, Callable[[], Type[BaseLLM]]]:\\n    return {\\n        \"ai21\": _import_ai21,\\n        \"aleph_alpha\": _import_aleph_alpha,\\n        \"amazon_api_gateway\": _import_amazon_api_gateway,\\n        \"amazon_bedrock\": _import_bedrock,\\n        \"anthropic\": _import_anthropic,\\n        \"anyscale\": _import_anyscale,\\n        \"arcee\": _import_arcee,\\n        \"aviary\": _import_aviary,\\n        \"azure\": _import_azure_openai,\\n        \"azureml_endpoint\": _import_azureml_endpoint,\\n        \"bananadev\": _import_bananadev,\\n        \"baseten\": _import_baseten,\\n        \"beam\": _import_beam,\\n        \"cerebriumai\": _import_cerebriumai,\\n        \"chat_glm\": _import_chatglm,\\n        \"clarifai\": _import_clarifai,\\n        \"cohere\": _import_cohere,\\n        \"ctransformers\": _import_ctransformers,\\n        \"ctranslate2\": _import_ctranslate2,\\n        \"databricks\": _import_databricks,\\n        \"databricks-chat\": _import_databricks_chat,\\n        \"deepinfra\": _import_deepinfra,\\n        \"deepsparse\": _import_deepsparse,\\n        \"edenai\": _import_edenai,\\n        \"fake-list\": _import_fake,\\n        \"forefrontai\": _import_forefrontai,\\n        \"giga-chat-model\": _import_gigachat,\\n        \"google_palm\": _import_google_palm,\\n        \"gooseai\": _import_gooseai,\\n        \"gradient\": _import_gradient_ai,\\n        \"gpt4all\": _import_gpt4all,\\n        \"huggingface_endpoint\": _import_huggingface_endpoint,\\n        \"huggingface_hub\": _import_huggingface_hub,\\n        \"huggingface_pipeline\": _import_huggingface_pipeline,\\n        \"huggingface_textgen_inference\": _import_huggingface_text_gen_inference,\\n        \"human-input\": _import_human,\\n        \"koboldai\": _import_koboldai,\\n        \"llamacpp\": _import_llamacpp,\\n        \"textgen\": _import_textgen,\\n        \"minimax\": _import_minimax,\\n        \"mlflow\": _import_mlflow,\\n        \"mlflow-chat\": _import_mlflow_chat,\\n        \"mlflow-ai-gateway\": _import_mlflow_ai_gateway,\\n        \"modal\": _import_modal,\\n        \"mosaic\": _import_mosaicml,\\n        \"nebula\": _import_symblai_nebula,' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"nibittensor\": _import_bittensor,\\n        \"nlpcloud\": _import_nlpcloud,\\n        \"ollama\": _import_ollama,\\n        \"openai\": _import_openai,\\n        \"openlm\": _import_openlm,\\n        \"pai_eas_endpoint\": _import_pai_eas_endpoint,\\n        \"petals\": _import_petals,\\n        \"pipelineai\": _import_pipelineai,\\n        \"predibase\": _import_predibase,\\n        \"opaqueprompts\": _import_opaqueprompts,\\n        \"replicate\": _import_replicate,\\n        \"rwkv\": _import_rwkv,\\n        \"sagemaker_endpoint\": _import_sagemaker_endpoint,\\n        \"self_hosted\": _import_self_hosted,\\n        \"self_hosted_hugging_face\": _import_self_hosted_hugging_face,\\n        \"stochasticai\": _import_stochasticai,\\n        \"together\": _import_together,\\n        \"tongyi\": _import_tongyi,\\n        \"titan_takeoff\": _import_titan_takeoff,\\n        \"titan_takeoff_pro\": _import_titan_takeoff_pro,\\n        \"vertexai\": _import_vertex,\\n        \"vertexai_model_garden\": _import_vertex_model_garden,\\n        \"openllm\": _import_openllm,\\n        \"openllm_client\": _import_openllm,\\n        \"vllm\": _import_vllm,\\n        \"vllm_openai\": _import_vllm_openai,\\n        \"watsonxllm\": _import_watsonxllm,\\n        \"writer\": _import_writer,\\n        \"xinference\": _import_xinference,\\n        \"javelin-ai-gateway\": _import_javelin_ai_gateway,\\n        \"qianfan_endpoint\": _import_baidu_qianfan_endpoint,\\n        \"yandex_gpt\": _import_yandex_gpt,\\n        \"VolcEngineMaasLLM\": _import_volcengine_maas,\\n    }' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"\\n**LLM** classes provide\\naccess to the large language model (**LLM**) APIs and services.\\n\\n**Class hierarchy:**\\n\\n.. code-block::\\n\\n    BaseLanguageModel --> BaseLLM --> LLM --> <name>  # Examples: AI21, HuggingFaceHub, OpenAI\\n\\n**Main helpers:**\\n\\n.. code-block::\\n\\n    LLMResult, PromptValue,\\n    CallbackManagerForLLMRun, AsyncCallbackManagerForLLMRun,\\n    CallbackManager, AsyncCallbackManager,\\n    AIMessage, BaseMessage\\n\"\"\"  # noqa: E501\\nimport warnings\\nfrom typing import Any, Callable, Dict, Type\\n\\nfrom langchain_core._api import LangChainDeprecationWarning\\nfrom langchain_core.language_models.llms import BaseLLM\\n\\nfrom langchain.utils.interactive_env import is_interactive_env\\n\\n\\n# Code for: def _import_ai21() -> Any:\\n\\n\\n# Code for: def _import_aleph_alpha() -> Any:\\n\\n\\n# Code for: def _import_amazon_api_gateway() -> Any:\\n\\n\\n# Code for: def _import_anthropic() -> Any:\\n\\n\\n# Code for: def _import_anyscale() -> Any:\\n\\n\\n# Code for: def _import_arcee() -> Any:\\n\\n\\n# Code for: def _import_aviary() -> Any:\\n\\n\\n# Code for: def _import_azureml_endpoint() -> Any:\\n\\n\\n# Code for: def _import_baidu_qianfan_endpoint() -> Any:\\n\\n\\n# Code for: def _import_bananadev() -> Any:\\n\\n\\n# Code for: def _import_baseten() -> Any:\\n\\n\\n# Code for: def _import_beam() -> Any:\\n\\n\\n# Code for: def _import_bedrock() -> Any:\\n\\n\\n# Code for: def _import_bittensor() -> Any:\\n\\n\\n# Code for: def _import_cerebriumai() -> Any:\\n\\n\\n# Code for: def _import_chatglm() -> Any:\\n\\n\\n# Code for: def _import_clarifai() -> Any:\\n\\n\\n# Code for: def _import_cohere() -> Any:\\n\\n\\n# Code for: def _import_ctransformers() -> Any:\\n\\n\\n# Code for: def _import_ctranslate2() -> Any:\\n\\n\\n# Code for: def _import_databricks() -> Any:\\n\\n\\n# Code for: def _import_databricks_chat() -> Any:\\n\\n\\n# Code for: def _import_deepinfra() -> Any:\\n\\n\\n# Code for: def _import_deepsparse() -> Any:\\n\\n\\n# Code for: def _import_edenai() -> Any:\\n\\n\\n# Code for: def _import_fake() -> Any:\\n\\n\\n# Code for: def _import_fireworks() -> Any:\\n\\n\\n# Code for: def _import_forefrontai() -> Any:\\n\\n\\n# Code for: def _import_gigachat() -> Any:' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# Code for: def _import_google_palm() -> Any:\\n\\n\\n# Code for: def _import_gooseai() -> Any:\\n\\n\\n# Code for: def _import_gpt4all() -> Any:\\n\\n\\n# Code for: def _import_gradient_ai() -> Any:\\n\\n\\n# Code for: def _import_huggingface_endpoint() -> Any:\\n\\n\\n# Code for: def _import_huggingface_hub() -> Any:\\n\\n\\n# Code for: def _import_huggingface_pipeline() -> Any:\\n\\n\\n# Code for: def _import_huggingface_text_gen_inference() -> Any:\\n\\n\\n# Code for: def _import_human() -> Any:\\n\\n\\n# Code for: def _import_javelin_ai_gateway() -> Any:\\n\\n\\n# Code for: def _import_koboldai() -> Any:\\n\\n\\n# Code for: def _import_llamacpp() -> Any:\\n\\n\\n# Code for: def _import_manifest() -> Any:\\n\\n\\n# Code for: def _import_minimax() -> Any:\\n\\n\\n# Code for: def _import_mlflow() -> Any:\\n\\n\\n# Code for: def _import_mlflow_chat() -> Any:\\n\\n\\n# Code for: def _import_mlflow_ai_gateway() -> Any:\\n\\n\\n# Code for: def _import_modal() -> Any:\\n\\n\\n# Code for: def _import_mosaicml() -> Any:\\n\\n\\n# Code for: def _import_nlpcloud() -> Any:\\n\\n\\n# Code for: def _import_octoai_endpoint() -> Any:\\n\\n\\n# Code for: def _import_ollama() -> Any:\\n\\n\\n# Code for: def _import_opaqueprompts() -> Any:\\n\\n\\n# Code for: def _import_azure_openai() -> Any:\\n\\n\\n# Code for: def _import_openai() -> Any:\\n\\n\\n# Code for: def _import_openai_chat() -> Any:\\n\\n\\n# Code for: def _import_openllm() -> Any:\\n\\n\\n# Code for: def _import_openlm() -> Any:\\n\\n\\n# Code for: def _import_pai_eas_endpoint() -> Any:\\n\\n\\n# Code for: def _import_petals() -> Any:\\n\\n\\n# Code for: def _import_pipelineai() -> Any:\\n\\n\\n# Code for: def _import_predibase() -> Any:\\n\\n\\n# Code for: def _import_predictionguard() -> Any:\\n\\n\\n# Code for: def _import_promptlayer() -> Any:\\n\\n\\n# Code for: def _import_promptlayer_chat() -> Any:\\n\\n\\n# Code for: def _import_replicate() -> Any:\\n\\n\\n# Code for: def _import_rwkv() -> Any:\\n\\n\\n# Code for: def _import_sagemaker_endpoint() -> Any:\\n\\n\\n# Code for: def _import_self_hosted() -> Any:\\n\\n\\n# Code for: def _import_self_hosted_hugging_face() -> Any:\\n\\n\\n# Code for: def _import_stochasticai() -> Any:\\n\\n\\n# Code for: def _import_symblai_nebula() -> Any:' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# Code for: def _import_textgen() -> Any:\\n\\n\\n# Code for: def _import_titan_takeoff() -> Any:\\n\\n\\n# Code for: def _import_titan_takeoff_pro() -> Any:\\n\\n\\n# Code for: def _import_together() -> Any:\\n\\n\\n# Code for: def _import_tongyi() -> Any:\\n\\n\\n# Code for: def _import_vertex() -> Any:\\n\\n\\n# Code for: def _import_vertex_model_garden() -> Any:\\n\\n\\n# Code for: def _import_vllm() -> Any:\\n\\n\\n# Code for: def _import_vllm_openai() -> Any:\\n\\n\\n# Code for: def _import_watsonxllm() -> Any:\\n\\n\\n# Code for: def _import_writer() -> Any:\\n\\n\\n# Code for: def _import_xinference() -> Any:\\n\\n\\n# Code for: def _import_yandex_gpt() -> Any:\\n\\n\\n# Code for: def _import_volcengine_maas() -> Any:\\n\\n\\n# Code for: def __getattr__(name: str) -> Any:' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='__all__ = [\\n    \"AI21\",\\n    \"AlephAlpha\",\\n    \"AmazonAPIGateway\",\\n    \"Anthropic\",\\n    \"Anyscale\",\\n    \"Arcee\",\\n    \"Aviary\",\\n    \"AzureMLOnlineEndpoint\",\\n    \"AzureOpenAI\",\\n    \"Banana\",\\n    \"Baseten\",\\n    \"Beam\",\\n    \"Bedrock\",\\n    \"CTransformers\",\\n    \"CTranslate2\",\\n    \"CerebriumAI\",\\n    \"ChatGLM\",\\n    \"Clarifai\",\\n    \"Cohere\",\\n    \"Databricks\",\\n    \"DeepInfra\",\\n    \"DeepSparse\",\\n    \"EdenAI\",\\n    \"FakeListLLM\",\\n    \"Fireworks\",\\n    \"ForefrontAI\",\\n    \"GigaChat\",\\n    \"GPT4All\",\\n    \"GooglePalm\",\\n    \"GooseAI\",\\n    \"GradientLLM\",\\n    \"HuggingFaceEndpoint\",\\n    \"HuggingFaceHub\",\\n    \"HuggingFacePipeline\",\\n    \"HuggingFaceTextGenInference\",\\n    \"HumanInputLLM\",\\n    \"KoboldApiLLM\",\\n    \"LlamaCpp\",\\n    \"TextGen\",\\n    \"ManifestWrapper\",\\n    \"Minimax\",\\n    \"MlflowAIGateway\",\\n    \"Modal\",\\n    \"MosaicML\",\\n    \"Nebula\",\\n    \"NIBittensorLLM\",\\n    \"NLPCloud\",\\n    \"Ollama\",\\n    \"OpenAI\",\\n    \"OpenAIChat\",\\n    \"OpenLLM\",\\n    \"OpenLM\",\\n    \"PaiEasEndpoint\",\\n    \"Petals\",\\n    \"PipelineAI\",\\n    \"Predibase\",\\n    \"PredictionGuard\",\\n    \"PromptLayerOpenAI\",\\n    \"PromptLayerOpenAIChat\",\\n    \"OpaquePrompts\",\\n    \"RWKV\",\\n    \"Replicate\",\\n    \"SagemakerEndpoint\",\\n    \"SelfHostedHuggingFaceLLM\",\\n    \"SelfHostedPipeline\",\\n    \"StochasticAI\",\\n    \"TitanTakeoff\",\\n    \"TitanTakeoffPro\",\\n    \"Tongyi\",\\n    \"VertexAI\",\\n    \"VertexAIModelGarden\",\\n    \"VLLM\",\\n    \"VLLMOpenAI\",\\n    \"WatsonxLLM\",\\n    \"Writer\",\\n    \"OctoAIEndpoint\",\\n    \"Xinference\",\\n    \"JavelinAIGateway\",\\n    \"QianfanLLMEndpoint\",\\n    \"YandexGPT\",\\n    \"VolcEngineMaasLLM\",\\n]\\n\\n\\n# Code for: def get_type_to_cls_dict() -> Dict[str, Callable[[], Type[BaseLLM]]]:' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\__init__.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.load.dump import default, dumpd, dumps\\n\\n__all__ = [\"default\", \"dumps\", \"dumpd\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\load\\\\dump.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.load.load import Reviver, load, loads\\n\\n__all__ = [\"Reviver\", \"loads\", \"load\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\load\\\\load.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.load.serializable import (\\n    BaseSerialized,\\n    Serializable,\\n    SerializedConstructor,\\n    SerializedNotImplemented,\\n    SerializedSecret,\\n    to_json_not_implemented,\\n    try_neq_default,\\n)\\n\\n__all__ = [\\n    \"BaseSerialized\",\\n    \"SerializedConstructor\",\\n    \"SerializedSecret\",\\n    \"SerializedNotImplemented\",\\n    \"try_neq_default\",\\n    \"Serializable\",\\n    \"to_json_not_implemented\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\load\\\\serializable.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Serialization and deserialization.\"\"\"\\nfrom langchain_core.load.dump import dumpd, dumps\\nfrom langchain_core.load.load import load, loads\\n\\n__all__ = [\\n    \"dumpd\",\\n    \"dumps\",\\n    \"load\",\\n    \"loads\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\load\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from typing import Any, Dict, List, Optional\\n\\nfrom langchain_core.messages import BaseMessage, get_buffer_string\\nfrom langchain_core.pydantic_v1 import root_validator\\n\\nfrom langchain.memory.chat_memory import BaseChatMemory, BaseMemory\\nfrom langchain.memory.utils import get_prompt_input_key\\n\\n\\nclass ConversationBufferMemory(BaseChatMemory):\\n    \"\"\"Buffer for storing conversation memory.\"\"\"\\n\\n    human_prefix: str = \"Human\"\\n    ai_prefix: str = \"AI\"\\n    memory_key: str = \"history\"  #: :meta private:\\n\\n    @property\\n    def buffer(self) -> Any:\\n        \"\"\"String buffer of memory.\"\"\"\\n        return self.buffer_as_messages if self.return_messages else self.buffer_as_str\\n\\n    @property\\n    def buffer_as_str(self) -> str:\\n        \"\"\"Exposes the buffer as a string in case return_messages is True.\"\"\"\\n        return get_buffer_string(\\n            self.chat_memory.messages,\\n            human_prefix=self.human_prefix,\\n            ai_prefix=self.ai_prefix,\\n        )\\n\\n    @property\\n    def buffer_as_messages(self) -> List[BaseMessage]:\\n        \"\"\"Exposes the buffer as a list of messages in case return_messages is False.\"\"\"\\n        return self.chat_memory.messages\\n\\n    @property\\n    def memory_variables(self) -> List[str]:\\n        \"\"\"Will always return list of memory variables.\\n\\n        :meta private:\\n        \"\"\"\\n        return [self.memory_key]\\n\\n    def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, Any]:\\n        \"\"\"Return history buffer.\"\"\"\\n        return {self.memory_key: self.buffer}\\n\\n\\nclass ConversationStringBufferMemory(BaseMemory):\\n    \"\"\"Buffer for storing conversation memory.\"\"\"\\n\\n    human_prefix: str = \"Human\"\\n    ai_prefix: str = \"AI\"\\n    \"\"\"Prefix to use for AI generated responses.\"\"\"\\n    buffer: str = \"\"\\n    output_key: Optional[str] = None\\n    input_key: Optional[str] = None\\n    memory_key: str = \"history\"  #: :meta private:' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\buffer.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@root_validator()\\n    def validate_chains(cls, values: Dict) -> Dict:\\n        \"\"\"Validate that return messages is not True.\"\"\"\\n        if values.get(\"return_messages\", False):\\n            raise ValueError(\\n                \"return_messages must be False for ConversationStringBufferMemory\"\\n            )\\n        return values\\n\\n    @property\\n    def memory_variables(self) -> List[str]:\\n        \"\"\"Will always return list of memory variables.\\n        :meta private:\\n        \"\"\"\\n        return [self.memory_key]\\n\\n    def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, str]:\\n        \"\"\"Return history buffer.\"\"\"\\n        return {self.memory_key: self.buffer}\\n\\n    def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:\\n        \"\"\"Save context from this conversation to buffer.\"\"\"\\n        if self.input_key is None:\\n            prompt_input_key = get_prompt_input_key(inputs, self.memory_variables)\\n        else:\\n            prompt_input_key = self.input_key\\n        if self.output_key is None:\\n            if len(outputs) != 1:\\n                raise ValueError(f\"One output key expected, got {outputs.keys()}\")\\n            output_key = list(outputs.keys())[0]\\n        else:\\n            output_key = self.output_key\\n        human = f\"{self.human_prefix}: \" + inputs[prompt_input_key]\\n        ai = f\"{self.ai_prefix}: \" + outputs[output_key]\\n        self.buffer += \"\\\\n\" + \"\\\\n\".join([human, ai])\\n\\n    def clear(self) -> None:\\n        \"\"\"Clear memory contents.\"\"\"\\n        self.buffer = \"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\buffer.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from typing import Any, Dict, List, Union\\n\\nfrom langchain_core.messages import BaseMessage, get_buffer_string\\n\\nfrom langchain.memory.chat_memory import BaseChatMemory\\n\\n\\nclass ConversationBufferWindowMemory(BaseChatMemory):\\n    \"\"\"Buffer for storing conversation memory inside a limited size window.\"\"\"\\n\\n    human_prefix: str = \"Human\"\\n    ai_prefix: str = \"AI\"\\n    memory_key: str = \"history\"  #: :meta private:\\n    k: int = 5\\n    \"\"\"Number of messages to store in buffer.\"\"\"\\n\\n    @property\\n    def buffer(self) -> Union[str, List[BaseMessage]]:\\n        \"\"\"String buffer of memory.\"\"\"\\n        return self.buffer_as_messages if self.return_messages else self.buffer_as_str\\n\\n    @property\\n    def buffer_as_str(self) -> str:\\n        \"\"\"Exposes the buffer as a string in case return_messages is True.\"\"\"\\n        messages = self.chat_memory.messages[-self.k * 2 :] if self.k > 0 else []\\n        return get_buffer_string(\\n            messages,\\n            human_prefix=self.human_prefix,\\n            ai_prefix=self.ai_prefix,\\n        )\\n\\n    @property\\n    def buffer_as_messages(self) -> List[BaseMessage]:\\n        \"\"\"Exposes the buffer as a list of messages in case return_messages is False.\"\"\"\\n        return self.chat_memory.messages[-self.k * 2 :] if self.k > 0 else []\\n\\n    @property\\n    def memory_variables(self) -> List[str]:\\n        \"\"\"Will always return list of memory variables.\\n\\n        :meta private:\\n        \"\"\"\\n        return [self.memory_key]\\n\\n    def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, Any]:\\n        \"\"\"Return history buffer.\"\"\"\\n        return {self.memory_key: self.buffer}' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\buffer_window.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from abc import ABC\\nfrom typing import Any, Dict, Optional, Tuple\\n\\nfrom langchain_community.chat_message_histories.in_memory import ChatMessageHistory\\nfrom langchain_core.chat_history import BaseChatMessageHistory\\nfrom langchain_core.memory import BaseMemory\\nfrom langchain_core.pydantic_v1 import Field\\n\\nfrom langchain.memory.utils import get_prompt_input_key\\n\\n\\nclass BaseChatMemory(BaseMemory, ABC):\\n    \"\"\"Abstract base class for chat memory.\"\"\"\\n\\n    chat_memory: BaseChatMessageHistory = Field(default_factory=ChatMessageHistory)\\n    output_key: Optional[str] = None\\n    input_key: Optional[str] = None\\n    return_messages: bool = False\\n\\n    def _get_input_output(\\n        self, inputs: Dict[str, Any], outputs: Dict[str, str]\\n    ) -> Tuple[str, str]:\\n        if self.input_key is None:\\n            prompt_input_key = get_prompt_input_key(inputs, self.memory_variables)\\n        else:\\n            prompt_input_key = self.input_key\\n        if self.output_key is None:\\n            if len(outputs) != 1:\\n                raise ValueError(f\"One output key expected, got {outputs.keys()}\")\\n            output_key = list(outputs.keys())[0]\\n        else:\\n            output_key = self.output_key\\n        return inputs[prompt_input_key], outputs[output_key]\\n\\n    def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:\\n        \"\"\"Save context from this conversation to buffer.\"\"\"\\n        input_str, output_str = self._get_input_output(inputs, outputs)\\n        self.chat_memory.add_user_message(input_str)\\n        self.chat_memory.add_ai_message(output_str)\\n\\n    def clear(self) -> None:\\n        \"\"\"Clear memory contents.\"\"\"\\n        self.chat_memory.clear()' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\chat_memory.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='import warnings\\nfrom typing import Any, Dict, List, Set\\n\\nfrom langchain_core.memory import BaseMemory\\nfrom langchain_core.pydantic_v1 import validator\\n\\nfrom langchain.memory.chat_memory import BaseChatMemory\\n\\n\\nclass CombinedMemory(BaseMemory):\\n    \"\"\"Combining multiple memories\\' data together.\"\"\"\\n\\n    memories: List[BaseMemory]\\n    \"\"\"For tracking all the memories that should be accessed.\"\"\"\\n\\n    @validator(\"memories\")\\n    def check_repeated_memory_variable(\\n        cls, value: List[BaseMemory]\\n    ) -> List[BaseMemory]:\\n        all_variables: Set[str] = set()\\n        for val in value:\\n            overlap = all_variables.intersection(val.memory_variables)\\n            if overlap:\\n                raise ValueError(\\n                    f\"The same variables {overlap} are found in multiple\"\\n                    \"memory object, which is not allowed by CombinedMemory.\"\\n                )\\n            all_variables |= set(val.memory_variables)\\n\\n        return value\\n\\n    @validator(\"memories\")\\n    def check_input_key(cls, value: List[BaseMemory]) -> List[BaseMemory]:\\n        \"\"\"Check that if memories are of type BaseChatMemory that input keys exist.\"\"\"\\n        for val in value:\\n            if isinstance(val, BaseChatMemory):\\n                if val.input_key is None:\\n                    warnings.warn(\\n                        \"When using CombinedMemory, \"\\n                        \"input keys should be so the input is known. \"\\n                        f\" Was not set on {val}\"\\n                    )\\n        return value\\n\\n    @property\\n    def memory_variables(self) -> List[str]:\\n        \"\"\"All the memory variables that this instance provides.\"\"\"\\n        \"\"\"Collected from the all the linked memories.\"\"\"\\n\\n        memory_variables = []\\n\\n        for memory in self.memories:\\n            memory_variables.extend(memory.memory_variables)\\n\\n        return memory_variables\\n\\n    def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, str]:\\n        \"\"\"Load all vars from sub-memories.\"\"\"\\n        memory_data: Dict[str, Any] = {}' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\combined.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# Collect vars from all sub-memories\\n        for memory in self.memories:\\n            data = memory.load_memory_variables(inputs)\\n            for key, value in data.items():\\n                if key in memory_data:\\n                    raise ValueError(\\n                        f\"The variable {key} is repeated in the CombinedMemory.\"\\n                    )\\n                memory_data[key] = value\\n\\n        return memory_data\\n\\n    def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:\\n        \"\"\"Save context from this session for every memory.\"\"\"\\n        # Save context for all sub-memories\\n        for memory in self.memories:\\n            memory.save_context(inputs, outputs)\\n\\n    def clear(self) -> None:\\n        \"\"\"Clear context from this session for every memory.\"\"\"\\n        for memory in self.memories:\\n            memory.clear()' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\combined.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='import logging\\nfrom abc import ABC, abstractmethod\\nfrom itertools import islice\\nfrom typing import Any, Dict, Iterable, List, Optional\\n\\nfrom langchain_community.utilities.redis import get_client\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.messages import BaseMessage, get_buffer_string\\nfrom langchain_core.prompts import BasePromptTemplate\\nfrom langchain_core.pydantic_v1 import BaseModel, Field\\n\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.memory.chat_memory import BaseChatMemory\\nfrom langchain.memory.prompt import (\\n    ENTITY_EXTRACTION_PROMPT,\\n    ENTITY_SUMMARIZATION_PROMPT,\\n)\\nfrom langchain.memory.utils import get_prompt_input_key\\n\\nlogger = logging.getLogger(__name__)\\n\\n\\nclass BaseEntityStore(BaseModel, ABC):\\n    \"\"\"Abstract base class for Entity store.\"\"\"\\n\\n    @abstractmethod\\n    def get(self, key: str, default: Optional[str] = None) -> Optional[str]:\\n        \"\"\"Get entity value from store.\"\"\"\\n        pass\\n\\n    @abstractmethod\\n    def set(self, key: str, value: Optional[str]) -> None:\\n        \"\"\"Set entity value in store.\"\"\"\\n        pass\\n\\n    @abstractmethod\\n    def delete(self, key: str) -> None:\\n        \"\"\"Delete entity value from store.\"\"\"\\n        pass\\n\\n    @abstractmethod\\n    def exists(self, key: str) -> bool:\\n        \"\"\"Check if entity exists in store.\"\"\"\\n        pass\\n\\n    @abstractmethod\\n    def clear(self) -> None:\\n        \"\"\"Delete all entities from store.\"\"\"\\n        pass\\n\\n\\nclass InMemoryEntityStore(BaseEntityStore):\\n    \"\"\"In-memory Entity store.\"\"\"\\n\\n    store: Dict[str, Optional[str]] = {}\\n\\n    def get(self, key: str, default: Optional[str] = None) -> Optional[str]:\\n        return self.store.get(key, default)\\n\\n    def set(self, key: str, value: Optional[str]) -> None:\\n        self.store[key] = value\\n\\n    def delete(self, key: str) -> None:\\n        del self.store[key]\\n\\n    def exists(self, key: str) -> bool:\\n        return key in self.store\\n\\n    def clear(self) -> None:\\n        return self.store.clear()' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\entity.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='class UpstashRedisEntityStore(BaseEntityStore):\\n    \"\"\"Upstash Redis backed Entity store.\\n\\n    Entities get a TTL of 1 day by default, and\\n    that TTL is extended by 3 days every time the entity is read back.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        session_id: str = \"default\",\\n        url: str = \"\",\\n        token: str = \"\",\\n        key_prefix: str = \"memory_store\",\\n        ttl: Optional[int] = 60 * 60 * 24,\\n        recall_ttl: Optional[int] = 60 * 60 * 24 * 3,\\n        *args: Any,\\n        **kwargs: Any,\\n    ):\\n        try:\\n            from upstash_redis import Redis\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import upstash_redis python package. \"\\n                \"Please install it with `pip install upstash_redis`.\"\\n            )\\n\\n        super().__init__(*args, **kwargs)\\n\\n        try:\\n            self.redis_client = Redis(url=url, token=token)\\n        except Exception:\\n            logger.error(\"Upstash Redis instance could not be initiated.\")\\n\\n        self.session_id = session_id\\n        self.key_prefix = key_prefix\\n        self.ttl = ttl\\n        self.recall_ttl = recall_ttl or ttl\\n\\n    @property\\n    def full_key_prefix(self) -> str:\\n        return f\"{self.key_prefix}:{self.session_id}\"\\n\\n    def get(self, key: str, default: Optional[str] = None) -> Optional[str]:\\n        res = (\\n            self.redis_client.getex(f\"{self.full_key_prefix}:{key}\", ex=self.recall_ttl)\\n            or default\\n            or \"\"\\n        )\\n        logger.debug(f\"Upstash Redis MEM get \\'{self.full_key_prefix}:{key}\\': \\'{res}\\'\")\\n        return res\\n\\n    def set(self, key: str, value: Optional[str]) -> None:\\n        if not value:\\n            return self.delete(key)\\n        self.redis_client.set(f\"{self.full_key_prefix}:{key}\", value, ex=self.ttl)\\n        logger.debug(\\n            f\"Redis MEM set \\'{self.full_key_prefix}:{key}\\': \\'{value}\\' EX {self.ttl}\"\\n        )\\n\\n    def delete(self, key: str) -> None:\\n        self.redis_client.delete(f\"{self.full_key_prefix}:{key}\")' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\entity.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def exists(self, key: str) -> bool:\\n        return self.redis_client.exists(f\"{self.full_key_prefix}:{key}\") == 1\\n\\n    def clear(self) -> None:\\n        def scan_and_delete(cursor: int) -> int:\\n            cursor, keys_to_delete = self.redis_client.scan(\\n                cursor, f\"{self.full_key_prefix}:*\"\\n            )\\n            self.redis_client.delete(*keys_to_delete)\\n            return cursor\\n\\n        cursor = scan_and_delete(0)\\n        while cursor != 0:\\n            scan_and_delete(cursor)\\n\\n\\nclass RedisEntityStore(BaseEntityStore):\\n    \"\"\"Redis-backed Entity store.\\n\\n    Entities get a TTL of 1 day by default, and\\n    that TTL is extended by 3 days every time the entity is read back.\\n    \"\"\"\\n\\n    redis_client: Any\\n    session_id: str = \"default\"\\n    key_prefix: str = \"memory_store\"\\n    ttl: Optional[int] = 60 * 60 * 24\\n    recall_ttl: Optional[int] = 60 * 60 * 24 * 3\\n\\n    def __init__(\\n        self,\\n        session_id: str = \"default\",\\n        url: str = \"redis://localhost:6379/0\",\\n        key_prefix: str = \"memory_store\",\\n        ttl: Optional[int] = 60 * 60 * 24,\\n        recall_ttl: Optional[int] = 60 * 60 * 24 * 3,\\n        *args: Any,\\n        **kwargs: Any,\\n    ):\\n        try:\\n            import redis\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import redis python package. \"\\n                \"Please install it with `pip install redis`.\"\\n            )\\n\\n        super().__init__(*args, **kwargs)\\n\\n        try:\\n            self.redis_client = get_client(redis_url=url, decode_responses=True)\\n        except redis.exceptions.ConnectionError as error:\\n            logger.error(error)\\n\\n        self.session_id = session_id\\n        self.key_prefix = key_prefix\\n        self.ttl = ttl\\n        self.recall_ttl = recall_ttl or ttl\\n\\n    @property\\n    def full_key_prefix(self) -> str:\\n        return f\"{self.key_prefix}:{self.session_id}\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\entity.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def get(self, key: str, default: Optional[str] = None) -> Optional[str]:\\n        res = (\\n            self.redis_client.getex(f\"{self.full_key_prefix}:{key}\", ex=self.recall_ttl)\\n            or default\\n            or \"\"\\n        )\\n        logger.debug(f\"REDIS MEM get \\'{self.full_key_prefix}:{key}\\': \\'{res}\\'\")\\n        return res\\n\\n    def set(self, key: str, value: Optional[str]) -> None:\\n        if not value:\\n            return self.delete(key)\\n        self.redis_client.set(f\"{self.full_key_prefix}:{key}\", value, ex=self.ttl)\\n        logger.debug(\\n            f\"REDIS MEM set \\'{self.full_key_prefix}:{key}\\': \\'{value}\\' EX {self.ttl}\"\\n        )\\n\\n    def delete(self, key: str) -> None:\\n        self.redis_client.delete(f\"{self.full_key_prefix}:{key}\")\\n\\n    def exists(self, key: str) -> bool:\\n        return self.redis_client.exists(f\"{self.full_key_prefix}:{key}\") == 1\\n\\n    def clear(self) -> None:\\n        # iterate a list in batches of size batch_size\\n        def batched(iterable: Iterable[Any], batch_size: int) -> Iterable[Any]:\\n            iterator = iter(iterable)\\n            while batch := list(islice(iterator, batch_size)):\\n                yield batch\\n\\n        for keybatch in batched(\\n            self.redis_client.scan_iter(f\"{self.full_key_prefix}:*\"), 500\\n        ):\\n            self.redis_client.delete(*keybatch)\\n\\n\\nclass SQLiteEntityStore(BaseEntityStore):\\n    \"\"\"SQLite-backed Entity store\"\"\"\\n\\n    session_id: str = \"default\"\\n    table_name: str = \"memory_store\"\\n    conn: Any = None\\n\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n\\n        arbitrary_types_allowed = True' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\entity.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def __init__(\\n        self,\\n        session_id: str = \"default\",\\n        db_file: str = \"entities.db\",\\n        table_name: str = \"memory_store\",\\n        *args: Any,\\n        **kwargs: Any,\\n    ):\\n        try:\\n            import sqlite3\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import sqlite3 python package. \"\\n                \"Please install it with `pip install sqlite3`.\"\\n            )\\n        super().__init__(*args, **kwargs)\\n\\n        self.conn = sqlite3.connect(db_file)\\n        self.session_id = session_id\\n        self.table_name = table_name\\n        self._create_table_if_not_exists()\\n\\n    @property\\n    def full_table_name(self) -> str:\\n        return f\"{self.table_name}_{self.session_id}\"\\n\\n    def _create_table_if_not_exists(self) -> None:\\n        create_table_query = f\"\"\"\\n            CREATE TABLE IF NOT EXISTS {self.full_table_name} (\\n                key TEXT PRIMARY KEY,\\n                value TEXT\\n            )\\n        \"\"\"\\n        with self.conn:\\n            self.conn.execute(create_table_query)\\n\\n    def get(self, key: str, default: Optional[str] = None) -> Optional[str]:\\n        query = f\"\"\"\\n            SELECT value\\n            FROM {self.full_table_name}\\n            WHERE key = ?\\n        \"\"\"\\n        cursor = self.conn.execute(query, (key,))\\n        result = cursor.fetchone()\\n        if result is not None:\\n            value = result[0]\\n            return value\\n        return default\\n\\n    def set(self, key: str, value: Optional[str]) -> None:\\n        if not value:\\n            return self.delete(key)\\n        query = f\"\"\"\\n            INSERT OR REPLACE INTO {self.full_table_name} (key, value)\\n            VALUES (?, ?)\\n        \"\"\"\\n        with self.conn:\\n            self.conn.execute(query, (key, value))\\n\\n    def delete(self, key: str) -> None:\\n        query = f\"\"\"\\n            DELETE FROM {self.full_table_name}\\n            WHERE key = ?\\n        \"\"\"\\n        with self.conn:\\n            self.conn.execute(query, (key,))' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\entity.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def exists(self, key: str) -> bool:\\n        query = f\"\"\"\\n            SELECT 1\\n            FROM {self.full_table_name}\\n            WHERE key = ?\\n            LIMIT 1\\n        \"\"\"\\n        cursor = self.conn.execute(query, (key,))\\n        result = cursor.fetchone()\\n        return result is not None\\n\\n    def clear(self) -> None:\\n        query = f\"\"\"\\n            DELETE FROM {self.full_table_name}\\n        \"\"\"\\n        with self.conn:\\n            self.conn.execute(query)\\n\\n\\nclass ConversationEntityMemory(BaseChatMemory):\\n    \"\"\"Entity extractor & summarizer memory.\\n\\n    Extracts named entities from the recent chat history and generates summaries.\\n    With a swappable entity store, persisting entities across conversations.\\n    Defaults to an in-memory entity store, and can be swapped out for a Redis,\\n    SQLite, or other entity store.\\n    \"\"\"\\n\\n    human_prefix: str = \"Human\"\\n    ai_prefix: str = \"AI\"\\n    llm: BaseLanguageModel\\n    entity_extraction_prompt: BasePromptTemplate = ENTITY_EXTRACTION_PROMPT\\n    entity_summarization_prompt: BasePromptTemplate = ENTITY_SUMMARIZATION_PROMPT\\n\\n    # Cache of recently detected entity names, if any\\n    # It is updated when load_memory_variables is called:\\n    entity_cache: List[str] = []\\n\\n    # Number of recent message pairs to consider when updating entities:\\n    k: int = 3\\n\\n    chat_history_key: str = \"history\"\\n\\n    # Store to manage entity-related data:\\n    entity_store: BaseEntityStore = Field(default_factory=InMemoryEntityStore)\\n\\n    @property\\n    def buffer(self) -> List[BaseMessage]:\\n        \"\"\"Access chat memory messages.\"\"\"\\n        return self.chat_memory.messages\\n\\n    @property\\n    def memory_variables(self) -> List[str]:\\n        \"\"\"Will always return list of memory variables.\\n\\n        :meta private:\\n        \"\"\"\\n        return [\"entities\", self.chat_history_key]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\entity.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, Any]:\\n        \"\"\"\\n        Returns chat history and all generated entities with summaries if available,\\n        and updates or clears the recent entity cache.\\n\\n        New entity name can be found when calling this method, before the entity\\n        summaries are generated, so the entity cache values may be empty if no entity\\n        descriptions are generated yet.\\n        \"\"\"\\n\\n        # Create an LLMChain for predicting entity names from the recent chat history:\\n        chain = LLMChain(llm=self.llm, prompt=self.entity_extraction_prompt)\\n\\n        if self.input_key is None:\\n            prompt_input_key = get_prompt_input_key(inputs, self.memory_variables)\\n        else:\\n            prompt_input_key = self.input_key\\n\\n        # Extract an arbitrary window of the last message pairs from\\n        # the chat history, where the hyperparameter k is the\\n        # number of message pairs:\\n        buffer_string = get_buffer_string(\\n            self.buffer[-self.k * 2 :],\\n            human_prefix=self.human_prefix,\\n            ai_prefix=self.ai_prefix,\\n        )\\n\\n        # Generates a comma-separated list of named entities,\\n        # e.g. \"Jane, White House, UFO\"\\n        # or \"NONE\" if no named entities are extracted:\\n        output = chain.predict(\\n            history=buffer_string,\\n            input=inputs[prompt_input_key],\\n        )\\n\\n        # If no named entities are extracted, assigns an empty list.\\n        if output.strip() == \"NONE\":\\n            entities = []\\n        else:\\n            # Make a list of the extracted entities:\\n            entities = [w.strip() for w in output.split(\",\")]\\n\\n        # Make a dictionary of entities with summary if exists:\\n        entity_summaries = {}\\n\\n        for entity in entities:\\n            entity_summaries[entity] = self.entity_store.get(entity, \"\")' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\entity.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# Replaces the entity name cache with the most recently discussed entities,\\n        # or if no entities were extracted, clears the cache:\\n        self.entity_cache = entities\\n\\n        # Should we return as message objects or as a string?\\n        if self.return_messages:\\n            # Get last `k` pair of chat messages:\\n            buffer: Any = self.buffer[-self.k * 2 :]\\n        else:\\n            # Reuse the string we made earlier:\\n            buffer = buffer_string\\n\\n        return {\\n            self.chat_history_key: buffer,\\n            \"entities\": entity_summaries,\\n        }\\n\\n    def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:\\n        \"\"\"\\n        Save context from this conversation history to the entity store.\\n\\n        Generates a summary for each entity in the entity cache by prompting\\n        the model, and saves these summaries to the entity store.\\n        \"\"\"\\n\\n        super().save_context(inputs, outputs)\\n\\n        if self.input_key is None:\\n            prompt_input_key = get_prompt_input_key(inputs, self.memory_variables)\\n        else:\\n            prompt_input_key = self.input_key\\n\\n        # Extract an arbitrary window of the last message pairs from\\n        # the chat history, where the hyperparameter k is the\\n        # number of message pairs:\\n        buffer_string = get_buffer_string(\\n            self.buffer[-self.k * 2 :],\\n            human_prefix=self.human_prefix,\\n            ai_prefix=self.ai_prefix,\\n        )\\n\\n        input_data = inputs[prompt_input_key]\\n\\n        # Create an LLMChain for predicting entity summarization from the context\\n        chain = LLMChain(llm=self.llm, prompt=self.entity_summarization_prompt)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\entity.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# Generate new summaries for entities and save them in the entity store\\n        for entity in self.entity_cache:\\n            # Get existing summary if it exists\\n            existing_summary = self.entity_store.get(entity, \"\")\\n            output = chain.predict(\\n                summary=existing_summary,\\n                entity=entity,\\n                history=buffer_string,\\n                input=input_data,\\n            )\\n            # Save the updated summary to the entity store\\n            self.entity_store.set(entity, output.strip())\\n\\n    def clear(self) -> None:\\n        \"\"\"Clear memory contents.\"\"\"\\n        self.chat_memory.clear()\\n        self.entity_cache.clear()\\n        self.entity_store.clear()' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\entity.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from typing import Any, Dict, List, Type, Union\\n\\nfrom langchain_community.graphs import NetworkxEntityGraph\\nfrom langchain_community.graphs.networkx_graph import (\\n    KnowledgeTriple,\\n    get_entities,\\n    parse_triples,\\n)\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.messages import BaseMessage, SystemMessage, get_buffer_string\\nfrom langchain_core.prompts import BasePromptTemplate\\nfrom langchain_core.pydantic_v1 import Field\\n\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.memory.chat_memory import BaseChatMemory\\nfrom langchain.memory.prompt import (\\n    ENTITY_EXTRACTION_PROMPT,\\n    KNOWLEDGE_TRIPLE_EXTRACTION_PROMPT,\\n)\\nfrom langchain.memory.utils import get_prompt_input_key\\n\\n\\nclass ConversationKGMemory(BaseChatMemory):\\n    \"\"\"Knowledge graph conversation memory.\\n\\n    Integrates with external knowledge graph to store and retrieve\\n    information about knowledge triples in the conversation.\\n    \"\"\"\\n\\n    k: int = 2\\n    human_prefix: str = \"Human\"\\n    ai_prefix: str = \"AI\"\\n    kg: NetworkxEntityGraph = Field(default_factory=NetworkxEntityGraph)\\n    knowledge_extraction_prompt: BasePromptTemplate = KNOWLEDGE_TRIPLE_EXTRACTION_PROMPT\\n    entity_extraction_prompt: BasePromptTemplate = ENTITY_EXTRACTION_PROMPT\\n    llm: BaseLanguageModel\\n    summary_message_cls: Type[BaseMessage] = SystemMessage\\n    \"\"\"Number of previous utterances to include in the context.\"\"\"\\n    memory_key: str = \"history\"  #: :meta private:\\n\\n    def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, Any]:\\n        \"\"\"Return history buffer.\"\"\"\\n        entities = self._get_current_entities(inputs)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\kg.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='summary_strings = []\\n        for entity in entities:\\n            knowledge = self.kg.get_entity_knowledge(entity)\\n            if knowledge:\\n                summary = f\"On {entity}: {\\'. \\'.join(knowledge)}.\"\\n                summary_strings.append(summary)\\n        context: Union[str, List]\\n        if not summary_strings:\\n            context = [] if self.return_messages else \"\"\\n        elif self.return_messages:\\n            context = [\\n                self.summary_message_cls(content=text) for text in summary_strings\\n            ]\\n        else:\\n            context = \"\\\\n\".join(summary_strings)\\n\\n        return {self.memory_key: context}\\n\\n    @property\\n    def memory_variables(self) -> List[str]:\\n        \"\"\"Will always return list of memory variables.\\n\\n        :meta private:\\n        \"\"\"\\n        return [self.memory_key]\\n\\n    def _get_prompt_input_key(self, inputs: Dict[str, Any]) -> str:\\n        \"\"\"Get the input key for the prompt.\"\"\"\\n        if self.input_key is None:\\n            return get_prompt_input_key(inputs, self.memory_variables)\\n        return self.input_key\\n\\n    def _get_prompt_output_key(self, outputs: Dict[str, Any]) -> str:\\n        \"\"\"Get the output key for the prompt.\"\"\"\\n        if self.output_key is None:\\n            if len(outputs) != 1:\\n                raise ValueError(f\"One output key expected, got {outputs.keys()}\")\\n            return list(outputs.keys())[0]\\n        return self.output_key\\n\\n    def get_current_entities(self, input_string: str) -> List[str]:\\n        chain = LLMChain(llm=self.llm, prompt=self.entity_extraction_prompt)\\n        buffer_string = get_buffer_string(\\n            self.chat_memory.messages[-self.k * 2 :],\\n            human_prefix=self.human_prefix,\\n            ai_prefix=self.ai_prefix,\\n        )\\n        output = chain.predict(\\n            history=buffer_string,\\n            input=input_string,\\n        )\\n        return get_entities(output)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\kg.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_current_entities(self, inputs: Dict[str, Any]) -> List[str]:\\n        \"\"\"Get the current entities in the conversation.\"\"\"\\n        prompt_input_key = self._get_prompt_input_key(inputs)\\n        return self.get_current_entities(inputs[prompt_input_key])\\n\\n    def get_knowledge_triplets(self, input_string: str) -> List[KnowledgeTriple]:\\n        chain = LLMChain(llm=self.llm, prompt=self.knowledge_extraction_prompt)\\n        buffer_string = get_buffer_string(\\n            self.chat_memory.messages[-self.k * 2 :],\\n            human_prefix=self.human_prefix,\\n            ai_prefix=self.ai_prefix,\\n        )\\n        output = chain.predict(\\n            history=buffer_string,\\n            input=input_string,\\n            verbose=True,\\n        )\\n        knowledge = parse_triples(output)\\n        return knowledge\\n\\n    def _get_and_update_kg(self, inputs: Dict[str, Any]) -> None:\\n        \"\"\"Get and update knowledge graph from the conversation history.\"\"\"\\n        prompt_input_key = self._get_prompt_input_key(inputs)\\n        knowledge = self.get_knowledge_triplets(inputs[prompt_input_key])\\n        for triple in knowledge:\\n            self.kg.add_triple(triple)\\n\\n    def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:\\n        \"\"\"Save context from this conversation to buffer.\"\"\"\\n        super().save_context(inputs, outputs)\\n        self._get_and_update_kg(inputs)\\n\\n    def clear(self) -> None:\\n        \"\"\"Clear memory contents.\"\"\"\\n        super().clear()\\n        self.kg.clear()' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\kg.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from typing import Any, Dict, List, Optional\\n\\nimport requests\\nfrom langchain_core.messages import get_buffer_string\\n\\nfrom langchain.memory.chat_memory import BaseChatMemory\\n\\nMANAGED_URL = \"https://api.getmetal.io/v1/motorhead\"\\n# LOCAL_URL = \"http://localhost:8080\"\\n\\n\\nclass MotorheadMemory(BaseChatMemory):\\n    \"\"\"Chat message memory backed by Motorhead service.\"\"\"\\n\\n    url: str = MANAGED_URL\\n    timeout: int = 3000\\n    memory_key: str = \"history\"\\n    session_id: str\\n    context: Optional[str] = None\\n\\n    # Managed Params\\n    api_key: Optional[str] = None\\n    client_id: Optional[str] = None\\n\\n    def __get_headers(self) -> Dict[str, str]:\\n        is_managed = self.url == MANAGED_URL\\n\\n        headers = {\\n            \"Content-Type\": \"application/json\",\\n        }\\n\\n        if is_managed and not (self.api_key and self.client_id):\\n            raise ValueError(\\n                \"\"\"\\n                You must provide an API key or a client ID to use the managed\\n                version of Motorhead. Visit https://getmetal.io for more information.\\n                \"\"\"\\n            )\\n\\n        if is_managed and self.api_key and self.client_id:\\n            headers[\"x-metal-api-key\"] = self.api_key\\n            headers[\"x-metal-client-id\"] = self.client_id\\n\\n        return headers\\n\\n    async def init(self) -> None:\\n        res = requests.get(\\n            f\"{self.url}/sessions/{self.session_id}/memory\",\\n            timeout=self.timeout,\\n            headers=self.__get_headers(),\\n        )\\n        res_data = res.json()\\n        res_data = res_data.get(\"data\", res_data)  # Handle Managed Version\\n\\n        messages = res_data.get(\"messages\", [])\\n        context = res_data.get(\"context\", \"NONE\")\\n\\n        for message in reversed(messages):\\n            if message[\"role\"] == \"AI\":\\n                self.chat_memory.add_ai_message(message[\"content\"])\\n            else:\\n                self.chat_memory.add_user_message(message[\"content\"])\\n\\n        if context and context != \"NONE\":\\n            self.context = context' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\motorhead_memory.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def load_memory_variables(self, values: Dict[str, Any]) -> Dict[str, Any]:\\n        if self.return_messages:\\n            return {self.memory_key: self.chat_memory.messages}\\n        else:\\n            return {self.memory_key: get_buffer_string(self.chat_memory.messages)}\\n\\n    @property\\n    def memory_variables(self) -> List[str]:\\n        return [self.memory_key]\\n\\n    def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:\\n        input_str, output_str = self._get_input_output(inputs, outputs)\\n        requests.post(\\n            f\"{self.url}/sessions/{self.session_id}/memory\",\\n            timeout=self.timeout,\\n            json={\\n                \"messages\": [\\n                    {\"role\": \"Human\", \"content\": f\"{input_str}\"},\\n                    {\"role\": \"AI\", \"content\": f\"{output_str}\"},\\n                ]\\n            },\\n            headers=self.__get_headers(),\\n        )\\n        super().save_context(inputs, outputs)\\n\\n    def delete_session(self) -> None:\\n        \"\"\"Delete a session\"\"\"\\n        requests.delete(f\"{self.url}/sessions/{self.session_id}/memory\")' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\motorhead_memory.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# flake8: noqa\\nfrom langchain_core.prompts.prompt import PromptTemplate\\n\\n_DEFAULT_ENTITY_MEMORY_CONVERSATION_TEMPLATE = \"\"\"You are an assistant to a human, powered by a large language model trained by OpenAI.\\n\\nYou are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\\n\\nYou are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\\n\\nOverall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\\n\\nContext:\\n{entities}\\n\\nCurrent conversation:\\n{history}\\nLast line:\\nHuman: {input}\\nYou:\"\"\"\\n\\nENTITY_MEMORY_CONVERSATION_TEMPLATE = PromptTemplate(\\n    input_variables=[\"entities\", \"history\", \"input\"],\\n    template=_DEFAULT_ENTITY_MEMORY_CONVERSATION_TEMPLATE,\\n)\\n\\n_DEFAULT_SUMMARIZER_TEMPLATE = \"\"\"Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\\n\\nEXAMPLE\\nCurrent summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='New lines of conversation:\\nHuman: Why do you think artificial intelligence is a force for good?\\nAI: Because artificial intelligence will help humans reach their full potential.\\n\\nNew summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\\nEND OF EXAMPLE\\n\\nCurrent summary:\\n{summary}\\n\\nNew lines of conversation:\\n{new_lines}\\n\\nNew summary:\"\"\"\\nSUMMARY_PROMPT = PromptTemplate(\\n    input_variables=[\"summary\", \"new_lines\"], template=_DEFAULT_SUMMARIZER_TEMPLATE\\n)\\n\\n_DEFAULT_ENTITY_EXTRACTION_TEMPLATE = \"\"\"You are an AI assistant reading the transcript of a conversation between an AI and a human. Extract all of the proper nouns from the last line of conversation. As a guideline, a proper noun is generally capitalized. You should definitely extract all names and places.\\n\\nThe conversation history is provided just in case of a coreference (e.g. \"What do you know about him\" where \"him\" is defined in a previous line) -- ignore items mentioned there that are not in the last line.\\n\\nReturn the output as a single comma-separated list, or NONE if there is nothing of note to return (e.g. the user is just issuing a greeting or having a simple conversation).\\n\\nEXAMPLE\\nConversation history:\\nPerson #1: how\\'s it going today?\\nAI: \"It\\'s going great! How about you?\"\\nPerson #1: good! busy working on Langchain. lots to do.\\nAI: \"That sounds like a lot of work! What kind of things are you doing to make Langchain better?\"\\nLast line:\\nPerson #1: i\\'m trying to improve Langchain\\'s interfaces, the UX, its integrations with various products the user might want ... a lot of stuff.\\nOutput: Langchain\\nEND OF EXAMPLE' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='EXAMPLE\\nConversation history:\\nPerson #1: how\\'s it going today?\\nAI: \"It\\'s going great! How about you?\"\\nPerson #1: good! busy working on Langchain. lots to do.\\nAI: \"That sounds like a lot of work! What kind of things are you doing to make Langchain better?\"\\nLast line:\\nPerson #1: i\\'m trying to improve Langchain\\'s interfaces, the UX, its integrations with various products the user might want ... a lot of stuff. I\\'m working with Person #2.\\nOutput: Langchain, Person #2\\nEND OF EXAMPLE\\n\\nConversation history (for reference only):\\n{history}\\nLast line of conversation (for extraction):\\nHuman: {input}\\n\\nOutput:\"\"\"\\nENTITY_EXTRACTION_PROMPT = PromptTemplate(\\n    input_variables=[\"history\", \"input\"], template=_DEFAULT_ENTITY_EXTRACTION_TEMPLATE\\n)\\n\\n_DEFAULT_ENTITY_SUMMARIZATION_TEMPLATE = \"\"\"You are an AI assistant helping a human keep track of facts about relevant people, places, and concepts in their life. Update the summary of the provided entity in the \"Entity\" section based on the last line of your conversation with the human. If you are writing the summary for the first time, return a single sentence.\\nThe update should only include facts that are relayed in the last line of conversation about the provided entity, and should only contain facts about the provided entity.\\n\\nIf there is no new information about the provided entity or the information is not worth noting (not an important or relevant fact to remember long-term), return the existing summary unchanged.\\n\\nFull conversation history (for context):\\n{history}\\n\\nEntity to summarize:\\n{entity}\\n\\nExisting summary of {entity}:\\n{summary}\\n\\nLast line of conversation:\\nHuman: {input}\\nUpdated summary:\"\"\"\\n\\nENTITY_SUMMARIZATION_PROMPT = PromptTemplate(\\n    input_variables=[\"entity\", \"summary\", \"history\", \"input\"],\\n    template=_DEFAULT_ENTITY_SUMMARIZATION_TEMPLATE,\\n)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='KG_TRIPLE_DELIMITER = \"<|>\"\\n_DEFAULT_KNOWLEDGE_TRIPLE_EXTRACTION_TEMPLATE = (\\n    \"You are a networked intelligence helping a human track knowledge triples\"\\n    \" about all relevant people, things, concepts, etc. and integrating\"\\n    \" them with your knowledge stored within your weights\"\\n    \" as well as that stored in a knowledge graph.\"\\n    \" Extract all of the knowledge triples from the last line of conversation.\"\\n    \" A knowledge triple is a clause that contains a subject, a predicate,\"\\n    \" and an object. The subject is the entity being described,\"\\n    \" the predicate is the property of the subject that is being\"\\n    \" described, and the object is the value of the property.\\\\n\\\\n\"\\n    \"EXAMPLE\\\\n\"\\n    \"Conversation history:\\\\n\"\\n    \"Person #1: Did you hear aliens landed in Area 51?\\\\n\"\\n    \"AI: No, I didn\\'t hear that. What do you know about Area 51?\\\\n\"\\n    \"Person #1: It\\'s a secret military base in Nevada.\\\\n\"\\n    \"AI: What do you know about Nevada?\\\\n\"\\n    \"Last line of conversation:\\\\n\"\\n    \"Person #1: It\\'s a state in the US. It\\'s also the number 1 producer of gold in the US.\\\\n\\\\n\"\\n    f\"Output: (Nevada, is a, state){KG_TRIPLE_DELIMITER}(Nevada, is in, US)\"\\n    f\"{KG_TRIPLE_DELIMITER}(Nevada, is the number 1 producer of, gold)\\\\n\"\\n    \"END OF EXAMPLE\\\\n\\\\n\"\\n    \"EXAMPLE\\\\n\"\\n    \"Conversation history:\\\\n\"\\n    \"Person #1: Hello.\\\\n\"\\n    \"AI: Hi! How are you?\\\\n\"\\n    \"Person #1: I\\'m good. How are you?\\\\n\"\\n    \"AI: I\\'m good too.\\\\n\"\\n    \"Last line of conversation:\\\\n\"\\n    \"Person #1: I\\'m going to the store.\\\\n\\\\n\"\\n    \"Output: NONE\\\\n\"\\n    \"END OF EXAMPLE\\\\n\\\\n\"\\n    \"EXAMPLE\\\\n\"\\n    \"Conversation history:\\\\n\"\\n    \"Person #1: What do you know about Descartes?\\\\n\"\\n    \"AI: Descartes was a French philosopher, mathematician, and scientist who lived in the 17th century.\\\\n\"\\n    \"Person #1: The Descartes I\\'m referring to is a standup comedian and interior designer from Montreal.\\\\n\"\\n    \"AI: Oh yes, He is a comedian and an interior designer. He has been in the industry for 30 years. His favorite food is baked bean pie.\\\\n\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"Last line of conversation:\\\\n\"\\n    \"Person #1: Oh huh. I know Descartes likes to drive antique scooters and play the mandolin.\\\\n\"\\n    f\"Output: (Descartes, likes to drive, antique scooters){KG_TRIPLE_DELIMITER}(Descartes, plays, mandolin)\\\\n\"\\n    \"END OF EXAMPLE\\\\n\\\\n\"\\n    \"Conversation history (for reference only):\\\\n\"\\n    \"{history}\"\\n    \"\\\\nLast line of conversation (for extraction):\\\\n\"\\n    \"Human: {input}\\\\n\\\\n\"\\n    \"Output:\"\\n)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='KNOWLEDGE_TRIPLE_EXTRACTION_PROMPT = PromptTemplate(\\n    input_variables=[\"history\", \"input\"],\\n    template=_DEFAULT_KNOWLEDGE_TRIPLE_EXTRACTION_TEMPLATE,\\n)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from typing import Any, Dict, List\\n\\nfrom langchain_core.memory import BaseMemory\\n\\n\\nclass ReadOnlySharedMemory(BaseMemory):\\n    \"\"\"A memory wrapper that is read-only and cannot be changed.\"\"\"\\n\\n    memory: BaseMemory\\n\\n    @property\\n    def memory_variables(self) -> List[str]:\\n        \"\"\"Return memory variables.\"\"\"\\n        return self.memory.memory_variables\\n\\n    def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, str]:\\n        \"\"\"Load memory variables from memory.\"\"\"\\n        return self.memory.load_memory_variables(inputs)\\n\\n    def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:\\n        \"\"\"Nothing should be saved or changed\"\"\"\\n        pass\\n\\n    def clear(self) -> None:\\n        \"\"\"Nothing to clear, got a memory like a vault.\"\"\"\\n        pass' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\readonly.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from typing import Any, Dict, List\\n\\nfrom langchain_core.memory import BaseMemory\\n\\n\\nclass SimpleMemory(BaseMemory):\\n    \"\"\"Simple memory for storing context or other information that shouldn\\'t\\n    ever change between prompts.\\n    \"\"\"\\n\\n    memories: Dict[str, Any] = dict()\\n\\n    @property\\n    def memory_variables(self) -> List[str]:\\n        return list(self.memories.keys())\\n\\n    def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, str]:\\n        return self.memories\\n\\n    def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:\\n        \"\"\"Nothing should be saved or changed, my memory is set in stone.\"\"\"\\n        pass\\n\\n    def clear(self) -> None:\\n        \"\"\"Nothing to clear, got a memory like a vault.\"\"\"\\n        pass' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\simple.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from __future__ import annotations\\n\\nfrom typing import Any, Dict, List, Type\\n\\nfrom langchain_core.chat_history import BaseChatMessageHistory\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.messages import BaseMessage, SystemMessage, get_buffer_string\\nfrom langchain_core.prompts import BasePromptTemplate\\nfrom langchain_core.pydantic_v1 import BaseModel, root_validator\\n\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.memory.chat_memory import BaseChatMemory\\nfrom langchain.memory.prompt import SUMMARY_PROMPT\\n\\n\\nclass SummarizerMixin(BaseModel):\\n    \"\"\"Mixin for summarizer.\"\"\"\\n\\n    human_prefix: str = \"Human\"\\n    ai_prefix: str = \"AI\"\\n    llm: BaseLanguageModel\\n    prompt: BasePromptTemplate = SUMMARY_PROMPT\\n    summary_message_cls: Type[BaseMessage] = SystemMessage\\n\\n    def predict_new_summary(\\n        self, messages: List[BaseMessage], existing_summary: str\\n    ) -> str:\\n        new_lines = get_buffer_string(\\n            messages,\\n            human_prefix=self.human_prefix,\\n            ai_prefix=self.ai_prefix,\\n        )\\n\\n        chain = LLMChain(llm=self.llm, prompt=self.prompt)\\n        return chain.predict(summary=existing_summary, new_lines=new_lines)\\n\\n\\nclass ConversationSummaryMemory(BaseChatMemory, SummarizerMixin):\\n    \"\"\"Conversation summarizer to chat memory.\"\"\"\\n\\n    buffer: str = \"\"\\n    memory_key: str = \"history\"  #: :meta private:\\n\\n    @classmethod\\n    def from_messages(\\n        cls,\\n        llm: BaseLanguageModel,\\n        chat_memory: BaseChatMessageHistory,\\n        *,\\n        summarize_step: int = 2,\\n        **kwargs: Any,\\n    ) -> ConversationSummaryMemory:\\n        obj = cls(llm=llm, chat_memory=chat_memory, **kwargs)\\n        for i in range(0, len(obj.chat_memory.messages), summarize_step):\\n            obj.buffer = obj.predict_new_summary(\\n                obj.chat_memory.messages[i : i + summarize_step], obj.buffer\\n            )\\n        return obj\\n\\n    @property\\n    def memory_variables(self) -> List[str]:\\n        \"\"\"Will always return list of memory variables.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\summary.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content=':meta private:\\n        \"\"\"\\n        return [self.memory_key]\\n\\n    def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, Any]:\\n        \"\"\"Return history buffer.\"\"\"\\n        if self.return_messages:\\n            buffer: Any = [self.summary_message_cls(content=self.buffer)]\\n        else:\\n            buffer = self.buffer\\n        return {self.memory_key: buffer}\\n\\n    @root_validator()\\n    def validate_prompt_input_variables(cls, values: Dict) -> Dict:\\n        \"\"\"Validate that prompt input variables are consistent.\"\"\"\\n        prompt_variables = values[\"prompt\"].input_variables\\n        expected_keys = {\"summary\", \"new_lines\"}\\n        if expected_keys != set(prompt_variables):\\n            raise ValueError(\\n                \"Got unexpected prompt input variables. The prompt expects \"\\n                f\"{prompt_variables}, but it should have {expected_keys}.\"\\n            )\\n        return values\\n\\n    def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:\\n        \"\"\"Save context from this conversation to buffer.\"\"\"\\n        super().save_context(inputs, outputs)\\n        self.buffer = self.predict_new_summary(\\n            self.chat_memory.messages[-2:], self.buffer\\n        )\\n\\n    def clear(self) -> None:\\n        \"\"\"Clear memory contents.\"\"\"\\n        super().clear()\\n        self.buffer = \"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\summary.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from typing import Any, Dict, List\\n\\nfrom langchain_core.messages import BaseMessage, get_buffer_string\\nfrom langchain_core.pydantic_v1 import root_validator\\n\\nfrom langchain.memory.chat_memory import BaseChatMemory\\nfrom langchain.memory.summary import SummarizerMixin\\n\\n\\nclass ConversationSummaryBufferMemory(BaseChatMemory, SummarizerMixin):\\n    \"\"\"Buffer with summarizer for storing conversation memory.\"\"\"\\n\\n    max_token_limit: int = 2000\\n    moving_summary_buffer: str = \"\"\\n    memory_key: str = \"history\"\\n\\n    @property\\n    def buffer(self) -> List[BaseMessage]:\\n        return self.chat_memory.messages\\n\\n    @property\\n    def memory_variables(self) -> List[str]:\\n        \"\"\"Will always return list of memory variables.\\n\\n        :meta private:\\n        \"\"\"\\n        return [self.memory_key]\\n\\n    def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, Any]:\\n        \"\"\"Return history buffer.\"\"\"\\n        buffer = self.buffer\\n        if self.moving_summary_buffer != \"\":\\n            first_messages: List[BaseMessage] = [\\n                self.summary_message_cls(content=self.moving_summary_buffer)\\n            ]\\n            buffer = first_messages + buffer\\n        if self.return_messages:\\n            final_buffer: Any = buffer\\n        else:\\n            final_buffer = get_buffer_string(\\n                buffer, human_prefix=self.human_prefix, ai_prefix=self.ai_prefix\\n            )\\n        return {self.memory_key: final_buffer}\\n\\n    @root_validator()\\n    def validate_prompt_input_variables(cls, values: Dict) -> Dict:\\n        \"\"\"Validate that prompt input variables are consistent.\"\"\"\\n        prompt_variables = values[\"prompt\"].input_variables\\n        expected_keys = {\"summary\", \"new_lines\"}\\n        if expected_keys != set(prompt_variables):\\n            raise ValueError(\\n                \"Got unexpected prompt input variables. The prompt expects \"\\n                f\"{prompt_variables}, but it should have {expected_keys}.\"\\n            )\\n        return values' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\summary_buffer.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:\\n        \"\"\"Save context from this conversation to buffer.\"\"\"\\n        super().save_context(inputs, outputs)\\n        self.prune()\\n\\n    def prune(self) -> None:\\n        \"\"\"Prune buffer if it exceeds max token limit\"\"\"\\n        buffer = self.chat_memory.messages\\n        curr_buffer_length = self.llm.get_num_tokens_from_messages(buffer)\\n        if curr_buffer_length > self.max_token_limit:\\n            pruned_memory = []\\n            while curr_buffer_length > self.max_token_limit:\\n                pruned_memory.append(buffer.pop(0))\\n                curr_buffer_length = self.llm.get_num_tokens_from_messages(buffer)\\n            self.moving_summary_buffer = self.predict_new_summary(\\n                pruned_memory, self.moving_summary_buffer\\n            )\\n\\n    def clear(self) -> None:\\n        \"\"\"Clear memory contents.\"\"\"\\n        super().clear()\\n        self.moving_summary_buffer = \"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\summary_buffer.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from typing import Any, Dict, List\\n\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.messages import BaseMessage, get_buffer_string\\n\\nfrom langchain.memory.chat_memory import BaseChatMemory\\n\\n\\nclass ConversationTokenBufferMemory(BaseChatMemory):\\n    \"\"\"Conversation chat memory with token limit.\"\"\"\\n\\n    human_prefix: str = \"Human\"\\n    ai_prefix: str = \"AI\"\\n    llm: BaseLanguageModel\\n    memory_key: str = \"history\"\\n    max_token_limit: int = 2000\\n\\n    @property\\n    def buffer(self) -> Any:\\n        \"\"\"String buffer of memory.\"\"\"\\n        return self.buffer_as_messages if self.return_messages else self.buffer_as_str\\n\\n    @property\\n    def buffer_as_str(self) -> str:\\n        \"\"\"Exposes the buffer as a string in case return_messages is False.\"\"\"\\n        return get_buffer_string(\\n            self.chat_memory.messages,\\n            human_prefix=self.human_prefix,\\n            ai_prefix=self.ai_prefix,\\n        )\\n\\n    @property\\n    def buffer_as_messages(self) -> List[BaseMessage]:\\n        \"\"\"Exposes the buffer as a list of messages in case return_messages is True.\"\"\"\\n        return self.chat_memory.messages\\n\\n    @property\\n    def memory_variables(self) -> List[str]:\\n        \"\"\"Will always return list of memory variables.\\n\\n        :meta private:\\n        \"\"\"\\n        return [self.memory_key]\\n\\n    def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, Any]:\\n        \"\"\"Return history buffer.\"\"\"\\n        return {self.memory_key: self.buffer}' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\token_buffer.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:\\n        \"\"\"Save context from this conversation to buffer. Pruned.\"\"\"\\n        super().save_context(inputs, outputs)\\n        # Prune buffer if it exceeds max token limit\\n        buffer = self.chat_memory.messages\\n        curr_buffer_length = self.llm.get_num_tokens_from_messages(buffer)\\n        if curr_buffer_length > self.max_token_limit:\\n            pruned_memory = []\\n            while curr_buffer_length > self.max_token_limit:\\n                pruned_memory.append(buffer.pop(0))\\n                curr_buffer_length = self.llm.get_num_tokens_from_messages(buffer)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\token_buffer.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from typing import Any, Dict, List\\n\\n\\ndef get_prompt_input_key(inputs: Dict[str, Any], memory_variables: List[str]) -> str:\\n    \"\"\"\\n    Get the prompt input key.\\n\\n    Args:\\n        inputs: Dict[str, Any]\\n        memory_variables: List[str]\\n\\n    Returns:\\n        A prompt input key.\\n    \"\"\"\\n    # \"stop\" is a special key that can be passed as input but is not used to\\n    # format the prompt.\\n    prompt_input_keys = list(set(inputs).difference(memory_variables + [\"stop\"]))\\n    if len(prompt_input_keys) != 1:\\n        raise ValueError(f\"One input key expected got {prompt_input_keys}\")\\n    return prompt_input_keys[0]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\utils.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Class for a VectorStore-backed memory object.\"\"\"\\n\\nfrom typing import Any, Dict, List, Optional, Sequence, Union\\n\\nfrom langchain_core.documents import Document\\nfrom langchain_core.pydantic_v1 import Field\\nfrom langchain_core.vectorstores import VectorStoreRetriever\\n\\nfrom langchain.memory.chat_memory import BaseMemory\\nfrom langchain.memory.utils import get_prompt_input_key\\n\\n\\nclass VectorStoreRetrieverMemory(BaseMemory):\\n    \"\"\"VectorStoreRetriever-backed memory.\"\"\"\\n\\n    retriever: VectorStoreRetriever = Field(exclude=True)\\n    \"\"\"VectorStoreRetriever object to connect to.\"\"\"\\n\\n    memory_key: str = \"history\"  #: :meta private:\\n    \"\"\"Key name to locate the memories in the result of load_memory_variables.\"\"\"\\n\\n    input_key: Optional[str] = None\\n    \"\"\"Key name to index the inputs to load_memory_variables.\"\"\"\\n\\n    return_docs: bool = False\\n    \"\"\"Whether or not to return the result of querying the database directly.\"\"\"\\n\\n    exclude_input_keys: Sequence[str] = Field(default_factory=tuple)\\n    \"\"\"Input keys to exclude in addition to memory key when constructing the document\"\"\"\\n\\n    @property\\n    def memory_variables(self) -> List[str]:\\n        \"\"\"The list of keys emitted from the load_memory_variables method.\"\"\"\\n        return [self.memory_key]\\n\\n    def _get_prompt_input_key(self, inputs: Dict[str, Any]) -> str:\\n        \"\"\"Get the input key for the prompt.\"\"\"\\n        if self.input_key is None:\\n            return get_prompt_input_key(inputs, self.memory_variables)\\n        return self.input_key\\n\\n    def load_memory_variables(\\n        self, inputs: Dict[str, Any]\\n    ) -> Dict[str, Union[List[Document], str]]:\\n        \"\"\"Return history buffer.\"\"\"\\n        input_key = self._get_prompt_input_key(inputs)\\n        query = inputs[input_key]\\n        docs = self.retriever.get_relevant_documents(query)\\n        result: Union[List[Document], str]\\n        if not self.return_docs:\\n            result = \"\\\\n\".join([doc.page_content for doc in docs])\\n        else:\\n            result = docs\\n        return {self.memory_key: result}' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\vectorstore.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _form_documents(\\n        self, inputs: Dict[str, Any], outputs: Dict[str, str]\\n    ) -> List[Document]:\\n        \"\"\"Format context from this conversation to buffer.\"\"\"\\n        # Each document should only include the current turn, not the chat history\\n        exclude = set(self.exclude_input_keys)\\n        exclude.add(self.memory_key)\\n        filtered_inputs = {k: v for k, v in inputs.items() if k not in exclude}\\n        texts = [\\n            f\"{k}: {v}\"\\n            for k, v in list(filtered_inputs.items()) + list(outputs.items())\\n        ]\\n        page_content = \"\\\\n\".join(texts)\\n        return [Document(page_content=page_content)]\\n\\n    def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:\\n        \"\"\"Save context from this conversation to buffer.\"\"\"\\n        documents = self._form_documents(inputs, outputs)\\n        self.retriever.add_documents(documents)\\n\\n    def clear(self) -> None:\\n        \"\"\"Nothing to clear.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\vectorstore.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from __future__ import annotations\\n\\nfrom typing import Any, Dict, Optional\\n\\nfrom langchain_community.chat_message_histories import ZepChatMessageHistory\\n\\nfrom langchain.memory import ConversationBufferMemory\\n\\n\\nclass ZepMemory(ConversationBufferMemory):\\n    \"\"\"Persist your chain history to the Zep MemoryStore.\\n\\n    The number of messages returned by Zep and when the Zep server summarizes chat\\n    histories is configurable. See the Zep documentation for more details.\\n\\n    Documentation: https://docs.getzep.com\\n\\n    Example:\\n        .. code-block:: python\\n\\n        memory = ZepMemory(\\n                    session_id=session_id,  # Identifies your user or a user\\'s session\\n                    url=ZEP_API_URL,        # Your Zep server\\'s URL\\n                    api_key=<your_api_key>, # Optional\\n                    memory_key=\"history\",   # Ensure this matches the key used in\\n                                            # chain\\'s prompt template\\n                    return_messages=True,   # Does your prompt template expect a string\\n                                            # or a list of Messages?\\n                )\\n        chain = LLMChain(memory=memory,...) # Configure your chain to use the ZepMemory\\n                                              instance\\n\\n\\n    Note:\\n        To persist metadata alongside your chat history, your will need to create a\\n    custom Chain class that overrides the `prep_outputs` method to include the metadata\\n    in the call to `self.memory.save_context`.\\n\\n\\n    Zep - Fast, scalable building blocks for LLM Apps\\n    =========\\n    Zep is an open source platform for productionizing LLM apps. Go from a prototype\\n    built in LangChain or LlamaIndex, or a custom app, to production in minutes without\\n    rewriting code.\\n\\n    For server installation instructions and more, see:\\n    https://docs.getzep.com/deployment/quickstart/\\n\\n    For more information on the zep-python package, see:\\n    https://github.com/getzep/zep-python\\n\\n    \"\"\"\\n\\n    chat_memory: ZepChatMessageHistory' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\zep_memory.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def __init__(\\n        self,\\n        session_id: str,\\n        url: str = \"http://localhost:8000\",\\n        api_key: Optional[str] = None,\\n        output_key: Optional[str] = None,\\n        input_key: Optional[str] = None,\\n        return_messages: bool = False,\\n        human_prefix: str = \"Human\",\\n        ai_prefix: str = \"AI\",\\n        memory_key: str = \"history\",\\n    ):\\n        \"\"\"Initialize ZepMemory.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\zep_memory.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Args:\\n            session_id (str): Identifies your user or a user\\'s session\\n            url (str, optional): Your Zep server\\'s URL. Defaults to\\n                                 \"http://localhost:8000\".\\n            api_key (Optional[str], optional): Your Zep API key. Defaults to None.\\n            output_key (Optional[str], optional): The key to use for the output message.\\n                                              Defaults to None.\\n            input_key (Optional[str], optional): The key to use for the input message.\\n                                              Defaults to None.\\n            return_messages (bool, optional): Does your prompt template expect a string\\n                                              or a list of Messages? Defaults to False\\n                                              i.e. return a string.\\n            human_prefix (str, optional): The prefix to use for human messages.\\n                                          Defaults to \"Human\".\\n            ai_prefix (str, optional): The prefix to use for AI messages.\\n                                       Defaults to \"AI\".\\n            memory_key (str, optional): The key to use for the memory.\\n                                        Defaults to \"history\".\\n                                        Ensure that this matches the key used in\\n                                        chain\\'s prompt template.\\n        \"\"\"\\n        chat_message_history = ZepChatMessageHistory(\\n            session_id=session_id,\\n            url=url,\\n            api_key=api_key,\\n        )\\n        super().__init__(\\n            chat_memory=chat_message_history,\\n            output_key=output_key,\\n            input_key=input_key,\\n            return_messages=return_messages,\\n            human_prefix=human_prefix,\\n            ai_prefix=ai_prefix,\\n            memory_key=memory_key,\\n        )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\zep_memory.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def save_context(\\n        self,\\n        inputs: Dict[str, Any],\\n        outputs: Dict[str, str],\\n        metadata: Optional[Dict[str, Any]] = None,\\n    ) -> None:\\n        \"\"\"Save context from this conversation to buffer.\\n\\n        Args:\\n            inputs (Dict[str, Any]): The inputs to the chain.\\n            outputs (Dict[str, str]): The outputs from the chain.\\n            metadata (Optional[Dict[str, Any]], optional): Any metadata to save with\\n                                                           the context. Defaults to None\\n\\n        Returns:\\n            None\\n        \"\"\"\\n        input_str, output_str = self._get_input_output(inputs, outputs)\\n        self.chat_memory.add_user_message(input_str, metadata=metadata)\\n        self.chat_memory.add_ai_message(output_str, metadata=metadata)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\zep_memory.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"**Memory** maintains Chain state, incorporating context from past runs.\\n\\n**Class hierarchy for Memory:**\\n\\n.. code-block::\\n\\n    BaseMemory --> BaseChatMemory --> <name>Memory  # Examples: ZepMemory, MotorheadMemory\\n\\n**Main helpers:**\\n\\n.. code-block::\\n\\n    BaseChatMessageHistory\\n\\n**Chat Message History** stores the chat message history in different stores.\\n\\n**Class hierarchy for ChatMessageHistory:**\\n\\n.. code-block::\\n\\n    BaseChatMessageHistory --> <name>ChatMessageHistory  # Example: ZepChatMessageHistory\\n\\n**Main helpers:**\\n\\n.. code-block::\\n\\n    AIMessage, BaseMessage, HumanMessage\\n\"\"\"  # noqa: E501\\nfrom langchain_community.chat_message_histories import (\\n    AstraDBChatMessageHistory,\\n    CassandraChatMessageHistory,\\n    ChatMessageHistory,\\n    CosmosDBChatMessageHistory,\\n    DynamoDBChatMessageHistory,\\n    ElasticsearchChatMessageHistory,\\n    FileChatMessageHistory,\\n    MomentoChatMessageHistory,\\n    MongoDBChatMessageHistory,\\n    PostgresChatMessageHistory,\\n    RedisChatMessageHistory,\\n    SingleStoreDBChatMessageHistory,\\n    SQLChatMessageHistory,\\n    StreamlitChatMessageHistory,\\n    UpstashRedisChatMessageHistory,\\n    XataChatMessageHistory,\\n    ZepChatMessageHistory,\\n)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain.memory.buffer import (\\n    ConversationBufferMemory,\\n    ConversationStringBufferMemory,\\n)\\nfrom langchain.memory.buffer_window import ConversationBufferWindowMemory\\nfrom langchain.memory.combined import CombinedMemory\\nfrom langchain.memory.entity import (\\n    ConversationEntityMemory,\\n    InMemoryEntityStore,\\n    RedisEntityStore,\\n    SQLiteEntityStore,\\n    UpstashRedisEntityStore,\\n)\\nfrom langchain.memory.kg import ConversationKGMemory\\nfrom langchain.memory.motorhead_memory import MotorheadMemory\\nfrom langchain.memory.readonly import ReadOnlySharedMemory\\nfrom langchain.memory.simple import SimpleMemory\\nfrom langchain.memory.summary import ConversationSummaryMemory\\nfrom langchain.memory.summary_buffer import ConversationSummaryBufferMemory\\nfrom langchain.memory.token_buffer import ConversationTokenBufferMemory\\nfrom langchain.memory.vectorstore import VectorStoreRetrieverMemory\\nfrom langchain.memory.zep_memory import ZepMemory' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='__all__ = [\\n    \"AstraDBChatMessageHistory\",\\n    \"CassandraChatMessageHistory\",\\n    \"ChatMessageHistory\",\\n    \"CombinedMemory\",\\n    \"ConversationBufferMemory\",\\n    \"ConversationBufferWindowMemory\",\\n    \"ConversationEntityMemory\",\\n    \"ConversationKGMemory\",\\n    \"ConversationStringBufferMemory\",\\n    \"ConversationSummaryBufferMemory\",\\n    \"ConversationSummaryMemory\",\\n    \"ConversationTokenBufferMemory\",\\n    \"CosmosDBChatMessageHistory\",\\n    \"DynamoDBChatMessageHistory\",\\n    \"ElasticsearchChatMessageHistory\",\\n    \"FileChatMessageHistory\",\\n    \"InMemoryEntityStore\",\\n    \"MomentoChatMessageHistory\",\\n    \"MongoDBChatMessageHistory\",\\n    \"MotorheadMemory\",\\n    \"PostgresChatMessageHistory\",\\n    \"ReadOnlySharedMemory\",\\n    \"RedisChatMessageHistory\",\\n    \"RedisEntityStore\",\\n    \"SingleStoreDBChatMessageHistory\",\\n    \"SQLChatMessageHistory\",\\n    \"SQLiteEntityStore\",\\n    \"SimpleMemory\",\\n    \"StreamlitChatMessageHistory\",\\n    \"VectorStoreRetrieverMemory\",\\n    \"XataChatMessageHistory\",\\n    \"ZepChatMessageHistory\",\\n    \"ZepMemory\",\\n    \"UpstashRedisEntityStore\",\\n    \"UpstashRedisChatMessageHistory\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_message_histories.astradb import (\\n    AstraDBChatMessageHistory,\\n)\\n\\n__all__ = [\"AstraDBChatMessageHistory\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\chat_message_histories\\\\astradb.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_message_histories.cassandra import (\\n    CassandraChatMessageHistory,\\n)\\n\\n__all__ = [\"CassandraChatMessageHistory\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\chat_message_histories\\\\cassandra.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_message_histories.cosmos_db import (\\n    CosmosDBChatMessageHistory,\\n)\\n\\n__all__ = [\"CosmosDBChatMessageHistory\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\chat_message_histories\\\\cosmos_db.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_message_histories.dynamodb import (\\n    DynamoDBChatMessageHistory,\\n)\\n\\n__all__ = [\"DynamoDBChatMessageHistory\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\chat_message_histories\\\\dynamodb.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_message_histories.elasticsearch import (\\n    ElasticsearchChatMessageHistory,\\n)\\n\\n__all__ = [\"ElasticsearchChatMessageHistory\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\chat_message_histories\\\\elasticsearch.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_message_histories.file import FileChatMessageHistory\\n\\n__all__ = [\"FileChatMessageHistory\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\chat_message_histories\\\\file.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_message_histories.firestore import (\\n    FirestoreChatMessageHistory,\\n)\\n\\n__all__ = [\"FirestoreChatMessageHistory\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\chat_message_histories\\\\firestore.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_message_histories.in_memory import ChatMessageHistory\\n\\n__all__ = [\"ChatMessageHistory\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\chat_message_histories\\\\in_memory.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_message_histories.momento import (\\n    MomentoChatMessageHistory,\\n)\\n\\n__all__ = [\"MomentoChatMessageHistory\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\chat_message_histories\\\\momento.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_message_histories.mongodb import (\\n    MongoDBChatMessageHistory,\\n)\\n\\n__all__ = [\"MongoDBChatMessageHistory\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\chat_message_histories\\\\mongodb.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_message_histories.neo4j import Neo4jChatMessageHistory\\n\\n__all__ = [\"Neo4jChatMessageHistory\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\chat_message_histories\\\\neo4j.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_message_histories.postgres import (\\n    PostgresChatMessageHistory,\\n)\\n\\n__all__ = [\"PostgresChatMessageHistory\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\chat_message_histories\\\\postgres.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_message_histories.redis import RedisChatMessageHistory\\n\\n__all__ = [\"RedisChatMessageHistory\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\chat_message_histories\\\\redis.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_message_histories.rocksetdb import (\\n    RocksetChatMessageHistory,\\n)\\n\\n__all__ = [\"RocksetChatMessageHistory\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\chat_message_histories\\\\rocksetdb.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_message_histories.singlestoredb import (\\n    SingleStoreDBChatMessageHistory,\\n)\\n\\n__all__ = [\"SingleStoreDBChatMessageHistory\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\chat_message_histories\\\\singlestoredb.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_message_histories.sql import (\\n    BaseMessageConverter,\\n    DefaultMessageConverter,\\n    SQLChatMessageHistory,\\n)\\n\\n__all__ = [\\n    \"BaseMessageConverter\",\\n    \"DefaultMessageConverter\",\\n    \"SQLChatMessageHistory\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\chat_message_histories\\\\sql.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_message_histories.streamlit import (\\n    StreamlitChatMessageHistory,\\n)\\n\\n__all__ = [\"StreamlitChatMessageHistory\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\chat_message_histories\\\\streamlit.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_message_histories.upstash_redis import (\\n    UpstashRedisChatMessageHistory,\\n)\\n\\n__all__ = [\"UpstashRedisChatMessageHistory\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\chat_message_histories\\\\upstash_redis.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_message_histories.xata import XataChatMessageHistory\\n\\n__all__ = [\"XataChatMessageHistory\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\chat_message_histories\\\\xata.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.chat_message_histories.zep import ZepChatMessageHistory\\n\\n__all__ = [\"ZepChatMessageHistory\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\chat_message_histories\\\\zep.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='import warnings\\nfrom typing import Any\\n\\nfrom langchain_core._api import LangChainDeprecationWarning\\n\\nfrom langchain.utils.interactive_env import is_interactive_env\\n\\n\\ndef __getattr__(name: str) -> Any:\\n    from langchain_community import chat_message_histories\\n\\n    # If not in interactive env, raise warning.\\n    if not is_interactive_env():\\n        warnings.warn(\\n            \"Importing chat message histories from langchain is deprecated. Importing \"\\n            \"from langchain will no longer be supported as of langchain==0.2.0. \"\\n            \"Please import from langchain-community instead:\\\\n\\\\n\"\\n            f\"`from langchain_community.chat_message_histories import {name}`.\\\\n\\\\n\"\\n            \"To install langchain-community run `pip install -U langchain-community`.\",\\n            category=LangChainDeprecationWarning,\\n        )\\n\\n    return getattr(chat_message_histories, name)\\n\\n\\n__all__ = [\\n    \"AstraDBChatMessageHistory\",\\n    \"ChatMessageHistory\",\\n    \"CassandraChatMessageHistory\",\\n    \"CosmosDBChatMessageHistory\",\\n    \"DynamoDBChatMessageHistory\",\\n    \"ElasticsearchChatMessageHistory\",\\n    \"FileChatMessageHistory\",\\n    \"FirestoreChatMessageHistory\",\\n    \"MomentoChatMessageHistory\",\\n    \"MongoDBChatMessageHistory\",\\n    \"PostgresChatMessageHistory\",\\n    \"RedisChatMessageHistory\",\\n    \"RocksetChatMessageHistory\",\\n    \"SQLChatMessageHistory\",\\n    \"StreamlitChatMessageHistory\",\\n    \"SingleStoreDBChatMessageHistory\",\\n    \"XataChatMessageHistory\",\\n    \"ZepChatMessageHistory\",\\n    \"UpstashRedisChatMessageHistory\",\\n    \"Neo4jChatMessageHistory\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\memory\\\\chat_message_histories\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.output_parsers import BaseOutputParser\\n\\n\\nclass BooleanOutputParser(BaseOutputParser[bool]):\\n    \"\"\"Parse the output of an LLM call to a boolean.\"\"\"\\n\\n    true_val: str = \"YES\"\\n    \"\"\"The string value that should be parsed as True.\"\"\"\\n    false_val: str = \"NO\"\\n    \"\"\"The string value that should be parsed as False.\"\"\"\\n\\n    def parse(self, text: str) -> bool:\\n        \"\"\"Parse the output of an LLM call to a boolean.\\n\\n        Args:\\n            text: output of a language model\\n\\n        Returns:\\n            boolean\\n\\n        \"\"\"\\n        cleaned_text = text.strip()\\n        if cleaned_text.upper() not in (self.true_val.upper(), self.false_val.upper()):\\n            raise ValueError(\\n                f\"BooleanOutputParser expected output value to either be \"\\n                f\"{self.true_val} or {self.false_val}. Received {cleaned_text}.\"\\n            )\\n        return cleaned_text.upper() == self.true_val.upper()\\n\\n    @property\\n    def _type(self) -> str:\\n        \"\"\"Snake-case string identifier for an output parser type.\"\"\"\\n        return \"boolean_output_parser\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\output_parsers\\\\boolean.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from __future__ import annotations\\n\\nfrom typing import Any, Dict, List\\n\\nfrom langchain_core.output_parsers import BaseOutputParser\\nfrom langchain_core.pydantic_v1 import root_validator\\n\\n\\nclass CombiningOutputParser(BaseOutputParser):\\n    \"\"\"Combine multiple output parsers into one.\"\"\"\\n\\n    parsers: List[BaseOutputParser]\\n\\n    @classmethod\\n    def is_lc_serializable(cls) -> bool:\\n        return True\\n\\n    @root_validator()\\n    def validate_parsers(cls, values: Dict[str, Any]) -> Dict[str, Any]:\\n        \"\"\"Validate the parsers.\"\"\"\\n        parsers = values[\"parsers\"]\\n        if len(parsers) < 2:\\n            raise ValueError(\"Must have at least two parsers\")\\n        for parser in parsers:\\n            if parser._type == \"combining\":\\n                raise ValueError(\"Cannot nest combining parsers\")\\n            if parser._type == \"list\":\\n                raise ValueError(\"Cannot combine list parsers\")\\n        return values\\n\\n    @property\\n    def _type(self) -> str:\\n        \"\"\"Return the type key.\"\"\"\\n        return \"combining\"\\n\\n    def get_format_instructions(self) -> str:\\n        \"\"\"Instructions on how the LLM output should be formatted.\"\"\"\\n\\n        initial = f\"For your first output: {self.parsers[0].get_format_instructions()}\"\\n        subsequent = \"\\\\n\".join(\\n            f\"Complete that output fully. Then produce another output, separated by two newline characters: {p.get_format_instructions()}\"  # noqa: E501\\n            for p in self.parsers[1:]\\n        )\\n        return f\"{initial}\\\\n{subsequent}\"\\n\\n    def parse(self, text: str) -> Dict[str, Any]:\\n        \"\"\"Parse the output of an LLM call.\"\"\"\\n        texts = text.split(\"\\\\n\\\\n\")\\n        output = dict()\\n        for txt, parser in zip(texts, self.parsers):\\n            output.update(parser.parse(txt.strip()))\\n        return output' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\output_parsers\\\\combining.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='import random\\nfrom datetime import datetime, timedelta\\nfrom typing import List\\n\\nfrom langchain_core.exceptions import OutputParserException\\nfrom langchain_core.output_parsers import BaseOutputParser\\nfrom langchain_core.utils import comma_list\\n\\n\\ndef _generate_random_datetime_strings(\\n    pattern: str,\\n    n: int = 3,\\n    start_date: datetime = datetime(1, 1, 1),\\n    end_date: datetime = datetime.now() + timedelta(days=3650),\\n) -> List[str]:\\n    \"\"\"Generates n random datetime strings conforming to the\\n    given pattern within the specified date range.\\n\\n    Pattern should be a string containing the desired format codes.\\n    start_date and end_date should be datetime objects representing\\n    the start and end of the date range.\\n    \"\"\"\\n    examples = []\\n    delta = end_date - start_date\\n    for i in range(n):\\n        random_delta = random.uniform(0, delta.total_seconds())\\n        dt = start_date + timedelta(seconds=random_delta)\\n        date_string = dt.strftime(pattern)\\n        examples.append(date_string)\\n    return examples\\n\\n\\nclass DatetimeOutputParser(BaseOutputParser[datetime]):\\n    \"\"\"Parse the output of an LLM call to a datetime.\"\"\"\\n\\n    format: str = \"%Y-%m-%dT%H:%M:%S.%fZ\"\\n    \"\"\"The string value that used as the datetime format.\"\"\"\\n\\n    def get_format_instructions(self) -> str:\\n        examples = comma_list(_generate_random_datetime_strings(self.format))\\n        return (\\n            f\"Write a datetime string that matches the \"\\n            f\"following pattern: \\'{self.format}\\'.\\\\n\\\\n\"\\n            f\"Examples: {examples}\\\\n\\\\n\"\\n            f\"Return ONLY this string, no other words!\"\\n        )\\n\\n    def parse(self, response: str) -> datetime:\\n        try:\\n            return datetime.strptime(response.strip(), self.format)\\n        except ValueError as e:\\n            raise OutputParserException(\\n                f\"Could not parse datetime string: {response}\"\\n            ) from e\\n\\n    @property\\n    def _type(self) -> str:\\n        return \"datetime\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\output_parsers\\\\datetime.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from enum import Enum\\nfrom typing import Any, Dict, List, Type\\n\\nfrom langchain_core.exceptions import OutputParserException\\nfrom langchain_core.output_parsers import BaseOutputParser\\nfrom langchain_core.pydantic_v1 import root_validator\\n\\n\\nclass EnumOutputParser(BaseOutputParser):\\n    \"\"\"Parse an output that is one of a set of values.\"\"\"\\n\\n    enum: Type[Enum]\\n    \"\"\"The enum to parse. Its values must be strings.\"\"\"\\n\\n    @root_validator()\\n    def raise_deprecation(cls, values: Dict) -> Dict:\\n        enum = values[\"enum\"]\\n        if not all(isinstance(e.value, str) for e in enum):\\n            raise ValueError(\"Enum values must be strings\")\\n        return values\\n\\n    @property\\n    def _valid_values(self) -> List[str]:\\n        return [e.value for e in self.enum]\\n\\n    def parse(self, response: str) -> Any:\\n        try:\\n            return self.enum(response.strip())\\n        except ValueError:\\n            raise OutputParserException(\\n                f\"Response \\'{response}\\' is not one of the \"\\n                f\"expected values: {self._valid_values}\"\\n            )\\n\\n    def get_format_instructions(self) -> str:\\n        return f\"Select one of the following options: {\\', \\'.join(self._valid_values)}\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\output_parsers\\\\enum.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.output_parsers.ernie_functions import (\\n    JsonKeyOutputFunctionsParser,\\n    JsonOutputFunctionsParser,\\n    OutputFunctionsParser,\\n    PydanticAttrOutputFunctionsParser,\\n    PydanticOutputFunctionsParser,\\n)\\n\\n__all__ = [\\n    \"JsonKeyOutputFunctionsParser\",\\n    \"JsonOutputFunctionsParser\",\\n    \"OutputFunctionsParser\",\\n    \"PydanticAttrOutputFunctionsParser\",\\n    \"PydanticOutputFunctionsParser\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\output_parsers\\\\ernie_functions.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from __future__ import annotations\\n\\nfrom typing import Any, TypeVar\\n\\nfrom langchain_core.exceptions import OutputParserException\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.output_parsers import BaseOutputParser\\nfrom langchain_core.prompts import BasePromptTemplate\\n\\nfrom langchain.output_parsers.prompts import NAIVE_FIX_PROMPT\\n\\nT = TypeVar(\"T\")\\n\\n\\nclass OutputFixingParser(BaseOutputParser[T]):\\n    \"\"\"Wraps a parser and tries to fix parsing errors.\"\"\"\\n\\n    @classmethod\\n    def is_lc_serializable(cls) -> bool:\\n        return True\\n\\n    parser: BaseOutputParser[T]\\n    \"\"\"The parser to use to parse the output.\"\"\"\\n    # Should be an LLMChain but we want to avoid top-level imports from langchain.chains\\n    retry_chain: Any\\n    \"\"\"The LLMChain to use to retry the completion.\"\"\"\\n    max_retries: int = 1\\n    \"\"\"The maximum number of times to retry the parse.\"\"\"\\n\\n    @classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        parser: BaseOutputParser[T],\\n        prompt: BasePromptTemplate = NAIVE_FIX_PROMPT,\\n        max_retries: int = 1,\\n    ) -> OutputFixingParser[T]:\\n        \"\"\"Create an OutputFixingParser from a language model and a parser.\\n\\n        Args:\\n            llm: llm to use for fixing\\n            parser: parser to use for parsing\\n            prompt: prompt to use for fixing\\n            max_retries: Maximum number of retries to parse.\\n\\n        Returns:\\n            OutputFixingParser\\n        \"\"\"\\n        from langchain.chains.llm import LLMChain\\n\\n        chain = LLMChain(llm=llm, prompt=prompt)\\n        return cls(parser=parser, retry_chain=chain, max_retries=max_retries)\\n\\n    def parse(self, completion: str) -> T:\\n        retries = 0' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\output_parsers\\\\fix.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='while retries <= self.max_retries:\\n            try:\\n                return self.parser.parse(completion)\\n            except OutputParserException as e:\\n                if retries == self.max_retries:\\n                    raise e\\n                else:\\n                    retries += 1\\n                    completion = self.retry_chain.run(\\n                        instructions=self.parser.get_format_instructions(),\\n                        completion=completion,\\n                        error=repr(e),\\n                    )\\n\\n        raise OutputParserException(\"Failed to parse\")\\n\\n    async def aparse(self, completion: str) -> T:\\n        retries = 0\\n\\n        while retries <= self.max_retries:\\n            try:\\n                return await self.parser.aparse(completion)\\n            except OutputParserException as e:\\n                if retries == self.max_retries:\\n                    raise e\\n                else:\\n                    retries += 1\\n                    completion = await self.retry_chain.arun(\\n                        instructions=self.parser.get_format_instructions(),\\n                        completion=completion,\\n                        error=repr(e),\\n                    )\\n\\n        raise OutputParserException(\"Failed to parse\")\\n\\n    def get_format_instructions(self) -> str:\\n        return self.parser.get_format_instructions()\\n\\n    @property\\n    def _type(self) -> str:\\n        return \"output_fixing\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\output_parsers\\\\fix.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# flake8: noqa\\n\\nSTRUCTURED_FORMAT_INSTRUCTIONS = \"\"\"The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\\n\\n```json\\n{{\\n{format}\\n}}\\n```\"\"\"\\n\\nSTRUCTURED_FORMAT_SIMPLE_INSTRUCTIONS = \"\"\"\\n```json\\n{{\\n{format}\\n}}\\n```\"\"\"\\n\\n\\nPYDANTIC_FORMAT_INSTRUCTIONS = \"\"\"The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {{\"properties\": {{\"foo\": {{\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {{\"type\": \"string\"}}}}}}, \"required\": [\"foo\"]}}\\nthe object {{\"foo\": [\"bar\", \"baz\"]}} is a well-formatted instance of the schema. The object {{\"properties\": {{\"foo\": [\"bar\", \"baz\"]}}}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{schema}\\n```\"\"\"\\n\\nYAML_FORMAT_INSTRUCTIONS = \"\"\"The output should be formatted as a YAML instance that conforms to the given JSON schema below.\\n\\n# Examples\\n## Schema\\n```\\n{{\"title\": \"Players\", \"description\": \"A list of players\", \"type\": \"array\", \"items\": {{\"$ref\": \"#/definitions/Player\"}}, \"definitions\": {{\"Player\": {{\"title\": \"Player\", \"type\": \"object\", \"properties\": {{\"name\": {{\"title\": \"Name\", \"description\": \"Player name\", \"type\": \"string\"}}, \"avg\": {{\"title\": \"Avg\", \"description\": \"Batting average\", \"type\": \"number\"}}}}, \"required\": [\"name\", \"avg\"]}}}}}}\\n```\\n## Well formatted instance\\n```\\n- name: John Doe\\n  avg: 0.3\\n- name: Jane Maxfield\\n  avg: 1.4\\n```\\n\\n## Schema\\n```\\n{{\"properties\": {{\"habit\": {{ \"description\": \"A common daily habit\", \"type\": \"string\" }}, \"sustainable_alternative\": {{ \"description\": \"An environmentally friendly alternative to the habit\", \"type\": \"string\"}}}}, \"required\": [\"habit\", \"sustainable_alternative\"]}}\\n```\\n## Well formatted instance\\n```\\nhabit: Using disposable water bottles for daily hydration.\\nsustainable_alternative: Switch to a reusable water bottle to reduce plastic waste and decrease your environmental footprint.\\n```' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\output_parsers\\\\format_instructions.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Please follow the standard YAML formatting conventions with an indent of 2 spaces and make sure that the data types adhere strictly to the following JSON schema: \\n```\\n{schema}\\n```\\n\\nMake sure to always enclose the YAML output in triple backticks (```). Please do not add anything other than valid YAML output!\"\"\"\\n\\n\\nPANDAS_DATAFRAME_FORMAT_INSTRUCTIONS = \"\"\"The output should be formatted as a string as the operation, followed by a colon, followed by the column or row to be queried on, followed by optional array parameters.\\n1. The column names are limited to the possible columns below.\\n2. Arrays must either be a comma-separated list of numbers formatted as [1,3,5], or it must be in range of numbers formatted as [0..4].\\n3. Remember that arrays are optional and not necessarily required.\\n4. If the column is not in the possible columns or the operation is not a valid Pandas DataFrame operation, return why it is invalid as a sentence starting with either \"Invalid column\" or \"Invalid operation\".\\n\\nAs an example, for the formats:\\n1. String \"column:num_legs\" is a well-formatted instance which gets the column num_legs, where num_legs is a possible column.\\n2. String \"row:1\" is a well-formatted instance which gets row 1.\\n3. String \"column:num_legs[1,2]\" is a well-formatted instance which gets the column num_legs for rows 1 and 2, where num_legs is a possible column.\\n4. String \"row:1[num_legs]\" is a well-formatted instance which gets row 1, but for just column num_legs, where num_legs is a possible column.\\n5. String \"mean:num_legs[1..3]\" is a well-formatted instance which takes the mean of num_legs from rows 1 to 3, where num_legs is a possible column and mean is a valid Pandas DataFrame operation.\\n6. String \"do_something:num_legs\" is a badly-formatted instance, where do_something is not a valid Pandas DataFrame operation.\\n7. String \"mean:invalid_col\" is a badly-formatted instance, where invalid_col is not a possible column.\\n\\nHere are the possible columns:\\n```\\n{columns}\\n```\\n\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\output_parsers\\\\format_instructions.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.output_parsers.json import (\\n    SimpleJsonOutputParser,\\n    parse_and_check_json_markdown,\\n    parse_json_markdown,\\n    parse_partial_json,\\n)\\n\\n__all__ = [\\n    \"SimpleJsonOutputParser\",\\n    \"parse_partial_json\",\\n    \"parse_json_markdown\",\\n    \"parse_and_check_json_markdown\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\output_parsers\\\\json.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.output_parsers.list import (\\n    CommaSeparatedListOutputParser,\\n    ListOutputParser,\\n    MarkdownListOutputParser,\\n    NumberedListOutputParser,\\n)\\n\\n__all__ = [\\n    \"ListOutputParser\",\\n    \"CommaSeparatedListOutputParser\",\\n    \"NumberedListOutputParser\",\\n    \"MarkdownListOutputParser\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\output_parsers\\\\list.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain.output_parsers.regex import RegexParser\\n\\n\\ndef load_output_parser(config: dict) -> dict:\\n    \"\"\"Load an output parser.\\n\\n    Args:\\n        config: config dict\\n\\n    Returns:\\n        config dict with output parser loaded\\n    \"\"\"\\n    if \"output_parsers\" in config:\\n        if config[\"output_parsers\"] is not None:\\n            _config = config[\"output_parsers\"]\\n            output_parser_type = _config[\"_type\"]\\n            if output_parser_type == \"regex_parser\":\\n                output_parser = RegexParser(**_config)\\n            else:\\n                raise ValueError(f\"Unsupported output parser {output_parser_type}\")\\n            config[\"output_parsers\"] = output_parser\\n    return config' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\output_parsers\\\\loading.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='import copy\\nimport json\\nfrom typing import Any, Dict, List, Optional, Type, Union\\n\\nimport jsonpatch\\nfrom langchain_core.exceptions import OutputParserException\\nfrom langchain_core.output_parsers import (\\n    BaseCumulativeTransformOutputParser,\\n    BaseGenerationOutputParser,\\n)\\nfrom langchain_core.output_parsers.json import parse_partial_json\\nfrom langchain_core.outputs import ChatGeneration, Generation\\nfrom langchain_core.pydantic_v1 import BaseModel, root_validator\\n\\n\\nclass OutputFunctionsParser(BaseGenerationOutputParser[Any]):\\n    \"\"\"Parse an output that is one of sets of values.\"\"\"\\n\\n    args_only: bool = True\\n    \"\"\"Whether to only return the arguments to the function call.\"\"\"\\n\\n    def parse_result(self, result: List[Generation], *, partial: bool = False) -> Any:\\n        generation = result[0]\\n        if not isinstance(generation, ChatGeneration):\\n            raise OutputParserException(\\n                \"This output parser can only be used with a chat generation.\"\\n            )\\n        message = generation.message\\n        try:\\n            func_call = copy.deepcopy(message.additional_kwargs[\"function_call\"])\\n        except KeyError as exc:\\n            raise OutputParserException(f\"Could not parse function call: {exc}\")\\n\\n        if self.args_only:\\n            return func_call[\"arguments\"]\\n        return func_call\\n\\n\\nclass JsonOutputFunctionsParser(BaseCumulativeTransformOutputParser[Any]):\\n    \"\"\"Parse an output as the Json object.\"\"\"\\n\\n    strict: bool = False\\n    \"\"\"Whether to allow non-JSON-compliant strings.\\n    \\n    See: https://docs.python.org/3/library/json.html#encoders-and-decoders\\n    \\n    Useful when the parsed output may include unicode characters or new lines.\\n    \"\"\"\\n\\n    args_only: bool = True\\n    \"\"\"Whether to only return the arguments to the function call.\"\"\"\\n\\n    @property\\n    def _type(self) -> str:\\n        return \"json_functions\"\\n\\n    def _diff(self, prev: Optional[Any], next: Any) -> Any:\\n        return jsonpatch.make_patch(prev, next).patch' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\output_parsers\\\\openai_functions.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def parse_result(self, result: List[Generation], *, partial: bool = False) -> Any:\\n        if len(result) != 1:\\n            raise OutputParserException(\\n                f\"Expected exactly one result, but got {len(result)}\"\\n            )\\n        generation = result[0]\\n        if not isinstance(generation, ChatGeneration):\\n            raise OutputParserException(\\n                \"This output parser can only be used with a chat generation.\"\\n            )\\n        message = generation.message\\n        try:\\n            function_call = message.additional_kwargs[\"function_call\"]\\n        except KeyError as exc:\\n            if partial:\\n                return None\\n            else:\\n                raise OutputParserException(f\"Could not parse function call: {exc}\")\\n        try:\\n            if partial:\\n                try:\\n                    if self.args_only:\\n                        return parse_partial_json(\\n                            function_call[\"arguments\"], strict=self.strict\\n                        )\\n                    else:\\n                        return {\\n                            **function_call,\\n                            \"arguments\": parse_partial_json(\\n                                function_call[\"arguments\"], strict=self.strict\\n                            ),\\n                        }\\n                except json.JSONDecodeError:\\n                    return None\\n            else:\\n                if self.args_only:\\n                    try:\\n                        return json.loads(\\n                            function_call[\"arguments\"], strict=self.strict\\n                        )\\n                    except (json.JSONDecodeError, TypeError) as exc:\\n                        raise OutputParserException(\\n                            f\"Could not parse function call data: {exc}\"\\n                        )\\n                else:\\n                    try:\\n                        return {\\n                            **function_call,\\n                            \"arguments\": json.loads(' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\output_parsers\\\\openai_functions.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='function_call[\"arguments\"], strict=self.strict\\n                            ),\\n                        }\\n                    except (json.JSONDecodeError, TypeError) as exc:\\n                        raise OutputParserException(\\n                            f\"Could not parse function call data: {exc}\"\\n                        )\\n        except KeyError:\\n            return None' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\output_parsers\\\\openai_functions.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# This method would be called by the default implementation of `parse_result`\\n    # but we\\'re overriding that method so it\\'s not needed.\\n    def parse(self, text: str) -> Any:\\n        raise NotImplementedError()\\n\\n\\nclass JsonKeyOutputFunctionsParser(JsonOutputFunctionsParser):\\n    \"\"\"Parse an output as the element of the Json object.\"\"\"\\n\\n    key_name: str\\n    \"\"\"The name of the key to return.\"\"\"\\n\\n    def parse_result(self, result: List[Generation], *, partial: bool = False) -> Any:\\n        res = super().parse_result(result, partial=partial)\\n        if partial and res is None:\\n            return None\\n        return res.get(self.key_name) if partial else res[self.key_name]\\n\\n\\nclass PydanticOutputFunctionsParser(OutputFunctionsParser):\\n    \"\"\"Parse an output as a pydantic object.\\n\\n    This parser is used to parse the output of a ChatModel that uses\\n    OpenAI function format to invoke functions.\\n\\n    The parser extracts the function call invocation and matches\\n    them to the pydantic schema provided.\\n\\n    An exception will be raised if the function call does not match\\n    the provided schema.\\n\\n    Example:\\n\\n        ... code-block:: python\\n\\n            message = AIMessage(\\n                content=\"This is a test message\",\\n                additional_kwargs={\\n                    \"function_call\": {\\n                        \"name\": \"cookie\",\\n                        \"arguments\": json.dumps({\"name\": \"value\", \"age\": 10}),\\n                    }\\n                },\\n            )\\n            chat_generation = ChatGeneration(message=message)\\n\\n            class Cookie(BaseModel):\\n                name: str\\n                age: int\\n\\n            class Dog(BaseModel):\\n                species: str\\n\\n            # Full output\\n            parser = PydanticOutputFunctionsParser(\\n                pydantic_schema={\"cookie\": Cookie, \"dog\": Dog}\\n            )\\n            result = parser.parse_result([chat_generation])\\n    \"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\output_parsers\\\\openai_functions.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='pydantic_schema: Union[Type[BaseModel], Dict[str, Type[BaseModel]]]\\n    \"\"\"The pydantic schema to parse the output with.\\n    \\n    If multiple schemas are provided, then the function name will be used to\\n    determine which schema to use.\\n    \"\"\"\\n\\n    @root_validator(pre=True)\\n    def validate_schema(cls, values: Dict) -> Dict:\\n        schema = values[\"pydantic_schema\"]\\n        if \"args_only\" not in values:\\n            values[\"args_only\"] = isinstance(schema, type) and issubclass(\\n                schema, BaseModel\\n            )\\n        elif values[\"args_only\"] and isinstance(schema, Dict):\\n            raise ValueError(\\n                \"If multiple pydantic schemas are provided then args_only should be\"\\n                \" False.\"\\n            )\\n        return values\\n\\n    def parse_result(self, result: List[Generation], *, partial: bool = False) -> Any:\\n        _result = super().parse_result(result)\\n        if self.args_only:\\n            pydantic_args = self.pydantic_schema.parse_raw(_result)  # type: ignore\\n        else:\\n            fn_name = _result[\"name\"]\\n            _args = _result[\"arguments\"]\\n            pydantic_args = self.pydantic_schema[fn_name].parse_raw(_args)  # type: ignore  # noqa: E501\\n        return pydantic_args\\n\\n\\nclass PydanticAttrOutputFunctionsParser(PydanticOutputFunctionsParser):\\n    \"\"\"Parse an output as an attribute of a pydantic object.\"\"\"\\n\\n    attr_name: str\\n    \"\"\"The name of the attribute to return.\"\"\"\\n\\n    def parse_result(self, result: List[Generation], *, partial: bool = False) -> Any:\\n        result = super().parse_result(result)\\n        return getattr(result, self.attr_name)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\output_parsers\\\\openai_functions.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='import copy\\nimport json\\nfrom json import JSONDecodeError\\nfrom typing import Any, List, Type\\n\\nfrom langchain_core.exceptions import OutputParserException\\nfrom langchain_core.output_parsers import (\\n    BaseGenerationOutputParser,\\n)\\nfrom langchain_core.outputs import ChatGeneration, Generation\\nfrom langchain_core.pydantic_v1 import BaseModel\\n\\n\\nclass JsonOutputToolsParser(BaseGenerationOutputParser[Any]):\\n    \"\"\"Parse tools from OpenAI response.\"\"\"\\n\\n    strict: bool = False\\n    \"\"\"Whether to allow non-JSON-compliant strings.\\n\\n    See: https://docs.python.org/3/library/json.html#encoders-and-decoders\\n\\n    Useful when the parsed output may include unicode characters or new lines.\\n    \"\"\"\\n    return_id: bool = False\\n    \"\"\"Whether to return the tool call id.\"\"\"\\n\\n    def parse_result(self, result: List[Generation], *, partial: bool = False) -> Any:\\n        generation = result[0]\\n        if not isinstance(generation, ChatGeneration):\\n            raise OutputParserException(\\n                \"This output parser can only be used with a chat generation.\"\\n            )\\n        message = generation.message\\n        try:\\n            tool_calls = copy.deepcopy(message.additional_kwargs[\"tool_calls\"])\\n        except KeyError:\\n            return []' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\output_parsers\\\\openai_tools.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='final_tools = []\\n        exceptions = []\\n        for tool_call in tool_calls:\\n            if \"function\" not in tool_call:\\n                continue\\n            try:\\n                function_args = json.loads(\\n                    tool_call[\"function\"][\"arguments\"], strict=self.strict\\n                )\\n            except JSONDecodeError as e:\\n                exceptions.append(\\n                    f\"Function {tool_call[\\'function\\'][\\'name\\']} arguments:\\\\n\\\\n\"\\n                    f\"{tool_call[\\'function\\'][\\'arguments\\']}\\\\n\\\\nare not valid JSON. \"\\n                    f\"Received JSONDecodeError {e}\"\\n                )\\n                continue\\n            parsed = {\\n                \"type\": tool_call[\"function\"][\"name\"],\\n                \"args\": function_args,\\n            }\\n            if self.return_id:\\n                parsed[\"id\"] = tool_call[\"id\"]\\n            final_tools.append(parsed)\\n        if exceptions:\\n            raise OutputParserException(\"\\\\n\\\\n\".join(exceptions))\\n        return final_tools\\n\\n\\nclass JsonOutputKeyToolsParser(JsonOutputToolsParser):\\n    \"\"\"Parse tools from OpenAI response.\"\"\"\\n\\n    key_name: str\\n    \"\"\"The type of tools to return.\"\"\"\\n    return_single: bool = False\\n    \"\"\"Whether to return only the first tool call.\"\"\"\\n\\n    def parse_result(self, result: List[Generation], *, partial: bool = False) -> Any:\\n        results = super().parse_result(result)\\n        results = [res for res in results if res[\"type\"] == self.key_name]\\n        if not self.return_id:\\n            results = [res[\"args\"] for res in results]\\n        if self.return_single:\\n            return results[0] if results else None\\n        return results\\n\\n\\nclass PydanticToolsParser(JsonOutputToolsParser):\\n    \"\"\"Parse tools from OpenAI response.\"\"\"\\n\\n    tools: List[Type[BaseModel]]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\output_parsers\\\\openai_tools.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def parse_result(self, result: List[Generation], *, partial: bool = False) -> Any:\\n        results = super().parse_result(result)\\n        name_dict = {tool.__name__: tool for tool in self.tools}\\n        return [name_dict[res[\"type\"]](**res[\"args\"]) for res in results]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\output_parsers\\\\openai_tools.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='import re\\nfrom typing import Any, Dict, List, Tuple, Union\\n\\nfrom langchain_core.exceptions import OutputParserException\\nfrom langchain_core.output_parsers.base import BaseOutputParser\\nfrom langchain_core.pydantic_v1 import validator\\n\\nfrom langchain.output_parsers.format_instructions import (\\n    PANDAS_DATAFRAME_FORMAT_INSTRUCTIONS,\\n)\\n\\n\\nclass PandasDataFrameOutputParser(BaseOutputParser):\\n    \"\"\"Parse an output using Pandas DataFrame format.\"\"\"\\n\\n    \"\"\"The Pandas DataFrame to parse.\"\"\"\\n    dataframe: Any\\n\\n    @validator(\"dataframe\")\\n    def validate_dataframe(cls, val: Any) -> Any:\\n        import pandas as pd\\n\\n        if issubclass(type(val), pd.DataFrame):\\n            return val\\n        if pd.DataFrame(val).empty:\\n            raise ValueError(\"DataFrame cannot be empty.\")\\n\\n        raise TypeError(\\n            \"Wrong type for \\'dataframe\\', must be a subclass \\\\\\n                of Pandas DataFrame (pd.DataFrame)\"\\n        )\\n\\n    def parse_array(\\n        self, array: str, original_request_params: str\\n    ) -> Tuple[List[Union[int, str]], str]:\\n        parsed_array: List[Union[int, str]] = []' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\output_parsers\\\\pandas_dataframe.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# Check if the format is [1,3,5]\\n        if re.match(r\"\\\\[\\\\d+(,\\\\s*\\\\d+)*\\\\]\", array):\\n            parsed_array = [int(i) for i in re.findall(r\"\\\\d+\", array)]\\n        # Check if the format is [1..5]\\n        elif re.match(r\"\\\\[(\\\\d+)\\\\.\\\\.(\\\\d+)\\\\]\", array):\\n            match = re.match(r\"\\\\[(\\\\d+)\\\\.\\\\.(\\\\d+)\\\\]\", array)\\n            if match:\\n                start, end = map(int, match.groups())\\n                parsed_array = list(range(start, end + 1))\\n            else:\\n                raise OutputParserException(\\n                    f\"Unable to parse the array provided in {array}. \\\\\\n                        Please check the format instructions.\"\\n                )\\n        # Check if the format is [\"column_name\"]\\n        elif re.match(r\"\\\\[[a-zA-Z0-9_]+(?:,[a-zA-Z0-9_]+)*\\\\]\", array):\\n            match = re.match(r\"\\\\[[a-zA-Z0-9_]+(?:,[a-zA-Z0-9_]+)*\\\\]\", array)\\n            if match:\\n                parsed_array = list(map(str, match.group().strip(\"[]\").split(\",\")))\\n            else:\\n                raise OutputParserException(\\n                    f\"Unable to parse the array provided in {array}. \\\\\\n                        Please check the format instructions.\"\\n                )\\n\\n        # Validate the array\\n        if not parsed_array:\\n            raise OutputParserException(\\n                f\"Invalid array format in \\'{original_request_params}\\'. \\\\\\n                    Please check the format instructions.\"\\n            )\\n        elif (\\n            isinstance(parsed_array[0], int)\\n            and parsed_array[-1] > self.dataframe.index.max()\\n        ):\\n            raise OutputParserException(\\n                f\"The maximum index {parsed_array[-1]} exceeds the maximum index of \\\\\\n                    the Pandas DataFrame {self.dataframe.index.max()}.\"\\n            )\\n\\n        return parsed_array, original_request_params.split(\"[\")[0]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\output_parsers\\\\pandas_dataframe.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def parse(self, request: str) -> Dict[str, Any]:\\n        stripped_request_params = None\\n        splitted_request = request.strip().split(\":\")\\n        if len(splitted_request) != 2:\\n            raise OutputParserException(\\n                f\"Request \\'{request}\\' is not correctly formatted. \\\\\\n                    Please refer to the format instructions.\"\\n            )\\n        result = {}\\n        try:\\n            request_type, request_params = splitted_request\\n            if request_type in {\"Invalid column\", \"Invalid operation\"}:\\n                raise OutputParserException(\\n                    f\"{request}. Please check the format instructions.\"\\n                )\\n            array_exists = re.search(r\"(\\\\[.*?\\\\])\", request_params)\\n            if array_exists:\\n                parsed_array, stripped_request_params = self.parse_array(\\n                    array_exists.group(1), request_params\\n                )\\n                if request_type == \"column\":\\n                    filtered_df = self.dataframe[\\n                        self.dataframe.index.isin(parsed_array)\\n                    ]\\n                    if len(parsed_array) == 1:\\n                        result[stripped_request_params] = filtered_df[\\n                            stripped_request_params\\n                        ].iloc[parsed_array[0]]\\n                    else:\\n                        result[stripped_request_params] = filtered_df[\\n                            stripped_request_params\\n                        ]\\n                elif request_type == \"row\":\\n                    filtered_df = self.dataframe[\\n                        self.dataframe.columns.intersection(parsed_array)\\n                    ]\\n                    if len(parsed_array) == 1:\\n                        result[stripped_request_params] = filtered_df.iloc[\\n                            int(stripped_request_params)\\n                        ][parsed_array[0]]\\n                    else:\\n                        result[stripped_request_params] = filtered_df.iloc[' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\output_parsers\\\\pandas_dataframe.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='int(stripped_request_params)\\n                        ]\\n                else:\\n                    filtered_df = self.dataframe[\\n                        self.dataframe.index.isin(parsed_array)\\n                    ]\\n                    result[request_type] = getattr(\\n                        filtered_df[stripped_request_params], request_type\\n                    )()\\n            else:\\n                if request_type == \"column\":\\n                    result[request_params] = self.dataframe[request_params]\\n                elif request_type == \"row\":\\n                    result[request_params] = self.dataframe.iloc[int(request_params)]\\n                else:\\n                    result[request_type] = getattr(\\n                        self.dataframe[request_params], request_type\\n                    )()\\n        except (AttributeError, IndexError, KeyError):\\n            if request_type not in {\"column\", \"row\"}:\\n                raise OutputParserException(\\n                    f\"Unsupported request type \\'{request_type}\\'. \\\\\\n                        Please check the format instructions.\"\\n                )\\n            raise OutputParserException(\\n                f\"\"\"Requested index {\\n                    request_params\\n                    if stripped_request_params is None\\n                    else stripped_request_params\\n                } is out of bounds.\"\"\"\\n            )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\output_parsers\\\\pandas_dataframe.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='return result\\n\\n    def get_format_instructions(self) -> str:\\n        return PANDAS_DATAFRAME_FORMAT_INSTRUCTIONS.format(\\n            columns=\", \".join(self.dataframe.columns)\\n        )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\output_parsers\\\\pandas_dataframe.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# flake8: noqa\\nfrom langchain_core.prompts.prompt import PromptTemplate\\n\\nNAIVE_FIX = \"\"\"Instructions:\\n--------------\\n{instructions}\\n--------------\\nCompletion:\\n--------------\\n{completion}\\n--------------\\n\\nAbove, the Completion did not satisfy the constraints given in the Instructions.\\nError:\\n--------------\\n{error}\\n--------------\\n\\nPlease try again. Please only respond with an answer that satisfies the constraints laid out in the Instructions:\"\"\"\\n\\n\\nNAIVE_FIX_PROMPT = PromptTemplate.from_template(NAIVE_FIX)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\output_parsers\\\\prompts.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='import json\\nimport re\\nfrom typing import Type, TypeVar\\n\\nfrom langchain_core.exceptions import OutputParserException\\nfrom langchain_core.output_parsers import BaseOutputParser\\nfrom langchain_core.pydantic_v1 import BaseModel, ValidationError\\n\\nfrom langchain.output_parsers.format_instructions import PYDANTIC_FORMAT_INSTRUCTIONS\\n\\nT = TypeVar(\"T\", bound=BaseModel)\\n\\n\\nclass PydanticOutputParser(BaseOutputParser[T]):\\n    \"\"\"Parse an output using a pydantic model.\"\"\"\\n\\n    pydantic_object: Type[T]\\n    \"\"\"The pydantic model to parse.\\n    \\n    Attention: To avoid potential compatibility issues, it\\'s recommended to use\\n        pydantic <2 or leverage the v1 namespace in pydantic >= 2.\\n    \"\"\"\\n\\n    def parse(self, text: str) -> T:\\n        try:\\n            # Greedy search for 1st json candidate.\\n            match = re.search(\\n                r\"\\\\{.*\\\\}\", text.strip(), re.MULTILINE | re.IGNORECASE | re.DOTALL\\n            )\\n            json_str = \"\"\\n            if match:\\n                json_str = match.group()\\n            json_object = json.loads(json_str, strict=False)\\n            return self.pydantic_object.parse_obj(json_object)\\n\\n        except (json.JSONDecodeError, ValidationError) as e:\\n            name = self.pydantic_object.__name__\\n            msg = f\"Failed to parse {name} from completion {text}. Got: {e}\"\\n            raise OutputParserException(msg, llm_output=text)\\n\\n    def get_format_instructions(self) -> str:\\n        schema = self.pydantic_object.schema()\\n\\n        # Remove extraneous fields.\\n        reduced_schema = schema\\n        if \"title\" in reduced_schema:\\n            del reduced_schema[\"title\"]\\n        if \"type\" in reduced_schema:\\n            del reduced_schema[\"type\"]\\n        # Ensure json in context is well-formed with double quotes.\\n        schema_str = json.dumps(reduced_schema)\\n\\n        return PYDANTIC_FORMAT_INSTRUCTIONS.format(schema=schema_str)\\n\\n    @property\\n    def _type(self) -> str:\\n        return \"pydantic\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\output_parsers\\\\pydantic.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@property\\n    def OutputType(self) -> Type[T]:\\n        \"\"\"Return the pydantic model.\"\"\"\\n        return self.pydantic_object' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\output_parsers\\\\pydantic.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.output_parsers.rail_parser import (\\n    GuardrailsOutputParser,\\n)\\n\\n__all__ = [\"GuardrailsOutputParser\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\output_parsers\\\\rail_parser.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from __future__ import annotations\\n\\nimport re\\nfrom typing import Dict, List, Optional\\n\\nfrom langchain_core.output_parsers import BaseOutputParser\\n\\n\\nclass RegexParser(BaseOutputParser):\\n    \"\"\"Parse the output of an LLM call using a regex.\"\"\"\\n\\n    @classmethod\\n    def is_lc_serializable(cls) -> bool:\\n        return True\\n\\n    regex: str\\n    \"\"\"The regex to use to parse the output.\"\"\"\\n    output_keys: List[str]\\n    \"\"\"The keys to use for the output.\"\"\"\\n    default_output_key: Optional[str] = None\\n    \"\"\"The default key to use for the output.\"\"\"\\n\\n    @property\\n    def _type(self) -> str:\\n        \"\"\"Return the type key.\"\"\"\\n        return \"regex_parser\"\\n\\n    def parse(self, text: str) -> Dict[str, str]:\\n        \"\"\"Parse the output of an LLM call.\"\"\"\\n        match = re.search(self.regex, text)\\n        if match:\\n            return {key: match.group(i + 1) for i, key in enumerate(self.output_keys)}\\n        else:\\n            if self.default_output_key is None:\\n                raise ValueError(f\"Could not parse output: {text}\")\\n            else:\\n                return {\\n                    key: text if key == self.default_output_key else \"\"\\n                    for key in self.output_keys\\n                }' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\output_parsers\\\\regex.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from __future__ import annotations\\n\\nimport re\\nfrom typing import Dict, Optional\\n\\nfrom langchain_core.output_parsers import BaseOutputParser\\n\\n\\nclass RegexDictParser(BaseOutputParser):\\n    \"\"\"Parse the output of an LLM call into a Dictionary using a regex.\"\"\"\\n\\n    regex_pattern: str = r\"{}:\\\\s?([^.\\'\\\\n\\']*)\\\\.?\"  # : :meta private:\\n    \"\"\"The regex pattern to use to parse the output.\"\"\"\\n    output_key_to_format: Dict[str, str]\\n    \"\"\"The keys to use for the output.\"\"\"\\n    no_update_value: Optional[str] = None\\n    \"\"\"The default key to use for the output.\"\"\"\\n\\n    @property\\n    def _type(self) -> str:\\n        \"\"\"Return the type key.\"\"\"\\n        return \"regex_dict_parser\"\\n\\n    def parse(self, text: str) -> Dict[str, str]:\\n        \"\"\"Parse the output of an LLM call.\"\"\"\\n        result = {}\\n        for output_key, expected_format in self.output_key_to_format.items():\\n            specific_regex = self.regex_pattern.format(re.escape(expected_format))\\n            matches = re.findall(specific_regex, text)\\n            if not matches:\\n                raise ValueError(\\n                    f\"No match found for output key: {output_key} with expected format \\\\\\n                        {expected_format} on text {text}\"\\n                )\\n            elif len(matches) > 1:\\n                raise ValueError(\\n                    f\"Multiple matches found for output key: {output_key} with \\\\\\n                        expected format {expected_format} on text {text}\"\\n                )\\n            elif (\\n                self.no_update_value is not None and matches[0] == self.no_update_value\\n            ):\\n                continue\\n            else:\\n                result[output_key] = matches[0]\\n        return result' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\output_parsers\\\\regex_dict.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from __future__ import annotations\\n\\nfrom typing import Any, TypeVar\\n\\nfrom langchain_core.exceptions import OutputParserException\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.output_parsers import BaseOutputParser\\nfrom langchain_core.prompt_values import PromptValue\\nfrom langchain_core.prompts import BasePromptTemplate, PromptTemplate\\n\\nNAIVE_COMPLETION_RETRY = \"\"\"Prompt:\\n{prompt}\\nCompletion:\\n{completion}\\n\\nAbove, the Completion did not satisfy the constraints given in the Prompt.\\nPlease try again:\"\"\"\\n\\nNAIVE_COMPLETION_RETRY_WITH_ERROR = \"\"\"Prompt:\\n{prompt}\\nCompletion:\\n{completion}\\n\\nAbove, the Completion did not satisfy the constraints given in the Prompt.\\nDetails: {error}\\nPlease try again:\"\"\"\\n\\nNAIVE_RETRY_PROMPT = PromptTemplate.from_template(NAIVE_COMPLETION_RETRY)\\nNAIVE_RETRY_WITH_ERROR_PROMPT = PromptTemplate.from_template(\\n    NAIVE_COMPLETION_RETRY_WITH_ERROR\\n)\\n\\nT = TypeVar(\"T\")\\n\\n\\nclass RetryOutputParser(BaseOutputParser[T]):\\n    \"\"\"Wraps a parser and tries to fix parsing errors.\\n\\n    Does this by passing the original prompt and the completion to another\\n    LLM, and telling it the completion did not satisfy criteria in the prompt.\\n    \"\"\"\\n\\n    parser: BaseOutputParser[T]\\n    \"\"\"The parser to use to parse the output.\"\"\"\\n    # Should be an LLMChain but we want to avoid top-level imports from langchain.chains\\n    retry_chain: Any\\n    \"\"\"The LLMChain to use to retry the completion.\"\"\"\\n    max_retries: int = 1\\n    \"\"\"The maximum number of times to retry the parse.\"\"\"\\n\\n    @classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        parser: BaseOutputParser[T],\\n        prompt: BasePromptTemplate = NAIVE_RETRY_PROMPT,\\n        max_retries: int = 1,\\n    ) -> RetryOutputParser[T]:\\n        \"\"\"Create an RetryOutputParser from a language model and a parser.\\n\\n        Args:\\n            llm: llm to use for fixing\\n            parser: parser to use for parsing\\n            prompt: prompt to use for fixing\\n            max_retries: Maximum number of retries to parse.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\output_parsers\\\\retry.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Returns:\\n            RetryOutputParser\\n        \"\"\"\\n        from langchain.chains.llm import LLMChain\\n\\n        chain = LLMChain(llm=llm, prompt=prompt)\\n        return cls(parser=parser, retry_chain=chain, max_retries=max_retries)\\n\\n    def parse_with_prompt(self, completion: str, prompt_value: PromptValue) -> T:\\n        \"\"\"Parse the output of an LLM call using a wrapped parser.\\n\\n        Args:\\n            completion: The chain completion to parse.\\n            prompt_value: The prompt to use to parse the completion.\\n\\n        Returns:\\n            The parsed completion.\\n        \"\"\"\\n        retries = 0\\n\\n        while retries <= self.max_retries:\\n            try:\\n                return self.parser.parse(completion)\\n            except OutputParserException as e:\\n                if retries == self.max_retries:\\n                    raise e\\n                else:\\n                    retries += 1\\n                    completion = self.retry_chain.run(\\n                        prompt=prompt_value.to_string(), completion=completion\\n                    )\\n\\n        raise OutputParserException(\"Failed to parse\")\\n\\n    async def aparse_with_prompt(self, completion: str, prompt_value: PromptValue) -> T:\\n        \"\"\"Parse the output of an LLM call using a wrapped parser.\\n\\n        Args:\\n            completion: The chain completion to parse.\\n            prompt_value: The prompt to use to parse the completion.\\n\\n        Returns:\\n            The parsed completion.\\n        \"\"\"\\n        retries = 0\\n\\n        while retries <= self.max_retries:\\n            try:\\n                return await self.parser.aparse(completion)\\n            except OutputParserException as e:\\n                if retries == self.max_retries:\\n                    raise e\\n                else:\\n                    retries += 1\\n                    completion = await self.retry_chain.arun(\\n                        prompt=prompt_value.to_string(), completion=completion\\n                    )\\n\\n        raise OutputParserException(\"Failed to parse\")' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\output_parsers\\\\retry.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def parse(self, completion: str) -> T:\\n        raise NotImplementedError(\\n            \"This OutputParser can only be called by the `parse_with_prompt` method.\"\\n        )\\n\\n    def get_format_instructions(self) -> str:\\n        return self.parser.get_format_instructions()\\n\\n    @property\\n    def _type(self) -> str:\\n        return \"retry\"\\n\\n\\nclass RetryWithErrorOutputParser(BaseOutputParser[T]):\\n    \"\"\"Wraps a parser and tries to fix parsing errors.\\n\\n    Does this by passing the original prompt, the completion, AND the error\\n    that was raised to another language model and telling it that the completion\\n    did not work, and raised the given error. Differs from RetryOutputParser\\n    in that this implementation provides the error that was raised back to the\\n    LLM, which in theory should give it more information on how to fix it.\\n    \"\"\"\\n\\n    parser: BaseOutputParser[T]\\n    \"\"\"The parser to use to parse the output.\"\"\"\\n    # Should be an LLMChain but we want to avoid top-level imports from langchain.chains\\n    retry_chain: Any\\n    \"\"\"The LLMChain to use to retry the completion.\"\"\"\\n    max_retries: int = 1\\n    \"\"\"The maximum number of times to retry the parse.\"\"\"\\n\\n    @classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        parser: BaseOutputParser[T],\\n        prompt: BasePromptTemplate = NAIVE_RETRY_WITH_ERROR_PROMPT,\\n        max_retries: int = 1,\\n    ) -> RetryWithErrorOutputParser[T]:\\n        \"\"\"Create a RetryWithErrorOutputParser from an LLM.\\n\\n        Args:\\n            llm: The LLM to use to retry the completion.\\n            parser: The parser to use to parse the output.\\n            prompt: The prompt to use to retry the completion.\\n            max_retries: The maximum number of times to retry the completion.\\n\\n        Returns:\\n            A RetryWithErrorOutputParser.\\n        \"\"\"\\n        from langchain.chains.llm import LLMChain\\n\\n        chain = LLMChain(llm=llm, prompt=prompt)\\n        return cls(parser=parser, retry_chain=chain, max_retries=max_retries)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\output_parsers\\\\retry.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def parse_with_prompt(self, completion: str, prompt_value: PromptValue) -> T:\\n        retries = 0\\n\\n        while retries <= self.max_retries:\\n            try:\\n                return self.parser.parse(completion)\\n            except OutputParserException as e:\\n                if retries == self.max_retries:\\n                    raise e\\n                else:\\n                    retries += 1\\n                    completion = self.retry_chain.run(\\n                        prompt=prompt_value.to_string(),\\n                        completion=completion,\\n                        error=repr(e),\\n                    )\\n\\n        raise OutputParserException(\"Failed to parse\")\\n\\n    async def aparse_with_prompt(self, completion: str, prompt_value: PromptValue) -> T:\\n        retries = 0\\n\\n        while retries <= self.max_retries:\\n            try:\\n                return await self.parser.aparse(completion)\\n            except OutputParserException as e:\\n                if retries == self.max_retries:\\n                    raise e\\n                else:\\n                    retries += 1\\n                    completion = await self.retry_chain.arun(\\n                        prompt=prompt_value.to_string(),\\n                        completion=completion,\\n                        error=repr(e),\\n                    )\\n\\n        raise OutputParserException(\"Failed to parse\")\\n\\n    def parse(self, completion: str) -> T:\\n        raise NotImplementedError(\\n            \"This OutputParser can only be called by the `parse_with_prompt` method.\"\\n        )\\n\\n    def get_format_instructions(self) -> str:\\n        return self.parser.get_format_instructions()\\n\\n    @property\\n    def _type(self) -> str:\\n        return \"retry_with_error\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\output_parsers\\\\retry.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from __future__ import annotations\\n\\nfrom typing import Any, List\\n\\nfrom langchain_core.output_parsers import BaseOutputParser\\nfrom langchain_core.output_parsers.json import parse_and_check_json_markdown\\nfrom langchain_core.pydantic_v1 import BaseModel\\n\\nfrom langchain.output_parsers.format_instructions import (\\n    STRUCTURED_FORMAT_INSTRUCTIONS,\\n    STRUCTURED_FORMAT_SIMPLE_INSTRUCTIONS,\\n)\\n\\nline_template = \\'\\\\t\"{name}\": {type}  // {description}\\'\\n\\n\\nclass ResponseSchema(BaseModel):\\n    \"\"\"A schema for a response from a structured output parser.\"\"\"\\n\\n    name: str\\n    \"\"\"The name of the schema.\"\"\"\\n    description: str\\n    \"\"\"The description of the schema.\"\"\"\\n    type: str = \"string\"\\n    \"\"\"The type of the response.\"\"\"\\n\\n\\ndef _get_sub_string(schema: ResponseSchema) -> str:\\n    return line_template.format(\\n        name=schema.name, description=schema.description, type=schema.type\\n    )\\n\\n\\nclass StructuredOutputParser(BaseOutputParser):\\n    \"\"\"Parse the output of an LLM call to a structured output.\"\"\"\\n\\n    response_schemas: List[ResponseSchema]\\n    \"\"\"The schemas for the response.\"\"\"\\n\\n    @classmethod\\n    def from_response_schemas(\\n        cls, response_schemas: List[ResponseSchema]\\n    ) -> StructuredOutputParser:\\n        return cls(response_schemas=response_schemas)\\n\\n    def get_format_instructions(self, only_json: bool = False) -> str:\\n        \"\"\"Get format instructions for the output parser.\\n\\n        example:\\n        ```python\\n        from langchain.output_parsers.structured import (\\n            StructuredOutputParser, ResponseSchema\\n        )\\n\\n        response_schemas = [\\n            ResponseSchema(\\n                name=\"foo\",\\n                description=\"a list of strings\",\\n                type=\"List[string]\"\\n                ),\\n            ResponseSchema(\\n                name=\"bar\",\\n                description=\"a string\",\\n                type=\"string\"\\n                ),\\n        ]\\n\\n        parser = StructuredOutputParser.from_response_schemas(response_schemas)\\n\\n        print(parser.get_format_instructions())' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\output_parsers\\\\structured.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='output:\\n        # The output should be a Markdown code snippet formatted in the following\\n        # schema, including the leading and trailing \"```json\" and \"```\":\\n        #\\n        # ```json\\n        # {\\n        #     \"foo\": List[string]  // a list of strings\\n        #     \"bar\": string  // a string\\n        # }\\n        # ```\\n\\n        Args:\\n            only_json (bool): If True, only the json in the Markdown code snippet\\n                will be returned, without the introducing text. Defaults to False.\\n        \"\"\"\\n        schema_str = \"\\\\n\".join(\\n            [_get_sub_string(schema) for schema in self.response_schemas]\\n        )\\n        if only_json:\\n            return STRUCTURED_FORMAT_SIMPLE_INSTRUCTIONS.format(format=schema_str)\\n        else:\\n            return STRUCTURED_FORMAT_INSTRUCTIONS.format(format=schema_str)\\n\\n    def parse(self, text: str) -> Any:\\n        expected_keys = [rs.name for rs in self.response_schemas]\\n        return parse_and_check_json_markdown(text, expected_keys)\\n\\n    @property\\n    def _type(self) -> str:\\n        return \"structured\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\output_parsers\\\\structured.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.output_parsers.xml import XMLOutputParser\\n\\n__all__ = [\"XMLOutputParser\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\output_parsers\\\\xml.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='import json\\nimport re\\nfrom typing import Type, TypeVar\\n\\nimport yaml\\nfrom langchain_core.exceptions import OutputParserException\\nfrom langchain_core.output_parsers import BaseOutputParser\\nfrom langchain_core.pydantic_v1 import BaseModel, ValidationError\\n\\nfrom langchain.output_parsers.format_instructions import YAML_FORMAT_INSTRUCTIONS\\n\\nT = TypeVar(\"T\", bound=BaseModel)\\n\\n\\nclass YamlOutputParser(BaseOutputParser[T]):\\n    \"\"\"Parse YAML output using a pydantic model.\"\"\"\\n\\n    pydantic_object: Type[T]\\n    \"\"\"The pydantic model to parse.\"\"\"\\n    pattern: re.Pattern = re.compile(\\n        r\"^```(?:ya?ml)?(?P<yaml>[^`]*)\", re.MULTILINE | re.DOTALL\\n    )\\n    \"\"\"Regex pattern to match yaml code blocks \\n    within triple backticks with optional yaml or yml prefix.\"\"\"\\n\\n    def parse(self, text: str) -> T:\\n        try:\\n            # Greedy search for 1st yaml candidate.\\n            match = re.search(self.pattern, text.strip())\\n            yaml_str = \"\"\\n            if match:\\n                yaml_str = match.group(\"yaml\")\\n            else:\\n                # If no backticks were present, try to parse the entire output as yaml.\\n                yaml_str = text\\n\\n            json_object = yaml.safe_load(yaml_str)\\n            return self.pydantic_object.parse_obj(json_object)\\n\\n        except (yaml.YAMLError, ValidationError) as e:\\n            name = self.pydantic_object.__name__\\n            msg = f\"Failed to parse {name} from completion {text}. Got: {e}\"\\n            raise OutputParserException(msg, llm_output=text) from e\\n\\n    def get_format_instructions(self) -> str:\\n        schema = self.pydantic_object.schema()\\n\\n        # Remove extraneous fields.\\n        reduced_schema = schema\\n        if \"title\" in reduced_schema:\\n            del reduced_schema[\"title\"]\\n        if \"type\" in reduced_schema:\\n            del reduced_schema[\"type\"]\\n        # Ensure yaml in context is well-formed with double quotes.\\n        schema_str = json.dumps(reduced_schema)\\n\\n        return YAML_FORMAT_INSTRUCTIONS.format(schema=schema_str)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\output_parsers\\\\yaml.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@property\\n    def _type(self) -> str:\\n        return \"yaml\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\output_parsers\\\\yaml.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"**OutputParser** classes parse the output of an LLM call.\\n\\n**Class hierarchy:**\\n\\n.. code-block::\\n\\n    BaseLLMOutputParser --> BaseOutputParser --> <name>OutputParser  # ListOutputParser, PydanticOutputParser\\n\\n**Main helpers:**\\n\\n.. code-block::\\n\\n    Serializable, Generation, PromptValue\\n\"\"\"  # noqa: E501\\nfrom langchain.output_parsers.boolean import BooleanOutputParser\\nfrom langchain.output_parsers.combining import CombiningOutputParser\\nfrom langchain.output_parsers.datetime import DatetimeOutputParser\\nfrom langchain.output_parsers.enum import EnumOutputParser\\nfrom langchain.output_parsers.fix import OutputFixingParser\\nfrom langchain.output_parsers.list import (\\n    CommaSeparatedListOutputParser,\\n    ListOutputParser,\\n    MarkdownListOutputParser,\\n    NumberedListOutputParser,\\n)\\nfrom langchain.output_parsers.openai_tools import (\\n    JsonOutputKeyToolsParser,\\n    JsonOutputToolsParser,\\n    PydanticToolsParser,\\n)\\nfrom langchain.output_parsers.pandas_dataframe import PandasDataFrameOutputParser\\nfrom langchain.output_parsers.pydantic import PydanticOutputParser\\nfrom langchain.output_parsers.rail_parser import GuardrailsOutputParser\\nfrom langchain.output_parsers.regex import RegexParser\\nfrom langchain.output_parsers.regex_dict import RegexDictParser\\nfrom langchain.output_parsers.retry import RetryOutputParser, RetryWithErrorOutputParser\\nfrom langchain.output_parsers.structured import ResponseSchema, StructuredOutputParser\\nfrom langchain.output_parsers.xml import XMLOutputParser\\nfrom langchain.output_parsers.yaml import YamlOutputParser' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\output_parsers\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='__all__ = [\\n    \"BooleanOutputParser\",\\n    \"CombiningOutputParser\",\\n    \"CommaSeparatedListOutputParser\",\\n    \"DatetimeOutputParser\",\\n    \"EnumOutputParser\",\\n    \"GuardrailsOutputParser\",\\n    \"ListOutputParser\",\\n    \"MarkdownListOutputParser\",\\n    \"NumberedListOutputParser\",\\n    \"OutputFixingParser\",\\n    \"PandasDataFrameOutputParser\",\\n    \"PydanticOutputParser\",\\n    \"RegexDictParser\",\\n    \"RegexParser\",\\n    \"ResponseSchema\",\\n    \"RetryOutputParser\",\\n    \"RetryWithErrorOutputParser\",\\n    \"StructuredOutputParser\",\\n    \"XMLOutputParser\",\\n    \"JsonOutputToolsParser\",\\n    \"PydanticToolsParser\",\\n    \"JsonOutputKeyToolsParser\",\\n    \"YamlOutputParser\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\output_parsers\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.prompt_values import StringPromptValue\\nfrom langchain_core.prompts import (\\n    BasePromptTemplate,\\n    StringPromptTemplate,\\n    check_valid_template,\\n    get_template_variables,\\n    jinja2_formatter,\\n    validate_jinja2,\\n)\\nfrom langchain_core.prompts.string import _get_jinja2_variables_from_template\\n\\n__all__ = [\\n    \"jinja2_formatter\",\\n    \"validate_jinja2\",\\n    \"check_valid_template\",\\n    \"get_template_variables\",\\n    \"StringPromptTemplate\",\\n    \"BasePromptTemplate\",\\n    \"StringPromptValue\",\\n    \"_get_jinja2_variables_from_template\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\prompts\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.prompt_values import ChatPromptValue, ChatPromptValueConcrete\\nfrom langchain_core.prompts.chat import (\\n    AIMessagePromptTemplate,\\n    BaseChatPromptTemplate,\\n    BaseMessagePromptTemplate,\\n    BaseStringMessagePromptTemplate,\\n    ChatMessagePromptTemplate,\\n    ChatPromptTemplate,\\n    HumanMessagePromptTemplate,\\n    MessageLike,\\n    MessageLikeRepresentation,\\n    MessagePromptTemplateT,\\n    MessagesPlaceholder,\\n    SystemMessagePromptTemplate,\\n    _convert_to_message,\\n    _create_template_from_message_type,\\n)\\n\\n__all__ = [\\n    \"BaseMessagePromptTemplate\",\\n    \"MessagesPlaceholder\",\\n    \"BaseStringMessagePromptTemplate\",\\n    \"ChatMessagePromptTemplate\",\\n    \"HumanMessagePromptTemplate\",\\n    \"AIMessagePromptTemplate\",\\n    \"SystemMessagePromptTemplate\",\\n    \"BaseChatPromptTemplate\",\\n    \"ChatPromptTemplate\",\\n    \"ChatPromptValue\",\\n    \"ChatPromptValueConcrete\",\\n    \"_convert_to_message\",\\n    \"_create_template_from_message_type\",\\n    \"MessagePromptTemplateT\",\\n    \"MessageLike\",\\n    \"MessageLikeRepresentation\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\prompts\\\\chat.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.prompts.few_shot import (\\n    FewShotChatMessagePromptTemplate,\\n    FewShotPromptTemplate,\\n    _FewShotPromptTemplateMixin,\\n)\\n\\n__all__ = [\\n    \"FewShotPromptTemplate\",\\n    \"FewShotChatMessagePromptTemplate\",\\n    \"_FewShotPromptTemplateMixin\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\prompts\\\\few_shot.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.prompts.few_shot_with_templates import FewShotPromptWithTemplates\\n\\n__all__ = [\"FewShotPromptWithTemplates\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\prompts\\\\few_shot_with_templates.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.prompts.loading import (\\n    _load_examples,\\n    _load_few_shot_prompt,\\n    _load_output_parser,\\n    _load_prompt,\\n    _load_prompt_from_file,\\n    _load_template,\\n    load_prompt,\\n    load_prompt_from_config,\\n)\\nfrom langchain_core.utils.loading import try_load_from_hub\\n\\n__all__ = [\\n    \"load_prompt_from_config\",\\n    \"load_prompt\",\\n    \"try_load_from_hub\",\\n    \"_load_examples\",\\n    \"_load_few_shot_prompt\",\\n    \"_load_output_parser\",\\n    \"_load_prompt\",\\n    \"_load_prompt_from_file\",\\n    \"_load_template\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\prompts\\\\loading.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.prompts.pipeline import PipelinePromptTemplate, _get_inputs\\n\\n__all__ = [\"PipelinePromptTemplate\", \"_get_inputs\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\prompts\\\\pipeline.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.prompts.prompt import PromptTemplate\\n\\n# For backwards compatibility.\\nPrompt = PromptTemplate\\n\\n__all__ = [\"PromptTemplate\", \"Prompt\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\prompts\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"**Prompt** is the input to the model.\\n\\nPrompt is often constructed\\nfrom multiple components. Prompt classes and functions make constructing\\n and working with prompts easy.\\n\\n**Class hierarchy:**\\n\\n.. code-block::\\n\\n    BasePromptTemplate --> PipelinePromptTemplate\\n                           StringPromptTemplate --> PromptTemplate\\n                                                    FewShotPromptTemplate\\n                                                    FewShotPromptWithTemplates\\n                           BaseChatPromptTemplate --> AutoGPTPrompt\\n                                                      ChatPromptTemplate --> AgentScratchPadChatPromptTemplate\\n\\n\\n\\n    BaseMessagePromptTemplate --> MessagesPlaceholder\\n                                  BaseStringMessagePromptTemplate --> ChatMessagePromptTemplate\\n                                                                      HumanMessagePromptTemplate\\n                                                                      AIMessagePromptTemplate\\n                                                                      SystemMessagePromptTemplate\\n\\n    PromptValue --> StringPromptValue\\n                    ChatPromptValue\\n\\n\"\"\"  # noqa: E501\\nfrom langchain_core.example_selectors import (\\n    LengthBasedExampleSelector,\\n    MaxMarginalRelevanceExampleSelector,\\n    SemanticSimilarityExampleSelector,\\n)\\nfrom langchain_core.prompts import (\\n    AIMessagePromptTemplate,\\n    BaseChatPromptTemplate,\\n    BasePromptTemplate,\\n    ChatMessagePromptTemplate,\\n    ChatPromptTemplate,\\n    FewShotChatMessagePromptTemplate,\\n    FewShotPromptTemplate,\\n    FewShotPromptWithTemplates,\\n    HumanMessagePromptTemplate,\\n    MessagesPlaceholder,\\n    PipelinePromptTemplate,\\n    PromptTemplate,\\n    StringPromptTemplate,\\n    SystemMessagePromptTemplate,\\n    load_prompt,\\n)\\n\\nfrom langchain.prompts.example_selector import NGramOverlapExampleSelector\\nfrom langchain.prompts.prompt import Prompt' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\prompts\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='__all__ = [\\n    \"AIMessagePromptTemplate\",\\n    \"BaseChatPromptTemplate\",\\n    \"BasePromptTemplate\",\\n    \"ChatMessagePromptTemplate\",\\n    \"ChatPromptTemplate\",\\n    \"FewShotPromptTemplate\",\\n    \"FewShotPromptWithTemplates\",\\n    \"HumanMessagePromptTemplate\",\\n    \"LengthBasedExampleSelector\",\\n    \"MaxMarginalRelevanceExampleSelector\",\\n    \"MessagesPlaceholder\",\\n    \"NGramOverlapExampleSelector\",\\n    \"PipelinePromptTemplate\",\\n    \"PromptTemplate\",\\n    \"SemanticSimilarityExampleSelector\",\\n    \"StringPromptTemplate\",\\n    \"SystemMessagePromptTemplate\",\\n    \"load_prompt\",\\n    \"FewShotChatMessagePromptTemplate\",\\n    \"Prompt\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\prompts\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.example_selectors.base import BaseExampleSelector\\n\\n__all__ = [\"BaseExampleSelector\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\prompts\\\\example_selector\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.example_selectors.length_based import (\\n    LengthBasedExampleSelector,\\n)\\n\\n__all__ = [\"LengthBasedExampleSelector\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\prompts\\\\example_selector\\\\length_based.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Select and order examples based on ngram overlap score (sentence_bleu score).\\n\\nhttps://www.nltk.org/_modules/nltk/translate/bleu_score.html\\nhttps://aclanthology.org/P02-1040.pdf\\n\"\"\"\\nfrom typing import Dict, List\\n\\nimport numpy as np\\nfrom langchain_core.example_selectors.base import BaseExampleSelector\\nfrom langchain_core.prompts import PromptTemplate\\nfrom langchain_core.pydantic_v1 import BaseModel, root_validator\\n\\n\\ndef ngram_overlap_score(source: List[str], example: List[str]) -> float:\\n    \"\"\"Compute ngram overlap score of source and example as sentence_bleu score.\\n\\n    Use sentence_bleu with method1 smoothing function and auto reweighting.\\n    Return float value between 0.0 and 1.0 inclusive.\\n    https://www.nltk.org/_modules/nltk/translate/bleu_score.html\\n    https://aclanthology.org/P02-1040.pdf\\n    \"\"\"\\n    from nltk.translate.bleu_score import (\\n        SmoothingFunction,  # type: ignore\\n        sentence_bleu,\\n    )\\n\\n    hypotheses = source[0].split()\\n    references = [s.split() for s in example]\\n\\n    return float(\\n        sentence_bleu(\\n            references,\\n            hypotheses,\\n            smoothing_function=SmoothingFunction().method1,\\n            auto_reweigh=True,\\n        )\\n    )\\n\\n\\nclass NGramOverlapExampleSelector(BaseExampleSelector, BaseModel):\\n    \"\"\"Select and order examples based on ngram overlap score (sentence_bleu score).\\n\\n    https://www.nltk.org/_modules/nltk/translate/bleu_score.html\\n    https://aclanthology.org/P02-1040.pdf\\n    \"\"\"\\n\\n    examples: List[dict]\\n    \"\"\"A list of the examples that the prompt template expects.\"\"\"\\n\\n    example_prompt: PromptTemplate\\n    \"\"\"Prompt template used to format the examples.\"\"\"\\n\\n    threshold: float = -1.0\\n    \"\"\"Threshold at which algorithm stops. Set to -1.0 by default.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\prompts\\\\example_selector\\\\ngram_overlap.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='For negative threshold:\\n    select_examples sorts examples by ngram_overlap_score, but excludes none.\\n    For threshold greater than 1.0:\\n    select_examples excludes all examples, and returns an empty list.\\n    For threshold equal to 0.0:\\n    select_examples sorts examples by ngram_overlap_score,\\n    and excludes examples with no ngram overlap with input.\\n    \"\"\"\\n\\n    @root_validator(pre=True)\\n    def check_dependencies(cls, values: Dict) -> Dict:\\n        \"\"\"Check that valid dependencies exist.\"\"\"\\n        try:\\n            from nltk.translate.bleu_score import (  # noqa: F401\\n                SmoothingFunction,\\n                sentence_bleu,\\n            )\\n        except ImportError as e:\\n            raise ImportError(\\n                \"Not all the correct dependencies for this ExampleSelect exist.\"\\n                \"Please install nltk with `pip install nltk`.\"\\n            ) from e\\n\\n        return values\\n\\n    def add_example(self, example: Dict[str, str]) -> None:\\n        \"\"\"Add new example to list.\"\"\"\\n        self.examples.append(example)\\n\\n    def select_examples(self, input_variables: Dict[str, str]) -> List[dict]:\\n        \"\"\"Return list of examples sorted by ngram_overlap_score with input.\\n\\n        Descending order.\\n        Excludes any examples with ngram_overlap_score less than or equal to threshold.\\n        \"\"\"\\n        inputs = list(input_variables.values())\\n        examples = []\\n        k = len(self.examples)\\n        score = [0.0] * k\\n        first_prompt_template_key = self.example_prompt.input_variables[0]\\n\\n        for i in range(k):\\n            score[i] = ngram_overlap_score(\\n                inputs, [self.examples[i][first_prompt_template_key]]\\n            )\\n\\n        while True:\\n            arg_max = np.argmax(score)\\n            if (score[arg_max] < self.threshold) or abs(\\n                score[arg_max] - self.threshold\\n            ) < 1e-9:\\n                break\\n\\n            examples.append(self.examples[arg_max])\\n            score[arg_max] = self.threshold - 1.0\\n\\n        return examples' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\prompts\\\\example_selector\\\\ngram_overlap.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.example_selectors.semantic_similarity import (\\n    MaxMarginalRelevanceExampleSelector,\\n    SemanticSimilarityExampleSelector,\\n    sorted_values,\\n)\\n\\n__all__ = [\\n    \"sorted_values\",\\n    \"SemanticSimilarityExampleSelector\",\\n    \"MaxMarginalRelevanceExampleSelector\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\prompts\\\\example_selector\\\\semantic_similarity.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Logic for selecting examples to include in prompts.\"\"\"\\nfrom langchain_core.example_selectors.length_based import (\\n    LengthBasedExampleSelector,\\n)\\nfrom langchain_core.example_selectors.semantic_similarity import (\\n    MaxMarginalRelevanceExampleSelector,\\n    SemanticSimilarityExampleSelector,\\n)\\n\\nfrom langchain.prompts.example_selector.ngram_overlap import (\\n    NGramOverlapExampleSelector,\\n)\\n\\n__all__ = [\\n    \"LengthBasedExampleSelector\",\\n    \"MaxMarginalRelevanceExampleSelector\",\\n    \"NGramOverlapExampleSelector\",\\n    \"SemanticSimilarityExampleSelector\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\prompts\\\\example_selector\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='try:\\n    from pydantic.v1.dataclasses import *  # noqa: F403\\nexcept ImportError:\\n    from pydantic.dataclasses import *  # noqa: F403' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\pydantic_v1\\\\dataclasses.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='try:\\n    from pydantic.v1.main import *  # noqa: F403\\nexcept ImportError:\\n    from pydantic.main import *  # noqa: F403' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\pydantic_v1\\\\main.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from importlib import metadata\\n\\n## Create namespaces for pydantic v1 and v2.\\n# This code must stay at the top of the file before other modules may\\n# attempt to import pydantic since it adds pydantic_v1 and pydantic_v2 to sys.modules.\\n#\\n# This hack is done for the following reasons:\\n# * Langchain will attempt to remain compatible with both pydantic v1 and v2 since\\n#   both dependencies and dependents may be stuck on either version of v1 or v2.\\n# * Creating namespaces for pydantic v1 and v2 should allow us to write code that\\n#   unambiguously uses either v1 or v2 API.\\n# * This change is easier to roll out and roll back.\\n\\ntry:\\n    from pydantic.v1 import *  # noqa: F403\\nexcept ImportError:\\n    from pydantic import *  # noqa: F403\\n\\n\\ntry:\\n    _PYDANTIC_MAJOR_VERSION: int = int(metadata.version(\"pydantic\").split(\".\")[0])\\nexcept metadata.PackageNotFoundError:\\n    _PYDANTIC_MAJOR_VERSION = 0' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\pydantic_v1\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.retrievers.arcee import ArceeRetriever\\n\\n__all__ = [\"ArceeRetriever\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\arcee.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.retrievers.arxiv import ArxivRetriever\\n\\n__all__ = [\"ArxivRetriever\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\arxiv.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.retrievers.azure_cognitive_search import (\\n    AzureCognitiveSearchRetriever,\\n)\\n\\n__all__ = [\"AzureCognitiveSearchRetriever\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\azure_cognitive_search.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.retrievers.bedrock import (\\n    AmazonKnowledgeBasesRetriever,\\n    RetrievalConfig,\\n    VectorSearchConfig,\\n)\\n\\n__all__ = [\"VectorSearchConfig\", \"RetrievalConfig\", \"AmazonKnowledgeBasesRetriever\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\bedrock.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.retrievers.bm25 import (\\n    BM25Retriever,\\n    default_preprocessing_func,\\n)\\n\\n__all__ = [\"default_preprocessing_func\", \"BM25Retriever\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\bm25.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.retrievers.chaindesk import ChaindeskRetriever\\n\\n__all__ = [\"ChaindeskRetriever\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\chaindesk.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.retrievers.chatgpt_plugin_retriever import (\\n    ChatGPTPluginRetriever,\\n)\\n\\n__all__ = [\"ChatGPTPluginRetriever\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\chatgpt_plugin_retriever.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.retrievers.cohere_rag_retriever import (\\n    CohereRagRetriever,\\n)\\n\\n__all__ = [\"CohereRagRetriever\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\cohere_rag_retriever.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from typing import Any, List\\n\\nfrom langchain_core.callbacks import (\\n    AsyncCallbackManagerForRetrieverRun,\\n    CallbackManagerForRetrieverRun,\\n)\\nfrom langchain_core.documents import Document\\nfrom langchain_core.retrievers import BaseRetriever\\n\\nfrom langchain.retrievers.document_compressors.base import (\\n    BaseDocumentCompressor,\\n)\\n\\n\\nclass ContextualCompressionRetriever(BaseRetriever):\\n    \"\"\"Retriever that wraps a base retriever and compresses the results.\"\"\"\\n\\n    base_compressor: BaseDocumentCompressor\\n    \"\"\"Compressor for compressing retrieved documents.\"\"\"\\n\\n    base_retriever: BaseRetriever\\n    \"\"\"Base Retriever to use for getting relevant documents.\"\"\"\\n\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n\\n        arbitrary_types_allowed = True\\n\\n    def _get_relevant_documents(\\n        self,\\n        query: str,\\n        *,\\n        run_manager: CallbackManagerForRetrieverRun,\\n        **kwargs: Any,\\n    ) -> List[Document]:\\n        \"\"\"Get documents relevant for a query.\\n\\n        Args:\\n            query: string to find relevant documents for\\n\\n        Returns:\\n            Sequence of relevant documents\\n        \"\"\"\\n        docs = self.base_retriever.get_relevant_documents(\\n            query, callbacks=run_manager.get_child(), **kwargs\\n        )\\n        if docs:\\n            compressed_docs = self.base_compressor.compress_documents(\\n                docs, query, callbacks=run_manager.get_child()\\n            )\\n            return list(compressed_docs)\\n        else:\\n            return []\\n\\n    async def _aget_relevant_documents(\\n        self,\\n        query: str,\\n        *,\\n        run_manager: AsyncCallbackManagerForRetrieverRun,\\n        **kwargs: Any,\\n    ) -> List[Document]:\\n        \"\"\"Get documents relevant for a query.\\n\\n        Args:\\n            query: string to find relevant documents for' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\contextual_compression.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Returns:\\n            List of relevant documents\\n        \"\"\"\\n        docs = await self.base_retriever.aget_relevant_documents(\\n            query, callbacks=run_manager.get_child(), **kwargs\\n        )\\n        if docs:\\n            compressed_docs = await self.base_compressor.acompress_documents(\\n                docs, query, callbacks=run_manager.get_child()\\n            )\\n            return list(compressed_docs)\\n        else:\\n            return []' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\contextual_compression.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.retrievers.databerry import DataberryRetriever\\n\\n__all__ = [\"DataberryRetriever\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\databerry.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.retrievers.docarray import DocArrayRetriever, SearchType\\n\\n__all__ = [\"SearchType\", \"DocArrayRetriever\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\docarray.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.retrievers.elastic_search_bm25 import (\\n    ElasticSearchBM25Retriever,\\n)\\n\\n__all__ = [\"ElasticSearchBM25Retriever\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\elastic_search_bm25.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.retrievers.embedchain import EmbedchainRetriever\\n\\n__all__ = [\"EmbedchainRetriever\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\embedchain.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"\\nEnsemble retriever that ensemble the results of \\nmultiple retrievers by using weighted  Reciprocal Rank Fusion\\n\"\"\"\\nimport asyncio\\nfrom typing import Any, Dict, List, Optional\\n\\nfrom langchain_core.callbacks import (\\n    AsyncCallbackManagerForRetrieverRun,\\n    CallbackManagerForRetrieverRun,\\n)\\nfrom langchain_core.documents import Document\\nfrom langchain_core.load.dump import dumpd\\nfrom langchain_core.pydantic_v1 import root_validator\\nfrom langchain_core.retrievers import BaseRetriever, RetrieverLike\\nfrom langchain_core.runnables import RunnableConfig\\nfrom langchain_core.runnables.config import ensure_config, patch_config\\nfrom langchain_core.runnables.utils import (\\n    ConfigurableFieldSpec,\\n    get_unique_config_specs,\\n)\\n\\n\\nclass EnsembleRetriever(BaseRetriever):\\n    \"\"\"Retriever that ensembles the multiple retrievers.\\n\\n    It uses a rank fusion.\\n\\n    Args:\\n        retrievers: A list of retrievers to ensemble.\\n        weights: A list of weights corresponding to the retrievers. Defaults to equal\\n            weighting for all retrievers.\\n        c: A constant added to the rank, controlling the balance between the importance\\n            of high-ranked items and the consideration given to lower-ranked items.\\n            Default is 60.\\n    \"\"\"\\n\\n    retrievers: List[RetrieverLike]\\n    weights: List[float]\\n    c: int = 60\\n\\n    @property\\n    def config_specs(self) -> List[ConfigurableFieldSpec]:\\n        \"\"\"List configurable fields for this runnable.\"\"\"\\n        return get_unique_config_specs(\\n            spec for retriever in self.retrievers for spec in retriever.config_specs\\n        )\\n\\n    @root_validator(pre=True)\\n    def set_weights(cls, values: Dict[str, Any]) -> Dict[str, Any]:\\n        if not values.get(\"weights\"):\\n            n_retrievers = len(values[\"retrievers\"])\\n            values[\"weights\"] = [1 / n_retrievers] * n_retrievers\\n        return values' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\ensemble.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def invoke(\\n        self, input: str, config: Optional[RunnableConfig] = None, **kwargs: Any\\n    ) -> List[Document]:\\n        from langchain_core.callbacks import CallbackManager\\n\\n        config = ensure_config(config)\\n        callback_manager = CallbackManager.configure(\\n            config.get(\"callbacks\"),\\n            None,\\n            verbose=kwargs.get(\"verbose\", False),\\n            inheritable_tags=config.get(\"tags\", []),\\n            local_tags=self.tags,\\n            inheritable_metadata=config.get(\"metadata\", {}),\\n            local_metadata=self.metadata,\\n        )\\n        run_manager = callback_manager.on_retriever_start(\\n            dumpd(self),\\n            input,\\n            name=config.get(\"run_name\"),\\n            **kwargs,\\n        )\\n        try:\\n            result = self.rank_fusion(input, run_manager=run_manager, config=config)\\n        except Exception as e:\\n            run_manager.on_retriever_error(e)\\n            raise e\\n        else:\\n            run_manager.on_retriever_end(\\n                result,\\n                **kwargs,\\n            )\\n            return result\\n\\n    async def ainvoke(\\n        self, input: str, config: Optional[RunnableConfig] = None, **kwargs: Any\\n    ) -> List[Document]:\\n        from langchain_core.callbacks import AsyncCallbackManager' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\ensemble.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='config = ensure_config(config)\\n        callback_manager = AsyncCallbackManager.configure(\\n            config.get(\"callbacks\"),\\n            None,\\n            verbose=kwargs.get(\"verbose\", False),\\n            inheritable_tags=config.get(\"tags\", []),\\n            local_tags=self.tags,\\n            inheritable_metadata=config.get(\"metadata\", {}),\\n            local_metadata=self.metadata,\\n        )\\n        run_manager = await callback_manager.on_retriever_start(\\n            dumpd(self),\\n            input,\\n            name=config.get(\"run_name\"),\\n            **kwargs,\\n        )\\n        try:\\n            result = await self.arank_fusion(\\n                input, run_manager=run_manager, config=config\\n            )\\n        except Exception as e:\\n            await run_manager.on_retriever_error(e)\\n            raise e\\n        else:\\n            await run_manager.on_retriever_end(\\n                result,\\n                **kwargs,\\n            )\\n            return result\\n\\n    def _get_relevant_documents(\\n        self,\\n        query: str,\\n        *,\\n        run_manager: CallbackManagerForRetrieverRun,\\n    ) -> List[Document]:\\n        \"\"\"\\n        Get the relevant documents for a given query.\\n\\n        Args:\\n            query: The query to search for.\\n\\n        Returns:\\n            A list of reranked documents.\\n        \"\"\"\\n\\n        # Get fused result of the retrievers.\\n        fused_documents = self.rank_fusion(query, run_manager)\\n\\n        return fused_documents\\n\\n    async def _aget_relevant_documents(\\n        self,\\n        query: str,\\n        *,\\n        run_manager: AsyncCallbackManagerForRetrieverRun,\\n    ) -> List[Document]:\\n        \"\"\"\\n        Asynchronously get the relevant documents for a given query.\\n\\n        Args:\\n            query: The query to search for.\\n\\n        Returns:\\n            A list of reranked documents.\\n        \"\"\"\\n\\n        # Get fused result of the retrievers.\\n        fused_documents = await self.arank_fusion(query, run_manager)\\n\\n        return fused_documents' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\ensemble.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def rank_fusion(\\n        self,\\n        query: str,\\n        run_manager: CallbackManagerForRetrieverRun,\\n        *,\\n        config: Optional[RunnableConfig] = None,\\n    ) -> List[Document]:\\n        \"\"\"\\n        Retrieve the results of the retrievers and use rank_fusion_func to get\\n        the final result.\\n\\n        Args:\\n            query: The query to search for.\\n\\n        Returns:\\n            A list of reranked documents.\\n        \"\"\"\\n\\n        # Get the results of all retrievers.\\n        retriever_docs = [\\n            retriever.invoke(\\n                query,\\n                patch_config(\\n                    config, callbacks=run_manager.get_child(tag=f\"retriever_{i+1}\")\\n                ),\\n            )\\n            for i, retriever in enumerate(self.retrievers)\\n        ]\\n\\n        # Enforce that retrieved docs are Documents for each list in retriever_docs\\n        for i in range(len(retriever_docs)):\\n            retriever_docs[i] = [\\n                Document(page_content=doc) if not isinstance(doc, Document) else doc\\n                for doc in retriever_docs[i]\\n            ]\\n\\n        # apply rank fusion\\n        fused_documents = self.weighted_reciprocal_rank(retriever_docs)\\n\\n        return fused_documents\\n\\n    async def arank_fusion(\\n        self,\\n        query: str,\\n        run_manager: AsyncCallbackManagerForRetrieverRun,\\n        *,\\n        config: Optional[RunnableConfig] = None,\\n    ) -> List[Document]:\\n        \"\"\"\\n        Asynchronously retrieve the results of the retrievers\\n        and use rank_fusion_func to get the final result.\\n\\n        Args:\\n            query: The query to search for.\\n\\n        Returns:\\n            A list of reranked documents.\\n        \"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\ensemble.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# Get the results of all retrievers.\\n        retriever_docs = await asyncio.gather(\\n            *[\\n                retriever.ainvoke(\\n                    query,\\n                    patch_config(\\n                        config, callbacks=run_manager.get_child(tag=f\"retriever_{i+1}\")\\n                    ),\\n                )\\n                for i, retriever in enumerate(self.retrievers)\\n            ]\\n        )\\n\\n        # Enforce that retrieved docs are Documents for each list in retriever_docs\\n        for i in range(len(retriever_docs)):\\n            retriever_docs[i] = [\\n                Document(page_content=doc) if not isinstance(doc, Document) else doc\\n                for doc in retriever_docs[i]\\n            ]\\n\\n        # apply rank fusion\\n        fused_documents = self.weighted_reciprocal_rank(retriever_docs)\\n\\n        return fused_documents\\n\\n    def weighted_reciprocal_rank(\\n        self, doc_lists: List[List[Document]]\\n    ) -> List[Document]:\\n        \"\"\"\\n        Perform weighted Reciprocal Rank Fusion on multiple rank lists.\\n        You can find more details about RRF here:\\n        https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf\\n\\n        Args:\\n            doc_lists: A list of rank lists, where each rank list contains unique items.\\n\\n        Returns:\\n            list: The final aggregated list of items sorted by their weighted RRF\\n                    scores in descending order.\\n        \"\"\"\\n        if len(doc_lists) != len(self.weights):\\n            raise ValueError(\\n                \"Number of rank lists must be equal to the number of weights.\"\\n            )\\n\\n        # Create a union of all unique documents in the input doc_lists\\n        all_documents = set()\\n        for doc_list in doc_lists:\\n            for doc in doc_list:\\n                all_documents.add(doc.page_content)\\n\\n        # Initialize the RRF score dictionary for each document\\n        rrf_score_dic = {doc: 0.0 for doc in all_documents}' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\ensemble.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# Calculate RRF scores for each document\\n        for doc_list, weight in zip(doc_lists, self.weights):\\n            for rank, doc in enumerate(doc_list, start=1):\\n                rrf_score = weight * (1 / (rank + self.c))\\n                rrf_score_dic[doc.page_content] += rrf_score\\n\\n        # Sort documents by their RRF scores in descending order\\n        sorted_documents = sorted(\\n            rrf_score_dic.keys(), key=lambda x: rrf_score_dic[x], reverse=True\\n        )\\n\\n        # Map the sorted page_content back to the original document objects\\n        page_content_to_doc_map = {\\n            doc.page_content: doc for doc_list in doc_lists for doc in doc_list\\n        }\\n        sorted_docs = [\\n            page_content_to_doc_map[page_content] for page_content in sorted_documents\\n        ]\\n\\n        return sorted_docs' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\ensemble.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.retrievers.google_cloud_documentai_warehouse import (\\n    GoogleDocumentAIWarehouseRetriever,\\n)\\n\\n__all__ = [\"GoogleDocumentAIWarehouseRetriever\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\google_cloud_documentai_warehouse.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.retrievers.google_vertex_ai_search import (\\n    GoogleCloudEnterpriseSearchRetriever,\\n    GoogleVertexAIMultiTurnSearchRetriever,\\n    GoogleVertexAISearchRetriever,\\n)\\n\\n__all__ = [\\n    \"GoogleVertexAISearchRetriever\",\\n    \"GoogleVertexAIMultiTurnSearchRetriever\",\\n    \"GoogleCloudEnterpriseSearchRetriever\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\google_vertex_ai_search.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.retrievers.kay import KayAiRetriever\\n\\n__all__ = [\"KayAiRetriever\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\kay.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.retrievers.kendra import (\\n    AdditionalResultAttribute,\\n    AdditionalResultAttributeValue,\\n    AmazonKendraRetriever,\\n    DocumentAttribute,\\n    DocumentAttributeValue,\\n    DocumentAttributeValueType,\\n    Highlight,\\n    QueryResult,\\n    QueryResultItem,\\n    ResultItem,\\n    RetrieveResult,\\n    RetrieveResultItem,\\n    TextWithHighLights,\\n    clean_excerpt,\\n    combined_text,\\n)\\n\\n__all__ = [\\n    \"clean_excerpt\",\\n    \"combined_text\",\\n    \"DocumentAttributeValueType\",\\n    \"Highlight\",\\n    \"TextWithHighLights\",\\n    \"AdditionalResultAttributeValue\",\\n    \"AdditionalResultAttribute\",\\n    \"DocumentAttributeValue\",\\n    \"DocumentAttribute\",\\n    \"ResultItem\",\\n    \"QueryResultItem\",\\n    \"RetrieveResultItem\",\\n    \"QueryResult\",\\n    \"RetrieveResult\",\\n    \"AmazonKendraRetriever\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\kendra.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.retrievers.knn import KNNRetriever\\n\\n__all__ = [\"KNNRetriever\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\knn.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.retrievers.llama_index import (\\n    LlamaIndexGraphRetriever,\\n    LlamaIndexRetriever,\\n)\\n\\n__all__ = [\"LlamaIndexRetriever\", \"LlamaIndexGraphRetriever\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\llama_index.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='import asyncio\\nfrom typing import List\\n\\nfrom langchain_core.callbacks import (\\n    AsyncCallbackManagerForRetrieverRun,\\n    CallbackManagerForRetrieverRun,\\n)\\nfrom langchain_core.documents import Document\\nfrom langchain_core.retrievers import BaseRetriever\\n\\n\\nclass MergerRetriever(BaseRetriever):\\n    \"\"\"Retriever that merges the results of multiple retrievers.\"\"\"\\n\\n    retrievers: List[BaseRetriever]\\n    \"\"\"A list of retrievers to merge.\"\"\"\\n\\n    def _get_relevant_documents(\\n        self,\\n        query: str,\\n        *,\\n        run_manager: CallbackManagerForRetrieverRun,\\n    ) -> List[Document]:\\n        \"\"\"\\n        Get the relevant documents for a given query.\\n\\n        Args:\\n            query: The query to search for.\\n\\n        Returns:\\n            A list of relevant documents.\\n        \"\"\"\\n\\n        # Merge the results of the retrievers.\\n        merged_documents = self.merge_documents(query, run_manager)\\n\\n        return merged_documents\\n\\n    async def _aget_relevant_documents(\\n        self,\\n        query: str,\\n        *,\\n        run_manager: AsyncCallbackManagerForRetrieverRun,\\n    ) -> List[Document]:\\n        \"\"\"\\n        Asynchronously get the relevant documents for a given query.\\n\\n        Args:\\n            query: The query to search for.\\n\\n        Returns:\\n            A list of relevant documents.\\n        \"\"\"\\n\\n        # Merge the results of the retrievers.\\n        merged_documents = await self.amerge_documents(query, run_manager)\\n\\n        return merged_documents\\n\\n    def merge_documents(\\n        self, query: str, run_manager: CallbackManagerForRetrieverRun\\n    ) -> List[Document]:\\n        \"\"\"\\n        Merge the results of the retrievers.\\n\\n        Args:\\n            query: The query to search for.\\n\\n        Returns:\\n            A list of merged documents.\\n        \"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\merger_retriever.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# Get the results of all retrievers.\\n        retriever_docs = [\\n            retriever.get_relevant_documents(\\n                query, callbacks=run_manager.get_child(\"retriever_{}\".format(i + 1))\\n            )\\n            for i, retriever in enumerate(self.retrievers)\\n        ]\\n\\n        # Merge the results of the retrievers.\\n        merged_documents = []\\n        max_docs = max(len(docs) for docs in retriever_docs)\\n        for i in range(max_docs):\\n            for retriever, doc in zip(self.retrievers, retriever_docs):\\n                if i < len(doc):\\n                    merged_documents.append(doc[i])\\n\\n        return merged_documents\\n\\n    async def amerge_documents(\\n        self, query: str, run_manager: AsyncCallbackManagerForRetrieverRun\\n    ) -> List[Document]:\\n        \"\"\"\\n        Asynchronously merge the results of the retrievers.\\n\\n        Args:\\n            query: The query to search for.\\n\\n        Returns:\\n            A list of merged documents.\\n        \"\"\"\\n\\n        # Get the results of all retrievers.\\n        retriever_docs = await asyncio.gather(\\n            *(\\n                retriever.aget_relevant_documents(\\n                    query, callbacks=run_manager.get_child(\"retriever_{}\".format(i + 1))\\n                )\\n                for i, retriever in enumerate(self.retrievers)\\n            )\\n        )\\n\\n        # Merge the results of the retrievers.\\n        merged_documents = []\\n        max_docs = max(len(docs) for docs in retriever_docs)\\n        for i in range(max_docs):\\n            for retriever, doc in zip(self.retrievers, retriever_docs):\\n                if i < len(doc):\\n                    merged_documents.append(doc[i])\\n\\n        return merged_documents' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\merger_retriever.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.retrievers.metal import MetalRetriever\\n\\n__all__ = [\"MetalRetriever\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\metal.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.retrievers.milvus import MilvusRetreiver, MilvusRetriever\\n\\n__all__ = [\"MilvusRetriever\", \"MilvusRetreiver\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\milvus.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='import asyncio\\nimport logging\\nfrom typing import List, Sequence\\n\\nfrom langchain_core.callbacks import (\\n    AsyncCallbackManagerForRetrieverRun,\\n    CallbackManagerForRetrieverRun,\\n)\\nfrom langchain_core.documents import Document\\nfrom langchain_core.language_models import BaseLLM\\nfrom langchain_core.prompts.prompt import PromptTemplate\\nfrom langchain_core.pydantic_v1 import BaseModel, Field\\nfrom langchain_core.retrievers import BaseRetriever\\n\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.output_parsers.pydantic import PydanticOutputParser\\n\\nlogger = logging.getLogger(__name__)\\n\\n\\nclass LineList(BaseModel):\\n    \"\"\"List of lines.\"\"\"\\n\\n    lines: List[str] = Field(description=\"Lines of text\")\\n    \"\"\"List of lines.\"\"\"\\n\\n\\nclass LineListOutputParser(PydanticOutputParser):\\n    \"\"\"Output parser for a list of lines.\"\"\"\\n\\n    def __init__(self) -> None:\\n        super().__init__(pydantic_object=LineList)\\n\\n    def parse(self, text: str) -> LineList:\\n        lines = text.strip().split(\"\\\\n\")\\n        return LineList(lines=lines)\\n\\n\\n# Default prompt\\nDEFAULT_QUERY_PROMPT = PromptTemplate(\\n    input_variables=[\"question\"],\\n    template=\"\"\"You are an AI language model assistant. Your task is \\n    to generate 3 different versions of the given user \\n    question to retrieve relevant documents from a vector  database. \\n    By generating multiple perspectives on the user question, \\n    your goal is to help the user overcome some of the limitations \\n    of distance-based similarity search. Provide these alternative \\n    questions separated by newlines. Original question: {question}\"\"\",\\n)\\n\\n\\ndef _unique_documents(documents: Sequence[Document]) -> List[Document]:\\n    return [doc for i, doc in enumerate(documents) if doc not in documents[:i]]\\n\\n\\nclass MultiQueryRetriever(BaseRetriever):\\n    \"\"\"Given a query, use an LLM to write a set of queries.\\n\\n    Retrieve docs for each query. Return the unique union of all retrieved docs.\\n    \"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\multi_query.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='retriever: BaseRetriever\\n    llm_chain: LLMChain\\n    verbose: bool = True\\n    parser_key: str = \"lines\"\\n    include_original: bool = False\\n    \"\"\"Whether to include the original query in the list of generated queries.\"\"\"\\n\\n    @classmethod\\n    def from_llm(\\n        cls,\\n        retriever: BaseRetriever,\\n        llm: BaseLLM,\\n        prompt: PromptTemplate = DEFAULT_QUERY_PROMPT,\\n        parser_key: str = \"lines\",\\n        include_original: bool = False,\\n    ) -> \"MultiQueryRetriever\":\\n        \"\"\"Initialize from llm using default template.\\n\\n        Args:\\n            retriever: retriever to query documents from\\n            llm: llm for query generation using DEFAULT_QUERY_PROMPT\\n            include_original: Whether to include the original query in the list of\\n                generated queries.\\n\\n        Returns:\\n            MultiQueryRetriever\\n        \"\"\"\\n        output_parser = LineListOutputParser()\\n        llm_chain = LLMChain(llm=llm, prompt=prompt, output_parser=output_parser)\\n        return cls(\\n            retriever=retriever,\\n            llm_chain=llm_chain,\\n            parser_key=parser_key,\\n            include_original=include_original,\\n        )\\n\\n    async def _aget_relevant_documents(\\n        self,\\n        query: str,\\n        *,\\n        run_manager: AsyncCallbackManagerForRetrieverRun,\\n    ) -> List[Document]:\\n        \"\"\"Get relevant documents given a user query.\\n\\n        Args:\\n            question: user query\\n\\n        Returns:\\n            Unique union of relevant documents from all generated queries\\n        \"\"\"\\n        queries = await self.agenerate_queries(query, run_manager)\\n        if self.include_original:\\n            queries.append(query)\\n        documents = await self.aretrieve_documents(queries, run_manager)\\n        return self.unique_union(documents)\\n\\n    async def agenerate_queries(\\n        self, question: str, run_manager: AsyncCallbackManagerForRetrieverRun\\n    ) -> List[str]:\\n        \"\"\"Generate queries based upon user input.\\n\\n        Args:\\n            question: user query' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\multi_query.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Returns:\\n            List of LLM generated queries that are similar to the user input\\n        \"\"\"\\n        response = await self.llm_chain.acall(\\n            inputs={\"question\": question}, callbacks=run_manager.get_child()\\n        )\\n        lines = getattr(response[\"text\"], self.parser_key, [])\\n        if self.verbose:\\n            logger.info(f\"Generated queries: {lines}\")\\n        return lines\\n\\n    async def aretrieve_documents(\\n        self, queries: List[str], run_manager: AsyncCallbackManagerForRetrieverRun\\n    ) -> List[Document]:\\n        \"\"\"Run all LLM generated queries.\\n\\n        Args:\\n            queries: query list\\n\\n        Returns:\\n            List of retrieved Documents\\n        \"\"\"\\n        document_lists = await asyncio.gather(\\n            *(\\n                self.retriever.aget_relevant_documents(\\n                    query, callbacks=run_manager.get_child()\\n                )\\n                for query in queries\\n            )\\n        )\\n        return [doc for docs in document_lists for doc in docs]\\n\\n    def _get_relevant_documents(\\n        self,\\n        query: str,\\n        *,\\n        run_manager: CallbackManagerForRetrieverRun,\\n    ) -> List[Document]:\\n        \"\"\"Get relevant documents given a user query.\\n\\n        Args:\\n            question: user query\\n\\n        Returns:\\n            Unique union of relevant documents from all generated queries\\n        \"\"\"\\n        queries = self.generate_queries(query, run_manager)\\n        if self.include_original:\\n            queries.append(query)\\n        documents = self.retrieve_documents(queries, run_manager)\\n        return self.unique_union(documents)\\n\\n    def generate_queries(\\n        self, question: str, run_manager: CallbackManagerForRetrieverRun\\n    ) -> List[str]:\\n        \"\"\"Generate queries based upon user input.\\n\\n        Args:\\n            question: user query' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\multi_query.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Returns:\\n            List of LLM generated queries that are similar to the user input\\n        \"\"\"\\n        response = self.llm_chain(\\n            {\"question\": question}, callbacks=run_manager.get_child()\\n        )\\n        lines = getattr(response[\"text\"], self.parser_key, [])\\n        if self.verbose:\\n            logger.info(f\"Generated queries: {lines}\")\\n        return lines\\n\\n    def retrieve_documents(\\n        self, queries: List[str], run_manager: CallbackManagerForRetrieverRun\\n    ) -> List[Document]:\\n        \"\"\"Run all LLM generated queries.\\n\\n        Args:\\n            queries: query list\\n\\n        Returns:\\n            List of retrieved Documents\\n        \"\"\"\\n        documents = []\\n        for query in queries:\\n            docs = self.retriever.get_relevant_documents(\\n                query, callbacks=run_manager.get_child()\\n            )\\n            documents.extend(docs)\\n        return documents\\n\\n    def unique_union(self, documents: List[Document]) -> List[Document]:\\n        \"\"\"Get unique Documents.\\n\\n        Args:\\n            documents: List of retrieved Documents\\n\\n        Returns:\\n            List of unique retrieved Documents\\n        \"\"\"\\n        return _unique_documents(documents)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\multi_query.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from enum import Enum\\nfrom typing import Dict, List, Optional\\n\\nfrom langchain_core.callbacks import CallbackManagerForRetrieverRun\\nfrom langchain_core.documents import Document\\nfrom langchain_core.pydantic_v1 import Field, root_validator\\nfrom langchain_core.retrievers import BaseRetriever\\nfrom langchain_core.stores import BaseStore, ByteStore\\nfrom langchain_core.vectorstores import VectorStore\\n\\nfrom langchain.storage._lc_store import create_kv_docstore\\n\\n\\nclass SearchType(str, Enum):\\n    \"\"\"Enumerator of the types of search to perform.\"\"\"\\n\\n    similarity = \"similarity\"\\n    \"\"\"Similarity search.\"\"\"\\n    mmr = \"mmr\"\\n    \"\"\"Maximal Marginal Relevance reranking of similarity search.\"\"\"\\n\\n\\nclass MultiVectorRetriever(BaseRetriever):\\n    \"\"\"Retrieve from a set of multiple embeddings for the same document.\"\"\"\\n\\n    vectorstore: VectorStore\\n    \"\"\"The underlying vectorstore to use to store small chunks\\n    and their embedding vectors\"\"\"\\n    byte_store: Optional[ByteStore] = None\\n    \"\"\"The lower-level backing storage layer for the parent documents\"\"\"\\n    docstore: BaseStore[str, Document]\\n    \"\"\"The storage interface for the parent documents\"\"\"\\n    id_key: str = \"doc_id\"\\n    search_kwargs: dict = Field(default_factory=dict)\\n    \"\"\"Keyword arguments to pass to the search function.\"\"\"\\n    search_type: SearchType = SearchType.similarity\\n    \"\"\"Type of search to perform (similarity / mmr)\"\"\"\\n\\n    @root_validator(pre=True)\\n    def shim_docstore(cls, values: Dict) -> Dict:\\n        byte_store = values.get(\"byte_store\")\\n        docstore = values.get(\"docstore\")\\n        if byte_store is not None:\\n            docstore = create_kv_docstore(byte_store)\\n        elif docstore is None:\\n            raise Exception(\"You must pass a `byte_store` parameter.\")\\n        values[\"docstore\"] = docstore\\n        return values' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\multi_vector.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_relevant_documents(\\n        self, query: str, *, run_manager: CallbackManagerForRetrieverRun\\n    ) -> List[Document]:\\n        \"\"\"Get documents relevant to a query.\\n        Args:\\n            query: String to find relevant documents for\\n            run_manager: The callbacks handler to use\\n        Returns:\\n            List of relevant documents\\n        \"\"\"\\n        if self.search_type == SearchType.mmr:\\n            sub_docs = self.vectorstore.max_marginal_relevance_search(\\n                query, **self.search_kwargs\\n            )\\n        else:\\n            sub_docs = self.vectorstore.similarity_search(query, **self.search_kwargs)\\n\\n        # We do this to maintain the order of the ids that are returned\\n        ids = []\\n        for d in sub_docs:\\n            if self.id_key in d.metadata and d.metadata[self.id_key] not in ids:\\n                ids.append(d.metadata[self.id_key])\\n        docs = self.docstore.mget(ids)\\n        return [d for d in docs if d is not None]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\multi_vector.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.retrievers.outline import OutlineRetriever\\n\\n__all__ = [\"OutlineRetriever\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\outline.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='import uuid\\nfrom typing import List, Optional\\n\\nfrom langchain_core.documents import Document\\n\\nfrom langchain.retrievers import MultiVectorRetriever\\nfrom langchain.text_splitter import TextSplitter\\n\\n\\nclass ParentDocumentRetriever(MultiVectorRetriever):\\n    \"\"\"Retrieve small chunks then retrieve their parent documents.\\n\\n    When splitting documents for retrieval, there are often conflicting desires:\\n\\n    1. You may want to have small documents, so that their embeddings can most\\n        accurately reflect their meaning. If too long, then the embeddings can\\n        lose meaning.\\n    2. You want to have long enough documents that the context of each chunk is\\n        retained.\\n\\n    The ParentDocumentRetriever strikes that balance by splitting and storing\\n    small chunks of data. During retrieval, it first fetches the small chunks\\n    but then looks up the parent ids for those chunks and returns those larger\\n    documents.\\n\\n    Note that \"parent document\" refers to the document that a small chunk\\n    originated from. This can either be the whole raw document OR a larger\\n    chunk.\\n\\n    Examples:\\n\\n        .. code-block:: python\\n\\n            # Imports\\n            from langchain_community.vectorstores import Chroma\\n            from langchain_community.embeddings import OpenAIEmbeddings\\n            from langchain.text_splitter import RecursiveCharacterTextSplitter\\n            from langchain.storage import InMemoryStore\\n\\n            # This text splitter is used to create the parent documents\\n            parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)\\n            # This text splitter is used to create the child documents\\n            # It should create documents smaller than the parent\\n            child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\\n            # The vectorstore to use to index the child chunks\\n            vectorstore = Chroma(embedding_function=OpenAIEmbeddings())\\n            # The storage layer for the parent documents\\n            store = InMemoryStore()' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\parent_document_retriever.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# Initialize the retriever\\n            retriever = ParentDocumentRetriever(\\n                vectorstore=vectorstore,\\n                docstore=store,\\n                child_splitter=child_splitter,\\n                parent_splitter=parent_splitter,\\n            )\\n    \"\"\"\\n\\n    child_splitter: TextSplitter\\n    \"\"\"The text splitter to use to create child documents.\"\"\"\\n\\n    \"\"\"The key to use to track the parent id. This will be stored in the\\n    metadata of child documents.\"\"\"\\n    parent_splitter: Optional[TextSplitter] = None\\n    \"\"\"The text splitter to use to create parent documents.\\n    If none, then the parent documents will be the raw documents passed in.\"\"\"\\n\\n    def add_documents(\\n        self,\\n        documents: List[Document],\\n        ids: Optional[List[str]] = None,\\n        add_to_docstore: bool = True,\\n    ) -> None:\\n        \"\"\"Adds documents to the docstore and vectorstores.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\parent_document_retriever.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Args:\\n            documents: List of documents to add\\n            ids: Optional list of ids for documents. If provided should be the same\\n                length as the list of documents. Can provided if parent documents\\n                are already in the document store and you don\\'t want to re-add\\n                to the docstore. If not provided, random UUIDs will be used as\\n                ids.\\n            add_to_docstore: Boolean of whether to add documents to docstore.\\n                This can be false if and only if `ids` are provided. You may want\\n                to set this to False if the documents are already in the docstore\\n                and you don\\'t want to re-add them.\\n        \"\"\"\\n        if self.parent_splitter is not None:\\n            documents = self.parent_splitter.split_documents(documents)\\n        if ids is None:\\n            doc_ids = [str(uuid.uuid4()) for _ in documents]\\n            if not add_to_docstore:\\n                raise ValueError(\\n                    \"If ids are not passed in, `add_to_docstore` MUST be True\"\\n                )\\n        else:\\n            if len(documents) != len(ids):\\n                raise ValueError(\\n                    \"Got uneven list of documents and ids. \"\\n                    \"If `ids` is provided, should be same length as `documents`.\"\\n                )\\n            doc_ids = ids\\n\\n        docs = []\\n        full_docs = []\\n        for i, doc in enumerate(documents):\\n            _id = doc_ids[i]\\n            sub_docs = self.child_splitter.split_documents([doc])\\n            for _doc in sub_docs:\\n                _doc.metadata[self.id_key] = _id\\n            docs.extend(sub_docs)\\n            full_docs.append((_id, doc))\\n        self.vectorstore.add_documents(docs)\\n        if add_to_docstore:\\n            self.docstore.mset(full_docs)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\parent_document_retriever.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.retrievers.pinecone_hybrid_search import (\\n    PineconeHybridSearchRetriever,\\n)\\n\\n__all__ = [\"PineconeHybridSearchRetriever\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\pinecone_hybrid_search.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.retrievers.pubmed import PubMedRetriever\\n\\n__all__ = [\"PubMedRetriever\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\pubmed.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain.retrievers.pubmed import PubMedRetriever\\n\\n__all__ = [\\n    \"PubMedRetriever\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\pupmed.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.retrievers.remote_retriever import RemoteLangChainRetriever\\n\\n__all__ = [\"RemoteLangChainRetriever\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\remote_retriever.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='import logging\\nfrom typing import List\\n\\nfrom langchain_core.callbacks import (\\n    AsyncCallbackManagerForRetrieverRun,\\n    CallbackManagerForRetrieverRun,\\n)\\nfrom langchain_core.documents import Document\\nfrom langchain_core.language_models import BaseLLM\\nfrom langchain_core.prompts.prompt import PromptTemplate\\nfrom langchain_core.retrievers import BaseRetriever\\n\\nfrom langchain.chains.llm import LLMChain\\n\\nlogger = logging.getLogger(__name__)\\n\\n# Default template\\nDEFAULT_TEMPLATE = \"\"\"You are an assistant tasked with taking a natural language \\\\\\nquery from a user and converting it into a query for a vectorstore. \\\\\\nIn this process, you strip out information that is not relevant for \\\\\\nthe retrieval task. Here is the user query: {question}\"\"\"\\n\\n# Default prompt\\nDEFAULT_QUERY_PROMPT = PromptTemplate.from_template(DEFAULT_TEMPLATE)\\n\\n\\nclass RePhraseQueryRetriever(BaseRetriever):\\n    \"\"\"Given a query, use an LLM to re-phrase it.\\n    Then, retrieve docs for the re-phrased query.\"\"\"\\n\\n    retriever: BaseRetriever\\n    llm_chain: LLMChain\\n\\n    @classmethod\\n    def from_llm(\\n        cls,\\n        retriever: BaseRetriever,\\n        llm: BaseLLM,\\n        prompt: PromptTemplate = DEFAULT_QUERY_PROMPT,\\n    ) -> \"RePhraseQueryRetriever\":\\n        \"\"\"Initialize from llm using default template.\\n\\n        The prompt used here expects a single input: `question`\\n\\n        Args:\\n            retriever: retriever to query documents from\\n            llm: llm for query generation using DEFAULT_QUERY_PROMPT\\n            prompt: prompt template for query generation\\n\\n        Returns:\\n            RePhraseQueryRetriever\\n        \"\"\"\\n\\n        llm_chain = LLMChain(llm=llm, prompt=prompt)\\n        return cls(\\n            retriever=retriever,\\n            llm_chain=llm_chain,\\n        )\\n\\n    def _get_relevant_documents(\\n        self,\\n        query: str,\\n        *,\\n        run_manager: CallbackManagerForRetrieverRun,\\n    ) -> List[Document]:\\n        \"\"\"Get relevated documents given a user question.\\n\\n        Args:\\n            query: user question' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\re_phraser.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Returns:\\n            Relevant documents for re-phrased question\\n        \"\"\"\\n        response = self.llm_chain(query, callbacks=run_manager.get_child())\\n        re_phrased_question = response[\"text\"]\\n        logger.info(f\"Re-phrased question: {re_phrased_question}\")\\n        docs = self.retriever.get_relevant_documents(\\n            re_phrased_question, callbacks=run_manager.get_child()\\n        )\\n        return docs\\n\\n    async def _aget_relevant_documents(\\n        self,\\n        query: str,\\n        *,\\n        run_manager: AsyncCallbackManagerForRetrieverRun,\\n    ) -> List[Document]:\\n        raise NotImplementedError' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\re_phraser.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.retrievers.svm import SVMRetriever\\n\\n__all__ = [\"SVMRetriever\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\svm.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.retrievers.tavily_search_api import (\\n    SearchDepth,\\n    TavilySearchAPIRetriever,\\n)\\n\\n__all__ = [\"SearchDepth\", \"TavilySearchAPIRetriever\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\tavily_search_api.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.retrievers.tfidf import TFIDFRetriever\\n\\n__all__ = [\"TFIDFRetriever\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\tfidf.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='import datetime\\nfrom copy import deepcopy\\nfrom typing import Any, Dict, List, Optional, Tuple\\n\\nfrom langchain_core.callbacks import CallbackManagerForRetrieverRun\\nfrom langchain_core.documents import Document\\nfrom langchain_core.pydantic_v1 import Field\\nfrom langchain_core.retrievers import BaseRetriever\\nfrom langchain_core.vectorstores import VectorStore\\n\\n\\ndef _get_hours_passed(time: datetime.datetime, ref_time: datetime.datetime) -> float:\\n    \"\"\"Get the hours passed between two datetimes.\"\"\"\\n    return (time - ref_time).total_seconds() / 3600\\n\\n\\nclass TimeWeightedVectorStoreRetriever(BaseRetriever):\\n    \"\"\"Retriever that combines embedding similarity with\\n    recency in retrieving values.\"\"\"\\n\\n    vectorstore: VectorStore\\n    \"\"\"The vectorstore to store documents and determine salience.\"\"\"\\n\\n    search_kwargs: dict = Field(default_factory=lambda: dict(k=100))\\n    \"\"\"Keyword arguments to pass to the vectorstore similarity search.\"\"\"\\n\\n    # TODO: abstract as a queue\\n    memory_stream: List[Document] = Field(default_factory=list)\\n    \"\"\"The memory_stream of documents to search through.\"\"\"\\n\\n    decay_rate: float = Field(default=0.01)\\n    \"\"\"The exponential decay factor used as (1.0-decay_rate)**(hrs_passed).\"\"\"\\n\\n    k: int = 4\\n    \"\"\"The maximum number of documents to retrieve in a given call.\"\"\"\\n\\n    other_score_keys: List[str] = []\\n    \"\"\"Other keys in the metadata to factor into the score, e.g. \\'importance\\'.\"\"\"\\n\\n    default_salience: Optional[float] = None\\n    \"\"\"The salience to assign memories not retrieved from the vector store.\\n\\n    None assigns no salience to documents not fetched from the vector store.\\n    \"\"\"\\n\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n\\n        arbitrary_types_allowed = True' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\time_weighted_retriever.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _document_get_date(self, field: str, document: Document) -> datetime.datetime:\\n        \"\"\"Return the value of the date field of a document.\"\"\"\\n        if field in document.metadata:\\n            if isinstance(document.metadata[field], float):\\n                return datetime.datetime.fromtimestamp(document.metadata[field])\\n            return document.metadata[field]\\n        return datetime.datetime.now()\\n\\n    def _get_combined_score(\\n        self,\\n        document: Document,\\n        vector_relevance: Optional[float],\\n        current_time: datetime.datetime,\\n    ) -> float:\\n        \"\"\"Return the combined score for a document.\"\"\"\\n        hours_passed = _get_hours_passed(\\n            current_time,\\n            self._document_get_date(\"last_accessed_at\", document),\\n        )\\n        score = (1.0 - self.decay_rate) ** hours_passed\\n        for key in self.other_score_keys:\\n            if key in document.metadata:\\n                score += document.metadata[key]\\n        if vector_relevance is not None:\\n            score += vector_relevance\\n        return score\\n\\n    def get_salient_docs(self, query: str) -> Dict[int, Tuple[Document, float]]:\\n        \"\"\"Return documents that are salient to the query.\"\"\"\\n        docs_and_scores: List[Tuple[Document, float]]\\n        docs_and_scores = self.vectorstore.similarity_search_with_relevance_scores(\\n            query, **self.search_kwargs\\n        )\\n        results = {}\\n        for fetched_doc, relevance in docs_and_scores:\\n            if \"buffer_idx\" in fetched_doc.metadata:\\n                buffer_idx = fetched_doc.metadata[\"buffer_idx\"]\\n                doc = self.memory_stream[buffer_idx]\\n                results[buffer_idx] = (doc, relevance)\\n        return results' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\time_weighted_retriever.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_relevant_documents(\\n        self, query: str, *, run_manager: CallbackManagerForRetrieverRun\\n    ) -> List[Document]:\\n        \"\"\"Return documents that are relevant to the query.\"\"\"\\n        current_time = datetime.datetime.now()\\n        docs_and_scores = {\\n            doc.metadata[\"buffer_idx\"]: (doc, self.default_salience)\\n            for doc in self.memory_stream[-self.k :]\\n        }\\n        # If a doc is considered salient, update the salience score\\n        docs_and_scores.update(self.get_salient_docs(query))\\n        rescored_docs = [\\n            (doc, self._get_combined_score(doc, relevance, current_time))\\n            for doc, relevance in docs_and_scores.values()\\n        ]\\n        rescored_docs.sort(key=lambda x: x[1], reverse=True)\\n        result = []\\n        # Ensure frequently accessed memories aren\\'t forgotten\\n        for doc, _ in rescored_docs[: self.k]:\\n            # TODO: Update vector store doc once `update` method is exposed.\\n            buffered_doc = self.memory_stream[doc.metadata[\"buffer_idx\"]]\\n            buffered_doc.metadata[\"last_accessed_at\"] = current_time\\n            result.append(buffered_doc)\\n        return result\\n\\n    def add_documents(self, documents: List[Document], **kwargs: Any) -> List[str]:\\n        \"\"\"Add documents to vectorstore.\"\"\"\\n        current_time = kwargs.get(\"current_time\")\\n        if current_time is None:\\n            current_time = datetime.datetime.now()\\n        # Avoid mutating input documents\\n        dup_docs = [deepcopy(d) for d in documents]\\n        for i, doc in enumerate(dup_docs):\\n            if \"last_accessed_at\" not in doc.metadata:\\n                doc.metadata[\"last_accessed_at\"] = current_time\\n            if \"created_at\" not in doc.metadata:\\n                doc.metadata[\"created_at\"] = current_time\\n            doc.metadata[\"buffer_idx\"] = len(self.memory_stream) + i\\n        self.memory_stream.extend(dup_docs)\\n        return self.vectorstore.add_documents(dup_docs, **kwargs)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\time_weighted_retriever.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='async def aadd_documents(\\n        self, documents: List[Document], **kwargs: Any\\n    ) -> List[str]:\\n        \"\"\"Add documents to vectorstore.\"\"\"\\n        current_time = kwargs.get(\"current_time\")\\n        if current_time is None:\\n            current_time = datetime.datetime.now()\\n        # Avoid mutating input documents\\n        dup_docs = [deepcopy(d) for d in documents]\\n        for i, doc in enumerate(dup_docs):\\n            if \"last_accessed_at\" not in doc.metadata:\\n                doc.metadata[\"last_accessed_at\"] = current_time\\n            if \"created_at\" not in doc.metadata:\\n                doc.metadata[\"created_at\"] = current_time\\n            doc.metadata[\"buffer_idx\"] = len(self.memory_stream) + i\\n        self.memory_stream.extend(dup_docs)\\n        return await self.vectorstore.aadd_documents(dup_docs, **kwargs)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\time_weighted_retriever.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.retrievers.vespa_retriever import VespaRetriever\\n\\n__all__ = [\"VespaRetriever\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\vespa_retriever.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.retrievers.weaviate_hybrid_search import (\\n    WeaviateHybridSearchRetriever,\\n)\\n\\n__all__ = [\"WeaviateHybridSearchRetriever\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\weaviate_hybrid_search.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='import logging\\nimport re\\nfrom typing import List, Optional\\n\\nfrom langchain_community.document_loaders import AsyncHtmlLoader\\nfrom langchain_community.document_transformers import Html2TextTransformer\\nfrom langchain_community.llms import LlamaCpp\\nfrom langchain_community.utilities import GoogleSearchAPIWrapper\\nfrom langchain_core.callbacks import (\\n    AsyncCallbackManagerForRetrieverRun,\\n    CallbackManagerForRetrieverRun,\\n)\\nfrom langchain_core.documents import Document\\nfrom langchain_core.language_models import BaseLLM\\nfrom langchain_core.prompts import BasePromptTemplate, PromptTemplate\\nfrom langchain_core.pydantic_v1 import BaseModel, Field\\nfrom langchain_core.retrievers import BaseRetriever\\nfrom langchain_core.vectorstores import VectorStore\\n\\nfrom langchain.chains import LLMChain\\nfrom langchain.chains.prompt_selector import ConditionalPromptSelector\\nfrom langchain.output_parsers.pydantic import PydanticOutputParser\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter, TextSplitter\\n\\nlogger = logging.getLogger(__name__)\\n\\n\\nclass SearchQueries(BaseModel):\\n    \"\"\"Search queries to research for the user\\'s goal.\"\"\"\\n\\n    queries: List[str] = Field(\\n        ..., description=\"List of search queries to look up on Google\"\\n    )\\n\\n\\nDEFAULT_LLAMA_SEARCH_PROMPT = PromptTemplate(\\n    input_variables=[\"question\"],\\n    template=\"\"\"<<SYS>> \\\\n You are an assistant tasked with improving Google search \\\\\\nresults. \\\\n <</SYS>> \\\\n\\\\n [INST] Generate THREE Google search queries that \\\\\\nare similar to this question. The output should be a numbered list of questions \\\\\\nand each should have a question mark at the end: \\\\n\\\\n {question} [/INST]\"\"\",\\n)\\n\\nDEFAULT_SEARCH_PROMPT = PromptTemplate(\\n    input_variables=[\"question\"],\\n    template=\"\"\"You are an assistant tasked with improving Google search \\\\\\nresults. Generate THREE Google search queries that are similar to \\\\\\nthis question. The output should be a numbered list of questions and each \\\\\\nshould have a question mark at the end: {question}\"\"\",\\n)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\web_research.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='class LineList(BaseModel):\\n    \"\"\"List of questions.\"\"\"\\n\\n    lines: List[str] = Field(description=\"Questions\")\\n\\n\\nclass QuestionListOutputParser(PydanticOutputParser):\\n    \"\"\"Output parser for a list of numbered questions.\"\"\"\\n\\n    def __init__(self) -> None:\\n        super().__init__(pydantic_object=LineList)\\n\\n    def parse(self, text: str) -> LineList:\\n        lines = re.findall(r\"\\\\d+\\\\..*?(?:\\\\n|$)\", text)\\n        return LineList(lines=lines)\\n\\n\\nclass WebResearchRetriever(BaseRetriever):\\n    \"\"\"`Google Search API` retriever.\"\"\"\\n\\n    # Inputs\\n    vectorstore: VectorStore = Field(\\n        ..., description=\"Vector store for storing web pages\"\\n    )\\n    llm_chain: LLMChain\\n    search: GoogleSearchAPIWrapper = Field(..., description=\"Google Search API Wrapper\")\\n    num_search_results: int = Field(1, description=\"Number of pages per Google search\")\\n    text_splitter: TextSplitter = Field(\\n        RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=50),\\n        description=\"Text splitter for splitting web pages into chunks\",\\n    )\\n    url_database: List[str] = Field(\\n        default_factory=list, description=\"List of processed URLs\"\\n    )\\n\\n    @classmethod\\n    def from_llm(\\n        cls,\\n        vectorstore: VectorStore,\\n        llm: BaseLLM,\\n        search: GoogleSearchAPIWrapper,\\n        prompt: Optional[BasePromptTemplate] = None,\\n        num_search_results: int = 1,\\n        text_splitter: RecursiveCharacterTextSplitter = RecursiveCharacterTextSplitter(\\n            chunk_size=1500, chunk_overlap=150\\n        ),\\n    ) -> \"WebResearchRetriever\":\\n        \"\"\"Initialize from llm using default template.\\n\\n        Args:\\n            vectorstore: Vector store for storing web pages\\n            llm: llm for search question generation\\n            search: GoogleSearchAPIWrapper\\n            prompt: prompt to generating search questions\\n            num_search_results: Number of pages per Google search\\n            text_splitter: Text splitter for splitting web pages into chunks' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\web_research.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Returns:\\n            WebResearchRetriever\\n        \"\"\"\\n\\n        if not prompt:\\n            QUESTION_PROMPT_SELECTOR = ConditionalPromptSelector(\\n                default_prompt=DEFAULT_SEARCH_PROMPT,\\n                conditionals=[\\n                    (lambda llm: isinstance(llm, LlamaCpp), DEFAULT_LLAMA_SEARCH_PROMPT)\\n                ],\\n            )\\n            prompt = QUESTION_PROMPT_SELECTOR.get_prompt(llm)\\n\\n        # Use chat model prompt\\n        llm_chain = LLMChain(\\n            llm=llm,\\n            prompt=prompt,\\n            output_parser=QuestionListOutputParser(),\\n        )\\n\\n        return cls(\\n            vectorstore=vectorstore,\\n            llm_chain=llm_chain,\\n            search=search,\\n            num_search_results=num_search_results,\\n            text_splitter=text_splitter,\\n        )\\n\\n    def clean_search_query(self, query: str) -> str:\\n        # Some search tools (e.g., Google) will\\n        # fail to return results if query has a\\n        # leading digit: 1. \"LangCh...\"\\n        # Check if the first character is a digit\\n        if query[0].isdigit():\\n            # Find the position of the first quote\\n            first_quote_pos = query.find(\\'\"\\')\\n            if first_quote_pos != -1:\\n                # Extract the part of the string after the quote\\n                query = query[first_quote_pos + 1 :]\\n                # Remove the trailing quote if present\\n                if query.endswith(\\'\"\\'):\\n                    query = query[:-1]\\n        return query.strip()\\n\\n    def search_tool(self, query: str, num_search_results: int = 1) -> List[dict]:\\n        \"\"\"Returns num_search_results pages per Google search.\"\"\"\\n        query_clean = self.clean_search_query(query)\\n        result = self.search.results(query_clean, num_search_results)\\n        return result\\n\\n    def _get_relevant_documents(\\n        self,\\n        query: str,\\n        *,\\n        run_manager: CallbackManagerForRetrieverRun,\\n    ) -> List[Document]:\\n        \"\"\"Search Google for documents related to the query input.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\web_research.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Args:\\n            query: user query\\n\\n        Returns:\\n            Relevant documents from all various urls.\\n        \"\"\"\\n\\n        # Get search questions\\n        logger.info(\"Generating questions for Google Search ...\")\\n        result = self.llm_chain({\"question\": query})\\n        logger.info(f\"Questions for Google Search (raw): {result}\")\\n        questions = getattr(result[\"text\"], \"lines\", [])\\n        logger.info(f\"Questions for Google Search: {questions}\")\\n\\n        # Get urls\\n        logger.info(\"Searching for relevant urls...\")\\n        urls_to_look = []\\n        for query in questions:\\n            # Google search\\n            search_results = self.search_tool(query, self.num_search_results)\\n            logger.info(\"Searching for relevant urls...\")\\n            logger.info(f\"Search results: {search_results}\")\\n            for res in search_results:\\n                if res.get(\"link\", None):\\n                    urls_to_look.append(res[\"link\"])\\n\\n        # Relevant urls\\n        urls = set(urls_to_look)\\n\\n        # Check for any new urls that we have not processed\\n        new_urls = list(urls.difference(self.url_database))\\n\\n        logger.info(f\"New URLs to load: {new_urls}\")\\n        # Load, split, and add new urls to vectorstore\\n        if new_urls:\\n            loader = AsyncHtmlLoader(new_urls, ignore_load_errors=True)\\n            html2text = Html2TextTransformer()\\n            logger.info(\"Indexing new urls...\")\\n            docs = loader.load()\\n            docs = list(html2text.transform_documents(docs))\\n            docs = self.text_splitter.split_documents(docs)\\n            self.vectorstore.add_documents(docs)\\n            self.url_database.extend(new_urls)\\n\\n        # Search for relevant splits\\n        # TODO: make this async\\n        logger.info(\"Grabbing most relevant splits from urls...\")\\n        docs = []\\n        for query in questions:\\n            docs.extend(self.vectorstore.similarity_search(query))' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\web_research.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# Get unique docs\\n        unique_documents_dict = {\\n            (doc.page_content, tuple(sorted(doc.metadata.items()))): doc for doc in docs\\n        }\\n        unique_documents = list(unique_documents_dict.values())\\n        return unique_documents\\n\\n    async def _aget_relevant_documents(\\n        self,\\n        query: str,\\n        *,\\n        run_manager: AsyncCallbackManagerForRetrieverRun,\\n    ) -> List[Document]:\\n        raise NotImplementedError' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\web_research.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.retrievers.wikipedia import WikipediaRetriever\\n\\n__all__ = [\"WikipediaRetriever\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\wikipedia.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.retrievers.you import YouRetriever\\n\\n__all__ = [\"YouRetriever\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\you.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.retrievers.zep import SearchScope, SearchType, ZepRetriever\\n\\n__all__ = [\"SearchScope\", \"SearchType\", \"ZepRetriever\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\zep.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.retrievers.zilliz import ZillizRetreiver, ZillizRetriever\\n\\n__all__ = [\"ZillizRetriever\", \"ZillizRetreiver\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\zilliz.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"**Retriever** class returns Documents given a text **query**.\\n\\nIt is more general than a vector store. A retriever does not need to be able to\\nstore documents, only to return (or retrieve) it. Vector stores can be used as\\nthe backbone of a retriever, but there are other types of retrievers as well.\\n\\n**Class hierarchy:**\\n\\n.. code-block::\\n\\n    BaseRetriever --> <name>Retriever  # Examples: ArxivRetriever, MergerRetriever\\n\\n**Main helpers:**\\n\\n.. code-block::\\n\\n    Document, Serializable, Callbacks,\\n    CallbackManagerForRetrieverRun, AsyncCallbackManagerForRetrieverRun\\n\"\"\"\\nimport warnings\\nfrom typing import Any\\n\\nfrom langchain_core._api import LangChainDeprecationWarning\\n\\nfrom langchain.retrievers.contextual_compression import ContextualCompressionRetriever\\nfrom langchain.retrievers.ensemble import EnsembleRetriever\\nfrom langchain.retrievers.merger_retriever import MergerRetriever\\nfrom langchain.retrievers.multi_query import MultiQueryRetriever\\nfrom langchain.retrievers.multi_vector import MultiVectorRetriever\\nfrom langchain.retrievers.outline import OutlineRetriever\\nfrom langchain.retrievers.parent_document_retriever import ParentDocumentRetriever\\nfrom langchain.retrievers.re_phraser import RePhraseQueryRetriever\\nfrom langchain.retrievers.self_query.base import SelfQueryRetriever\\nfrom langchain.retrievers.time_weighted_retriever import (\\n    TimeWeightedVectorStoreRetriever,\\n)\\nfrom langchain.retrievers.web_research import WebResearchRetriever\\nfrom langchain.utils.interactive_env import is_interactive_env\\n\\n\\ndef __getattr__(name: str) -> Any:\\n    from langchain_community import retrievers' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# If not in interactive env, raise warning.\\n    if not is_interactive_env():\\n        warnings.warn(\\n            \"Importing this retriever from langchain is deprecated. Importing it from \"\\n            \"langchain will no longer be supported as of langchain==0.2.0. \"\\n            \"Please import from langchain-community instead:\\\\n\\\\n\"\\n            f\"`from langchain_community.retrievers import {name}`.\\\\n\\\\n\"\\n            \"To install langchain-community run `pip install -U langchain-community`.\",\\n            category=LangChainDeprecationWarning,\\n        )\\n\\n    return getattr(retrievers, name)\\n\\n\\n__all__ = [\\n    \"AmazonKendraRetriever\",\\n    \"AmazonKnowledgeBasesRetriever\",\\n    \"ArceeRetriever\",\\n    \"ArxivRetriever\",\\n    \"AzureCognitiveSearchRetriever\",\\n    \"ChatGPTPluginRetriever\",\\n    \"ContextualCompressionRetriever\",\\n    \"ChaindeskRetriever\",\\n    \"CohereRagRetriever\",\\n    \"ElasticSearchBM25Retriever\",\\n    \"EmbedchainRetriever\",\\n    \"GoogleDocumentAIWarehouseRetriever\",\\n    \"GoogleCloudEnterpriseSearchRetriever\",\\n    \"GoogleVertexAIMultiTurnSearchRetriever\",\\n    \"GoogleVertexAISearchRetriever\",\\n    \"KayAiRetriever\",\\n    \"KNNRetriever\",\\n    \"LlamaIndexGraphRetriever\",\\n    \"LlamaIndexRetriever\",\\n    \"MergerRetriever\",\\n    \"MetalRetriever\",\\n    \"MilvusRetriever\",\\n    \"MultiQueryRetriever\",\\n    \"OutlineRetriever\",\\n    \"PineconeHybridSearchRetriever\",\\n    \"PubMedRetriever\",\\n    \"RemoteLangChainRetriever\",\\n    \"SVMRetriever\",\\n    \"SelfQueryRetriever\",\\n    \"TavilySearchAPIRetriever\",\\n    \"TFIDFRetriever\",\\n    \"BM25Retriever\",\\n    \"TimeWeightedVectorStoreRetriever\",\\n    \"VespaRetriever\",\\n    \"WeaviateHybridSearchRetriever\",\\n    \"WikipediaRetriever\",\\n    \"ZepRetriever\",\\n    \"ZillizRetriever\",\\n    \"DocArrayRetriever\",\\n    \"RePhraseQueryRetriever\",\\n    \"WebResearchRetriever\",\\n    \"EnsembleRetriever\",\\n    \"ParentDocumentRetriever\",\\n    \"MultiVectorRetriever\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from abc import ABC, abstractmethod\\nfrom inspect import signature\\nfrom typing import List, Optional, Sequence, Union\\n\\nfrom langchain_core.documents import BaseDocumentTransformer, Document\\nfrom langchain_core.pydantic_v1 import BaseModel\\nfrom langchain_core.runnables.config import run_in_executor\\n\\nfrom langchain.callbacks.manager import Callbacks\\n\\n\\nclass BaseDocumentCompressor(BaseModel, ABC):\\n    \"\"\"Base class for document compressors.\"\"\"\\n\\n    @abstractmethod\\n    def compress_documents(\\n        self,\\n        documents: Sequence[Document],\\n        query: str,\\n        callbacks: Optional[Callbacks] = None,\\n    ) -> Sequence[Document]:\\n        \"\"\"Compress retrieved documents given the query context.\"\"\"\\n\\n    async def acompress_documents(\\n        self,\\n        documents: Sequence[Document],\\n        query: str,\\n        callbacks: Optional[Callbacks] = None,\\n    ) -> Sequence[Document]:\\n        \"\"\"Compress retrieved documents given the query context.\"\"\"\\n        return await run_in_executor(\\n            None, self.compress_documents, documents, query, callbacks\\n        )\\n\\n\\nclass DocumentCompressorPipeline(BaseDocumentCompressor):\\n    \"\"\"Document compressor that uses a pipeline of Transformers.\"\"\"\\n\\n    transformers: List[Union[BaseDocumentTransformer, BaseDocumentCompressor]]\\n    \"\"\"List of document filters that are chained together and run in sequence.\"\"\"\\n\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n\\n        arbitrary_types_allowed = True' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\document_compressors\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def compress_documents(\\n        self,\\n        documents: Sequence[Document],\\n        query: str,\\n        callbacks: Optional[Callbacks] = None,\\n    ) -> Sequence[Document]:\\n        \"\"\"Transform a list of documents.\"\"\"\\n        for _transformer in self.transformers:\\n            if isinstance(_transformer, BaseDocumentCompressor):\\n                accepts_callbacks = (\\n                    signature(_transformer.compress_documents).parameters.get(\\n                        \"callbacks\"\\n                    )\\n                    is not None\\n                )\\n                if accepts_callbacks:\\n                    documents = _transformer.compress_documents(\\n                        documents, query, callbacks=callbacks\\n                    )\\n                else:\\n                    documents = _transformer.compress_documents(documents, query)\\n            elif isinstance(_transformer, BaseDocumentTransformer):\\n                documents = _transformer.transform_documents(documents)\\n            else:\\n                raise ValueError(f\"Got unexpected transformer type: {_transformer}\")\\n        return documents' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\document_compressors\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='async def acompress_documents(\\n        self,\\n        documents: Sequence[Document],\\n        query: str,\\n        callbacks: Optional[Callbacks] = None,\\n    ) -> Sequence[Document]:\\n        \"\"\"Compress retrieved documents given the query context.\"\"\"\\n        for _transformer in self.transformers:\\n            if isinstance(_transformer, BaseDocumentCompressor):\\n                accepts_callbacks = (\\n                    signature(_transformer.acompress_documents).parameters.get(\\n                        \"callbacks\"\\n                    )\\n                    is not None\\n                )\\n                if accepts_callbacks:\\n                    documents = await _transformer.acompress_documents(\\n                        documents, query, callbacks=callbacks\\n                    )\\n                else:\\n                    documents = await _transformer.acompress_documents(documents, query)\\n            elif isinstance(_transformer, BaseDocumentTransformer):\\n                documents = await _transformer.atransform_documents(documents)\\n            else:\\n                raise ValueError(f\"Got unexpected transformer type: {_transformer}\")\\n        return documents' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\document_compressors\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"DocumentFilter that uses an LLM chain to extract the relevant parts of documents.\"\"\"\\nfrom __future__ import annotations\\n\\nimport asyncio\\nfrom typing import Any, Callable, Dict, Optional, Sequence\\n\\nfrom langchain_core.documents import Document\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.output_parsers import BaseOutputParser\\nfrom langchain_core.prompts import PromptTemplate\\n\\nfrom langchain.callbacks.manager import Callbacks\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.retrievers.document_compressors.base import BaseDocumentCompressor\\nfrom langchain.retrievers.document_compressors.chain_extract_prompt import (\\n    prompt_template,\\n)\\n\\n\\ndef default_get_input(query: str, doc: Document) -> Dict[str, Any]:\\n    \"\"\"Return the compression chain input.\"\"\"\\n    return {\"question\": query, \"context\": doc.page_content}\\n\\n\\nclass NoOutputParser(BaseOutputParser[str]):\\n    \"\"\"Parse outputs that could return a null string of some sort.\"\"\"\\n\\n    no_output_str: str = \"NO_OUTPUT\"\\n\\n    def parse(self, text: str) -> str:\\n        cleaned_text = text.strip()\\n        if cleaned_text == self.no_output_str:\\n            return \"\"\\n        return cleaned_text\\n\\n\\ndef _get_default_chain_prompt() -> PromptTemplate:\\n    output_parser = NoOutputParser()\\n    template = prompt_template.format(no_output_str=output_parser.no_output_str)\\n    return PromptTemplate(\\n        template=template,\\n        input_variables=[\"question\", \"context\"],\\n        output_parser=output_parser,\\n    )\\n\\n\\nclass LLMChainExtractor(BaseDocumentCompressor):\\n    \"\"\"Document compressor that uses an LLM chain to extract\\n    the relevant parts of documents.\"\"\"\\n\\n    llm_chain: LLMChain\\n    \"\"\"LLM wrapper to use for compressing documents.\"\"\"\\n\\n    get_input: Callable[[str, Document], dict] = default_get_input\\n    \"\"\"Callable for constructing the chain input from the query and a Document.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\document_compressors\\\\chain_extract.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def compress_documents(\\n        self,\\n        documents: Sequence[Document],\\n        query: str,\\n        callbacks: Optional[Callbacks] = None,\\n    ) -> Sequence[Document]:\\n        \"\"\"Compress page content of raw documents.\"\"\"\\n        compressed_docs = []\\n        for doc in documents:\\n            _input = self.get_input(query, doc)\\n            output = self.llm_chain.predict_and_parse(**_input, callbacks=callbacks)\\n            if len(output) == 0:\\n                continue\\n            compressed_docs.append(Document(page_content=output, metadata=doc.metadata))\\n        return compressed_docs\\n\\n    async def acompress_documents(\\n        self,\\n        documents: Sequence[Document],\\n        query: str,\\n        callbacks: Optional[Callbacks] = None,\\n    ) -> Sequence[Document]:\\n        \"\"\"Compress page content of raw documents asynchronously.\"\"\"\\n        outputs = await asyncio.gather(\\n            *[\\n                self.llm_chain.apredict_and_parse(\\n                    **self.get_input(query, doc), callbacks=callbacks\\n                )\\n                for doc in documents\\n            ]\\n        )\\n        compressed_docs = []\\n        for i, doc in enumerate(documents):\\n            if len(outputs[i]) == 0:\\n                continue\\n            compressed_docs.append(\\n                Document(page_content=outputs[i], metadata=doc.metadata)\\n            )\\n        return compressed_docs\\n\\n    @classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        prompt: Optional[PromptTemplate] = None,\\n        get_input: Optional[Callable[[str, Document], str]] = None,\\n        llm_chain_kwargs: Optional[dict] = None,\\n    ) -> LLMChainExtractor:\\n        \"\"\"Initialize from LLM.\"\"\"\\n        _prompt = prompt if prompt is not None else _get_default_chain_prompt()\\n        _get_input = get_input if get_input is not None else default_get_input\\n        llm_chain = LLMChain(llm=llm, prompt=_prompt, **(llm_chain_kwargs or {}))\\n        return cls(llm_chain=llm_chain, get_input=_get_input)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\document_compressors\\\\chain_extract.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# flake8: noqa\\nprompt_template = \"\"\"Given the following question and context, extract any part of the context *AS IS* that is relevant to answer the question. If none of the context is relevant return {no_output_str}. \\n\\nRemember, *DO NOT* edit the extracted parts of the context.\\n\\n> Question: {{question}}\\n> Context:\\n>>>\\n{{context}}\\n>>>\\nExtracted relevant parts:\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\document_compressors\\\\chain_extract_prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Filter that uses an LLM to drop documents that aren\\'t relevant to the query.\"\"\"\\nfrom typing import Any, Callable, Dict, Optional, Sequence\\n\\nfrom langchain_core.documents import Document\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.prompts import BasePromptTemplate, PromptTemplate\\n\\nfrom langchain.callbacks.manager import Callbacks\\nfrom langchain.chains import LLMChain\\nfrom langchain.output_parsers.boolean import BooleanOutputParser\\nfrom langchain.retrievers.document_compressors.base import BaseDocumentCompressor\\nfrom langchain.retrievers.document_compressors.chain_filter_prompt import (\\n    prompt_template,\\n)\\n\\n\\ndef _get_default_chain_prompt() -> PromptTemplate:\\n    return PromptTemplate(\\n        template=prompt_template,\\n        input_variables=[\"question\", \"context\"],\\n        output_parser=BooleanOutputParser(),\\n    )\\n\\n\\ndef default_get_input(query: str, doc: Document) -> Dict[str, Any]:\\n    \"\"\"Return the compression chain input.\"\"\"\\n    return {\"question\": query, \"context\": doc.page_content}\\n\\n\\nclass LLMChainFilter(BaseDocumentCompressor):\\n    \"\"\"Filter that drops documents that aren\\'t relevant to the query.\"\"\"\\n\\n    llm_chain: LLMChain\\n    \"\"\"LLM wrapper to use for filtering documents. \\n    The chain prompt is expected to have a BooleanOutputParser.\"\"\"\\n\\n    get_input: Callable[[str, Document], dict] = default_get_input\\n    \"\"\"Callable for constructing the chain input from the query and a Document.\"\"\"\\n\\n    def compress_documents(\\n        self,\\n        documents: Sequence[Document],\\n        query: str,\\n        callbacks: Optional[Callbacks] = None,\\n    ) -> Sequence[Document]:\\n        \"\"\"Filter down documents based on their relevance to the query.\"\"\"\\n        filtered_docs = []\\n        for doc in documents:\\n            _input = self.get_input(query, doc)\\n            include_doc = self.llm_chain.predict_and_parse(\\n                **_input, callbacks=callbacks\\n            )\\n            if include_doc:\\n                filtered_docs.append(doc)\\n        return filtered_docs' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\document_compressors\\\\chain_filter.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        prompt: Optional[BasePromptTemplate] = None,\\n        **kwargs: Any,\\n    ) -> \"LLMChainFilter\":\\n        \"\"\"Create a LLMChainFilter from a language model.\\n\\n        Args:\\n            llm: The language model to use for filtering.\\n            prompt: The prompt to use for the filter.\\n            **kwargs: Additional arguments to pass to the constructor.\\n\\n        Returns:\\n            A LLMChainFilter that uses the given language model.\\n        \"\"\"\\n        _prompt = prompt if prompt is not None else _get_default_chain_prompt()\\n        llm_chain = LLMChain(llm=llm, prompt=_prompt)\\n        return cls(llm_chain=llm_chain, **kwargs)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\document_compressors\\\\chain_filter.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# flake8: noqa\\nprompt_template = \"\"\"Given the following question and context, return YES if the context is relevant to the question and NO if it isn\\'t.\\n\\n> Question: {question}\\n> Context:\\n>>>\\n{context}\\n>>>\\n> Relevant (YES / NO):\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\document_compressors\\\\chain_filter_prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from __future__ import annotations\\n\\nfrom typing import TYPE_CHECKING, Dict, Optional, Sequence\\n\\nfrom langchain_core.documents import Document\\nfrom langchain_core.pydantic_v1 import Extra, root_validator\\n\\nfrom langchain.callbacks.manager import Callbacks\\nfrom langchain.retrievers.document_compressors.base import BaseDocumentCompressor\\nfrom langchain.utils import get_from_dict_or_env\\n\\nif TYPE_CHECKING:\\n    from cohere import Client\\nelse:\\n    # We do to avoid pydantic annotation issues when actually instantiating\\n    # while keeping this import optional\\n    try:\\n        from cohere import Client\\n    except ImportError:\\n        pass\\n\\n\\nclass CohereRerank(BaseDocumentCompressor):\\n    \"\"\"Document compressor that uses `Cohere Rerank API`.\"\"\"\\n\\n    client: Client\\n    \"\"\"Cohere client to use for compressing documents.\"\"\"\\n    top_n: int = 3\\n    \"\"\"Number of documents to return.\"\"\"\\n    model: str = \"rerank-english-v2.0\"\\n    \"\"\"Model to use for reranking.\"\"\"\\n\\n    cohere_api_key: Optional[str] = None\\n    user_agent: str = \"langchain\"\\n    \"\"\"Identifier for the application making the request.\"\"\"\\n\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n\\n        extra = Extra.forbid\\n        arbitrary_types_allowed = True\\n\\n    @root_validator(pre=True)\\n    def validate_environment(cls, values: Dict) -> Dict:\\n        \"\"\"Validate that api key and python package exists in environment.\"\"\"\\n        try:\\n            import cohere\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import cohere python package. \"\\n                \"Please install it with `pip install cohere`.\"\\n            )\\n        cohere_api_key = get_from_dict_or_env(\\n            values, \"cohere_api_key\", \"COHERE_API_KEY\"\\n        )\\n        client_name = values.get(\"user_agent\", \"langchain\")\\n        values[\"client\"] = cohere.Client(cohere_api_key, client_name=client_name)\\n        return values' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\document_compressors\\\\cohere_rerank.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def compress_documents(\\n        self,\\n        documents: Sequence[Document],\\n        query: str,\\n        callbacks: Optional[Callbacks] = None,\\n    ) -> Sequence[Document]:\\n        \"\"\"\\n        Compress documents using Cohere\\'s rerank API.\\n\\n        Args:\\n            documents: A sequence of documents to compress.\\n            query: The query to use for compressing the documents.\\n            callbacks: Callbacks to run during the compression process.\\n\\n        Returns:\\n            A sequence of compressed documents.\\n        \"\"\"\\n        if len(documents) == 0:  # to avoid empty api call\\n            return []\\n        doc_list = list(documents)\\n        _docs = [d.page_content for d in doc_list]\\n        results = self.client.rerank(\\n            model=self.model, query=query, documents=_docs, top_n=self.top_n\\n        )\\n        final_results = []\\n        for r in results:\\n            doc = doc_list[r.index]\\n            doc.metadata[\"relevance_score\"] = r.relevance_score\\n            final_results.append(doc)\\n        return final_results' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\document_compressors\\\\cohere_rerank.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from typing import Callable, Dict, Optional, Sequence\\n\\nimport numpy as np\\nfrom langchain_community.document_transformers.embeddings_redundant_filter import (\\n    _get_embeddings_from_stateful_docs,\\n    get_stateful_documents,\\n)\\nfrom langchain_core.documents import Document\\nfrom langchain_core.embeddings import Embeddings\\nfrom langchain_core.pydantic_v1 import root_validator\\n\\nfrom langchain.callbacks.manager import Callbacks\\nfrom langchain.retrievers.document_compressors.base import (\\n    BaseDocumentCompressor,\\n)\\nfrom langchain.utils.math import cosine_similarity\\n\\n\\nclass EmbeddingsFilter(BaseDocumentCompressor):\\n    \"\"\"Document compressor that uses embeddings to drop documents\\n    unrelated to the query.\"\"\"\\n\\n    embeddings: Embeddings\\n    \"\"\"Embeddings to use for embedding document contents and queries.\"\"\"\\n    similarity_fn: Callable = cosine_similarity\\n    \"\"\"Similarity function for comparing documents. Function expected to take as input\\n    two matrices (List[List[float]]) and return a matrix of scores where higher values\\n    indicate greater similarity.\"\"\"\\n    k: Optional[int] = 20\\n    \"\"\"The number of relevant documents to return. Can be set to None, in which case\\n    `similarity_threshold` must be specified. Defaults to 20.\"\"\"\\n    similarity_threshold: Optional[float]\\n    \"\"\"Threshold for determining when two documents are similar enough\\n    to be considered redundant. Defaults to None, must be specified if `k` is set\\n    to None.\"\"\"\\n\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n\\n        arbitrary_types_allowed = True\\n\\n    @root_validator()\\n    def validate_params(cls, values: Dict) -> Dict:\\n        \"\"\"Validate similarity parameters.\"\"\"\\n        if values[\"k\"] is None and values[\"similarity_threshold\"] is None:\\n            raise ValueError(\"Must specify one of `k` or `similarity_threshold`.\")\\n        return values' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\document_compressors\\\\embeddings_filter.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def compress_documents(\\n        self,\\n        documents: Sequence[Document],\\n        query: str,\\n        callbacks: Optional[Callbacks] = None,\\n    ) -> Sequence[Document]:\\n        \"\"\"Filter documents based on similarity of their embeddings to the query.\"\"\"\\n        stateful_documents = get_stateful_documents(documents)\\n        embedded_documents = _get_embeddings_from_stateful_docs(\\n            self.embeddings, stateful_documents\\n        )\\n        embedded_query = self.embeddings.embed_query(query)\\n        similarity = self.similarity_fn([embedded_query], embedded_documents)[0]\\n        included_idxs = np.arange(len(embedded_documents))\\n        if self.k is not None:\\n            included_idxs = np.argsort(similarity)[::-1][: self.k]\\n        if self.similarity_threshold is not None:\\n            similar_enough = np.where(\\n                similarity[included_idxs] > self.similarity_threshold\\n            )\\n            included_idxs = included_idxs[similar_enough]\\n        for i in included_idxs:\\n            stateful_documents[i].state[\"query_similarity_score\"] = similarity[i]\\n        return [stateful_documents[i] for i in included_idxs]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\document_compressors\\\\embeddings_filter.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain.retrievers.document_compressors.base import DocumentCompressorPipeline\\nfrom langchain.retrievers.document_compressors.chain_extract import (\\n    LLMChainExtractor,\\n)\\nfrom langchain.retrievers.document_compressors.chain_filter import (\\n    LLMChainFilter,\\n)\\nfrom langchain.retrievers.document_compressors.cohere_rerank import CohereRerank\\nfrom langchain.retrievers.document_compressors.embeddings_filter import (\\n    EmbeddingsFilter,\\n)\\n\\n__all__ = [\\n    \"DocumentCompressorPipeline\",\\n    \"EmbeddingsFilter\",\\n    \"LLMChainExtractor\",\\n    \"LLMChainFilter\",\\n    \"CohereRerank\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\document_compressors\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Logic for converting internal query language to a valid AstraDB query.\"\"\"\\nfrom typing import Dict, Tuple, Union\\n\\nfrom langchain.chains.query_constructor.ir import (\\n    Comparator,\\n    Comparison,\\n    Operation,\\n    Operator,\\n    StructuredQuery,\\n    Visitor,\\n)\\n\\nMULTIPLE_ARITY_COMPARATORS = [Comparator.IN, Comparator.NIN]\\n\\n\\nclass AstraDBTranslator(Visitor):\\n    \"\"\"Translate AstraDB internal query language elements to valid filters.\"\"\"\\n\\n    \"\"\"Subset of allowed logical comparators.\"\"\"\\n    allowed_comparators = [\\n        Comparator.EQ,\\n        Comparator.NE,\\n        Comparator.GT,\\n        Comparator.GTE,\\n        Comparator.LT,\\n        Comparator.LTE,\\n        Comparator.IN,\\n        Comparator.NIN,\\n    ]\\n\\n    \"\"\"Subset of allowed logical operators.\"\"\"\\n    allowed_operators = [Operator.AND, Operator.OR]\\n\\n    def _format_func(self, func: Union[Operator, Comparator]) -> str:\\n        self._validate_func(func)\\n        map_dict = {\\n            Operator.AND: \"$and\",\\n            Operator.OR: \"$or\",\\n            Comparator.EQ: \"$eq\",\\n            Comparator.NE: \"$ne\",\\n            Comparator.GTE: \"$gte\",\\n            Comparator.LTE: \"$lte\",\\n            Comparator.LT: \"$lt\",\\n            Comparator.GT: \"$gt\",\\n            Comparator.IN: \"$in\",\\n            Comparator.NIN: \"$nin\",\\n        }\\n        return map_dict[func]\\n\\n    def visit_operation(self, operation: Operation) -> Dict:\\n        args = [arg.accept(self) for arg in operation.arguments]\\n        return {self._format_func(operation.operator): args}\\n\\n    def visit_comparison(self, comparison: Comparison) -> Dict:\\n        if comparison.comparator in MULTIPLE_ARITY_COMPARATORS and not isinstance(\\n            comparison.value, list\\n        ):\\n            comparison.value = [comparison.value]\\n\\n        comparator = self._format_func(comparison.comparator)\\n        return {comparison.attribute: {comparator: comparison.value}}' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\self_query\\\\astradb.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def visit_structured_query(\\n        self, structured_query: StructuredQuery\\n    ) -> Tuple[str, dict]:\\n        if structured_query.filter is None:\\n            kwargs = {}\\n        else:\\n            kwargs = {\"filter\": structured_query.filter.accept(self)}\\n        return structured_query.query, kwargs' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\self_query\\\\astradb.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Retriever that generates and executes structured queries over its own data source.\"\"\"\\nimport logging\\nfrom typing import Any, Dict, List, Optional, Sequence, Tuple, Type, Union\\n\\nfrom langchain_community.vectorstores import (\\n    AstraDB,\\n    Chroma,\\n    DashVector,\\n    DeepLake,\\n    ElasticsearchStore,\\n    Milvus,\\n    MongoDBAtlasVectorSearch,\\n    MyScale,\\n    OpenSearchVectorSearch,\\n    Pinecone,\\n    Qdrant,\\n    Redis,\\n    SupabaseVectorStore,\\n    TimescaleVector,\\n    Vectara,\\n    Weaviate,\\n)\\nfrom langchain_core.documents import Document\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.pydantic_v1 import Field, root_validator\\nfrom langchain_core.retrievers import BaseRetriever\\nfrom langchain_core.runnables import Runnable\\nfrom langchain_core.vectorstores import VectorStore' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\self_query\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain.callbacks.manager import (\\n    AsyncCallbackManagerForRetrieverRun,\\n    CallbackManagerForRetrieverRun,\\n)\\nfrom langchain.chains.query_constructor.base import load_query_constructor_runnable\\nfrom langchain.chains.query_constructor.ir import StructuredQuery, Visitor\\nfrom langchain.chains.query_constructor.schema import AttributeInfo\\nfrom langchain.retrievers.self_query.astradb import AstraDBTranslator\\nfrom langchain.retrievers.self_query.chroma import ChromaTranslator\\nfrom langchain.retrievers.self_query.dashvector import DashvectorTranslator\\nfrom langchain.retrievers.self_query.deeplake import DeepLakeTranslator\\nfrom langchain.retrievers.self_query.elasticsearch import ElasticsearchTranslator\\nfrom langchain.retrievers.self_query.milvus import MilvusTranslator\\nfrom langchain.retrievers.self_query.mongodb_atlas import MongoDBAtlasTranslator\\nfrom langchain.retrievers.self_query.myscale import MyScaleTranslator\\nfrom langchain.retrievers.self_query.opensearch import OpenSearchTranslator\\nfrom langchain.retrievers.self_query.pinecone import PineconeTranslator\\nfrom langchain.retrievers.self_query.qdrant import QdrantTranslator\\nfrom langchain.retrievers.self_query.redis import RedisTranslator\\nfrom langchain.retrievers.self_query.supabase import SupabaseVectorTranslator\\nfrom langchain.retrievers.self_query.timescalevector import TimescaleVectorTranslator\\nfrom langchain.retrievers.self_query.vectara import VectaraTranslator\\nfrom langchain.retrievers.self_query.weaviate import WeaviateTranslator\\n\\nlogger = logging.getLogger(__name__)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\self_query\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_builtin_translator(vectorstore: VectorStore) -> Visitor:\\n    \"\"\"Get the translator class corresponding to the vector store class.\"\"\"\\n    BUILTIN_TRANSLATORS: Dict[Type[VectorStore], Type[Visitor]] = {\\n        AstraDB: AstraDBTranslator,\\n        Pinecone: PineconeTranslator,\\n        Chroma: ChromaTranslator,\\n        DashVector: DashvectorTranslator,\\n        Weaviate: WeaviateTranslator,\\n        Vectara: VectaraTranslator,\\n        Qdrant: QdrantTranslator,\\n        MyScale: MyScaleTranslator,\\n        DeepLake: DeepLakeTranslator,\\n        ElasticsearchStore: ElasticsearchTranslator,\\n        Milvus: MilvusTranslator,\\n        SupabaseVectorStore: SupabaseVectorTranslator,\\n        TimescaleVector: TimescaleVectorTranslator,\\n        OpenSearchVectorSearch: OpenSearchTranslator,\\n        MongoDBAtlasVectorSearch: MongoDBAtlasTranslator,\\n    }\\n    if isinstance(vectorstore, Qdrant):\\n        return QdrantTranslator(metadata_key=vectorstore.metadata_payload_key)\\n    elif isinstance(vectorstore, MyScale):\\n        return MyScaleTranslator(metadata_key=vectorstore.metadata_column)\\n    elif isinstance(vectorstore, Redis):\\n        return RedisTranslator.from_vectorstore(vectorstore)\\n    elif vectorstore.__class__ in BUILTIN_TRANSLATORS:\\n        return BUILTIN_TRANSLATORS[vectorstore.__class__]()\\n    else:\\n        raise ValueError(\\n            f\"Self query retriever with Vector Store type {vectorstore.__class__}\"\\n            f\" not supported.\"\\n        )\\n\\n\\nclass SelfQueryRetriever(BaseRetriever):\\n    \"\"\"Retriever that uses a vector store and an LLM to generate\\n    the vector store queries.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\self_query\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='vectorstore: VectorStore\\n    \"\"\"The underlying vector store from which documents will be retrieved.\"\"\"\\n    query_constructor: Runnable[dict, StructuredQuery] = Field(alias=\"llm_chain\")\\n    \"\"\"The query constructor chain for generating the vector store queries.\\n    \\n    llm_chain is legacy name kept for backwards compatibility.\"\"\"\\n    search_type: str = \"similarity\"\\n    \"\"\"The search type to perform on the vector store.\"\"\"\\n    search_kwargs: dict = Field(default_factory=dict)\\n    \"\"\"Keyword arguments to pass in to the vector store search.\"\"\"\\n    structured_query_translator: Visitor\\n    \"\"\"Translator for turning internal query language into vectorstore search params.\"\"\"\\n    verbose: bool = False\\n\\n    use_original_query: bool = False\\n    \"\"\"Use original query instead of the revised new query from LLM\"\"\"\\n\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n\\n        arbitrary_types_allowed = True\\n        allow_population_by_field_name = True\\n\\n    @root_validator(pre=True)\\n    def validate_translator(cls, values: Dict) -> Dict:\\n        \"\"\"Validate translator.\"\"\"\\n        if \"structured_query_translator\" not in values:\\n            values[\"structured_query_translator\"] = _get_builtin_translator(\\n                values[\"vectorstore\"]\\n            )\\n        return values\\n\\n    @property\\n    def llm_chain(self) -> Runnable:\\n        \"\"\"llm_chain is legacy name kept for backwards compatibility.\"\"\"\\n        return self.query_constructor\\n\\n    def _prepare_query(\\n        self, query: str, structured_query: StructuredQuery\\n    ) -> Tuple[str, Dict[str, Any]]:\\n        new_query, new_kwargs = self.structured_query_translator.visit_structured_query(\\n            structured_query\\n        )\\n        if structured_query.limit is not None:\\n            new_kwargs[\"k\"] = structured_query.limit\\n        if self.use_original_query:\\n            new_query = query\\n        search_kwargs = {**self.search_kwargs, **new_kwargs}\\n        return new_query, search_kwargs' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\self_query\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_docs_with_query(\\n        self, query: str, search_kwargs: Dict[str, Any]\\n    ) -> List[Document]:\\n        docs = self.vectorstore.search(query, self.search_type, **search_kwargs)\\n        return docs\\n\\n    async def _aget_docs_with_query(\\n        self, query: str, search_kwargs: Dict[str, Any]\\n    ) -> List[Document]:\\n        docs = await self.vectorstore.asearch(query, self.search_type, **search_kwargs)\\n        return docs\\n\\n    def _get_relevant_documents(\\n        self, query: str, *, run_manager: CallbackManagerForRetrieverRun\\n    ) -> List[Document]:\\n        \"\"\"Get documents relevant for a query.\\n\\n        Args:\\n            query: string to find relevant documents for\\n\\n        Returns:\\n            List of relevant documents\\n        \"\"\"\\n        structured_query = self.query_constructor.invoke(\\n            {\"query\": query}, config={\"callbacks\": run_manager.get_child()}\\n        )\\n        if self.verbose:\\n            logger.info(f\"Generated Query: {structured_query}\")\\n        new_query, search_kwargs = self._prepare_query(query, structured_query)\\n        docs = self._get_docs_with_query(new_query, search_kwargs)\\n        return docs\\n\\n    async def _aget_relevant_documents(\\n        self, query: str, *, run_manager: AsyncCallbackManagerForRetrieverRun\\n    ) -> List[Document]:\\n        \"\"\"Get documents relevant for a query.\\n\\n        Args:\\n            query: string to find relevant documents for\\n\\n        Returns:\\n            List of relevant documents\\n        \"\"\"\\n        structured_query = await self.query_constructor.ainvoke(\\n            {\"query\": query}, config={\"callbacks\": run_manager.get_child()}\\n        )\\n        if self.verbose:\\n            logger.info(f\"Generated Query: {structured_query}\")\\n        new_query, search_kwargs = self._prepare_query(query, structured_query)\\n        docs = await self._aget_docs_with_query(new_query, search_kwargs)\\n        return docs' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\self_query\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        vectorstore: VectorStore,\\n        document_contents: str,\\n        metadata_field_info: Sequence[Union[AttributeInfo, dict]],\\n        structured_query_translator: Optional[Visitor] = None,\\n        chain_kwargs: Optional[Dict] = None,\\n        enable_limit: bool = False,\\n        use_original_query: bool = False,\\n        **kwargs: Any,\\n    ) -> \"SelfQueryRetriever\":\\n        if structured_query_translator is None:\\n            structured_query_translator = _get_builtin_translator(vectorstore)\\n        chain_kwargs = chain_kwargs or {}\\n\\n        if (\\n            \"allowed_comparators\" not in chain_kwargs\\n            and structured_query_translator.allowed_comparators is not None\\n        ):\\n            chain_kwargs[\\n                \"allowed_comparators\"\\n            ] = structured_query_translator.allowed_comparators\\n        if (\\n            \"allowed_operators\" not in chain_kwargs\\n            and structured_query_translator.allowed_operators is not None\\n        ):\\n            chain_kwargs[\\n                \"allowed_operators\"\\n            ] = structured_query_translator.allowed_operators\\n        query_constructor = load_query_constructor_runnable(\\n            llm,\\n            document_contents,\\n            metadata_field_info,\\n            enable_limit=enable_limit,\\n            **chain_kwargs,\\n        )\\n        return cls(\\n            query_constructor=query_constructor,\\n            vectorstore=vectorstore,\\n            use_original_query=use_original_query,\\n            structured_query_translator=structured_query_translator,\\n            **kwargs,\\n        )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\self_query\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from typing import Dict, Tuple, Union\\n\\nfrom langchain.chains.query_constructor.ir import (\\n    Comparator,\\n    Comparison,\\n    Operation,\\n    Operator,\\n    StructuredQuery,\\n    Visitor,\\n)\\n\\n\\nclass ChromaTranslator(Visitor):\\n    \"\"\"Translate `Chroma` internal query language elements to valid filters.\"\"\"\\n\\n    allowed_operators = [Operator.AND, Operator.OR]\\n    \"\"\"Subset of allowed logical operators.\"\"\"\\n    allowed_comparators = [\\n        Comparator.EQ,\\n        Comparator.NE,\\n        Comparator.GT,\\n        Comparator.GTE,\\n        Comparator.LT,\\n        Comparator.LTE,\\n    ]\\n    \"\"\"Subset of allowed logical comparators.\"\"\"\\n\\n    def _format_func(self, func: Union[Operator, Comparator]) -> str:\\n        self._validate_func(func)\\n        return f\"${func.value}\"\\n\\n    def visit_operation(self, operation: Operation) -> Dict:\\n        args = [arg.accept(self) for arg in operation.arguments]\\n        return {self._format_func(operation.operator): args}\\n\\n    def visit_comparison(self, comparison: Comparison) -> Dict:\\n        return {\\n            comparison.attribute: {\\n                self._format_func(comparison.comparator): comparison.value\\n            }\\n        }\\n\\n    def visit_structured_query(\\n        self, structured_query: StructuredQuery\\n    ) -> Tuple[str, dict]:\\n        if structured_query.filter is None:\\n            kwargs = {}\\n        else:\\n            kwargs = {\"filter\": structured_query.filter.accept(self)}\\n        return structured_query.query, kwargs' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\self_query\\\\chroma.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Logic for converting internal query language to a valid DashVector query.\"\"\"\\nfrom typing import Tuple, Union\\n\\nfrom langchain.chains.query_constructor.ir import (\\n    Comparator,\\n    Comparison,\\n    Operation,\\n    Operator,\\n    StructuredQuery,\\n    Visitor,\\n)\\n\\n\\nclass DashvectorTranslator(Visitor):\\n    \"\"\"Logic for converting internal query language elements to valid filters.\"\"\"\\n\\n    allowed_operators = [Operator.AND, Operator.OR]\\n    allowed_comparators = [\\n        Comparator.EQ,\\n        Comparator.GT,\\n        Comparator.GTE,\\n        Comparator.LT,\\n        Comparator.LTE,\\n        Comparator.LIKE,\\n    ]\\n\\n    map_dict = {\\n        Operator.AND: \" AND \",\\n        Operator.OR: \" OR \",\\n        Comparator.EQ: \" = \",\\n        Comparator.GT: \" > \",\\n        Comparator.GTE: \" >= \",\\n        Comparator.LT: \" < \",\\n        Comparator.LTE: \" <= \",\\n        Comparator.LIKE: \" LIKE \",\\n    }\\n\\n    def _format_func(self, func: Union[Operator, Comparator]) -> str:\\n        self._validate_func(func)\\n        return self.map_dict[func]\\n\\n    def visit_operation(self, operation: Operation) -> str:\\n        args = [arg.accept(self) for arg in operation.arguments]\\n        return self._format_func(operation.operator).join(args)\\n\\n    def visit_comparison(self, comparison: Comparison) -> str:\\n        value = comparison.value\\n        if isinstance(value, str):\\n            if comparison.comparator == Comparator.LIKE:\\n                value = f\"\\'%{value}%\\'\"\\n            else:\\n                value = f\"\\'{value}\\'\"\\n        return (\\n            f\"{comparison.attribute}{self._format_func(comparison.comparator)}{value}\"\\n        )\\n\\n    def visit_structured_query(\\n        self, structured_query: StructuredQuery\\n    ) -> Tuple[str, dict]:\\n        if structured_query.filter is None:\\n            kwargs = {}\\n        else:\\n            kwargs = {\"filter\": structured_query.filter.accept(self)}\\n        return structured_query.query, kwargs' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\self_query\\\\dashvector.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Logic for converting internal query language to a valid Chroma query.\"\"\"\\nfrom typing import Tuple, Union\\n\\nfrom langchain.chains.query_constructor.ir import (\\n    Comparator,\\n    Comparison,\\n    Operation,\\n    Operator,\\n    StructuredQuery,\\n    Visitor,\\n)\\n\\nCOMPARATOR_TO_TQL = {\\n    Comparator.EQ: \"==\",\\n    Comparator.GT: \">\",\\n    Comparator.GTE: \">=\",\\n    Comparator.LT: \"<\",\\n    Comparator.LTE: \"<=\",\\n}\\n\\n\\nOPERATOR_TO_TQL = {\\n    Operator.AND: \"and\",\\n    Operator.OR: \"or\",\\n    Operator.NOT: \"NOT\",\\n}\\n\\n\\ndef can_cast_to_float(string: str) -> bool:\\n    \"\"\"Check if a string can be cast to a float.\"\"\"\\n    try:\\n        float(string)\\n        return True\\n    except ValueError:\\n        return False\\n\\n\\nclass DeepLakeTranslator(Visitor):\\n    \"\"\"Translate `DeepLake` internal query language elements to valid filters.\"\"\"\\n\\n    allowed_operators = [Operator.AND, Operator.OR, Operator.NOT]\\n    \"\"\"Subset of allowed logical operators.\"\"\"\\n    allowed_comparators = [\\n        Comparator.EQ,\\n        Comparator.GT,\\n        Comparator.GTE,\\n        Comparator.LT,\\n        Comparator.LTE,\\n    ]\\n    \"\"\"Subset of allowed logical comparators.\"\"\"\\n\\n    def _format_func(self, func: Union[Operator, Comparator]) -> str:\\n        self._validate_func(func)\\n        if isinstance(func, Operator):\\n            value = OPERATOR_TO_TQL[func.value]  # type: ignore\\n        elif isinstance(func, Comparator):\\n            value = COMPARATOR_TO_TQL[func.value]  # type: ignore\\n        return f\"{value}\"\\n\\n    def visit_operation(self, operation: Operation) -> str:\\n        args = [arg.accept(self) for arg in operation.arguments]\\n        operator = self._format_func(operation.operator)\\n        return \"(\" + (\" \" + operator + \" \").join(args) + \")\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\self_query\\\\deeplake.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def visit_comparison(self, comparison: Comparison) -> str:\\n        comparator = self._format_func(comparison.comparator)\\n        values = comparison.value\\n        if isinstance(values, list):\\n            tql = []\\n            for value in values:\\n                comparison.value = value\\n                tql.append(self.visit_comparison(comparison))\\n\\n            return \"(\" + (\" or \").join(tql) + \")\"\\n\\n        if not can_cast_to_float(comparison.value):\\n            values = f\"\\'{values}\\'\"\\n        return f\"metadata[\\'{comparison.attribute}\\'] {comparator} {values}\"\\n\\n    def visit_structured_query(\\n        self, structured_query: StructuredQuery\\n    ) -> Tuple[str, dict]:\\n        if structured_query.filter is None:\\n            kwargs = {}\\n        else:\\n            tqL = f\"SELECT * WHERE {structured_query.filter.accept(self)}\"\\n            kwargs = {\"tql\": tqL}\\n        return structured_query.query, kwargs' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\self_query\\\\deeplake.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from typing import Dict, Tuple, Union\\n\\nfrom langchain.chains.query_constructor.ir import (\\n    Comparator,\\n    Comparison,\\n    Operation,\\n    Operator,\\n    StructuredQuery,\\n    Visitor,\\n)\\n\\n\\nclass ElasticsearchTranslator(Visitor):\\n    \"\"\"Translate `Elasticsearch` internal query language elements to valid filters.\"\"\"\\n\\n    allowed_comparators = [\\n        Comparator.EQ,\\n        Comparator.GT,\\n        Comparator.GTE,\\n        Comparator.LT,\\n        Comparator.LTE,\\n        Comparator.CONTAIN,\\n        Comparator.LIKE,\\n    ]\\n    \"\"\"Subset of allowed logical comparators.\"\"\"\\n\\n    allowed_operators = [Operator.AND, Operator.OR, Operator.NOT]\\n    \"\"\"Subset of allowed logical operators.\"\"\"\\n\\n    def _format_func(self, func: Union[Operator, Comparator]) -> str:\\n        self._validate_func(func)\\n        map_dict = {\\n            Operator.OR: \"should\",\\n            Operator.NOT: \"must_not\",\\n            Operator.AND: \"must\",\\n            Comparator.EQ: \"term\",\\n            Comparator.GT: \"gt\",\\n            Comparator.GTE: \"gte\",\\n            Comparator.LT: \"lt\",\\n            Comparator.LTE: \"lte\",\\n            Comparator.CONTAIN: \"match\",\\n            Comparator.LIKE: \"match\",\\n        }\\n        return map_dict[func]\\n\\n    def visit_operation(self, operation: Operation) -> Dict:\\n        args = [arg.accept(self) for arg in operation.arguments]\\n\\n        return {\"bool\": {self._format_func(operation.operator): args}}\\n\\n    def visit_comparison(self, comparison: Comparison) -> Dict:\\n        # ElasticsearchStore filters require to target\\n        # the metadata object field\\n        field = f\"metadata.{comparison.attribute}\"\\n\\n        is_range_comparator = comparison.comparator in [\\n            Comparator.GT,\\n            Comparator.GTE,\\n            Comparator.LT,\\n            Comparator.LTE,\\n        ]\\n\\n        if is_range_comparator:\\n            return {\\n                \"range\": {\\n                    field: {self._format_func(comparison.comparator): comparison.value}\\n                }\\n            }' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\self_query\\\\elasticsearch.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='if comparison.comparator == Comparator.CONTAIN:\\n            return {\\n                self._format_func(comparison.comparator): {\\n                    field: {\"query\": comparison.value}\\n                }\\n            }\\n\\n        if comparison.comparator == Comparator.LIKE:\\n            return {\\n                self._format_func(comparison.comparator): {\\n                    field: {\"query\": comparison.value, \"fuzziness\": \"AUTO\"}\\n                }\\n            }\\n\\n        # we assume that if the value is a string,\\n        # we want to use the keyword field\\n        field = f\"{field}.keyword\" if isinstance(comparison.value, str) else field\\n\\n        return {self._format_func(comparison.comparator): {field: comparison.value}}\\n\\n    def visit_structured_query(\\n        self, structured_query: StructuredQuery\\n    ) -> Tuple[str, dict]:\\n        if structured_query.filter is None:\\n            kwargs = {}\\n        else:\\n            kwargs = {\"filter\": [structured_query.filter.accept(self)]}\\n        return structured_query.query, kwargs' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\self_query\\\\elasticsearch.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Logic for converting internal query language to a valid Milvus query.\"\"\"\\nfrom typing import Tuple, Union\\n\\nfrom langchain.chains.query_constructor.ir import (\\n    Comparator,\\n    Comparison,\\n    Operation,\\n    Operator,\\n    StructuredQuery,\\n    Visitor,\\n)\\n\\nCOMPARATOR_TO_BER = {\\n    Comparator.EQ: \"==\",\\n    Comparator.GT: \">\",\\n    Comparator.GTE: \">=\",\\n    Comparator.LT: \"<\",\\n    Comparator.LTE: \"<=\",\\n    Comparator.IN: \"in\",\\n    Comparator.LIKE: \"like\",\\n}\\n\\nUNARY_OPERATORS = [Operator.NOT]\\n\\n\\ndef process_value(value: Union[int, float, str], comparator: Comparator) -> str:\\n    \"\"\"Convert a value to a string and add double quotes if it is a string.\\n\\n    It required for comparators involving strings.\\n\\n    Args:\\n        value: The value to convert.\\n        comparator: The comparator.\\n\\n    Returns:\\n        The converted value as a string.\\n    \"\"\"\\n    #\\n    if isinstance(value, str):\\n        if comparator is Comparator.LIKE:\\n            # If the comparator is LIKE, add a percent sign after it for prefix matching\\n            # and add double quotes\\n            return f\\'\"{value}%\"\\'\\n        else:\\n            # If the value is already a string, add double quotes\\n            return f\\'\"{value}\"\\'\\n    else:\\n        # If the value is not a string, convert it to a string without double quotes\\n        return str(value)\\n\\n\\nclass MilvusTranslator(Visitor):\\n    \"\"\"Translate Milvus internal query language elements to valid filters.\"\"\"\\n\\n    \"\"\"Subset of allowed logical operators.\"\"\"\\n    allowed_operators = [Operator.AND, Operator.NOT, Operator.OR]\\n\\n    \"\"\"Subset of allowed logical comparators.\"\"\"\\n    allowed_comparators = [\\n        Comparator.EQ,\\n        Comparator.GT,\\n        Comparator.GTE,\\n        Comparator.LT,\\n        Comparator.LTE,\\n        Comparator.IN,\\n        Comparator.LIKE,\\n    ]\\n\\n    def _format_func(self, func: Union[Operator, Comparator]) -> str:\\n        self._validate_func(func)\\n        value = func.value\\n        if isinstance(func, Comparator):\\n            value = COMPARATOR_TO_BER[func]\\n        return f\"{value}\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\self_query\\\\milvus.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def visit_operation(self, operation: Operation) -> str:\\n        if operation.operator in UNARY_OPERATORS and len(operation.arguments) == 1:\\n            operator = self._format_func(operation.operator)\\n            return operator + \"(\" + operation.arguments[0].accept(self) + \")\"\\n        elif operation.operator in UNARY_OPERATORS:\\n            raise ValueError(\\n                f\\'\"{operation.operator.value}\" can have only one argument in Milvus\\'\\n            )\\n        else:\\n            args = [arg.accept(self) for arg in operation.arguments]\\n            operator = self._format_func(operation.operator)\\n            return \"(\" + (\" \" + operator + \" \").join(args) + \")\"\\n\\n    def visit_comparison(self, comparison: Comparison) -> str:\\n        comparator = self._format_func(comparison.comparator)\\n        processed_value = process_value(comparison.value, comparison.comparator)\\n        attribute = comparison.attribute\\n\\n        return \"( \" + attribute + \" \" + comparator + \" \" + processed_value + \" )\"\\n\\n    def visit_structured_query(\\n        self, structured_query: StructuredQuery\\n    ) -> Tuple[str, dict]:\\n        if structured_query.filter is None:\\n            kwargs = {}\\n        else:\\n            kwargs = {\"expr\": structured_query.filter.accept(self)}\\n        return structured_query.query, kwargs' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\self_query\\\\milvus.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Logic for converting internal query language to a valid MongoDB Atlas query.\"\"\"\\nfrom typing import Dict, Tuple, Union\\n\\nfrom langchain.chains.query_constructor.ir import (\\n    Comparator,\\n    Comparison,\\n    Operation,\\n    Operator,\\n    StructuredQuery,\\n    Visitor,\\n)\\n\\nMULTIPLE_ARITY_COMPARATORS = [Comparator.IN, Comparator.NIN]\\n\\n\\nclass MongoDBAtlasTranslator(Visitor):\\n    \"\"\"Translate Mongo internal query language elements to valid filters.\"\"\"\\n\\n    \"\"\"Subset of allowed logical comparators.\"\"\"\\n    allowed_comparators = [\\n        Comparator.EQ,\\n        Comparator.NE,\\n        Comparator.GT,\\n        Comparator.GTE,\\n        Comparator.LT,\\n        Comparator.LTE,\\n        Comparator.IN,\\n        Comparator.NIN,\\n    ]\\n\\n    \"\"\"Subset of allowed logical operators.\"\"\"\\n    allowed_operators = [Operator.AND, Operator.OR]\\n\\n    ## Convert a operator or a comparator to Mongo Query Format\\n    def _format_func(self, func: Union[Operator, Comparator]) -> str:\\n        self._validate_func(func)\\n        map_dict = {\\n            Operator.AND: \"$and\",\\n            Operator.OR: \"$or\",\\n            Comparator.EQ: \"$eq\",\\n            Comparator.NE: \"$ne\",\\n            Comparator.GTE: \"$gte\",\\n            Comparator.LTE: \"$lte\",\\n            Comparator.LT: \"$lt\",\\n            Comparator.GT: \"$gt\",\\n            Comparator.IN: \"$in\",\\n            Comparator.NIN: \"$nin\",\\n        }\\n        return map_dict[func]\\n\\n    def visit_operation(self, operation: Operation) -> Dict:\\n        args = [arg.accept(self) for arg in operation.arguments]\\n        return {self._format_func(operation.operator): args}\\n\\n    def visit_comparison(self, comparison: Comparison) -> Dict:\\n        if comparison.comparator in MULTIPLE_ARITY_COMPARATORS and not isinstance(\\n            comparison.value, list\\n        ):\\n            comparison.value = [comparison.value]\\n\\n        comparator = self._format_func(comparison.comparator)\\n\\n        attribute = comparison.attribute\\n\\n        return {attribute: {comparator: comparison.value}}' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\self_query\\\\mongodb_atlas.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def visit_structured_query(\\n        self, structured_query: StructuredQuery\\n    ) -> Tuple[str, dict]:\\n        if structured_query.filter is None:\\n            kwargs = {}\\n        else:\\n            kwargs = {\"pre_filter\": structured_query.filter.accept(self)}\\n        return structured_query.query, kwargs' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\self_query\\\\mongodb_atlas.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='import re\\nfrom typing import Any, Callable, Dict, Tuple\\n\\nfrom langchain.chains.query_constructor.ir import (\\n    Comparator,\\n    Comparison,\\n    Operation,\\n    Operator,\\n    StructuredQuery,\\n    Visitor,\\n)\\n\\n\\ndef _DEFAULT_COMPOSER(op_name: str) -> Callable:\\n    \"\"\"\\n    Default composer for logical operators.\\n\\n    Args:\\n        op_name: Name of the operator.\\n\\n    Returns:\\n        Callable that takes a list of arguments and returns a string.\\n    \"\"\"\\n\\n    def f(*args: Any) -> str:\\n        args_: map[str] = map(str, args)\\n        return f\" {op_name} \".join(args_)\\n\\n    return f\\n\\n\\ndef _FUNCTION_COMPOSER(op_name: str) -> Callable:\\n    \"\"\"\\n    Composer for functions.\\n\\n    Args:\\n        op_name: Name of the function.\\n\\n    Returns:\\n        Callable that takes a list of arguments and returns a string.\\n    \"\"\"\\n\\n    def f(*args: Any) -> str:\\n        args_: map[str] = map(str, args)\\n        return f\"{op_name}({\\',\\'.join(args_)})\"\\n\\n    return f\\n\\n\\nclass MyScaleTranslator(Visitor):\\n    \"\"\"Translate `MyScale` internal query language elements to valid filters.\"\"\"\\n\\n    allowed_operators = [Operator.AND, Operator.OR, Operator.NOT]\\n    \"\"\"Subset of allowed logical operators.\"\"\"\\n\\n    allowed_comparators = [\\n        Comparator.EQ,\\n        Comparator.GT,\\n        Comparator.GTE,\\n        Comparator.LT,\\n        Comparator.LTE,\\n        Comparator.CONTAIN,\\n        Comparator.LIKE,\\n    ]\\n\\n    map_dict = {\\n        Operator.AND: _DEFAULT_COMPOSER(\"AND\"),\\n        Operator.OR: _DEFAULT_COMPOSER(\"OR\"),\\n        Operator.NOT: _DEFAULT_COMPOSER(\"NOT\"),\\n        Comparator.EQ: _DEFAULT_COMPOSER(\"=\"),\\n        Comparator.GT: _DEFAULT_COMPOSER(\">\"),\\n        Comparator.GTE: _DEFAULT_COMPOSER(\">=\"),\\n        Comparator.LT: _DEFAULT_COMPOSER(\"<\"),\\n        Comparator.LTE: _DEFAULT_COMPOSER(\"<=\"),\\n        Comparator.CONTAIN: _FUNCTION_COMPOSER(\"has\"),\\n        Comparator.LIKE: _DEFAULT_COMPOSER(\"ILIKE\"),\\n    }\\n\\n    def __init__(self, metadata_key: str = \"metadata\") -> None:\\n        super().__init__()\\n        self.metadata_key = metadata_key' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\self_query\\\\myscale.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def visit_operation(self, operation: Operation) -> Dict:\\n        args = [arg.accept(self) for arg in operation.arguments]\\n        func = operation.operator\\n        self._validate_func(func)\\n        return self.map_dict[func](*args)\\n\\n    def visit_comparison(self, comparison: Comparison) -> Dict:\\n        regex = r\"\\\\((.*?)\\\\)\"\\n        matched = re.search(r\"\\\\(\\\\w+\\\\)\", comparison.attribute)\\n\\n        # If arbitrary function is applied to an attribute\\n        if matched:\\n            attr = re.sub(\\n                regex,\\n                f\"({self.metadata_key}.{matched.group(0)[1:-1]})\",\\n                comparison.attribute,\\n            )\\n        else:\\n            attr = f\"{self.metadata_key}.{comparison.attribute}\"\\n        value = comparison.value\\n        comp = comparison.comparator\\n\\n        value = f\"\\'{value}\\'\" if isinstance(value, str) else value\\n\\n        # convert timestamp for datetime objects\\n        if isinstance(value, dict) and value.get(\"type\") == \"date\":\\n            attr = f\"parseDateTime32BestEffort({attr})\"\\n            value = f\"parseDateTime32BestEffort(\\'{value[\\'date\\']}\\')\"\\n\\n        # string pattern match\\n        if comp is Comparator.LIKE:\\n            value = f\"\\'%{value[1:-1]}%\\'\"\\n        return self.map_dict[comp](attr, value)\\n\\n    def visit_structured_query(\\n        self, structured_query: StructuredQuery\\n    ) -> Tuple[str, dict]:\\n        print(structured_query)\\n        if structured_query.filter is None:\\n            kwargs = {}\\n        else:\\n            kwargs = {\"where_str\": structured_query.filter.accept(self)}\\n        return structured_query.query, kwargs' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\self_query\\\\myscale.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from typing import Dict, Tuple, Union\\n\\nfrom langchain.chains.query_constructor.ir import (\\n    Comparator,\\n    Comparison,\\n    Operation,\\n    Operator,\\n    StructuredQuery,\\n    Visitor,\\n)\\n\\n\\nclass OpenSearchTranslator(Visitor):\\n    \"\"\"Translate `OpenSearch` internal query domain-specific\\n    language elements to valid filters.\"\"\"\\n\\n    allowed_comparators = [\\n        Comparator.EQ,\\n        Comparator.LT,\\n        Comparator.LTE,\\n        Comparator.GT,\\n        Comparator.GTE,\\n        Comparator.CONTAIN,\\n        Comparator.LIKE,\\n    ]\\n    \"\"\"Subset of allowed logical comparators.\"\"\"\\n\\n    allowed_operators = [Operator.AND, Operator.OR, Operator.NOT]\\n    \"\"\"Subset of allowed logical operators.\"\"\"\\n\\n    def _format_func(self, func: Union[Operator, Comparator]) -> str:\\n        self._validate_func(func)\\n        comp_operator_map = {\\n            Comparator.EQ: \"term\",\\n            Comparator.LT: \"lt\",\\n            Comparator.LTE: \"lte\",\\n            Comparator.GT: \"gt\",\\n            Comparator.GTE: \"gte\",\\n            Comparator.CONTAIN: \"match\",\\n            Comparator.LIKE: \"fuzzy\",\\n            Operator.AND: \"must\",\\n            Operator.OR: \"should\",\\n            Operator.NOT: \"must_not\",\\n        }\\n        return comp_operator_map[func]\\n\\n    def visit_operation(self, operation: Operation) -> Dict:\\n        args = [arg.accept(self) for arg in operation.arguments]\\n\\n        return {\"bool\": {self._format_func(operation.operator): args}}\\n\\n    def visit_comparison(self, comparison: Comparison) -> Dict:\\n        field = f\"metadata.{comparison.attribute}\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\self_query\\\\opensearch.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='if comparison.comparator in [\\n            Comparator.LT,\\n            Comparator.LTE,\\n            Comparator.GT,\\n            Comparator.GTE,\\n        ]:\\n            if isinstance(comparison.value, dict):\\n                if \"date\" in comparison.value:\\n                    return {\\n                        \"range\": {\\n                            field: {\\n                                self._format_func(\\n                                    comparison.comparator\\n                                ): comparison.value[\"date\"]\\n                            }\\n                        }\\n                    }\\n            else:\\n                return {\\n                    \"range\": {\\n                        field: {\\n                            self._format_func(comparison.comparator): comparison.value\\n                        }\\n                    }\\n                }\\n\\n        if comparison.comparator == Comparator.LIKE:\\n            return {\\n                self._format_func(comparison.comparator): {\\n                    field: {\"value\": comparison.value}\\n                }\\n            }\\n\\n        field = f\"{field}.keyword\" if isinstance(comparison.value, str) else field\\n\\n        if isinstance(comparison.value, dict):\\n            if \"date\" in comparison.value:\\n                comparison.value = comparison.value[\"date\"]\\n\\n        return {self._format_func(comparison.comparator): {field: comparison.value}}\\n\\n    def visit_structured_query(\\n        self, structured_query: StructuredQuery\\n    ) -> Tuple[str, dict]:\\n        if structured_query.filter is None:\\n            kwargs = {}\\n        else:\\n            kwargs = {\"filter\": structured_query.filter.accept(self)}\\n\\n        return structured_query.query, kwargs' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\self_query\\\\opensearch.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from typing import Dict, Tuple, Union\\n\\nfrom langchain.chains.query_constructor.ir import (\\n    Comparator,\\n    Comparison,\\n    Operation,\\n    Operator,\\n    StructuredQuery,\\n    Visitor,\\n)\\n\\n\\nclass PineconeTranslator(Visitor):\\n    \"\"\"Translate `Pinecone` internal query language elements to valid filters.\"\"\"\\n\\n    allowed_comparators = (\\n        Comparator.EQ,\\n        Comparator.NE,\\n        Comparator.LT,\\n        Comparator.LTE,\\n        Comparator.GT,\\n        Comparator.GTE,\\n        Comparator.IN,\\n        Comparator.NIN,\\n    )\\n    \"\"\"Subset of allowed logical comparators.\"\"\"\\n    allowed_operators = (Operator.AND, Operator.OR)\\n    \"\"\"Subset of allowed logical operators.\"\"\"\\n\\n    def _format_func(self, func: Union[Operator, Comparator]) -> str:\\n        self._validate_func(func)\\n        return f\"${func.value}\"\\n\\n    def visit_operation(self, operation: Operation) -> Dict:\\n        args = [arg.accept(self) for arg in operation.arguments]\\n        return {self._format_func(operation.operator): args}\\n\\n    def visit_comparison(self, comparison: Comparison) -> Dict:\\n        if comparison.comparator in (Comparator.IN, Comparator.NIN) and not isinstance(\\n            comparison.value, list\\n        ):\\n            comparison.value = [comparison.value]\\n\\n        return {\\n            comparison.attribute: {\\n                self._format_func(comparison.comparator): comparison.value\\n            }\\n        }\\n\\n    def visit_structured_query(\\n        self, structured_query: StructuredQuery\\n    ) -> Tuple[str, dict]:\\n        if structured_query.filter is None:\\n            kwargs = {}\\n        else:\\n            kwargs = {\"filter\": structured_query.filter.accept(self)}\\n        return structured_query.query, kwargs' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\self_query\\\\pinecone.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from __future__ import annotations\\n\\nfrom typing import TYPE_CHECKING, Tuple\\n\\nfrom langchain.chains.query_constructor.ir import (\\n    Comparator,\\n    Comparison,\\n    Operation,\\n    Operator,\\n    StructuredQuery,\\n    Visitor,\\n)\\n\\nif TYPE_CHECKING:\\n    from qdrant_client.http import models as rest\\n\\n\\nclass QdrantTranslator(Visitor):\\n    \"\"\"Translate `Qdrant` internal query language elements to valid filters.\"\"\"\\n\\n    allowed_operators = (\\n        Operator.AND,\\n        Operator.OR,\\n        Operator.NOT,\\n    )\\n    \"\"\"Subset of allowed logical operators.\"\"\"\\n\\n    allowed_comparators = (\\n        Comparator.EQ,\\n        Comparator.LT,\\n        Comparator.LTE,\\n        Comparator.GT,\\n        Comparator.GTE,\\n    )\\n    \"\"\"Subset of allowed logical comparators.\"\"\"\\n\\n    def __init__(self, metadata_key: str):\\n        self.metadata_key = metadata_key\\n\\n    def visit_operation(self, operation: Operation) -> rest.Filter:\\n        try:\\n            from qdrant_client.http import models as rest\\n        except ImportError as e:\\n            raise ImportError(\\n                \"Cannot import qdrant_client. Please install with `pip install \"\\n                \"qdrant-client`.\"\\n            ) from e\\n\\n        args = [arg.accept(self) for arg in operation.arguments]\\n        operator = {\\n            Operator.AND: \"must\",\\n            Operator.OR: \"should\",\\n            Operator.NOT: \"must_not\",\\n        }[operation.operator]\\n        return rest.Filter(**{operator: args})\\n\\n    def visit_comparison(self, comparison: Comparison) -> rest.FieldCondition:\\n        try:\\n            from qdrant_client.http import models as rest\\n        except ImportError as e:\\n            raise ImportError(\\n                \"Cannot import qdrant_client. Please install with `pip install \"\\n                \"qdrant-client`.\"\\n            ) from e' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\self_query\\\\qdrant.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='self._validate_func(comparison.comparator)\\n        attribute = self.metadata_key + \".\" + comparison.attribute\\n        if comparison.comparator == Comparator.EQ:\\n            return rest.FieldCondition(\\n                key=attribute, match=rest.MatchValue(value=comparison.value)\\n            )\\n        kwargs = {comparison.comparator.value: comparison.value}\\n        return rest.FieldCondition(key=attribute, range=rest.Range(**kwargs))\\n\\n    def visit_structured_query(\\n        self, structured_query: StructuredQuery\\n    ) -> Tuple[str, dict]:\\n        try:\\n            from qdrant_client.http import models as rest\\n        except ImportError as e:\\n            raise ImportError(\\n                \"Cannot import qdrant_client. Please install with `pip install \"\\n                \"qdrant-client`.\"\\n            ) from e\\n\\n        if structured_query.filter is None:\\n            kwargs = {}\\n        else:\\n            filter = structured_query.filter.accept(self)\\n            if isinstance(filter, rest.FieldCondition):\\n                filter = rest.Filter(must=[filter])\\n            kwargs = {\"filter\": filter}\\n        return structured_query.query, kwargs' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\self_query\\\\qdrant.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from __future__ import annotations\\n\\nfrom typing import Any, Tuple\\n\\nfrom langchain_community.vectorstores.redis import Redis\\nfrom langchain_community.vectorstores.redis.filters import (\\n    RedisFilterExpression,\\n    RedisFilterField,\\n    RedisFilterOperator,\\n    RedisNum,\\n    RedisTag,\\n    RedisText,\\n)\\nfrom langchain_community.vectorstores.redis.schema import RedisModel\\n\\nfrom langchain.chains.query_constructor.ir import (\\n    Comparator,\\n    Comparison,\\n    Operation,\\n    Operator,\\n    StructuredQuery,\\n    Visitor,\\n)\\n\\n_COMPARATOR_TO_BUILTIN_METHOD = {\\n    Comparator.EQ: \"__eq__\",\\n    Comparator.NE: \"__ne__\",\\n    Comparator.LT: \"__lt__\",\\n    Comparator.GT: \"__gt__\",\\n    Comparator.LTE: \"__le__\",\\n    Comparator.GTE: \"__ge__\",\\n    Comparator.CONTAIN: \"__eq__\",\\n    Comparator.LIKE: \"__mod__\",\\n}\\n\\n\\nclass RedisTranslator(Visitor):\\n    \"\"\"Visitor for translating structured queries to Redis filter expressions.\"\"\"\\n\\n    allowed_comparators = (\\n        Comparator.EQ,\\n        Comparator.NE,\\n        Comparator.LT,\\n        Comparator.LTE,\\n        Comparator.GT,\\n        Comparator.GTE,\\n        Comparator.CONTAIN,\\n        Comparator.LIKE,\\n    )\\n    \"\"\"Subset of allowed logical comparators.\"\"\"\\n    allowed_operators = (Operator.AND, Operator.OR)\\n    \"\"\"Subset of allowed logical operators.\"\"\"\\n\\n    def __init__(self, schema: RedisModel) -> None:\\n        self._schema = schema\\n\\n    def _attribute_to_filter_field(self, attribute: str) -> RedisFilterField:\\n        if attribute in [tf.name for tf in self._schema.text]:\\n            return RedisText(attribute)\\n        elif attribute in [tf.name for tf in self._schema.tag or []]:\\n            return RedisTag(attribute)\\n        elif attribute in [tf.name for tf in self._schema.numeric or []]:\\n            return RedisNum(attribute)\\n        else:\\n            raise ValueError(\\n                f\"Invalid attribute {attribute} not in vector store schema. Schema is:\"\\n                f\"\\\\n{self._schema.as_dict()}\"\\n            )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\self_query\\\\redis.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def visit_comparison(self, comparison: Comparison) -> RedisFilterExpression:\\n        filter_field = self._attribute_to_filter_field(comparison.attribute)\\n        comparison_method = _COMPARATOR_TO_BUILTIN_METHOD[comparison.comparator]\\n        return getattr(filter_field, comparison_method)(comparison.value)\\n\\n    def visit_operation(self, operation: Operation) -> Any:\\n        left = operation.arguments[0].accept(self)\\n        if len(operation.arguments) > 2:\\n            right = self.visit_operation(\\n                Operation(\\n                    operator=operation.operator, arguments=operation.arguments[1:]\\n                )\\n            )\\n        else:\\n            right = operation.arguments[1].accept(self)\\n        redis_operator = (\\n            RedisFilterOperator.OR\\n            if operation.operator == Operator.OR\\n            else RedisFilterOperator.AND\\n        )\\n        return RedisFilterExpression(operator=redis_operator, left=left, right=right)\\n\\n    def visit_structured_query(\\n        self, structured_query: StructuredQuery\\n    ) -> Tuple[str, dict]:\\n        if structured_query.filter is None:\\n            kwargs = {}\\n        else:\\n            kwargs = {\"filter\": structured_query.filter.accept(self)}\\n        return structured_query.query, kwargs\\n\\n    @classmethod\\n    def from_vectorstore(cls, vectorstore: Redis) -> RedisTranslator:\\n        return cls(vectorstore._schema)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\self_query\\\\redis.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from typing import Any, Dict, Tuple\\n\\nfrom langchain.chains.query_constructor.ir import (\\n    Comparator,\\n    Comparison,\\n    Operation,\\n    Operator,\\n    StructuredQuery,\\n    Visitor,\\n)\\n\\n\\nclass SupabaseVectorTranslator(Visitor):\\n    \"\"\"Translate Langchain filters to Supabase PostgREST filters.\"\"\"\\n\\n    allowed_operators = [Operator.AND, Operator.OR]\\n    \"\"\"Subset of allowed logical operators.\"\"\"\\n\\n    allowed_comparators = [\\n        Comparator.EQ,\\n        Comparator.NE,\\n        Comparator.GT,\\n        Comparator.GTE,\\n        Comparator.LT,\\n        Comparator.LTE,\\n        Comparator.LIKE,\\n    ]\\n    \"\"\"Subset of allowed logical comparators.\"\"\"\\n\\n    metadata_column = \"metadata\"\\n\\n    def _map_comparator(self, comparator: Comparator) -> str:\\n        \"\"\"\\n        Maps Langchain comparator to PostgREST comparator:\\n\\n        https://postgrest.org/en/stable/references/api/tables_views.html#operators\\n        \"\"\"\\n        postgrest_comparator = {\\n            Comparator.EQ: \"eq\",\\n            Comparator.NE: \"neq\",\\n            Comparator.GT: \"gt\",\\n            Comparator.GTE: \"gte\",\\n            Comparator.LT: \"lt\",\\n            Comparator.LTE: \"lte\",\\n            Comparator.LIKE: \"like\",\\n        }.get(comparator)\\n\\n        if postgrest_comparator is None:\\n            raise Exception(\\n                f\"Comparator \\'{comparator}\\' is not currently \"\\n                \"supported in Supabase Vector\"\\n            )\\n\\n        return postgrest_comparator\\n\\n    def _get_json_operator(self, value: Any) -> str:\\n        if isinstance(value, str):\\n            return \"->>\"\\n        else:\\n            return \"->\"\\n\\n    def visit_operation(self, operation: Operation) -> str:\\n        args = [arg.accept(self) for arg in operation.arguments]\\n        return f\"{operation.operator.value}({\\',\\'.join(args)})\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\self_query\\\\supabase.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def visit_comparison(self, comparison: Comparison) -> str:\\n        if isinstance(comparison.value, list):\\n            return self.visit_operation(\\n                Operation(\\n                    operator=Operator.AND,\\n                    arguments=(\\n                        Comparison(\\n                            comparator=comparison.comparator,\\n                            attribute=comparison.attribute,\\n                            value=value,\\n                        )\\n                        for value in comparison.value\\n                    ),\\n                )\\n            )\\n\\n        return \".\".join(\\n            [\\n                f\"{self.metadata_column}{self._get_json_operator(comparison.value)}{comparison.attribute}\",\\n                f\"{self._map_comparator(comparison.comparator)}\",\\n                f\"{comparison.value}\",\\n            ]\\n        )\\n\\n    def visit_structured_query(\\n        self, structured_query: StructuredQuery\\n    ) -> Tuple[str, Dict[str, str]]:\\n        if structured_query.filter is None:\\n            kwargs = {}\\n        else:\\n            kwargs = {\"postgrest_filter\": structured_query.filter.accept(self)}\\n        return structured_query.query, kwargs' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\self_query\\\\supabase.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from __future__ import annotations\\n\\nfrom typing import TYPE_CHECKING, Tuple, Union\\n\\nfrom langchain.chains.query_constructor.ir import (\\n    Comparator,\\n    Comparison,\\n    Operation,\\n    Operator,\\n    StructuredQuery,\\n    Visitor,\\n)\\n\\nif TYPE_CHECKING:\\n    from timescale_vector import client\\n\\n\\nclass TimescaleVectorTranslator(Visitor):\\n    \"\"\"Translate the internal query language elements to valid filters.\"\"\"\\n\\n    allowed_operators = [Operator.AND, Operator.OR, Operator.NOT]\\n    \"\"\"Subset of allowed logical operators.\"\"\"\\n\\n    allowed_comparators = [\\n        Comparator.EQ,\\n        Comparator.GT,\\n        Comparator.GTE,\\n        Comparator.LT,\\n        Comparator.LTE,\\n    ]\\n\\n    COMPARATOR_MAP = {\\n        Comparator.EQ: \"==\",\\n        Comparator.GT: \">\",\\n        Comparator.GTE: \">=\",\\n        Comparator.LT: \"<\",\\n        Comparator.LTE: \"<=\",\\n    }\\n\\n    OPERATOR_MAP = {Operator.AND: \"AND\", Operator.OR: \"OR\", Operator.NOT: \"NOT\"}\\n\\n    def _format_func(self, func: Union[Operator, Comparator]) -> str:\\n        self._validate_func(func)\\n        if isinstance(func, Operator):\\n            value = self.OPERATOR_MAP[func.value]  # type: ignore\\n        elif isinstance(func, Comparator):\\n            value = self.COMPARATOR_MAP[func.value]  # type: ignore\\n        return f\"{value}\"\\n\\n    def visit_operation(self, operation: Operation) -> client.Predicates:\\n        try:\\n            from timescale_vector import client\\n        except ImportError as e:\\n            raise ImportError(\\n                \"Cannot import timescale-vector. Please install with `pip install \"\\n                \"timescale-vector`.\"\\n            ) from e\\n        args = [arg.accept(self) for arg in operation.arguments]\\n        return client.Predicates(*args, operator=self._format_func(operation.operator))' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\self_query\\\\timescalevector.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def visit_comparison(self, comparison: Comparison) -> client.Predicates:\\n        try:\\n            from timescale_vector import client\\n        except ImportError as e:\\n            raise ImportError(\\n                \"Cannot import timescale-vector. Please install with `pip install \"\\n                \"timescale-vector`.\"\\n            ) from e\\n        return client.Predicates(\\n            (\\n                comparison.attribute,\\n                self._format_func(comparison.comparator),\\n                comparison.value,\\n            )\\n        )\\n\\n    def visit_structured_query(\\n        self, structured_query: StructuredQuery\\n    ) -> Tuple[str, dict]:\\n        if structured_query.filter is None:\\n            kwargs = {}\\n        else:\\n            kwargs = {\"predicates\": structured_query.filter.accept(self)}\\n        return structured_query.query, kwargs' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\self_query\\\\timescalevector.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from typing import Tuple, Union\\n\\nfrom langchain.chains.query_constructor.ir import (\\n    Comparator,\\n    Comparison,\\n    Operation,\\n    Operator,\\n    StructuredQuery,\\n    Visitor,\\n)\\n\\n\\ndef process_value(value: Union[int, float, str]) -> str:\\n    \"\"\"Convert a value to a string and add single quotes if it is a string.\"\"\"\\n    if isinstance(value, str):\\n        return f\"\\'{value}\\'\"\\n    else:\\n        return str(value)\\n\\n\\nclass VectaraTranslator(Visitor):\\n    \"\"\"Translate `Vectara` internal query language elements to valid filters.\"\"\"\\n\\n    allowed_operators = [Operator.AND, Operator.OR]\\n    \"\"\"Subset of allowed logical operators.\"\"\"\\n    allowed_comparators = [\\n        Comparator.EQ,\\n        Comparator.NE,\\n        Comparator.GT,\\n        Comparator.GTE,\\n        Comparator.LT,\\n        Comparator.LTE,\\n    ]\\n    \"\"\"Subset of allowed logical comparators.\"\"\"\\n\\n    def _format_func(self, func: Union[Operator, Comparator]) -> str:\\n        map_dict = {\\n            Operator.AND: \" and \",\\n            Operator.OR: \" or \",\\n            Comparator.EQ: \"=\",\\n            Comparator.NE: \"!=\",\\n            Comparator.GT: \">\",\\n            Comparator.GTE: \">=\",\\n            Comparator.LT: \"<\",\\n            Comparator.LTE: \"<=\",\\n        }\\n        self._validate_func(func)\\n        return map_dict[func]\\n\\n    def visit_operation(self, operation: Operation) -> str:\\n        args = [arg.accept(self) for arg in operation.arguments]\\n        operator = self._format_func(operation.operator)\\n        return \"( \" + operator.join(args) + \" )\"\\n\\n    def visit_comparison(self, comparison: Comparison) -> str:\\n        comparator = self._format_func(comparison.comparator)\\n        processed_value = process_value(comparison.value)\\n        attribute = comparison.attribute\\n        return (\\n            \"( \" + \"doc.\" + attribute + \" \" + comparator + \" \" + processed_value + \" )\"\\n        )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\self_query\\\\vectara.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def visit_structured_query(\\n        self, structured_query: StructuredQuery\\n    ) -> Tuple[str, dict]:\\n        if structured_query.filter is None:\\n            kwargs = {}\\n        else:\\n            kwargs = {\"filter\": structured_query.filter.accept(self)}\\n        return structured_query.query, kwargs' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\self_query\\\\vectara.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from datetime import datetime\\nfrom typing import Dict, Tuple, Union\\n\\nfrom langchain.chains.query_constructor.ir import (\\n    Comparator,\\n    Comparison,\\n    Operation,\\n    Operator,\\n    StructuredQuery,\\n    Visitor,\\n)\\n\\n\\nclass WeaviateTranslator(Visitor):\\n    \"\"\"Translate `Weaviate` internal query language elements to valid filters.\"\"\"\\n\\n    allowed_operators = [Operator.AND, Operator.OR]\\n    \"\"\"Subset of allowed logical operators.\"\"\"\\n\\n    allowed_comparators = [\\n        Comparator.EQ,\\n        Comparator.NE,\\n        Comparator.GTE,\\n        Comparator.LTE,\\n        Comparator.LT,\\n        Comparator.GT,\\n    ]\\n\\n    def _format_func(self, func: Union[Operator, Comparator]) -> str:\\n        self._validate_func(func)\\n        # https://weaviate.io/developers/weaviate/api/graphql/filters\\n        map_dict = {\\n            Operator.AND: \"And\",\\n            Operator.OR: \"Or\",\\n            Comparator.EQ: \"Equal\",\\n            Comparator.NE: \"NotEqual\",\\n            Comparator.GTE: \"GreaterThanEqual\",\\n            Comparator.LTE: \"LessThanEqual\",\\n            Comparator.LT: \"LessThan\",\\n            Comparator.GT: \"GreaterThan\",\\n        }\\n        return map_dict[func]\\n\\n    def visit_operation(self, operation: Operation) -> Dict:\\n        args = [arg.accept(self) for arg in operation.arguments]\\n        return {\"operator\": self._format_func(operation.operator), \"operands\": args}' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\self_query\\\\weaviate.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def visit_comparison(self, comparison: Comparison) -> Dict:\\n        value_type = \"valueText\"\\n        value = comparison.value\\n        if isinstance(comparison.value, bool):\\n            value_type = \"valueBoolean\"\\n        elif isinstance(comparison.value, float):\\n            value_type = \"valueNumber\"\\n        elif isinstance(comparison.value, int):\\n            value_type = \"valueInt\"\\n        elif (\\n            isinstance(comparison.value, dict)\\n            and comparison.value.get(\"type\") == \"date\"\\n        ):\\n            value_type = \"valueDate\"\\n            # ISO 8601 timestamp, formatted as RFC3339\\n            date = datetime.strptime(comparison.value[\"date\"], \"%Y-%m-%d\")\\n            value = date.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\\n        filter = {\\n            \"path\": [comparison.attribute],\\n            \"operator\": self._format_func(comparison.comparator),\\n            value_type: value,\\n        }\\n        return filter\\n\\n    def visit_structured_query(\\n        self, structured_query: StructuredQuery\\n    ) -> Tuple[str, dict]:\\n        if structured_query.filter is None:\\n            kwargs = {}\\n        else:\\n            kwargs = {\"where_filter\": structured_query.filter.accept(self)}\\n        return structured_query.query, kwargs' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\retrievers\\\\self_query\\\\weaviate.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from typing import Any, Optional\\n\\nfrom langchain_core.runnables.base import Input, Output, RunnableBindingBase\\n\\n\\nclass HubRunnable(RunnableBindingBase[Input, Output]):\\n    \"\"\"\\n    An instance of a runnable stored in the LangChain Hub.\\n    \"\"\"\\n\\n    owner_repo_commit: str\\n\\n    def __init__(\\n        self,\\n        owner_repo_commit: str,\\n        *,\\n        api_url: Optional[str] = None,\\n        api_key: Optional[str] = None,\\n        **kwargs: Any,\\n    ) -> None:\\n        from langchain.hub import pull\\n\\n        pulled = pull(owner_repo_commit, api_url=api_url, api_key=api_key)\\n        super_kwargs = {\\n            \"kwargs\": {},\\n            \"config\": {},\\n            **kwargs,\\n            \"bound\": pulled,\\n            \"owner_repo_commit\": owner_repo_commit,\\n        }\\n        super().__init__(**super_kwargs)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\runnables\\\\hub.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from operator import itemgetter\\nfrom typing import Any, Callable, List, Mapping, Optional, Union\\n\\nfrom langchain_core.messages import BaseMessage\\nfrom langchain_core.runnables import RouterRunnable, Runnable\\nfrom langchain_core.runnables.base import RunnableBindingBase\\nfrom typing_extensions import TypedDict\\n\\nfrom langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\\n\\n\\nclass OpenAIFunction(TypedDict):\\n    \"\"\"A function description for ChatOpenAI\"\"\"\\n\\n    name: str\\n    \"\"\"The name of the function.\"\"\"\\n    description: str\\n    \"\"\"The description of the function.\"\"\"\\n    parameters: dict\\n    \"\"\"The parameters to the function.\"\"\"\\n\\n\\nclass OpenAIFunctionsRouter(RunnableBindingBase[BaseMessage, Any]):\\n    \"\"\"A runnable that routes to the selected function.\"\"\"\\n\\n    functions: Optional[List[OpenAIFunction]]\\n\\n    def __init__(\\n        self,\\n        runnables: Mapping[\\n            str,\\n            Union[\\n                Runnable[dict, Any],\\n                Callable[[dict], Any],\\n            ],\\n        ],\\n        functions: Optional[List[OpenAIFunction]] = None,\\n    ):\\n        if functions is not None:\\n            assert len(functions) == len(runnables)\\n            assert all(func[\"name\"] in runnables for func in functions)\\n        router = (\\n            JsonOutputFunctionsParser(args_only=False)\\n            | {\"key\": itemgetter(\"name\"), \"input\": itemgetter(\"arguments\")}\\n            | RouterRunnable(runnables)\\n        )\\n        super().__init__(bound=router, kwargs={}, functions=functions)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\runnables\\\\openai_functions.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.agents import AgentAction, AgentActionMessageLog, AgentFinish\\n\\n__all__ = [\"AgentAction\", \"AgentActionMessageLog\", \"AgentFinish\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\schema\\\\agent.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.caches import RETURN_VAL_TYPE, BaseCache\\n\\n__all__ = [\"BaseCache\", \"RETURN_VAL_TYPE\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\schema\\\\cache.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.chat_sessions import ChatSession\\n\\n__all__ = [\"ChatSession\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\schema\\\\chat.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.chat_history import BaseChatMessageHistory\\n\\n__all__ = [\"BaseChatMessageHistory\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\schema\\\\chat_history.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.documents import BaseDocumentTransformer, Document\\n\\n__all__ = [\"Document\", \"BaseDocumentTransformer\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\schema\\\\document.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.embeddings import Embeddings\\n\\n__all__ = [\"Embeddings\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\schema\\\\embeddings.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.exceptions import LangChainException\\n\\n__all__ = [\"LangChainException\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\schema\\\\exceptions.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.language_models import (\\n    BaseLanguageModel,\\n    LanguageModelInput,\\n    LanguageModelOutput,\\n    get_tokenizer,\\n)\\nfrom langchain_core.language_models.base import _get_token_ids_default_method\\n\\n__all__ = [\\n    \"get_tokenizer\",\\n    \"BaseLanguageModel\",\\n    \"_get_token_ids_default_method\",\\n    \"LanguageModelInput\",\\n    \"LanguageModelOutput\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\schema\\\\language_model.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.memory import BaseMemory\\n\\n__all__ = [\"BaseMemory\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\schema\\\\memory.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.messages import (\\n    AIMessage,\\n    AIMessageChunk,\\n    AnyMessage,\\n    BaseMessage,\\n    BaseMessageChunk,\\n    ChatMessage,\\n    ChatMessageChunk,\\n    FunctionMessage,\\n    FunctionMessageChunk,\\n    HumanMessage,\\n    HumanMessageChunk,\\n    SystemMessage,\\n    SystemMessageChunk,\\n    ToolMessage,\\n    ToolMessageChunk,\\n    _message_from_dict,\\n    get_buffer_string,\\n    merge_content,\\n    message_to_dict,\\n    messages_from_dict,\\n    messages_to_dict,\\n)\\n\\n# Backwards compatibility.\\n_message_to_dict = message_to_dict\\n\\n__all__ = [\\n    \"get_buffer_string\",\\n    \"BaseMessage\",\\n    \"merge_content\",\\n    \"BaseMessageChunk\",\\n    \"HumanMessage\",\\n    \"HumanMessageChunk\",\\n    \"AIMessage\",\\n    \"AIMessageChunk\",\\n    \"SystemMessage\",\\n    \"SystemMessageChunk\",\\n    \"FunctionMessage\",\\n    \"FunctionMessageChunk\",\\n    \"ToolMessage\",\\n    \"ToolMessageChunk\",\\n    \"ChatMessage\",\\n    \"ChatMessageChunk\",\\n    \"messages_to_dict\",\\n    \"messages_from_dict\",\\n    \"_message_to_dict\",\\n    \"_message_from_dict\",\\n    \"message_to_dict\",\\n    \"AnyMessage\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\schema\\\\messages.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.outputs import (\\n    ChatGeneration,\\n    ChatGenerationChunk,\\n    ChatResult,\\n    Generation,\\n    GenerationChunk,\\n    LLMResult,\\n    RunInfo,\\n)\\n\\n__all__ = [\\n    \"Generation\",\\n    \"GenerationChunk\",\\n    \"ChatGeneration\",\\n    \"ChatGenerationChunk\",\\n    \"RunInfo\",\\n    \"ChatResult\",\\n    \"LLMResult\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\schema\\\\output.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.exceptions import OutputParserException\\nfrom langchain_core.output_parsers import (\\n    BaseCumulativeTransformOutputParser,\\n    BaseGenerationOutputParser,\\n    BaseLLMOutputParser,\\n    BaseOutputParser,\\n    BaseTransformOutputParser,\\n    StrOutputParser,\\n)\\nfrom langchain_core.output_parsers.base import T\\n\\n# Backwards compatibility.\\nNoOpOutputParser = StrOutputParser\\n\\n__all__ = [\\n    \"BaseLLMOutputParser\",\\n    \"BaseGenerationOutputParser\",\\n    \"BaseOutputParser\",\\n    \"BaseTransformOutputParser\",\\n    \"BaseCumulativeTransformOutputParser\",\\n    \"NoOpOutputParser\",\\n    \"StrOutputParser\",\\n    \"OutputParserException\",\\n    \"T\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\schema\\\\output_parser.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.prompt_values import PromptValue\\n\\n__all__ = [\"PromptValue\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\schema\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.prompts import BasePromptTemplate, format_document\\n\\n__all__ = [\"BasePromptTemplate\", \"format_document\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\schema\\\\prompt_template.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.retrievers import BaseRetriever\\n\\n__all__ = [\"BaseRetriever\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\schema\\\\retriever.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.stores import BaseStore, K, V\\n\\n__all__ = [\"BaseStore\", \"K\", \"V\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\schema\\\\storage.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.vectorstores import VST, VectorStore, VectorStoreRetriever\\n\\n__all__ = [\"VectorStore\", \"VectorStoreRetriever\", \"VST\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\schema\\\\vectorstore.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"**Schemas** are the LangChain Base Classes and Interfaces.\"\"\"\\nfrom langchain_core.agents import AgentAction, AgentFinish\\nfrom langchain_core.caches import BaseCache\\nfrom langchain_core.chat_history import BaseChatMessageHistory\\nfrom langchain_core.documents import BaseDocumentTransformer, Document\\nfrom langchain_core.exceptions import LangChainException, OutputParserException\\nfrom langchain_core.memory import BaseMemory\\nfrom langchain_core.messages import (\\n    AIMessage,\\n    BaseMessage,\\n    ChatMessage,\\n    FunctionMessage,\\n    HumanMessage,\\n    SystemMessage,\\n    _message_from_dict,\\n    get_buffer_string,\\n    messages_from_dict,\\n    messages_to_dict,\\n)\\nfrom langchain_core.messages.base import message_to_dict\\nfrom langchain_core.output_parsers import (\\n    BaseLLMOutputParser,\\n    BaseOutputParser,\\n    StrOutputParser,\\n)\\nfrom langchain_core.outputs import (\\n    ChatGeneration,\\n    ChatResult,\\n    Generation,\\n    LLMResult,\\n    RunInfo,\\n)\\nfrom langchain_core.prompt_values import PromptValue\\nfrom langchain_core.prompts import BasePromptTemplate, format_document\\nfrom langchain_core.retrievers import BaseRetriever\\nfrom langchain_core.stores import BaseStore\\n\\nRUN_KEY = \"__run\"\\n\\n# Backwards compatibility.\\nMemory = BaseMemory\\n_message_to_dict = message_to_dict' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\schema\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='__all__ = [\\n    \"BaseCache\",\\n    \"BaseMemory\",\\n    \"BaseStore\",\\n    \"AgentFinish\",\\n    \"AgentAction\",\\n    \"Document\",\\n    \"BaseChatMessageHistory\",\\n    \"BaseDocumentTransformer\",\\n    \"BaseMessage\",\\n    \"ChatMessage\",\\n    \"FunctionMessage\",\\n    \"HumanMessage\",\\n    \"AIMessage\",\\n    \"SystemMessage\",\\n    \"messages_from_dict\",\\n    \"messages_to_dict\",\\n    \"message_to_dict\",\\n    \"_message_to_dict\",\\n    \"_message_from_dict\",\\n    \"get_buffer_string\",\\n    \"RunInfo\",\\n    \"LLMResult\",\\n    \"ChatResult\",\\n    \"ChatGeneration\",\\n    \"Generation\",\\n    \"PromptValue\",\\n    \"LangChainException\",\\n    \"BaseRetriever\",\\n    \"RUN_KEY\",\\n    \"Memory\",\\n    \"OutputParserException\",\\n    \"StrOutputParser\",\\n    \"BaseOutputParser\",\\n    \"BaseLLMOutputParser\",\\n    \"BasePromptTemplate\",\\n    \"format_document\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\schema\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.callbacks.base import (\\n    AsyncCallbackHandler,\\n    BaseCallbackHandler,\\n    BaseCallbackManager,\\n    CallbackManagerMixin,\\n    ChainManagerMixin,\\n    LLMManagerMixin,\\n    RetrieverManagerMixin,\\n    RunManagerMixin,\\n    ToolManagerMixin,\\n)\\n\\n__all__ = [\\n    \"RetrieverManagerMixin\",\\n    \"LLMManagerMixin\",\\n    \"ChainManagerMixin\",\\n    \"ToolManagerMixin\",\\n    \"CallbackManagerMixin\",\\n    \"RunManagerMixin\",\\n    \"BaseCallbackHandler\",\\n    \"AsyncCallbackHandler\",\\n    \"BaseCallbackManager\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\schema\\\\callbacks\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.callbacks.manager import (\\n    AsyncCallbackManager,\\n    AsyncCallbackManagerForChainGroup,\\n    AsyncCallbackManagerForChainRun,\\n    AsyncCallbackManagerForLLMRun,\\n    AsyncCallbackManagerForRetrieverRun,\\n    AsyncCallbackManagerForToolRun,\\n    AsyncParentRunManager,\\n    AsyncRunManager,\\n    BaseRunManager,\\n    CallbackManager,\\n    CallbackManagerForChainGroup,\\n    CallbackManagerForChainRun,\\n    CallbackManagerForLLMRun,\\n    CallbackManagerForRetrieverRun,\\n    CallbackManagerForToolRun,\\n    ParentRunManager,\\n    RunManager,\\n    handle_event,\\n    trace_as_chain_group,\\n)\\nfrom langchain_core.tracers.context import (\\n    collect_runs,\\n    register_configure_hook,\\n    tracing_enabled,\\n    tracing_v2_enabled,\\n)\\nfrom langchain_core.utils.env import env_var_is_set\\n\\n__all__ = [\\n    \"tracing_enabled\",\\n    \"tracing_v2_enabled\",\\n    \"collect_runs\",\\n    \"trace_as_chain_group\",\\n    \"handle_event\",\\n    \"BaseRunManager\",\\n    \"RunManager\",\\n    \"ParentRunManager\",\\n    \"AsyncRunManager\",\\n    \"AsyncParentRunManager\",\\n    \"CallbackManagerForLLMRun\",\\n    \"AsyncCallbackManagerForLLMRun\",\\n    \"CallbackManagerForChainRun\",\\n    \"AsyncCallbackManagerForChainRun\",\\n    \"CallbackManagerForToolRun\",\\n    \"AsyncCallbackManagerForToolRun\",\\n    \"CallbackManagerForRetrieverRun\",\\n    \"AsyncCallbackManagerForRetrieverRun\",\\n    \"CallbackManager\",\\n    \"CallbackManagerForChainGroup\",\\n    \"AsyncCallbackManager\",\\n    \"AsyncCallbackManagerForChainGroup\",\\n    \"register_configure_hook\",\\n    \"env_var_is_set\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\schema\\\\callbacks\\\\manager.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.callbacks.stdout import StdOutCallbackHandler\\n\\n__all__ = [\"StdOutCallbackHandler\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\schema\\\\callbacks\\\\stdout.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\\n\\n__all__ = [\"StreamingStdOutCallbackHandler\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\schema\\\\callbacks\\\\streaming_stdout.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.tracers.base import BaseTracer, TracerException\\n\\n__all__ = [\"TracerException\", \"BaseTracer\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\schema\\\\callbacks\\\\tracers\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.tracers.evaluation import (\\n    EvaluatorCallbackHandler,\\n    wait_for_all_evaluators,\\n)\\n\\n__all__ = [\"wait_for_all_evaluators\", \"EvaluatorCallbackHandler\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\schema\\\\callbacks\\\\tracers\\\\evaluation.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.tracers.langchain import (\\n    LangChainTracer,\\n    get_client,\\n    log_error_once,\\n    wait_for_all_tracers,\\n)\\n\\n__all__ = [\"log_error_once\", \"wait_for_all_tracers\", \"get_client\", \"LangChainTracer\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\schema\\\\callbacks\\\\tracers\\\\langchain.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.tracers.langchain_v1 import LangChainTracerV1, get_headers\\n\\n__all__ = [\"get_headers\", \"LangChainTracerV1\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\schema\\\\callbacks\\\\tracers\\\\langchain_v1.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.tracers.log_stream import (\\n    LogEntry,\\n    LogStreamCallbackHandler,\\n    RunLog,\\n    RunLogPatch,\\n    RunState,\\n)\\n\\n__all__ = [\"LogEntry\", \"RunState\", \"RunLogPatch\", \"RunLog\", \"LogStreamCallbackHandler\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\schema\\\\callbacks\\\\tracers\\\\log_stream.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.tracers.root_listeners import RootListenersTracer\\n\\n__all__ = [\"RootListenersTracer\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\schema\\\\callbacks\\\\tracers\\\\root_listeners.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.tracers.run_collector import RunCollectorCallbackHandler\\n\\n__all__ = [\"RunCollectorCallbackHandler\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\schema\\\\callbacks\\\\tracers\\\\run_collector.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.tracers.schemas import (\\n    BaseRun,\\n    ChainRun,\\n    LLMRun,\\n    Run,\\n    RunTypeEnum,\\n    ToolRun,\\n    TracerSession,\\n    TracerSessionBase,\\n    TracerSessionV1,\\n    TracerSessionV1Base,\\n    TracerSessionV1Create,\\n)\\n\\n__all__ = [\\n    \"RunTypeEnum\",\\n    \"TracerSessionV1Base\",\\n    \"TracerSessionV1Create\",\\n    \"TracerSessionV1\",\\n    \"TracerSessionBase\",\\n    \"TracerSession\",\\n    \"BaseRun\",\\n    \"LLMRun\",\\n    \"ChainRun\",\\n    \"ToolRun\",\\n    \"Run\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\schema\\\\callbacks\\\\tracers\\\\schemas.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.tracers.stdout import (\\n    ConsoleCallbackHandler,\\n    FunctionCallbackHandler,\\n    elapsed,\\n    try_json_stringify,\\n)\\n\\n__all__ = [\\n    \"try_json_stringify\",\\n    \"elapsed\",\\n    \"FunctionCallbackHandler\",\\n    \"ConsoleCallbackHandler\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\schema\\\\callbacks\\\\tracers\\\\stdout.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.runnables.base import (\\n    Other,\\n    Runnable,\\n    RunnableBinding,\\n    RunnableBindingBase,\\n    RunnableEach,\\n    RunnableEachBase,\\n    RunnableGenerator,\\n    RunnableLambda,\\n    RunnableLike,\\n    RunnableParallel,\\n    RunnableSequence,\\n    RunnableSerializable,\\n    coerce_to_runnable,\\n)\\nfrom langchain_core.runnables.utils import Input, Output\\n\\n# Backwards compatibility.\\nRunnableMap = RunnableParallel\\n\\n__all__ = [\\n    \"Input\",\\n    \"Output\",\\n    \"RunnableLike\",\\n    \"Other\",\\n    \"Runnable\",\\n    \"RunnableSerializable\",\\n    \"RunnableSequence\",\\n    \"RunnableParallel\",\\n    \"RunnableGenerator\",\\n    \"RunnableLambda\",\\n    \"RunnableEachBase\",\\n    \"RunnableEach\",\\n    \"RunnableBindingBase\",\\n    \"RunnableBinding\",\\n    \"RunnableMap\",\\n    \"coerce_to_runnable\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\schema\\\\runnable\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.runnables.branch import RunnableBranch\\n\\n__all__ = [\"RunnableBranch\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\schema\\\\runnable\\\\branch.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.runnables.config import (\\n    EmptyDict,\\n    RunnableConfig,\\n    acall_func_with_variable_args,\\n    call_func_with_variable_args,\\n    ensure_config,\\n    get_async_callback_manager_for_config,\\n    get_callback_manager_for_config,\\n    get_config_list,\\n    get_executor_for_config,\\n    merge_configs,\\n    patch_config,\\n)\\n\\n__all__ = [\\n    \"EmptyDict\",\\n    \"RunnableConfig\",\\n    \"ensure_config\",\\n    \"get_config_list\",\\n    \"patch_config\",\\n    \"merge_configs\",\\n    \"acall_func_with_variable_args\",\\n    \"call_func_with_variable_args\",\\n    \"get_callback_manager_for_config\",\\n    \"get_async_callback_manager_for_config\",\\n    \"get_executor_for_config\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\schema\\\\runnable\\\\config.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.runnables.configurable import (\\n    DynamicRunnable,\\n    RunnableConfigurableAlternatives,\\n    RunnableConfigurableFields,\\n    StrEnum,\\n    make_options_spec,\\n)\\n\\n__all__ = [\\n    \"DynamicRunnable\",\\n    \"RunnableConfigurableFields\",\\n    \"StrEnum\",\\n    \"RunnableConfigurableAlternatives\",\\n    \"make_options_spec\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\schema\\\\runnable\\\\configurable.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.runnables.fallbacks import RunnableWithFallbacks\\n\\n__all__ = [\"RunnableWithFallbacks\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\schema\\\\runnable\\\\fallbacks.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.runnables.history import (\\n    GetSessionHistoryCallable,\\n    MessagesOrDictWithMessages,\\n    RunnableWithMessageHistory,\\n)\\n\\n__all__ = [\\n    \"RunnableWithMessageHistory\",\\n    \"GetSessionHistoryCallable\",\\n    \"MessagesOrDictWithMessages\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\schema\\\\runnable\\\\history.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.runnables.passthrough import (\\n    RunnableAssign,\\n    RunnablePassthrough,\\n    aidentity,\\n    identity,\\n)\\n\\n__all__ = [\"aidentity\", \"identity\", \"RunnablePassthrough\", \"RunnableAssign\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\schema\\\\runnable\\\\passthrough.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.runnables.retry import RunnableRetry, U\\n\\n__all__ = [\"RunnableRetry\", \"U\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\schema\\\\runnable\\\\retry.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.runnables.router import RouterInput, RouterRunnable\\n\\n__all__ = [\"RouterInput\", \"RouterRunnable\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\schema\\\\runnable\\\\router.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.runnables.utils import (\\n    Addable,\\n    AddableDict,\\n    AnyConfigurableField,\\n    ConfigurableField,\\n    ConfigurableFieldMultiOption,\\n    ConfigurableFieldSingleOption,\\n    ConfigurableFieldSpec,\\n    GetLambdaSource,\\n    Input,\\n    IsFunctionArgDict,\\n    IsLocalDict,\\n    Output,\\n    SupportsAdd,\\n    aadd,\\n    accepts_config,\\n    accepts_run_manager,\\n    add,\\n    gated_coro,\\n    gather_with_concurrency,\\n    get_function_first_arg_dict_keys,\\n    get_lambda_source,\\n    get_unique_config_specs,\\n    indent_lines_after_first,\\n)\\n\\n__all__ = [\\n    \"accepts_run_manager\",\\n    \"accepts_config\",\\n    \"IsLocalDict\",\\n    \"IsFunctionArgDict\",\\n    \"GetLambdaSource\",\\n    \"get_function_first_arg_dict_keys\",\\n    \"get_lambda_source\",\\n    \"indent_lines_after_first\",\\n    \"AddableDict\",\\n    \"SupportsAdd\",\\n    \"add\",\\n    \"ConfigurableField\",\\n    \"ConfigurableFieldSingleOption\",\\n    \"ConfigurableFieldMultiOption\",\\n    \"ConfigurableFieldSpec\",\\n    \"get_unique_config_specs\",\\n    \"aadd\",\\n    \"gated_coro\",\\n    \"gather_with_concurrency\",\\n    \"Input\",\\n    \"Output\",\\n    \"Addable\",\\n    \"AnyConfigurableField\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\schema\\\\runnable\\\\utils.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"LangChain **Runnable** and the **LangChain Expression Language (LCEL)**.\\n\\nThe LangChain Expression Language (LCEL) offers a declarative method to build\\nproduction-grade programs that harness the power of LLMs.\\n\\nPrograms created using LCEL and LangChain Runnables inherently support\\nsynchronous, asynchronous, batch, and streaming operations.\\n\\nSupport for **async** allows servers hosting LCEL based programs to scale better\\nfor higher concurrent loads.\\n\\n**Streaming** of intermediate outputs as they\\'re being generated allows for\\ncreating more responsive UX.\\n\\nThis module contains schema and implementation of LangChain Runnables primitives.\\n\"\"\"\\nfrom langchain_core.runnables.base import (\\n    Runnable,\\n    RunnableBinding,\\n    RunnableGenerator,\\n    RunnableLambda,\\n    RunnableMap,\\n    RunnableParallel,\\n    RunnableSequence,\\n    RunnableSerializable,\\n)\\nfrom langchain_core.runnables.branch import RunnableBranch\\nfrom langchain_core.runnables.config import RunnableConfig, patch_config\\nfrom langchain_core.runnables.fallbacks import RunnableWithFallbacks\\nfrom langchain_core.runnables.passthrough import RunnablePassthrough\\nfrom langchain_core.runnables.router import RouterInput, RouterRunnable\\nfrom langchain_core.runnables.utils import (\\n    ConfigurableField,\\n    ConfigurableFieldMultiOption,\\n    ConfigurableFieldSingleOption,\\n)\\n\\n__all__ = [\\n    \"ConfigurableField\",\\n    \"ConfigurableFieldSingleOption\",\\n    \"ConfigurableFieldMultiOption\",\\n    \"patch_config\",\\n    \"RouterInput\",\\n    \"RouterRunnable\",\\n    \"Runnable\",\\n    \"RunnableSerializable\",\\n    \"RunnableBinding\",\\n    \"RunnableBranch\",\\n    \"RunnableConfig\",\\n    \"RunnableGenerator\",\\n    \"RunnableLambda\",\\n    \"RunnableMap\",\\n    \"RunnableParallel\",\\n    \"RunnablePassthrough\",\\n    \"RunnableSequence\",\\n    \"RunnableWithFallbacks\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\schema\\\\runnable\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"**LangSmith** utilities.\\n\\nThis module provides utilities for connecting to `LangSmith <https://smith.langchain.com/>`_. For more information on LangSmith, see the `LangSmith documentation <https://docs.smith.langchain.com/>`_.\\n\\n**Evaluation**\\n\\nLangSmith helps you evaluate Chains and other language model application components using a number of LangChain evaluators.\\nAn example of this is shown below, assuming you\\'ve created a LangSmith dataset called ``<my_dataset_name>``:\\n\\n.. code-block:: python\\n\\n    from langsmith import Client\\n    from langchain_community.chat_models import ChatOpenAI\\n    from langchain.chains import LLMChain\\n    from langchain.smith import RunEvalConfig, run_on_dataset\\n\\n    # Chains may have memory. Passing in a constructor function lets the\\n    # evaluation framework avoid cross-contamination between runs.\\n    def construct_chain():\\n        llm = ChatOpenAI(temperature=0)\\n        chain = LLMChain.from_string(\\n            llm,\\n            \"What\\'s the answer to {your_input_key}\"\\n        )\\n        return chain\\n\\n    # Load off-the-shelf evaluators via config or the EvaluatorType (string or enum)\\n    evaluation_config = RunEvalConfig(\\n        evaluators=[\\n            \"qa\",  # \"Correctness\" against a reference answer\\n            \"embedding_distance\",\\n            RunEvalConfig.Criteria(\"helpfulness\"),\\n            RunEvalConfig.Criteria({\\n                \"fifth-grader-score\": \"Do you have to be smarter than a fifth grader to answer this question?\"\\n            }),\\n        ]\\n    )\\n\\n    client = Client()\\n    run_on_dataset(\\n        client,\\n        \"<my_dataset_name>\",\\n        construct_chain,\\n        evaluation=evaluation_config,\\n    )\\n\\nYou can also create custom evaluators by subclassing the\\n:class:`StringEvaluator <langchain.evaluation.schema.StringEvaluator>`\\nor LangSmith\\'s `RunEvaluator` classes.\\n\\n.. code-block:: python\\n\\n    from typing import Optional\\n    from langchain.evaluation import StringEvaluator' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='class MyStringEvaluator(StringEvaluator):\\n        \\n        @property\\n        def requires_input(self) -> bool:\\n            return False\\n        \\n        @property\\n        def requires_reference(self) -> bool:\\n            return True\\n        \\n        @property\\n        def evaluation_name(self) -> str:\\n            return \"exact_match\"\\n        \\n        def _evaluate_strings(self, prediction, reference=None, input=None, **kwargs) -> dict:\\n            return {\"score\": prediction == reference}\\n\\n\\n    evaluation_config = RunEvalConfig(\\n        custom_evaluators = [MyStringEvaluator()],\\n    )\\n\\n    run_on_dataset(\\n        client,\\n        \"<my_dataset_name>\",\\n        construct_chain,\\n        evaluation=evaluation_config,\\n    )    \\n\\n**Primary Functions**\\n\\n- :func:`arun_on_dataset <langchain.smith.evaluation.runner_utils.arun_on_dataset>`: Asynchronous function to evaluate a chain, agent, or other LangChain component over a dataset.\\n- :func:`run_on_dataset <langchain.smith.evaluation.runner_utils.run_on_dataset>`: Function to evaluate a chain, agent, or other LangChain component over a dataset.\\n- :class:`RunEvalConfig <langchain.smith.evaluation.config.RunEvalConfig>`: Class representing the configuration for running evaluation. You can select evaluators by :class:`EvaluatorType <langchain.evaluation.schema.EvaluatorType>` or config, or you can pass in `custom_evaluators`\\n\"\"\"  # noqa: E501\\nfrom langchain.smith.evaluation import (\\n    RunEvalConfig,\\n    arun_on_dataset,\\n    run_on_dataset,\\n)\\n\\n__all__ = [\\n    \"arun_on_dataset\",\\n    \"run_on_dataset\",\\n    \"RunEvalConfig\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Configuration for run evaluators.\"\"\"\\n\\nfrom typing import Any, Dict, List, Optional, Union\\n\\nfrom langchain_core.embeddings import Embeddings\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.prompts import BasePromptTemplate\\nfrom langchain_core.pydantic_v1 import BaseModel, Field\\nfrom langsmith import RunEvaluator\\n\\nfrom langchain.evaluation.criteria.eval_chain import CRITERIA_TYPE\\nfrom langchain.evaluation.embedding_distance.base import (\\n    EmbeddingDistance as EmbeddingDistanceEnum,\\n)\\nfrom langchain.evaluation.schema import EvaluatorType, StringEvaluator\\nfrom langchain.evaluation.string_distance.base import (\\n    StringDistance as StringDistanceEnum,\\n)\\n\\n\\nclass EvalConfig(BaseModel):\\n    \"\"\"Configuration for a given run evaluator.\\n\\n    Parameters\\n    ----------\\n    evaluator_type : EvaluatorType\\n        The type of evaluator to use.\\n\\n    Methods\\n    -------\\n    get_kwargs()\\n        Get the keyword arguments for the evaluator configuration.\\n\\n    \"\"\"\\n\\n    evaluator_type: EvaluatorType\\n\\n    def get_kwargs(self) -> Dict[str, Any]:\\n        \"\"\"Get the keyword arguments for the load_evaluator call.\\n\\n        Returns\\n        -------\\n        Dict[str, Any]\\n            The keyword arguments for the load_evaluator call.\\n\\n        \"\"\"\\n        kwargs = {}\\n        for field, val in self:\\n            if field == \"evaluator_type\":\\n                continue\\n            elif val is None:\\n                continue\\n            kwargs[field] = val\\n        return kwargs\\n\\n\\nclass SingleKeyEvalConfig(EvalConfig):\\n    \"\"\"Configuration for a run evaluator that only requires a single key.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\config.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='reference_key: Optional[str] = None\\n    \"\"\"The key in the dataset run to use as the reference string.\\n    If not provided, we will attempt to infer automatically.\"\"\"\\n    prediction_key: Optional[str] = None\\n    \"\"\"The key from the traced run\\'s outputs dictionary to use to\\n    represent the prediction. If not provided, it will be inferred\\n    automatically.\"\"\"\\n    input_key: Optional[str] = None\\n    \"\"\"The key from the traced run\\'s inputs dictionary to use to represent the\\n    input. If not provided, it will be inferred automatically.\"\"\"\\n\\n    def get_kwargs(self) -> Dict[str, Any]:\\n        kwargs = super().get_kwargs()\\n        # Filer out the keys that are not needed for the evaluator.\\n        for key in [\"reference_key\", \"prediction_key\", \"input_key\"]:\\n            kwargs.pop(key, None)\\n        return kwargs\\n\\n\\nclass RunEvalConfig(BaseModel):\\n    \"\"\"Configuration for a run evaluation.\\n\\n    Parameters\\n    ----------\\n    evaluators : List[Union[EvaluatorType, EvalConfig]]\\n        Configurations for which evaluators to apply to the dataset run.\\n        Each can be the string of an :class:`EvaluatorType <langchain.evaluation.schema.EvaluatorType>`, such\\n        as EvaluatorType.QA, the evaluator type string (\"qa\"), or a configuration for a\\n        given evaluator (e.g., :class:`RunEvalConfig.QA <langchain.smith.evaluation.config.RunEvalConfig.QA>`).\\n\\n    custom_evaluators : Optional[List[Union[RunEvaluator, StringEvaluator]]]\\n        Custom evaluators to apply to the dataset run.\\n\\n    reference_key : Optional[str]\\n        The key in the dataset run to use as the reference string.\\n        If not provided, it will be inferred automatically.\\n\\n    prediction_key : Optional[str]\\n        The key from the traced run\\'s outputs dictionary to use to\\n        represent the prediction. If not provided, it will be inferred\\n        automatically.\\n\\n    input_key : Optional[str]\\n        The key from the traced run\\'s inputs dictionary to use to represent the\\n        input. If not provided, it will be inferred automatically.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\config.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='eval_llm : Optional[BaseLanguageModel]\\n        The language model to pass to any evaluators that use a language model.\\n    \"\"\"  # noqa: E501\\n\\n    evaluators: List[Union[EvaluatorType, str, EvalConfig]] = Field(\\n        default_factory=list\\n    )\\n    \"\"\"Configurations for which evaluators to apply to the dataset run.\\n    Each can be the string of an\\n    :class:`EvaluatorType <langchain.evaluation.schema.EvaluatorType>`, such\\n    as `EvaluatorType.QA`, the evaluator type string (\"qa\"), or a configuration for a\\n    given evaluator\\n    (e.g., \\n    :class:`RunEvalConfig.QA <langchain.smith.evaluation.config.RunEvalConfig.QA>`).\"\"\"  # noqa: E501\\n    custom_evaluators: Optional[List[Union[RunEvaluator, StringEvaluator]]] = None\\n    \"\"\"Custom evaluators to apply to the dataset run.\"\"\"\\n    reference_key: Optional[str] = None\\n    \"\"\"The key in the dataset run to use as the reference string.\\n    If not provided, we will attempt to infer automatically.\"\"\"\\n    prediction_key: Optional[str] = None\\n    \"\"\"The key from the traced run\\'s outputs dictionary to use to\\n    represent the prediction. If not provided, it will be inferred\\n    automatically.\"\"\"\\n    input_key: Optional[str] = None\\n    \"\"\"The key from the traced run\\'s inputs dictionary to use to represent the\\n    input. If not provided, it will be inferred automatically.\"\"\"\\n    eval_llm: Optional[BaseLanguageModel] = None\\n    \"\"\"The language model to pass to any evaluators that require one.\"\"\"\\n\\n    class Config:\\n        arbitrary_types_allowed = True\\n\\n    class Criteria(SingleKeyEvalConfig):\\n        \"\"\"Configuration for a reference-free criteria evaluator.\\n\\n        Parameters\\n        ----------\\n        criteria : Optional[CRITERIA_TYPE]\\n            The criteria to evaluate.\\n        llm : Optional[BaseLanguageModel]\\n            The language model to use for the evaluation chain.\\n\\n        \"\"\"\\n\\n        criteria: Optional[CRITERIA_TYPE] = None\\n        llm: Optional[BaseLanguageModel] = None\\n        evaluator_type: EvaluatorType = EvaluatorType.CRITERIA' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\config.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def __init__(\\n            self, criteria: Optional[CRITERIA_TYPE] = None, **kwargs: Any\\n        ) -> None:\\n            super().__init__(criteria=criteria, **kwargs)\\n\\n    class LabeledCriteria(SingleKeyEvalConfig):\\n        \"\"\"Configuration for a labeled (with references) criteria evaluator.\\n\\n        Parameters\\n        ----------\\n        criteria : Optional[CRITERIA_TYPE]\\n            The criteria to evaluate.\\n        llm : Optional[BaseLanguageModel]\\n            The language model to use for the evaluation chain.\\n        \"\"\"\\n\\n        criteria: Optional[CRITERIA_TYPE] = None\\n        llm: Optional[BaseLanguageModel] = None\\n        evaluator_type: EvaluatorType = EvaluatorType.LABELED_CRITERIA\\n\\n        def __init__(\\n            self, criteria: Optional[CRITERIA_TYPE] = None, **kwargs: Any\\n        ) -> None:\\n            super().__init__(criteria=criteria, **kwargs)\\n\\n    class EmbeddingDistance(SingleKeyEvalConfig):\\n        \"\"\"Configuration for an embedding distance evaluator.\\n\\n        Parameters\\n        ----------\\n        embeddings : Optional[Embeddings]\\n            The embeddings to use for computing the distance.\\n\\n        distance_metric : Optional[EmbeddingDistanceEnum]\\n            The distance metric to use for computing the distance.\\n\\n        \"\"\"\\n\\n        evaluator_type: EvaluatorType = EvaluatorType.EMBEDDING_DISTANCE\\n        embeddings: Optional[Embeddings] = None\\n        distance_metric: Optional[EmbeddingDistanceEnum] = None\\n\\n        class Config:\\n            arbitrary_types_allowed = True\\n\\n    class StringDistance(SingleKeyEvalConfig):\\n        \"\"\"Configuration for a string distance evaluator.\\n\\n        Parameters\\n        ----------\\n        distance : Optional[StringDistanceEnum]\\n            The string distance metric to use.\\n\\n        \"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\config.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='evaluator_type: EvaluatorType = EvaluatorType.STRING_DISTANCE\\n        distance: Optional[StringDistanceEnum] = None\\n        \"\"\"The string distance metric to use.\\n            damerau_levenshtein: The Damerau-Levenshtein distance.\\n            levenshtein: The Levenshtein distance.\\n            jaro: The Jaro distance.\\n            jaro_winkler: The Jaro-Winkler distance.\\n        \"\"\"\\n        normalize_score: bool = True\\n        \"\"\"Whether to normalize the distance to between 0 and 1.\\n        Applies only to the Levenshtein and Damerau-Levenshtein distances.\"\"\"\\n\\n    class QA(SingleKeyEvalConfig):\\n        \"\"\"Configuration for a QA evaluator.\\n\\n        Parameters\\n        ----------\\n        prompt : Optional[BasePromptTemplate]\\n            The prompt template to use for generating the question.\\n        llm : Optional[BaseLanguageModel]\\n            The language model to use for the evaluation chain.\\n        \"\"\"\\n\\n        evaluator_type: EvaluatorType = EvaluatorType.QA\\n        llm: Optional[BaseLanguageModel] = None\\n        prompt: Optional[BasePromptTemplate] = None\\n\\n    class ContextQA(SingleKeyEvalConfig):\\n        \"\"\"Configuration for a context-based QA evaluator.\\n\\n        Parameters\\n        ----------\\n        prompt : Optional[BasePromptTemplate]\\n            The prompt template to use for generating the question.\\n        llm : Optional[BaseLanguageModel]\\n            The language model to use for the evaluation chain.\\n\\n        \"\"\"\\n\\n        evaluator_type: EvaluatorType = EvaluatorType.CONTEXT_QA\\n        llm: Optional[BaseLanguageModel] = None\\n        prompt: Optional[BasePromptTemplate] = None\\n\\n    class CoTQA(SingleKeyEvalConfig):\\n        \"\"\"Configuration for a context-based QA evaluator.\\n\\n        Parameters\\n        ----------\\n        prompt : Optional[BasePromptTemplate]\\n            The prompt template to use for generating the question.\\n        llm : Optional[BaseLanguageModel]\\n            The language model to use for the evaluation chain.\\n\\n        \"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\config.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='evaluator_type: EvaluatorType = EvaluatorType.CONTEXT_QA\\n        llm: Optional[BaseLanguageModel] = None\\n        prompt: Optional[BasePromptTemplate] = None\\n\\n    class JsonValidity(SingleKeyEvalConfig):\\n        \"\"\"Configuration for a json validity evaluator.\\n\\n        Parameters\\n        ----------\\n        \"\"\"\\n\\n        evaluator_type: EvaluatorType = EvaluatorType.JSON_VALIDITY\\n\\n    class JsonEqualityEvaluator(EvalConfig):\\n        \"\"\"Configuration for a json equality evaluator.\\n\\n        Parameters\\n        ----------\\n        \"\"\"\\n\\n        evaluator_type: EvaluatorType = EvaluatorType.JSON_EQUALITY\\n\\n    class ExactMatch(SingleKeyEvalConfig):\\n        \"\"\"Configuration for an exact match string evaluator.\\n\\n        Parameters\\n        ----------\\n        ignore_case : bool\\n            Whether to ignore case when comparing strings.\\n        ignore_punctuation : bool\\n            Whether to ignore punctuation when comparing strings.\\n        ignore_numbers : bool\\n            Whether to ignore numbers when comparing strings.\\n        \"\"\"\\n\\n        evaluator_type: EvaluatorType = EvaluatorType.STRING_DISTANCE\\n        ignore_case: bool = False\\n        ignore_punctuation: bool = False\\n        ignore_numbers: bool = False\\n\\n    class RegexMatch(SingleKeyEvalConfig):\\n        \"\"\"Configuration for a regex match string evaluator.\\n\\n        Parameters\\n        ----------\\n        flags : int\\n            The flags to pass to the regex. Example: re.IGNORECASE.\\n        \"\"\"\\n\\n        evaluator_type: EvaluatorType = EvaluatorType.REGEX_MATCH\\n        flags: int = 0\\n\\n    class ScoreString(SingleKeyEvalConfig):\\n        \"\"\"Configuration for a score string evaluator.\\n        This is like the criteria evaluator but it is configured by\\n        default to return a score on the scale from 1-10.\\n\\n        It is recommended to normalize these scores\\n        by setting `normalize_by` to 10.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\config.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Parameters\\n        ----------\\n        criteria : Optional[CRITERIA_TYPE]\\n            The criteria to evaluate.\\n        llm : Optional[BaseLanguageModel]\\n            The language model to use for the evaluation chain.\\n        normalize_by: Optional[int] = None\\n            If you want to normalize the score, the denominator to use.\\n            If not provided, the score will be between 1 and 10 (by default).\\n        prompt : Optional[BasePromptTemplate]\\n\\n        \"\"\"\\n\\n        evaluator_type: EvaluatorType = EvaluatorType.SCORE_STRING\\n        criteria: Optional[CRITERIA_TYPE] = None\\n        llm: Optional[BaseLanguageModel] = None\\n        normalize_by: Optional[float] = None\\n        prompt: Optional[BasePromptTemplate] = None\\n\\n        def __init__(\\n            self,\\n            criteria: Optional[CRITERIA_TYPE] = None,\\n            normalize_by: Optional[float] = None,\\n            **kwargs: Any,\\n        ) -> None:\\n            super().__init__(criteria=criteria, normalize_by=normalize_by, **kwargs)\\n\\n    class LabeledScoreString(ScoreString):\\n        evaluator_type: EvaluatorType = EvaluatorType.LABELED_SCORE_STRING' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\config.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def random_name() -> str:\\n    \"\"\"Generate a random name.\"\"\"\\n    adjective = random.choice(adjectives)\\n    noun = random.choice(nouns)\\n    number = random.randint(1, 100)\\n    return f\"{adjective}-{noun}-{number}\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\name_generation.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='import random\\n\\nadjectives = [\\n    \"abandoned\",\\n    \"aching\",\\n    \"advanced\",\\n    \"ample\",\\n    \"artistic\",\\n    \"back\",\\n    \"best\",\\n    \"bold\",\\n    \"brief\",\\n    \"clear\",\\n    \"cold\",\\n    \"complicated\",\\n    \"cooked\",\\n    \"crazy\",\\n    \"crushing\",\\n    \"damp\",\\n    \"dear\",\\n    \"definite\",\\n    \"dependable\",\\n    \"diligent\",\\n    \"drab\",\\n    \"earnest\",\\n    \"elderly\",\\n    \"enchanted\",\\n    \"essential\",\\n    \"excellent\",\\n    \"extraneous\",\\n    \"fixed\",\\n    \"flowery\",\\n    \"formal\",\\n    \"fresh\",\\n    \"frosty\",\\n    \"giving\",\\n    \"glossy\",\\n    \"healthy\",\\n    \"helpful\",\\n    \"impressionable\",\\n    \"kind\",\\n    \"large\",\\n    \"left\",\\n    \"long\",\\n    \"loyal\",\\n    \"mealy\",\\n    \"memorable\",\\n    \"monthly\",\\n    \"new\",\\n    \"notable\",\\n    \"only\",\\n    \"ordinary\",\\n    \"passionate\",\\n    \"perfect\",\\n    \"pertinent\",\\n    \"proper\",\\n    \"puzzled\",\\n    \"reflecting\",\\n    \"respectful\",\\n    \"roasted\",\\n    \"scholarly\",\\n    \"shiny\",\\n    \"slight\",\\n    \"sparkling\",\\n    \"spotless\",\\n    \"stupendous\",\\n    \"sunny\",\\n    \"tart\",\\n    \"terrific\",\\n    \"timely\",\\n    \"unique\",\\n    \"upbeat\",\\n    \"vacant\",\\n    \"virtual\",\\n    \"warm\",\\n    \"weary\",\\n    \"whispered\",\\n    \"worthwhile\",\\n    \"yellow\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\name_generation.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='nouns = [\\n    \"account\",\\n    \"acknowledgment\",\\n    \"address\",\\n    \"advertising\",\\n    \"airplane\",\\n    \"animal\",\\n    \"appointment\",\\n    \"arrival\",\\n    \"artist\",\\n    \"attachment\",\\n    \"attitude\",\\n    \"availability\",\\n    \"backpack\",\\n    \"bag\",\\n    \"balance\",\\n    \"bass\",\\n    \"bean\",\\n    \"beauty\",\\n    \"bibliography\",\\n    \"bill\",\\n    \"bite\",\\n    \"blossom\",\\n    \"boat\",\\n    \"book\",\\n    \"box\",\\n    \"boy\",\\n    \"bread\",\\n    \"bridge\",\\n    \"broccoli\",\\n    \"building\",\\n    \"butter\",\\n    \"button\",\\n    \"cabbage\",\\n    \"cake\",\\n    \"camera\",\\n    \"camp\",\\n    \"candle\",\\n    \"candy\",\\n    \"canvas\",\\n    \"car\",\\n    \"card\",\\n    \"carrot\",\\n    \"cart\",\\n    \"case\",\\n    \"cat\",\\n    \"chain\",\\n    \"chair\",\\n    \"chalk\",\\n    \"chance\",\\n    \"change\",\\n    \"channel\",\\n    \"character\",\\n    \"charge\",\\n    \"charm\",\\n    \"chart\",\\n    \"check\",\\n    \"cheek\",\\n    \"cheese\",\\n    \"chef\",\\n    \"cherry\",\\n    \"chicken\",\\n    \"child\",\\n    \"church\",\\n    \"circle\",\\n    \"class\",\\n    \"clay\",\\n    \"click\",\\n    \"clock\",\\n    \"cloth\",\\n    \"cloud\",\\n    \"clove\",\\n    \"club\",\\n    \"coach\",\\n    \"coal\",\\n    \"coast\",\\n    \"coat\",\\n    \"cod\",\\n    \"coffee\",\\n    \"collar\",\\n    \"color\",\\n    \"comb\",\\n    \"comfort\",\\n    \"comic\",\\n    \"committee\",\\n    \"community\",\\n    \"company\",\\n    \"comparison\",\\n    \"competition\",\\n    \"condition\",\\n    \"connection\",\\n    \"control\",\\n    \"cook\",\\n    \"copper\",\\n    \"copy\",\\n    \"corn\",\\n    \"cough\",\\n    \"country\",\\n    \"cover\",\\n    \"crate\",\\n    \"crayon\",\\n    \"cream\",\\n    \"creator\",\\n    \"crew\",\\n    \"crown\",\\n    \"current\",\\n    \"curtain\",\\n    \"curve\",\\n    \"cushion\",\\n    \"dad\",\\n    \"daughter\",\\n    \"day\",\\n    \"death\",\\n    \"debt\",\\n    \"decision\",\\n    \"deer\",\\n    \"degree\",\\n    \"design\",\\n    \"desire\",\\n    \"desk\",\\n    \"detail\",\\n    \"development\",\\n    \"digestion\",\\n    \"dime\",\\n    \"dinner\",\\n    \"direction\",\\n    \"dirt\",\\n    \"discovery\",\\n    \"discussion\",\\n    \"disease\",\\n    \"disgust\",\\n    \"distance\",\\n    \"distribution\",\\n    \"division\",\\n    \"doctor\",\\n    \"dog\",\\n    \"door\",\\n    \"drain\",\\n    \"drawer\",\\n    \"dress\",\\n    \"drink\",\\n    \"driving\",\\n    \"dust\",\\n    \"ear\",\\n    \"earth\",\\n    \"edge\",' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\name_generation.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"education\",\\n    \"effect\",\\n    \"egg\",\\n    \"end\",\\n    \"energy\",\\n    \"engine\",\\n    \"error\",\\n    \"event\",\\n    \"example\",\\n    \"exchange\",\\n    \"existence\",\\n    \"expansion\",\\n    \"experience\",\\n    \"expert\",\\n    \"eye\",\\n    \"face\",\\n    \"fact\",\\n    \"fall\",\\n    \"family\",\\n    \"farm\",\\n    \"father\",\\n    \"fear\",\\n    \"feeling\",\\n    \"field\",\\n    \"finger\",\\n    \"fire\",\\n    \"fish\",\\n    \"flag\",\\n    \"flight\",\\n    \"floor\",\\n    \"flower\",\\n    \"fold\",\\n    \"food\",\\n    \"football\",\\n    \"force\",\\n    \"form\",\\n    \"frame\",\\n    \"friend\",\\n    \"frog\",\\n    \"fruit\",\\n    \"fuel\",\\n    \"furniture\",\\n    \"game\",\\n    \"garden\",\\n    \"gate\",\\n    \"girl\",\\n    \"glass\",\\n    \"glove\",\\n    \"goat\",\\n    \"gold\",\\n    \"government\",\\n    \"grade\",\\n    \"grain\",\\n    \"grass\",\\n    \"green\",\\n    \"grip\",\\n    \"group\",\\n    \"growth\",\\n    \"guide\",\\n    \"guitar\",\\n    \"hair\",\\n    \"hall\",\\n    \"hand\",\\n    \"harbor\",\\n    \"harmony\",\\n    \"hat\",\\n    \"head\",\\n    \"health\",\\n    \"heart\",\\n    \"heat\",\\n    \"hill\",\\n    \"history\",\\n    \"hobbies\",\\n    \"hole\",\\n    \"hope\",\\n    \"horn\",\\n    \"horse\",\\n    \"hospital\",\\n    \"hour\",\\n    \"house\",\\n    \"humor\",\\n    \"idea\",\\n    \"impulse\",\\n    \"income\",\\n    \"increase\",\\n    \"industry\",\\n    \"ink\",\\n    \"insect\",\\n    \"instrument\",\\n    \"insurance\",\\n    \"interest\",\\n    \"invention\",\\n    \"iron\",\\n    \"island\",\\n    \"jelly\",\\n    \"jet\",\\n    \"jewel\",\\n    \"join\",\\n    \"judge\",\\n    \"juice\",\\n    \"jump\",\\n    \"kettle\",\\n    \"key\",\\n    \"kick\",\\n    \"kiss\",\\n    \"kitten\",\\n    \"knee\",\\n    \"knife\",\\n    \"knowledge\",\\n    \"land\",\\n    \"language\",\\n    \"laugh\",\\n    \"law\",\\n    \"lead\",\\n    \"learning\",\\n    \"leather\",\\n    \"leg\",\\n    \"lettuce\",\\n    \"level\",\\n    \"library\",\\n    \"lift\",\\n    \"light\",\\n    \"limit\",\\n    \"line\",\\n    \"linen\",\\n    \"lip\",\\n    \"liquid\",\\n    \"list\",\\n    \"look\",\\n    \"loss\",\\n    \"love\",\\n    \"lunch\",\\n    \"machine\",\\n    \"man\",\\n    \"manager\",\\n    \"map\",\\n    \"marble\",\\n    \"mark\",\\n    \"market\",\\n    \"mass\",\\n    \"match\",\\n    \"meal\",\\n    \"measure\",\\n    \"meat\",\\n    \"meeting\",\\n    \"memory\",\\n    \"metal\",\\n    \"middle\",\\n    \"milk\",\\n    \"mind\",\\n    \"mine\",\\n    \"minute\",\\n    \"mist\",\\n    \"mitten\",' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\name_generation.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"mom\",\\n    \"money\",\\n    \"monkey\",\\n    \"month\",\\n    \"moon\",\\n    \"morning\",\\n    \"mother\",\\n    \"motion\",\\n    \"mountain\",\\n    \"mouth\",\\n    \"muscle\",\\n    \"music\",\\n    \"nail\",\\n    \"name\",\\n    \"nation\",\\n    \"neck\",\\n    \"need\",\\n    \"news\",\\n    \"night\",\\n    \"noise\",\\n    \"note\",\\n    \"number\",\\n    \"nut\",\\n    \"observation\",\\n    \"offer\",\\n    \"oil\",\\n    \"operation\",\\n    \"opinion\",\\n    \"orange\",\\n    \"order\",\\n    \"organization\",\\n    \"ornament\",\\n    \"oven\",\\n    \"page\",\\n    \"pail\",\\n    \"pain\",\\n    \"paint\",\\n    \"pan\",\\n    \"pancake\",\\n    \"paper\",\\n    \"parcel\",\\n    \"parent\",\\n    \"part\",\\n    \"passenger\",\\n    \"paste\",\\n    \"payment\",\\n    \"peace\",\\n    \"pear\",\\n    \"pen\",\\n    \"pencil\",\\n    \"person\",\\n    \"pest\",\\n    \"pet\",\\n    \"picture\",\\n    \"pie\",\\n    \"pin\",\\n    \"pipe\",\\n    \"pizza\",\\n    \"place\",\\n    \"plane\",\\n    \"plant\",\\n    \"plastic\",\\n    \"plate\",\\n    \"play\",\\n    \"pleasure\",\\n    \"plot\",\\n    \"plough\",\\n    \"pocket\",\\n    \"point\",\\n    \"poison\",\\n    \"police\",\\n    \"pollution\",\\n    \"popcorn\",\\n    \"porter\",\\n    \"position\",\\n    \"pot\",\\n    \"potato\",\\n    \"powder\",\\n    \"power\",\\n    \"price\",\\n    \"print\",\\n    \"process\",\\n    \"produce\",\\n    \"product\",\\n    \"profit\",\\n    \"property\",\\n    \"prose\",\\n    \"protest\",\\n    \"pull\",\\n    \"pump\",\\n    \"punishment\",\\n    \"purpose\",\\n    \"push\",\\n    \"quarter\",\\n    \"question\",\\n    \"quiet\",\\n    \"quill\",\\n    \"quilt\",\\n    \"quince\",\\n    \"rabbit\",\\n    \"rail\",\\n    \"rain\",\\n    \"range\",\\n    \"rat\",\\n    \"rate\",\\n    \"ray\",\\n    \"reaction\",\\n    \"reading\",\\n    \"reason\",\\n    \"record\",\\n    \"regret\",\\n    \"relation\",\\n    \"religion\",\\n    \"representative\",\\n    \"request\",\\n    \"respect\",\\n    \"rest\",\\n    \"reward\",\\n    \"rhythm\",\\n    \"rice\",\\n    \"river\",\\n    \"road\",\\n    \"roll\",\\n    \"room\",\\n    \"root\",\\n    \"rose\",\\n    \"route\",\\n    \"rub\",\\n    \"rule\",\\n    \"run\",\\n    \"sack\",\\n    \"sail\",\\n    \"salt\",\\n    \"sand\",\\n    \"scale\",\\n    \"scarecrow\",\\n    \"scarf\",\\n    \"scene\",\\n    \"scent\",\\n    \"school\",\\n    \"science\",\\n    \"scissors\",\\n    \"screw\",\\n    \"sea\",\\n    \"seat\",\\n    \"secretary\",\\n    \"seed\",\\n    \"selection\",\\n    \"self\",\\n    \"sense\",\\n    \"servant\",' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\name_generation.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"shade\",\\n    \"shake\",\\n    \"shame\",\\n    \"shape\",\\n    \"sheep\",\\n    \"sheet\",\\n    \"shelf\",\\n    \"ship\",\\n    \"shirt\",\\n    \"shock\",\\n    \"shoe\",\\n    \"shop\",\\n    \"show\",\\n    \"side\",\\n    \"sign\",\\n    \"silk\",\\n    \"sink\",\\n    \"sister\",\\n    \"size\",\\n    \"sky\",\\n    \"sleep\",\\n    \"smash\",\\n    \"smell\",\\n    \"smile\",\\n    \"smoke\",\\n    \"snail\",\\n    \"snake\",\\n    \"sneeze\",\\n    \"snow\",\\n    \"soap\",\\n    \"society\",\\n    \"sock\",\\n    \"soda\",\\n    \"sofa\",\\n    \"son\",\\n    \"song\",\\n    \"sort\",\\n    \"sound\",\\n    \"soup\",\\n    \"space\",\\n    \"spark\",\\n    \"speed\",\\n    \"sponge\",\\n    \"spoon\",\\n    \"spray\",\\n    \"spring\",\\n    \"spy\",\\n    \"square\",\\n    \"stamp\",\\n    \"star\",\\n    \"start\",\\n    \"statement\",\\n    \"station\",\\n    \"steam\",\\n    \"steel\",\\n    \"stem\",\\n    \"step\",\\n    \"stew\",\\n    \"stick\",\\n    \"stitch\",\\n    \"stocking\",\\n    \"stomach\",\\n    \"stone\",\\n    \"stop\",\\n    \"store\",\\n    \"story\",\\n    \"stove\",\\n    \"stranger\",\\n    \"straw\",\\n    \"stream\",\\n    \"street\",\\n    \"stretch\",\\n    \"string\",\\n    \"structure\",\\n    \"substance\",\\n    \"sugar\",\\n    \"suggestion\",\\n    \"suit\",\\n    \"summer\",\\n    \"sun\",\\n    \"support\",\\n    \"surprise\",\\n    \"sweater\",\\n    \"swim\",\\n    \"system\",\\n    \"table\",\\n    \"tail\",\\n    \"talk\",\\n    \"tank\",\\n    \"taste\",\\n    \"tax\",\\n    \"tea\",\\n    \"teaching\",\\n    \"team\",\\n    \"tendency\",\\n    \"test\",\\n    \"texture\",\\n    \"theory\",\\n    \"thing\",\\n    \"thought\",\\n    \"thread\",\\n    \"throat\",\\n    \"thumb\",\\n    \"thunder\",\\n    \"ticket\",\\n    \"time\",\\n    \"tin\",\\n    \"title\",\\n    \"toad\",\\n    \"toe\",\\n    \"tooth\",\\n    \"toothpaste\",\\n    \"touch\",\\n    \"town\",\\n    \"toy\",\\n    \"trade\",\\n    \"train\",\\n    \"transport\",\\n    \"tray\",\\n    \"treatment\",\\n    \"tree\",\\n    \"trick\",\\n    \"trip\",\\n    \"trouble\",\\n    \"trousers\",\\n    \"truck\",\\n    \"tub\",\\n    \"turkey\",\\n    \"turn\",\\n    \"twist\",\\n    \"umbrella\",\\n    \"uncle\",\\n    \"underwear\",\\n    \"unit\",\\n    \"use\",\\n    \"vacation\",\\n    \"value\",\\n    \"van\",\\n    \"vase\",\\n    \"vegetable\",\\n    \"veil\",\\n    \"vein\",\\n    \"verse\",\\n    \"vessel\",\\n    \"view\",\\n    \"visitor\",\\n    \"voice\",\\n    \"volcano\",\\n    \"walk\",\\n    \"wall\",\\n    \"war\",\\n    \"wash\",\\n    \"waste\",\\n    \"watch\",\\n    \"water\",' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\name_generation.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"wave\",\\n    \"wax\",\\n    \"way\",\\n    \"wealth\",\\n    \"weather\",\\n    \"week\",\\n    \"weight\",\\n    \"wheel\",\\n    \"whip\",\\n    \"whistle\",\\n    \"window\",\\n    \"wine\",\\n    \"wing\",\\n    \"winter\",\\n    \"wire\",\\n    \"wish\",\\n    \"woman\",\\n    \"wood\",\\n    \"wool\",\\n    \"word\",\\n    \"work\",\\n    \"worm\",\\n    \"wound\",\\n    \"wrist\",\\n    \"writer\",\\n    \"yard\",\\n    \"yoke\",\\n    \"zebra\",\\n    \"zinc\",\\n    \"zipper\",\\n    \"zone\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\name_generation.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# Code for: def random_name() -> str:' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\name_generation.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"A simple progress bar for the console.\"\"\"\\nimport threading\\nfrom typing import Any, Dict, Optional, Sequence\\nfrom uuid import UUID\\n\\nfrom langchain_core.documents import Document\\nfrom langchain_core.outputs import LLMResult\\n\\nfrom langchain.callbacks import base as base_callbacks\\n\\n\\nclass ProgressBarCallback(base_callbacks.BaseCallbackHandler):\\n    \"\"\"A simple progress bar for the console.\"\"\"\\n\\n    def __init__(self, total: int, ncols: int = 50, **kwargs: Any):\\n        \"\"\"Initialize the progress bar.\\n\\n        Args:\\n            total: int, the total number of items to be processed.\\n            ncols: int, the character width of the progress bar.\\n        \"\"\"\\n        self.total = total\\n        self.ncols = ncols\\n        self.counter = 0\\n        self.lock = threading.Lock()\\n        self._print_bar()\\n\\n    def increment(self) -> None:\\n        \"\"\"Increment the counter and update the progress bar.\"\"\"\\n        with self.lock:\\n            self.counter += 1\\n            self._print_bar()\\n\\n    def _print_bar(self) -> None:\\n        \"\"\"Print the progress bar to the console.\"\"\"\\n        progress = self.counter / self.total\\n        arrow = \"-\" * int(round(progress * self.ncols) - 1) + \">\"\\n        spaces = \" \" * (self.ncols - len(arrow))\\n        print(f\"\\\\r[{arrow + spaces}] {self.counter}/{self.total}\", end=\"\")\\n\\n    def on_chain_error(\\n        self,\\n        error: BaseException,\\n        *,\\n        run_id: UUID,\\n        parent_run_id: Optional[UUID] = None,\\n        **kwargs: Any,\\n    ) -> Any:\\n        if parent_run_id is None:\\n            self.increment()\\n\\n    def on_chain_end(\\n        self,\\n        outputs: Dict[str, Any],\\n        *,\\n        run_id: UUID,\\n        parent_run_id: Optional[UUID] = None,\\n        **kwargs: Any,\\n    ) -> Any:\\n        if parent_run_id is None:\\n            self.increment()' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\progress.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def on_retriever_error(\\n        self,\\n        error: BaseException,\\n        *,\\n        run_id: UUID,\\n        parent_run_id: Optional[UUID] = None,\\n        **kwargs: Any,\\n    ) -> Any:\\n        if parent_run_id is None:\\n            self.increment()\\n\\n    def on_retriever_end(\\n        self,\\n        documents: Sequence[Document],\\n        *,\\n        run_id: UUID,\\n        parent_run_id: Optional[UUID] = None,\\n        **kwargs: Any,\\n    ) -> Any:\\n        if parent_run_id is None:\\n            self.increment()\\n\\n    def on_llm_error(\\n        self,\\n        error: BaseException,\\n        *,\\n        run_id: UUID,\\n        parent_run_id: Optional[UUID] = None,\\n        **kwargs: Any,\\n    ) -> Any:\\n        if parent_run_id is None:\\n            self.increment()\\n\\n    def on_llm_end(\\n        self,\\n        response: LLMResult,\\n        *,\\n        run_id: UUID,\\n        parent_run_id: Optional[UUID] = None,\\n        **kwargs: Any,\\n    ) -> Any:\\n        if parent_run_id is None:\\n            self.increment()\\n\\n    def on_tool_error(\\n        self,\\n        error: BaseException,\\n        *,\\n        run_id: UUID,\\n        parent_run_id: Optional[UUID] = None,\\n        **kwargs: Any,\\n    ) -> Any:\\n        if parent_run_id is None:\\n            self.increment()\\n\\n    def on_tool_end(\\n        self,\\n        output: str,\\n        *,\\n        run_id: UUID,\\n        parent_run_id: Optional[UUID] = None,\\n        **kwargs: Any,\\n    ) -> Any:\\n        if parent_run_id is None:\\n            self.increment()' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\progress.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='class InputFormatError(Exception):\\n    \"\"\"Raised when the input format is invalid.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\runner_utils.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='class TestResult(dict):\\n    \"\"\"A dictionary of the results of a single test run.\"\"\"\\n\\n    def get_aggregate_feedback(\\n        self,\\n    ) -> pd.DataFrame:\\n        \"\"\"Return quantiles for the feedback scores.\\n\\n        This method calculates and prints the quantiles for the feedback scores\\n        across all feedback keys.\\n\\n        Returns:\\n            A DataFrame containing the quantiles for each feedback key.\\n        \"\"\"\\n        df = self.to_dataframe()\\n        # Drop all things starting with inputs., outputs., and reference\\n        to_drop = [\\n            col\\n            for col in df.columns\\n            if col.startswith(\"inputs.\")\\n            or col.startswith(\"outputs.\")\\n            or col in {\"input\", \"output\"}\\n            or col.startswith(\"reference\")\\n        ]\\n        return df.describe(include=\"all\").drop(to_drop, axis=1)\\n\\n    def to_dataframe(self) -> pd.DataFrame:\\n        \"\"\"Convert the results to a dataframe.\"\"\"\\n        try:\\n            import pandas as pd\\n        except ImportError as e:\\n            raise ImportError(\\n                \"Pandas is required to convert the results to a dataframe.\"\\n                \" to install pandas, run `pip install pandas`.\"\\n            ) from e\\n\\n        indices = []\\n        records = []\\n        for example_id, result in self[\"results\"].items():\\n            feedback = result[\"feedback\"]\\n            output_ = result.get(\"output\")\\n            if isinstance(output_, dict):\\n                output = {f\"outputs.{k}\": v for k, v in output_.items()}\\n            elif output_ is None:\\n                output = {}\\n            else:\\n                output = {\"output\": output_}' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\runner_utils.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='r = {\\n                **{f\"inputs.{k}\": v for k, v in result[\"input\"].items()},\\n                **output,\\n            }\\n            if \"reference\" in result:\\n                if isinstance(result[\"reference\"], dict):\\n                    r.update(\\n                        {f\"reference.{k}\": v for k, v in result[\"reference\"].items()}\\n                    )\\n                else:\\n                    r[\"reference\"] = result[\"reference\"]\\n            r.update(\\n                {\\n                    **{f\"feedback.{f.key}\": f.score for f in feedback},\\n                    \"error\": result.get(\"Error\"),\\n                    \"execution_time\": result[\"execution_time\"],\\n                    \"run_id\": result.get(\"run_id\"),\\n                }\\n            )\\n            records.append(r)\\n            indices.append(example_id)\\n\\n        return pd.DataFrame(records, index=indices)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\runner_utils.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='class EvalError(dict):\\n    \"\"\"Your architecture raised an error.\"\"\"\\n\\n    def __init__(self, Error: BaseException, **kwargs: Any) -> None:\\n        super().__init__(Error=Error, **kwargs)\\n\\n    def __getattr__(self, name: str) -> Any:\\n        try:\\n            return self[name]\\n        except KeyError:\\n            raise AttributeError(f\"\\'EvalError\\' object has no attribute \\'{name}\\'\")' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\runner_utils.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _wrap_in_chain_factory(\\n    llm_or_chain_factory: MODEL_OR_CHAIN_FACTORY,\\n    dataset_name: str = \"<my_dataset>\",\\n) -> MCF:\\n    \"\"\"Forgive the user if they pass in a chain without memory instead of a chain\\n    factory. It\\'s a common mistake. Raise a more helpful error message as well.\"\"\"\\n    if isinstance(llm_or_chain_factory, Chain):\\n        chain = llm_or_chain_factory\\n        chain_class = chain.__class__.__name__\\n        if llm_or_chain_factory.memory is not None:\\n            memory_class = chain.memory.__class__.__name__\\n            raise ValueError(\\n                \"Cannot directly evaluate a chain with stateful memory.\"\\n                \" To evaluate this chain, pass in a chain constructor\"\\n                \" that initializes fresh memory each time it is called.\"\\n                \"  This will safegaurd against information\"\\n                \" leakage between dataset examples.\"\\n                \"\\\\nFor example:\\\\n\\\\n\"\\n                \"def chain_constructor():\\\\n\"\\n                f\"    new_memory = {memory_class}(...)\\\\n\"\\n                f\"    return {chain_class}\"\\n                \"(memory=new_memory, ...)\\\\n\\\\n\"\\n                f\\'run_on_dataset(\"{dataset_name}\", chain_constructor, ...)\\'\\n            )\\n        return lambda: chain\\n    elif isinstance(llm_or_chain_factory, BaseLanguageModel):\\n        return llm_or_chain_factory\\n    elif isinstance(llm_or_chain_factory, Runnable):\\n        # Memory may exist here, but it\\'s not elegant to check all those cases.\\n        lcf = llm_or_chain_factory\\n        return lambda: lcf\\n    elif callable(llm_or_chain_factory):\\n        if is_traceable_function(llm_or_chain_factory):\\n            runnable_ = as_runnable(cast(Callable, llm_or_chain_factory))\\n            return lambda: runnable_\\n        try:\\n            _model = llm_or_chain_factory()  # type: ignore[call-arg]\\n        except TypeError:\\n            # It\\'s an arbitrary function, wrap it in a RunnableLambda\\n            user_func = cast(Callable, llm_or_chain_factory)\\n            sig = inspect.signature(user_func)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\runner_utils.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='logger.info(f\"Wrapping function {sig} as RunnableLambda.\")\\n            wrapped = RunnableLambda(user_func)\\n            return lambda: wrapped\\n        constructor = cast(Callable, llm_or_chain_factory)\\n        if isinstance(_model, BaseLanguageModel):\\n            # It\\'s not uncommon to do an LLM constructor instead of raw LLM,\\n            # so we\\'ll unpack it for the user.\\n            return _model\\n        elif is_traceable_function(cast(Callable, _model)):\\n            runnable_ = as_runnable(cast(Callable, _model))\\n            return lambda: runnable_\\n        elif not isinstance(_model, Runnable):\\n            # This is unlikely to happen - a constructor for a model function\\n            return lambda: RunnableLambda(constructor)\\n        else:\\n            # Typical correct case\\n            return constructor  # noqa\\n    return llm_or_chain_factory' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\runner_utils.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_prompt(inputs: Dict[str, Any]) -> str:\\n    \"\"\"Get prompt from inputs.\\n\\n    Args:\\n        inputs: The input dictionary.\\n\\n    Returns:\\n        A string prompt.\\n    Raises:\\n        InputFormatError: If the input format is invalid.\\n    \"\"\"\\n    if not inputs:\\n        raise InputFormatError(\"Inputs should not be empty.\")\\n\\n    prompts = []\\n    if \"prompt\" in inputs:\\n        if not isinstance(inputs[\"prompt\"], str):\\n            raise InputFormatError(\\n                \"Expected string for \\'prompt\\', got\"\\n                f\" {type(inputs[\\'prompt\\']).__name__}\"\\n            )\\n        prompts = [inputs[\"prompt\"]]\\n    elif \"prompts\" in inputs:\\n        if not isinstance(inputs[\"prompts\"], list) or not all(\\n            isinstance(i, str) for i in inputs[\"prompts\"]\\n        ):\\n            raise InputFormatError(\\n                \"Expected list of strings for \\'prompts\\',\"\\n                f\" got {type(inputs[\\'prompts\\']).__name__}\"\\n            )\\n        prompts = inputs[\"prompts\"]\\n    elif len(inputs) == 1:\\n        prompt_ = next(iter(inputs.values()))\\n        if isinstance(prompt_, str):\\n            prompts = [prompt_]\\n        elif isinstance(prompt_, list) and all(isinstance(i, str) for i in prompt_):\\n            prompts = prompt_\\n        else:\\n            raise InputFormatError(f\"LLM Run expects string prompt input. Got {inputs}\")\\n    else:\\n        raise InputFormatError(\\n            f\"LLM Run expects \\'prompt\\' or \\'prompts\\' in inputs. Got {inputs}\"\\n        )\\n    if len(prompts) == 1:\\n        return prompts[0]\\n    else:\\n        raise InputFormatError(\\n            f\"LLM Run expects single prompt input. Got {len(prompts)} prompts.\"\\n        )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\runner_utils.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_messages(inputs: Dict[str, Any]) -> List[BaseMessage]:\\n    \"\"\"Get Chat Messages from inputs.\\n\\n    Args:\\n        inputs: The input dictionary.\\n\\n    Returns:\\n        A list of chat messages.\\n    Raises:\\n        InputFormatError: If the input format is invalid.\\n    \"\"\"\\n    if not inputs:\\n        raise InputFormatError(\"Inputs should not be empty.\")\\n\\n    if \"messages\" in inputs:\\n        single_input = inputs[\"messages\"]\\n    elif len(inputs) == 1:\\n        single_input = next(iter(inputs.values()))\\n    else:\\n        raise InputFormatError(\\n            f\"Chat Run expects \\'messages\\' in inputs when example has multiple\"\\n            f\" input keys. Got {inputs}\"\\n        )\\n    if isinstance(single_input, list) and all(\\n        isinstance(i, dict) for i in single_input\\n    ):\\n        raw_messages = [single_input]\\n    elif isinstance(single_input, list) and all(\\n        isinstance(i, list) for i in single_input\\n    ):\\n        raw_messages = single_input\\n    else:\\n        raise InputFormatError(\\n            f\"Chat Run expects List[dict] or List[List[dict]] values for\"\\n            f\" \\'messages\\' key input. Got {inputs}\"\\n        )\\n    if len(raw_messages) == 1:\\n        return messages_from_dict(raw_messages[0])\\n    else:\\n        raise InputFormatError(\\n            f\"Chat Run expects single List[dict] or List[List[dict]] \\'messages\\'\"\\n            f\" input. Got {len(raw_messages)} messages from inputs {inputs}\"\\n        )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\runner_utils.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _validate_example_inputs_for_language_model(\\n    first_example: Example,\\n    input_mapper: Optional[Callable[[Dict], Any]],\\n) -> None:\\n    if input_mapper:\\n        prompt_input = input_mapper(first_example.inputs)\\n        if not isinstance(prompt_input, str) and not (\\n            isinstance(prompt_input, list)\\n            and all(isinstance(msg, BaseMessage) for msg in prompt_input)\\n        ):\\n            raise InputFormatError(\\n                \"When using an input_mapper to prepare dataset example inputs\"\\n                \" for an LLM or chat model, the output must a single string or\"\\n                \" a list of chat messages.\"\\n                f\"\\\\nGot: {prompt_input} of type {type(prompt_input)}.\"\\n            )\\n    else:\\n        try:\\n            _get_prompt(first_example.inputs)\\n        except InputFormatError:\\n            try:\\n                _get_messages(first_example.inputs)\\n            except InputFormatError:\\n                raise InputFormatError(\\n                    \"Example inputs do not match language model input format. \"\\n                    \"Expected a dictionary with messages or a single prompt.\"\\n                    f\" Got: {first_example.inputs}\"\\n                    \" Please update your dataset OR provide an input_mapper\"\\n                    \" to convert the example.inputs to a compatible format\"\\n                    \" for the llm or chat model you wish to evaluate.\"\\n                )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\runner_utils.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _validate_example_inputs_for_chain(\\n    first_example: Example,\\n    chain: Chain,\\n    input_mapper: Optional[Callable[[Dict], Any]],\\n) -> None:\\n    \"\"\"Validate that the example inputs match the chain input keys.\"\"\"\\n    if input_mapper:\\n        first_inputs = input_mapper(first_example.inputs)\\n        missing_keys = set(chain.input_keys).difference(first_inputs)\\n        if not isinstance(first_inputs, dict):\\n            raise InputFormatError(\\n                \"When using an input_mapper to prepare dataset example\"\\n                \" inputs for a chain, the mapped value must be a dictionary.\"\\n                f\"\\\\nGot: {first_inputs} of type {type(first_inputs)}.\"\\n            )\\n        if missing_keys:\\n            raise InputFormatError(\\n                \"Missing keys after loading example using input_mapper.\"\\n                f\"\\\\nExpected: {chain.input_keys}. Got: {first_inputs.keys()}\"\\n            )\\n    else:\\n        first_inputs = first_example.inputs\\n        missing_keys = set(chain.input_keys).difference(first_inputs)\\n        if len(first_inputs) == 1 and len(chain.input_keys) == 1:\\n            # We can pass this through the run method.\\n            # Refrain from calling to validate.\\n            pass\\n        elif missing_keys:\\n            raise InputFormatError(\\n                \"Example inputs missing expected chain input keys.\"\\n                \" Please provide an input_mapper to convert the example.inputs\"\\n                \" to a compatible format for the chain you wish to evaluate.\"\\n                f\"Expected: {chain.input_keys}. \"\\n                f\"Got: {first_inputs.keys()}\"\\n            )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\runner_utils.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _validate_example_inputs(\\n    example: Example,\\n    llm_or_chain_factory: MCF,\\n    input_mapper: Optional[Callable[[Dict], Any]],\\n) -> None:\\n    \"\"\"Validate that the example inputs are valid for the model.\"\"\"\\n    if isinstance(llm_or_chain_factory, BaseLanguageModel):\\n        _validate_example_inputs_for_language_model(example, input_mapper)\\n    else:\\n        chain = llm_or_chain_factory()\\n        if isinstance(chain, Chain):\\n            # Otherwise it\\'s a runnable\\n            _validate_example_inputs_for_chain(example, chain, input_mapper)\\n        elif isinstance(chain, Runnable):\\n            logger.debug(f\"Skipping input validation for {chain}\")' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\runner_utils.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _setup_evaluation(\\n    llm_or_chain_factory: MCF,\\n    examples: List[Example],\\n    evaluation: Optional[smith_eval.RunEvalConfig],\\n    data_type: DataType,\\n) -> Optional[List[RunEvaluator]]:\\n    \"\"\"Configure the evaluators to run on the results of the chain.\"\"\"\\n    if evaluation:\\n        if isinstance(llm_or_chain_factory, BaseLanguageModel):\\n            run_inputs, run_outputs = None, None\\n            run_type = \"llm\"\\n        else:\\n            run_type = \"chain\"\\n            if data_type in (DataType.chat, DataType.llm):\\n                val = data_type.value if isinstance(data_type, Enum) else data_type\\n                raise ValueError(\\n                    \"Cannot evaluate a chain on dataset with \"\\n                    f\"data_type={val}. \"\\n                    \"Please specify a dataset with the default \\'kv\\' data type.\"\\n                )\\n            chain = llm_or_chain_factory()\\n            run_inputs = chain.input_keys if isinstance(chain, Chain) else None\\n            run_outputs = chain.output_keys if isinstance(chain, Chain) else None\\n        run_evaluators = _load_run_evaluators(\\n            evaluation,\\n            run_type,\\n            data_type,\\n            list(examples[0].outputs) if examples[0].outputs else None,\\n            run_inputs,\\n            run_outputs,\\n        )\\n    else:\\n        # TODO: Create a default helpfulness evaluator\\n        run_evaluators = None\\n    return run_evaluators' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\runner_utils.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _determine_input_key(\\n    config: smith_eval.RunEvalConfig,\\n    run_inputs: Optional[List[str]],\\n) -> Optional[str]:\\n    input_key = None\\n    if config.input_key:\\n        input_key = config.input_key\\n        if run_inputs and input_key not in run_inputs:\\n            logger.warning(\\n                f\"Input key {input_key} not in chain\\'s specified\"\\n                f\" input keys {run_inputs}. Evaluation behavior may be undefined.\"\\n            )\\n    elif run_inputs and len(run_inputs) == 1:\\n        input_key = run_inputs[0]\\n    elif run_inputs is not None and len(run_inputs) > 1:\\n        logger.warning(\\n            f\"Chain expects multiple input keys: {run_inputs},\"\\n            f\" Evaluator is likely to fail. Evaluation behavior may be undefined.\"\\n            \" Specify an input_key in the RunEvalConfig to avoid this warning.\"\\n        )\\n\\n    return input_key' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\runner_utils.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _determine_prediction_key(\\n    config: smith_eval.RunEvalConfig,\\n    run_outputs: Optional[List[str]],\\n) -> Optional[str]:\\n    prediction_key = None\\n    if config.prediction_key:\\n        prediction_key = config.prediction_key\\n        if run_outputs and prediction_key not in run_outputs:\\n            logger.warning(\\n                f\"Prediction key {prediction_key} not in chain\\'s specified\"\\n                f\" output keys {run_outputs}. Evaluation behavior may be undefined.\"\\n            )\\n    elif run_outputs and len(run_outputs) == 1:\\n        prediction_key = run_outputs[0]\\n    elif run_outputs is not None and len(run_outputs) > 1:\\n        logger.warning(\\n            f\"Chain expects multiple output keys: {run_outputs},\"\\n            f\" Evaluation behavior may be undefined. Specify a prediction_key\"\\n            \" in the RunEvalConfig to avoid this warning.\"\\n        )\\n    return prediction_key' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\runner_utils.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _determine_reference_key(\\n    config: smith_eval.RunEvalConfig,\\n    example_outputs: Optional[List[str]],\\n) -> Optional[str]:\\n    if config.reference_key:\\n        reference_key = config.reference_key\\n        if example_outputs and reference_key not in example_outputs:\\n            raise ValueError(\\n                f\"Reference key {reference_key} not in Dataset\"\\n                f\" example outputs: {example_outputs}\"\\n            )\\n    elif example_outputs and len(example_outputs) == 1:\\n        reference_key = list(example_outputs)[0]\\n    else:\\n        reference_key = None\\n    return reference_key' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\runner_utils.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _construct_run_evaluator(\\n    eval_config: Union[EvaluatorType, str, smith_eval_config.EvalConfig],\\n    eval_llm: Optional[BaseLanguageModel],\\n    run_type: str,\\n    data_type: DataType,\\n    example_outputs: Optional[List[str]],\\n    reference_key: Optional[str],\\n    input_key: Optional[str],\\n    prediction_key: Optional[str],\\n) -> RunEvaluator:\\n    if isinstance(eval_config, (EvaluatorType, str)):\\n        if not isinstance(eval_config, EvaluatorType):\\n            eval_config = EvaluatorType(eval_config)\\n        evaluator_ = load_evaluator(eval_config, llm=eval_llm)\\n        eval_type_tag = eval_config.value\\n    else:\\n        kwargs = {\"llm\": eval_llm, **eval_config.get_kwargs()}\\n        evaluator_ = load_evaluator(eval_config.evaluator_type, **kwargs)\\n        eval_type_tag = eval_config.evaluator_type.value\\n        # Override keys if specified in the config\\n        if isinstance(eval_config, smith_eval_config.SingleKeyEvalConfig):\\n            input_key = eval_config.input_key or input_key\\n            prediction_key = eval_config.prediction_key or prediction_key\\n            reference_key = eval_config.reference_key or reference_key' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\runner_utils.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='if isinstance(evaluator_, StringEvaluator):\\n        if evaluator_.requires_reference and reference_key is None:\\n            raise ValueError(\\n                f\"Must specify reference_key in smith_eval.RunEvalConfig to use\"\\n                f\" evaluator of type {eval_type_tag} with\"\\n                f\" dataset with multiple output keys: {example_outputs}.\"\\n            )\\n        run_evaluator = smith_eval.StringRunEvaluatorChain.from_run_and_data_type(\\n            evaluator_,\\n            run_type,\\n            data_type,\\n            input_key=input_key,\\n            prediction_key=prediction_key,\\n            reference_key=reference_key,\\n            tags=[eval_type_tag],\\n        )\\n    elif isinstance(evaluator_, PairwiseStringEvaluator):\\n        raise NotImplementedError(\\n            f\"Run evaluator for {eval_type_tag} is not implemented.\"\\n            \" PairwiseStringEvaluators compare the outputs of two different models\"\\n            \" rather than the output of a single model.\"\\n            \" Did you mean to use a StringEvaluator instead?\"\\n            \"\\\\nSee: https://python.langchain.com/docs/guides/evaluation/string/\"\\n        )\\n\\n    else:\\n        raise NotImplementedError(\\n            f\"Run evaluator for {eval_type_tag} is not implemented\"\\n        )\\n    return run_evaluator' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\runner_utils.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _get_keys(\\n    config: smith_eval.RunEvalConfig,\\n    run_inputs: Optional[List[str]],\\n    run_outputs: Optional[List[str]],\\n    example_outputs: Optional[List[str]],\\n) -> Tuple[Optional[str], Optional[str], Optional[str]]:\\n    input_key = _determine_input_key(config, run_inputs)\\n    prediction_key = _determine_prediction_key(config, run_outputs)\\n    reference_key = _determine_reference_key(config, example_outputs)\\n    return input_key, prediction_key, reference_key' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\runner_utils.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _load_run_evaluators(\\n    config: smith_eval.RunEvalConfig,\\n    run_type: str,\\n    data_type: DataType,\\n    example_outputs: Optional[List[str]],\\n    run_inputs: Optional[List[str]],\\n    run_outputs: Optional[List[str]],\\n) -> List[RunEvaluator]:\\n    \"\"\"\\n    Load run evaluators from a configuration.\\n\\n    Args:\\n        config: Configuration for the run evaluators.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\runner_utils.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Returns:\\n        A list of run evaluators.\\n    \"\"\"\\n    run_evaluators = []\\n    input_key, prediction_key, reference_key = None, None, None\\n    if (\\n        config.evaluators\\n        or any([isinstance(e, EvaluatorType) for e in config.evaluators])\\n        or (\\n            config.custom_evaluators\\n            and any([isinstance(e, StringEvaluator) for e in config.custom_evaluators])\\n        )\\n    ):\\n        input_key, prediction_key, reference_key = _get_keys(\\n            config, run_inputs, run_outputs, example_outputs\\n        )\\n    for eval_config in config.evaluators:\\n        run_evaluator = _construct_run_evaluator(\\n            eval_config,\\n            config.eval_llm,\\n            run_type,\\n            data_type,\\n            example_outputs,\\n            reference_key,\\n            input_key,\\n            prediction_key,\\n        )\\n        run_evaluators.append(run_evaluator)\\n    custom_evaluators = config.custom_evaluators or []\\n    for custom_evaluator in custom_evaluators:\\n        if isinstance(custom_evaluator, RunEvaluator):\\n            run_evaluators.append(custom_evaluator)\\n        elif isinstance(custom_evaluator, StringEvaluator):\\n            run_evaluators.append(\\n                smith_eval.StringRunEvaluatorChain.from_run_and_data_type(\\n                    custom_evaluator,\\n                    run_type,\\n                    data_type,\\n                    input_key=input_key,\\n                    prediction_key=prediction_key,\\n                    reference_key=reference_key,\\n                )\\n            )\\n        else:\\n            raise ValueError(\\n                f\"Unsupported custom evaluator: {custom_evaluator}.\"\\n                f\" Expected RunEvaluator or StringEvaluator.\"\\n            )\\n\\n    return run_evaluators' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\runner_utils.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='async def _arun_llm(\\n    llm: BaseLanguageModel,\\n    inputs: Dict[str, Any],\\n    *,\\n    tags: Optional[List[str]] = None,\\n    callbacks: Callbacks = None,\\n    input_mapper: Optional[Callable[[Dict], Any]] = None,\\n    metadata: Optional[Dict[str, Any]] = None,\\n) -> Union[str, BaseMessage]:\\n    \"\"\"Asynchronously run the language model.\\n\\n    Args:\\n        llm: The language model to run.\\n        inputs: The input dictionary.\\n        tags: Optional tags to add to the run.\\n        callbacks: Optional callbacks to use during the run.\\n        input_mapper: Optional function to map inputs to the expected format.\\n\\n    Returns:\\n        The LLMResult or ChatResult.\\n    Raises:\\n        ValueError: If the LLM type is unsupported.\\n        InputFormatError: If the input format is invalid.\\n    \"\"\"\\n    if input_mapper is not None:\\n        prompt_or_messages = input_mapper(inputs)\\n        if (\\n            isinstance(prompt_or_messages, str)\\n            or isinstance(prompt_or_messages, list)\\n            and all(isinstance(msg, BaseMessage) for msg in prompt_or_messages)\\n        ):\\n            return await llm.ainvoke(\\n                prompt_or_messages,\\n                config=RunnableConfig(\\n                    callbacks=callbacks, tags=tags or [], metadata=metadata or {}\\n                ),\\n            )\\n        else:\\n            raise InputFormatError(\\n                \"Input mapper returned invalid format\"\\n                f\" {prompt_or_messages}\"\\n                \"\\\\nExpected a single string or list of chat messages.\"\\n            )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\runner_utils.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='else:\\n        try:\\n            prompt = _get_prompt(inputs)\\n            llm_output: Union[str, BaseMessage] = await llm.ainvoke(\\n                prompt,\\n                config=RunnableConfig(\\n                    callbacks=callbacks, tags=tags or [], metadata=metadata or {}\\n                ),\\n            )\\n        except InputFormatError:\\n            messages = _get_messages(inputs)\\n            llm_output = await llm.ainvoke(\\n                messages,\\n                config=RunnableConfig(\\n                    callbacks=callbacks, tags=tags or [], metadata=metadata or {}\\n                ),\\n            )\\n    return llm_output' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\runner_utils.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='async def _arun_chain(\\n    chain: Union[Chain, Runnable],\\n    inputs: Dict[str, Any],\\n    callbacks: Callbacks,\\n    *,\\n    tags: Optional[List[str]] = None,\\n    input_mapper: Optional[Callable[[Dict], Any]] = None,\\n    metadata: Optional[Dict[str, Any]] = None,\\n) -> Union[dict, str]:\\n    \"\"\"Run a chain asynchronously on inputs.\"\"\"\\n    inputs_ = inputs if input_mapper is None else input_mapper(inputs)\\n    if (\\n        isinstance(chain, Chain)\\n        and isinstance(inputs_, dict)\\n        and len(inputs_) == 1\\n        and chain.input_keys\\n    ):\\n        val = next(iter(inputs_.values()))\\n        output = await chain.ainvoke(\\n            val,\\n            config=RunnableConfig(\\n                callbacks=callbacks, tags=tags or [], metadata=metadata or {}\\n            ),\\n        )\\n    else:\\n        runnable_config = RunnableConfig(\\n            tags=tags or [], callbacks=callbacks, metadata=metadata or {}\\n        )\\n        output = await chain.ainvoke(inputs_, config=runnable_config)\\n    return output' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\runner_utils.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='async def _arun_llm_or_chain(\\n    example: Example,\\n    config: RunnableConfig,\\n    *,\\n    llm_or_chain_factory: MCF,\\n    input_mapper: Optional[Callable[[Dict], Any]] = None,\\n) -> Union[dict, str, LLMResult, ChatResult]:\\n    \"\"\"Asynchronously run the Chain or language model.\\n\\n    Args:\\n        example: The example to run.\\n        llm_or_chain_factory: The Chain or language model constructor to run.\\n        tags: Optional tags to add to the run.\\n        callbacks: Optional callbacks to use during the run.\\n        input_mapper: Optional function to map the input to the expected format.\\n\\n    Returns:\\n        A list of outputs.\\n    \"\"\"\\n    chain_or_llm = (\\n        \"LLM\" if isinstance(llm_or_chain_factory, BaseLanguageModel) else \"Chain\"\\n    )\\n    result = None\\n    try:\\n        if isinstance(llm_or_chain_factory, BaseLanguageModel):\\n            output: Any = await _arun_llm(\\n                llm_or_chain_factory,\\n                example.inputs,\\n                tags=config[\"tags\"],\\n                callbacks=config[\"callbacks\"],\\n                input_mapper=input_mapper,\\n                metadata=config.get(\"metadata\"),\\n            )\\n        else:\\n            chain = llm_or_chain_factory()\\n            output = await _arun_chain(\\n                chain,\\n                example.inputs,\\n                tags=config[\"tags\"],\\n                callbacks=config[\"callbacks\"],\\n                input_mapper=input_mapper,\\n                metadata=config.get(\"metadata\"),\\n            )\\n        result = output\\n    except Exception as e:\\n        logger.warning(\\n            f\"{chain_or_llm} failed for example {example.id} \"\\n            f\"with inputs {example.inputs}\"\\n            f\"\\\\n{repr(e)}\"\\n        )\\n        result = EvalError(Error=e)\\n    return result' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\runner_utils.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _run_llm(\\n    llm: BaseLanguageModel,\\n    inputs: Dict[str, Any],\\n    callbacks: Callbacks,\\n    *,\\n    tags: Optional[List[str]] = None,\\n    input_mapper: Optional[Callable[[Dict], Any]] = None,\\n    metadata: Optional[Dict[str, Any]] = None,\\n) -> Union[str, BaseMessage]:\\n    \"\"\"\\n    Run the language model on the example.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\runner_utils.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Args:\\n        llm: The language model to run.\\n        inputs: The input dictionary.\\n        callbacks: The callbacks to use during the run.\\n        tags: Optional tags to add to the run.\\n        input_mapper: function to map to the inputs dictionary from an Example\\n    Returns:\\n        The LLMResult or ChatResult.\\n    Raises:\\n        ValueError: If the LLM type is unsupported.\\n        InputFormatError: If the input format is invalid.\\n    \"\"\"\\n    # Most of this is legacy code; we could probably remove a lot of it.\\n    if input_mapper is not None:\\n        prompt_or_messages = input_mapper(inputs)\\n        if (\\n            isinstance(prompt_or_messages, str)\\n            or isinstance(prompt_or_messages, list)\\n            and all(isinstance(msg, BaseMessage) for msg in prompt_or_messages)\\n        ):\\n            llm_output: Union[str, BaseMessage] = llm.invoke(\\n                prompt_or_messages,\\n                config=RunnableConfig(\\n                    callbacks=callbacks, tags=tags or [], metadata=metadata or {}\\n                ),\\n            )\\n        else:\\n            raise InputFormatError(\\n                \"Input mapper returned invalid format: \"\\n                f\" {prompt_or_messages}\"\\n                \"\\\\nExpected a single string or list of chat messages.\"\\n            )\\n    else:\\n        try:\\n            llm_prompts = _get_prompt(inputs)\\n            llm_output = llm.invoke(\\n                llm_prompts,\\n                config=RunnableConfig(\\n                    callbacks=callbacks, tags=tags or [], metadata=metadata or {}\\n                ),\\n            )\\n        except InputFormatError:\\n            llm_messages = _get_messages(inputs)\\n            llm_output = llm.invoke(\\n                llm_messages,\\n                config=RunnableConfig(callbacks=callbacks, metadata=metadata or {}),\\n            )\\n    return llm_output' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\runner_utils.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _run_chain(\\n    chain: Union[Chain, Runnable],\\n    inputs: Dict[str, Any],\\n    callbacks: Callbacks,\\n    *,\\n    tags: Optional[List[str]] = None,\\n    input_mapper: Optional[Callable[[Dict], Any]] = None,\\n    metadata: Optional[Dict[str, Any]] = None,\\n) -> Union[Dict, str]:\\n    \"\"\"Run a chain on inputs.\"\"\"\\n    inputs_ = inputs if input_mapper is None else input_mapper(inputs)\\n    if (\\n        isinstance(chain, Chain)\\n        and isinstance(inputs_, dict)\\n        and len(inputs_) == 1\\n        and chain.input_keys\\n    ):\\n        val = next(iter(inputs_.values()))\\n        output = chain.invoke(\\n            val,\\n            config=RunnableConfig(\\n                callbacks=callbacks, tags=tags or [], metadata=metadata or {}\\n            ),\\n        )\\n    else:\\n        runnable_config = RunnableConfig(\\n            tags=tags or [], callbacks=callbacks, metadata=metadata or {}\\n        )\\n        output = chain.invoke(inputs_, config=runnable_config)\\n    return output' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\runner_utils.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _run_llm_or_chain(\\n    example: Example,\\n    config: RunnableConfig,\\n    *,\\n    llm_or_chain_factory: MCF,\\n    input_mapper: Optional[Callable[[Dict], Any]] = None,\\n) -> Union[dict, str, LLMResult, ChatResult]:\\n    \"\"\"\\n    Run the Chain or language model synchronously.\\n\\n    Args:\\n        example: The example to run.\\n        llm_or_chain_factory: The Chain or language model constructor to run.\\n        tags: Optional tags to add to the run.\\n        callbacks: Optional callbacks to use during the run.\\n\\n    Returns:\\n        Union[List[dict], List[str], List[LLMResult], List[ChatResult]]:\\n          The outputs of the model or chain.\\n    \"\"\"\\n    chain_or_llm = (\\n        \"LLM\" if isinstance(llm_or_chain_factory, BaseLanguageModel) else \"Chain\"\\n    )\\n    result = None\\n    try:\\n        if isinstance(llm_or_chain_factory, BaseLanguageModel):\\n            output: Any = _run_llm(\\n                llm_or_chain_factory,\\n                example.inputs,\\n                config[\"callbacks\"],\\n                tags=config[\"tags\"],\\n                input_mapper=input_mapper,\\n                metadata=config.get(\"metadata\"),\\n            )\\n        else:\\n            chain = llm_or_chain_factory()\\n            output = _run_chain(\\n                chain,\\n                example.inputs,\\n                config[\"callbacks\"],\\n                tags=config[\"tags\"],\\n                input_mapper=input_mapper,\\n                metadata=config.get(\"metadata\"),\\n            )\\n        result = output\\n    except Exception as e:\\n        error_type = type(e).__name__\\n        logger.warning(\\n            f\"{chain_or_llm} failed for example {example.id} \"\\n            f\"with inputs {example.inputs}\"\\n            f\"\\\\nError Type: {error_type}, Message: {e}\"\\n        )\\n        result = EvalError(Error=e)\\n    return result' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\runner_utils.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _prepare_eval_run(\\n    client: Client,\\n    dataset_name: str,\\n    llm_or_chain_factory: MODEL_OR_CHAIN_FACTORY,\\n    project_name: str,\\n    project_metadata: Optional[Dict[str, Any]] = None,\\n    tags: Optional[List[str]] = None,\\n) -> Tuple[MCF, TracerSession, Dataset, List[Example]]:\\n    wrapped_model = _wrap_in_chain_factory(llm_or_chain_factory, dataset_name)\\n    dataset = client.read_dataset(dataset_name=dataset_name)\\n    examples = list(client.list_examples(dataset_id=dataset.id))\\n    if not examples:\\n        raise ValueError(f\"Dataset {dataset_name} has no example rows.\")\\n\\n    try:\\n        git_info = get_git_info()\\n        if git_info:\\n            project_metadata = project_metadata or {}\\n            project_metadata = {\\n                **project_metadata,\\n                \"git\": git_info,\\n            }\\n        project = client.create_project(\\n            project_name,\\n            reference_dataset_id=dataset.id,\\n            project_extra={\"tags\": tags} if tags else {},\\n            metadata=project_metadata,\\n        )\\n    except (HTTPError, ValueError, LangSmithError) as e:\\n        if \"already exists \" not in str(e):\\n            raise e\\n        uid = uuid.uuid4()\\n        example_msg = f\"\"\"\\nrun_on_dataset(\\n    ...\\n    project_name=\"{project_name} - {uid}\", # Update since {project_name} already exists\\n)\\n\"\"\"\\n        raise ValueError(\\n            f\"Test project {project_name} already exists. Please use a different name:\"\\n            f\"\\\\n\\\\n{example_msg}\"\\n        )\\n    comparison_url = dataset.url + f\"/compare?selectedSessions={project.id}\"\\n    print(\\n        f\"View the evaluation results for project \\'{project_name}\\'\"\\n        f\" at:\\\\n{comparison_url}\\\\n\\\\n\"\\n        f\"View all tests for Dataset {dataset_name} at:\\\\n{dataset.url}\",\\n        flush=True,\\n    )\\n    return wrapped_model, project, dataset, examples' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\runner_utils.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='class _RowResult(TypedDict, total=False):\\n    \"\"\"A dictionary of the results for a single example row.\"\"\"\\n\\n    feedback: Optional[List[EvaluationResult]]\\n    execution_time: Optional[float]\\n    run_id: Optional[str]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\runner_utils.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='class _DatasetRunContainer:\\n    \"\"\"A container to help manage the state of a eval run.\"\"\"\\n\\n    client: Client\\n    project: TracerSession\\n    wrapped_model: MCF\\n    examples: List[Example]\\n    configs: List[RunnableConfig]\\n\\n    def _merge_test_outputs(\\n        self,\\n        batch_results: list,\\n        all_eval_results: Dict[str, _RowResult],\\n    ) -> dict:\\n        results: dict = {}\\n        for example, output in zip(self.examples, batch_results):\\n            row_result = cast(_RowResult, all_eval_results.get(str(example.id), {}))\\n            results[str(example.id)] = {\\n                \"input\": example.inputs,\\n                \"feedback\": row_result.get(\"feedback\", []),\\n                \"execution_time\": row_result.get(\"execution_time\"),\\n                \"run_id\": row_result.get(\"run_id\"),\\n            }\\n            if isinstance(output, EvalError):\\n                results[str(example.id)][\"Error\"] = output.Error\\n            else:\\n                results[str(example.id)][\"output\"] = output\\n            if example.outputs:\\n                results[str(example.id)][\"reference\"] = example.outputs\\n        return results' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\runner_utils.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _collect_metrics(self) -> Dict[str, _RowResult]:\\n        all_eval_results: dict = {}\\n        for c in self.configs:\\n            for callback in cast(list, c[\"callbacks\"]):\\n                if isinstance(callback, EvaluatorCallbackHandler):\\n                    eval_results = callback.logged_eval_results\\n                    for (_, example_id), v in eval_results.items():\\n                        all_eval_results.setdefault(str(example_id), {}).update(\\n                            {\"feedback\": v}\\n                        )\\n                elif isinstance(callback, LangChainTracer):\\n                    run = callback.latest_run\\n                    execution_time = (\\n                        (run.end_time - run.start_time).total_seconds()\\n                        if run and run.end_time\\n                        else None\\n                    )\\n                    run_id = str(run.id) if run else None\\n                    all_eval_results.setdefault(str(callback.example_id), {}).update(\\n                        {\\n                            \"execution_time\": execution_time,\\n                            \"run_id\": run_id,\\n                        }\\n                    )\\n        return cast(Dict[str, _RowResult], all_eval_results)\\n\\n    def _collect_test_results(\\n        self,\\n        batch_results: List[Union[dict, str, LLMResult, ChatResult]],\\n    ) -> TestResult:\\n        wait_for_all_evaluators()\\n        all_eval_results = self._collect_metrics()\\n        results = self._merge_test_outputs(batch_results, all_eval_results)\\n        return TestResult(\\n            project_name=self.project.name,\\n            results=results,\\n        )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\runner_utils.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def finish(self, batch_results: list, verbose: bool = False) -> TestResult:\\n        results = self._collect_test_results(batch_results)\\n        if verbose:\\n            try:\\n                agg_feedback = results.get_aggregate_feedback()\\n                _display_aggregate_results(agg_feedback)\\n            except Exception as e:\\n                logger.debug(f\"Failed to print aggregate feedback: {repr(e)}\")\\n        try:\\n            # Closing the project permits name changing and metric optimizations\\n            self.client.update_project(\\n                self.project.id, end_time=datetime.now(timezone.utc)\\n            )\\n        except Exception as e:\\n            logger.debug(f\"Failed to close project: {repr(e)}\")\\n        return results' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\runner_utils.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@classmethod\\n    def prepare(\\n        cls,\\n        client: Client,\\n        dataset_name: str,\\n        llm_or_chain_factory: MODEL_OR_CHAIN_FACTORY,\\n        project_name: Optional[str],\\n        evaluation: Optional[smith_eval.RunEvalConfig] = None,\\n        tags: Optional[List[str]] = None,\\n        input_mapper: Optional[Callable[[Dict], Any]] = None,\\n        concurrency_level: int = 5,\\n        project_metadata: Optional[Dict[str, Any]] = None,\\n        revision_id: Optional[str] = None,\\n    ) -> _DatasetRunContainer:\\n        project_name = project_name or name_generation.random_name()\\n        if revision_id:\\n            if not project_metadata:\\n                project_metadata = {}\\n            project_metadata.update({\"revision_id\": revision_id})\\n        wrapped_model, project, dataset, examples = _prepare_eval_run(\\n            client,\\n            dataset_name,\\n            llm_or_chain_factory,\\n            project_name,\\n            project_metadata=project_metadata,\\n            tags=tags,\\n        )\\n        tags = tags or []\\n        for k, v in (project.metadata.get(\"git\") or {}).items():\\n            tags.append(f\"git:{k}={v}\")\\n        wrapped_model = _wrap_in_chain_factory(llm_or_chain_factory)\\n        run_evaluators = _setup_evaluation(\\n            wrapped_model, examples, evaluation, dataset.data_type or DataType.kv\\n        )\\n        _validate_example_inputs(examples[0], wrapped_model, input_mapper)\\n        progress_bar = progress.ProgressBarCallback(len(examples))\\n        configs = [\\n            RunnableConfig(\\n                callbacks=[\\n                    LangChainTracer(\\n                        project_name=project.name,\\n                        client=client,\\n                        use_threading=False,\\n                        example_id=example.id,\\n                    ),\\n                    EvaluatorCallbackHandler(\\n                        evaluators=run_evaluators or [],\\n                        client=client,\\n                        example_id=example.id,\\n                        max_concurrency=0,' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\runner_utils.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='),\\n                    progress_bar,\\n                ],\\n                tags=tags,\\n                max_concurrency=concurrency_level,\\n                metadata={\"revision_id\": revision_id} if revision_id else {},\\n            )\\n            for example in examples\\n        ]\\n        return cls(\\n            client=client,\\n            project=project,\\n            wrapped_model=wrapped_model,\\n            examples=examples,\\n            configs=configs,\\n        )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\runner_utils.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _is_jupyter_environment() -> bool:\\n    try:\\n        from IPython import get_ipython\\n\\n        res = get_ipython()\\n        return get_ipython() is not None and \"zmqshell\" in str(type(res))\\n    except ImportError:\\n        return False' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\runner_utils.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _display_aggregate_results(aggregate_results: pd.DataFrame) -> None:\\n    if _is_jupyter_environment():\\n        from IPython.display import HTML, display\\n\\n        display(HTML(\"<h3>Experiment Results:</h3>\"))\\n        display(aggregate_results)\\n    else:\\n        formatted_string = aggregate_results.to_string(\\n            float_format=lambda x: f\"{x:.2f}\", justify=\"right\"\\n        )\\n        print(\"\\\\n Experiment Results:\")\\n        print(formatted_string)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\runner_utils.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='async def arun_on_dataset(\\n    client: Optional[Client],\\n    dataset_name: str,\\n    llm_or_chain_factory: MODEL_OR_CHAIN_FACTORY,\\n    *,\\n    evaluation: Optional[smith_eval.RunEvalConfig] = None,\\n    concurrency_level: int = 5,\\n    project_name: Optional[str] = None,\\n    project_metadata: Optional[Dict[str, Any]] = None,\\n    verbose: bool = False,\\n    tags: Optional[List[str]] = None,\\n    revision_id: Optional[str] = None,\\n    **kwargs: Any,\\n) -> Dict[str, Any]:\\n    input_mapper = kwargs.pop(\"input_mapper\", None)\\n    if input_mapper:\\n        warn_deprecated(\"0.0.305\", message=_INPUT_MAPPER_DEP_WARNING, pending=True)\\n    if revision_id is None:\\n        revision_id = get_langchain_env_var_metadata().get(\"revision_id\")\\n\\n    if kwargs:\\n        warn_deprecated(\\n            \"0.0.305\",\\n            message=\"The following arguments are deprecated and \"\\n            \"will be removed in a future release: \"\\n            f\"{kwargs.keys()}.\",\\n            removal=\"0.0.305\",\\n        )\\n    client = client or Client()\\n    container = _DatasetRunContainer.prepare(\\n        client,\\n        dataset_name,\\n        llm_or_chain_factory,\\n        project_name,\\n        evaluation,\\n        tags,\\n        input_mapper,\\n        concurrency_level,\\n        project_metadata=project_metadata,\\n        revision_id=revision_id,\\n    )\\n    batch_results = await runnable_utils.gather_with_concurrency(\\n        container.configs[0].get(\"max_concurrency\"),\\n        *map(\\n            functools.partial(\\n                _arun_llm_or_chain,\\n                llm_or_chain_factory=container.wrapped_model,\\n                input_mapper=input_mapper,\\n            ),\\n            container.examples,\\n            container.configs,\\n        ),\\n    )\\n    return container.finish(batch_results, verbose=verbose)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\runner_utils.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def run_on_dataset(\\n    client: Optional[Client],\\n    dataset_name: str,\\n    llm_or_chain_factory: MODEL_OR_CHAIN_FACTORY,\\n    *,\\n    evaluation: Optional[smith_eval.RunEvalConfig] = None,\\n    concurrency_level: int = 5,\\n    project_name: Optional[str] = None,\\n    project_metadata: Optional[Dict[str, Any]] = None,\\n    verbose: bool = False,\\n    tags: Optional[List[str]] = None,\\n    revision_id: Optional[str] = None,\\n    **kwargs: Any,\\n) -> Dict[str, Any]:\\n    input_mapper = kwargs.pop(\"input_mapper\", None)\\n    if input_mapper:\\n        warn_deprecated(\"0.0.305\", message=_INPUT_MAPPER_DEP_WARNING, pending=True)\\n    if revision_id is None:\\n        revision_id = get_langchain_env_var_metadata().get(\"revision_id\")' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\runner_utils.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='if kwargs:\\n        warn_deprecated(\\n            \"0.0.305\",\\n            message=\"The following arguments are deprecated and \"\\n            \"will be removed in a future release: \"\\n            f\"{kwargs.keys()}.\",\\n            removal=\"0.0.305\",\\n        )\\n    client = client or Client()\\n    container = _DatasetRunContainer.prepare(\\n        client,\\n        dataset_name,\\n        llm_or_chain_factory,\\n        project_name,\\n        evaluation,\\n        tags,\\n        input_mapper,\\n        concurrency_level,\\n        project_metadata=project_metadata,\\n        revision_id=revision_id,\\n    )\\n    if concurrency_level == 0:\\n        batch_results = [\\n            _run_llm_or_chain(\\n                example,\\n                config,\\n                llm_or_chain_factory=container.wrapped_model,\\n                input_mapper=input_mapper,\\n            )\\n            for example, config in zip(container.examples, container.configs)\\n        ]\\n    else:\\n        with runnable_config.get_executor_for_config(container.configs[0]) as executor:\\n            batch_results = list(\\n                executor.map(\\n                    functools.partial(\\n                        _run_llm_or_chain,\\n                        llm_or_chain_factory=container.wrapped_model,\\n                        input_mapper=input_mapper,\\n                    ),\\n                    container.examples,\\n                    container.configs,\\n                )\\n            )\\n\\n    return container.finish(batch_results, verbose=verbose)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\runner_utils.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Utilities for running language models or Chains over datasets.\"\"\"\\n\\nfrom __future__ import annotations\\n\\nimport dataclasses\\nimport functools\\nimport inspect\\nimport logging\\nimport uuid\\nfrom datetime import datetime, timezone\\nfrom enum import Enum\\nfrom typing import (\\n    TYPE_CHECKING,\\n    Any,\\n    Callable,\\n    Dict,\\n    List,\\n    Optional,\\n    Tuple,\\n    Union,\\n    cast,\\n)\\n\\nfrom langchain_core._api import warn_deprecated\\nfrom langchain_core.language_models import BaseLanguageModel\\nfrom langchain_core.messages import BaseMessage, messages_from_dict\\nfrom langchain_core.outputs import ChatResult, LLMResult\\nfrom langchain_core.runnables import Runnable, RunnableConfig, RunnableLambda\\nfrom langchain_core.runnables import config as runnable_config\\nfrom langchain_core.runnables import utils as runnable_utils\\nfrom langchain_core.tracers.evaluation import (\\n    EvaluatorCallbackHandler,\\n    wait_for_all_evaluators,\\n)\\nfrom langchain_core.tracers.langchain import LangChainTracer\\nfrom langsmith.client import Client\\nfrom langsmith.env import get_git_info, get_langchain_env_var_metadata\\nfrom langsmith.evaluation import EvaluationResult, RunEvaluator\\nfrom langsmith.run_helpers import as_runnable, is_traceable_function\\nfrom langsmith.schemas import Dataset, DataType, Example, TracerSession\\nfrom langsmith.utils import LangSmithError\\nfrom requests import HTTPError\\nfrom typing_extensions import TypedDict\\n\\nfrom langchain.callbacks.manager import Callbacks\\nfrom langchain.chains.base import Chain\\nfrom langchain.evaluation.loading import load_evaluator\\nfrom langchain.evaluation.schema import (\\n    EvaluatorType,\\n    PairwiseStringEvaluator,\\n    StringEvaluator,\\n)\\nfrom langchain.smith import evaluation as smith_eval\\nfrom langchain.smith.evaluation import config as smith_eval_config\\nfrom langchain.smith.evaluation import name_generation, progress\\n\\nif TYPE_CHECKING:\\n    import pandas as pd\\n\\nlogger = logging.getLogger(__name__)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\runner_utils.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='MODEL_OR_CHAIN_FACTORY = Union[\\n    Callable[[], Union[Chain, Runnable]],\\n    BaseLanguageModel,\\n    Callable[[dict], Any],\\n    Runnable,\\n    Chain,\\n]\\nMCF = Union[Callable[[], Union[Chain, Runnable]], BaseLanguageModel]\\n\\n\\n# Code for: class InputFormatError(Exception):\\n\\n\\n## Shared Utilities\\n\\n\\n# Code for: class TestResult(dict):\\n\\n\\n# Code for: class EvalError(dict):\\n\\n\\n# Code for: def _wrap_in_chain_factory(\\n\\n\\n# Code for: def _get_prompt(inputs: Dict[str, Any]) -> str:\\n\\n\\n# Code for: def _get_messages(inputs: Dict[str, Any]) -> List[BaseMessage]:\\n\\n\\n## Shared data validation utilities\\n# Code for: def _validate_example_inputs_for_language_model(\\n\\n\\n# Code for: def _validate_example_inputs_for_chain(\\n\\n\\n# Code for: def _validate_example_inputs(\\n\\n\\n## Shared Evaluator Setup Utilities\\n\\n\\n# Code for: def _setup_evaluation(\\n\\n\\n# Code for: def _determine_input_key(\\n\\n\\n# Code for: def _determine_prediction_key(\\n\\n\\n# Code for: def _determine_reference_key(\\n\\n\\n# Code for: def _construct_run_evaluator(\\n\\n\\n# Code for: def _get_keys(\\n\\n\\n# Code for: def _load_run_evaluators(\\n\\n\\n### Async Helpers\\n\\n\\n# Code for: async def _arun_llm(\\n\\n\\n# Code for: async def _arun_chain(\\n\\n\\n# Code for: async def _arun_llm_or_chain(\\n\\n\\n## Sync Utilities\\n\\n\\n# Code for: def _run_llm(\\n\\n\\n# Code for: def _run_chain(\\n\\n\\n# Code for: def _run_llm_or_chain(\\n\\n\\n## Public API\\n\\n\\n# Code for: def _prepare_eval_run(\\n\\n\\n# Code for: class _RowResult(TypedDict, total=False):\\n\\n\\n@dataclasses.dataclass\\n# Code for: class _DatasetRunContainer:\\n\\n\\n# Code for: def _is_jupyter_environment() -> bool:\\n\\n\\n# Code for: def _display_aggregate_results(aggregate_results: pd.DataFrame) -> None:' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\runner_utils.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='_INPUT_MAPPER_DEP_WARNING = (\\n    \"The input_mapper argument is deprecated and \"\\n    \"will be removed in a future release. Please add a \"\\n    \" RunnableLambda to your chain to map inputs to the expected format\"\\n    \" instead. Example:\\\\n\"\\n    \"def construct_chain():\\\\n\"\\n    \"    my_chain = ...\\\\n\"\\n    \"    input_mapper = {\\'other_key\\': \\'MyOtherInput\\', \\'my_input_key\\': x}\\\\n\"\\n    \"    return input_mapper | my_chain\\\\n\"\\n    \"run_on_dataset(..., llm_or_chain_factory=construct_chain)\\\\n\"\\n    \"(See https://api.python.langchain.com/en/latest/schema/\"\\n    \"langchain.schema.runnable.base.RunnableLambda.html)\"\\n)\\n\\n\\n# Code for: async def arun_on_dataset(\\n\\n\\n# Code for: def run_on_dataset(\\n\\n\\n_RUN_ON_DATASET_DOCSTRING = \"\"\"\\nRun the Chain or language model on a dataset and store traces\\nto the specified project name.\\n\\nArgs:\\n    dataset_name: Name of the dataset to run the chain on.\\n    llm_or_chain_factory: Language model or Chain constructor to run\\n        over the dataset. The Chain constructor is used to permit\\n        independent calls on each example without carrying over state.\\n    evaluation: Configuration for evaluators to run on the\\n        results of the chain\\n    concurrency_level: The number of async tasks to run concurrently.\\n    project_name: Name of the project to store the traces in.\\n        Defaults to {dataset_name}-{chain class name}-{datetime}.\\n    project_metadata: Optional metadata to add to the project.\\n        Useful for storing information the test variant.\\n        (prompt version, model version, etc.)\\n    client: LangSmith client to use to access the dataset and to\\n        log feedback and run traces.\\n    verbose: Whether to print progress.\\n    tags: Tags to add to each run in the project.\\n    revision_id: Optional revision identifier to assign this test run to\\n        track the performance of different versions of your system.\\nReturns:\\n    A dictionary containing the run\\'s project name and the resulting model outputs.\\n\\n\\nFor the (usually faster) async version of this function, see :func:`arun_on_dataset`.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\runner_utils.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Examples\\n--------\\n\\n.. code-block:: python\\n\\n    from langsmith import Client\\n    from langchain_openai import ChatOpenAI\\n    from langchain.chains import LLMChain\\n    from langchain.smith import smith_eval.RunEvalConfig, run_on_dataset\\n\\n    # Chains may have memory. Passing in a constructor function lets the\\n    # evaluation framework avoid cross-contamination between runs.\\n    def construct_chain():\\n        llm = ChatOpenAI(temperature=0)\\n        chain = LLMChain.from_string(\\n            llm,\\n            \"What\\'s the answer to {your_input_key}\"\\n        )\\n        return chain\\n\\n    # Load off-the-shelf evaluators via config or the EvaluatorType (string or enum)\\n    evaluation_config = smith_eval.RunEvalConfig(\\n        evaluators=[\\n            \"qa\",  # \"Correctness\" against a reference answer\\n            \"embedding_distance\",\\n            smith_eval.RunEvalConfig.Criteria(\"helpfulness\"),\\n            smith_eval.RunEvalConfig.Criteria({\\n                \"fifth-grader-score\": \"Do you have to be smarter than a fifth grader to answer this question?\"\\n            }),\\n        ]\\n    )\\n\\n    client = Client()\\n    run_on_dataset(\\n        client,\\n        \"<my_dataset_name>\",\\n        construct_chain,\\n        evaluation=evaluation_config,\\n    )\\n\\nYou can also create custom evaluators by subclassing the\\n:class:`StringEvaluator <langchain.evaluation.schema.StringEvaluator>`\\nor LangSmith\\'s `RunEvaluator` classes.\\n\\n.. code-block:: python\\n\\n    from typing import Optional\\n    from langchain.evaluation import StringEvaluator\\n\\n    class MyStringEvaluator(StringEvaluator):\\n\\n        @property\\n        def requires_input(self) -> bool:\\n            return False\\n\\n        @property\\n        def requires_reference(self) -> bool:\\n            return True\\n\\n        @property\\n        def evaluation_name(self) -> str:\\n            return \"exact_match\"\\n\\n        def _evaluate_strings(self, prediction, reference=None, input=None, **kwargs) -> dict:\\n            return {\"score\": prediction == reference}' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\runner_utils.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='evaluation_config = smith_eval.RunEvalConfig(\\n        custom_evaluators = [MyStringEvaluator()],\\n    )\\n\\n    run_on_dataset(\\n        client,\\n        \"<my_dataset_name>\",\\n        construct_chain,\\n        evaluation=evaluation_config,\\n    )\\n\"\"\"  # noqa: E501\\nrun_on_dataset.__doc__ = _RUN_ON_DATASET_DOCSTRING\\narun_on_dataset.__doc__ = _RUN_ON_DATASET_DOCSTRING.replace(\\n    \"run_on_dataset(\", \"await arun_on_dataset(\"\\n)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\runner_utils.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Run evaluator wrapper for string evaluators.\"\"\"\\nfrom __future__ import annotations\\n\\nfrom abc import abstractmethod\\nfrom typing import Any, Dict, List, Optional\\n\\nfrom langchain_core.load.dump import dumpd\\nfrom langchain_core.load.load import load\\nfrom langchain_core.load.serializable import Serializable\\nfrom langchain_core.messages import BaseMessage, get_buffer_string, messages_from_dict\\nfrom langsmith import EvaluationResult, RunEvaluator\\nfrom langsmith.schemas import DataType, Example, Run\\n\\nfrom langchain.callbacks.manager import (\\n    AsyncCallbackManagerForChainRun,\\n    CallbackManagerForChainRun,\\n)\\nfrom langchain.chains.base import Chain\\nfrom langchain.evaluation.schema import StringEvaluator\\nfrom langchain.schema import RUN_KEY\\n\\n\\ndef _get_messages_from_run_dict(messages: List[dict]) -> List[BaseMessage]:\\n    if not messages:\\n        return []\\n    first_message = messages[0]\\n    if \"lc\" in first_message:\\n        return [load(dumpd(message)) for message in messages]\\n    else:\\n        return messages_from_dict(messages)\\n\\n\\nclass StringRunMapper(Serializable):\\n    \"\"\"Extract items to evaluate from the run object.\"\"\"\\n\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"The keys to extract from the run.\"\"\"\\n        return [\"prediction\", \"input\"]\\n\\n    @abstractmethod\\n    def map(self, run: Run) -> Dict[str, str]:\\n        \"\"\"Maps the Run to a dictionary.\"\"\"\\n\\n    def __call__(self, run: Run) -> Dict[str, str]:\\n        \"\"\"Maps the Run to a dictionary.\"\"\"\\n        if not run.outputs:\\n            raise ValueError(f\"Run {run.id} has no outputs to evaluate.\")\\n        return self.map(run)\\n\\n\\nclass LLMStringRunMapper(StringRunMapper):\\n    \"\"\"Extract items to evaluate from the run object.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\string_run_evaluator.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def serialize_chat_messages(self, messages: List[Dict]) -> str:\\n        \"\"\"Extract the input messages from the run.\"\"\"\\n        if isinstance(messages, list) and messages:\\n            if isinstance(messages[0], dict):\\n                chat_messages = _get_messages_from_run_dict(messages)\\n            elif isinstance(messages[0], list):\\n                # Runs from Tracer have messages as a list of lists of dicts\\n                chat_messages = _get_messages_from_run_dict(messages[0])\\n            else:\\n                raise ValueError(f\"Could not extract messages to evaluate {messages}\")\\n            return get_buffer_string(chat_messages)\\n        raise ValueError(f\"Could not extract messages to evaluate {messages}\")\\n\\n    def serialize_inputs(self, inputs: Dict) -> str:\\n        if \"prompts\" in inputs:  # Should we even accept this?\\n            input_ = \"\\\\n\\\\n\".join(inputs[\"prompts\"])\\n        elif \"prompt\" in inputs:\\n            input_ = inputs[\"prompt\"]\\n        elif \"messages\" in inputs:\\n            input_ = self.serialize_chat_messages(inputs[\"messages\"])\\n        else:\\n            raise ValueError(\"LLM Run must have either messages or prompts as inputs.\")\\n        return input_\\n\\n    def serialize_outputs(self, outputs: Dict) -> str:\\n        if not outputs.get(\"generations\"):\\n            raise ValueError(\"Cannot evaluate LLM Run without generations.\")\\n        generations: List[Dict] = outputs[\"generations\"]\\n        if not generations:\\n            raise ValueError(\"Cannot evaluate LLM run with empty generations.\")\\n        first_generation: Dict = generations[0]\\n        if isinstance(first_generation, list):\\n            # Runs from Tracer have generations as a list of lists of dicts\\n            # Whereas Runs from the API have a list of dicts\\n            first_generation = first_generation[0]\\n        if \"message\" in first_generation:\\n            output_ = self.serialize_chat_messages([first_generation[\"message\"]])\\n        else:\\n            output_ = first_generation[\"text\"]\\n        return output_' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\string_run_evaluator.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def map(self, run: Run) -> Dict[str, str]:\\n        \"\"\"Maps the Run to a dictionary.\"\"\"\\n        if run.run_type != \"llm\":\\n            raise ValueError(\"LLM RunMapper only supports LLM runs.\")\\n        elif not run.outputs:\\n            if run.error:\\n                raise ValueError(\\n                    f\"Cannot evaluate errored LLM run {run.id}: {run.error}\"\\n                )\\n            else:\\n                raise ValueError(\\n                    f\"Run {run.id} has no outputs. Cannot evaluate this run.\"\\n                )\\n        else:\\n            try:\\n                inputs = self.serialize_inputs(run.inputs)\\n            except Exception as e:\\n                raise ValueError(\\n                    f\"Could not parse LM input from run inputs {run.inputs}\"\\n                ) from e\\n            try:\\n                output_ = self.serialize_outputs(run.outputs)\\n            except Exception as e:\\n                raise ValueError(\\n                    f\"Could not parse LM prediction from run outputs {run.outputs}\"\\n                ) from e\\n            return {\"input\": inputs, \"prediction\": output_}\\n\\n\\nclass ChainStringRunMapper(StringRunMapper):\\n    \"\"\"Extract items to evaluate from the run object from a chain.\"\"\"\\n\\n    input_key: Optional[str] = None\\n    \"\"\"The key from the model Run\\'s inputs to use as the eval input.\\n    If not provided, will use the only input key or raise an\\n    error if there are multiple.\"\"\"\\n    prediction_key: Optional[str] = None\\n    \"\"\"The key from the model Run\\'s outputs to use as the eval prediction.\\n    If not provided, will use the only output key or raise an error\\n    if there are multiple.\"\"\"\\n\\n    def _get_key(self, source: Dict, key: Optional[str], which: str) -> str:\\n        if key is not None:\\n            return source[key]\\n        elif len(source) == 1:\\n            return next(iter(source.values()))\\n        else:\\n            raise ValueError(\\n                f\"Could not map run {which} with multiple keys: \"\\n                f\"{source}\\\\nPlease manually specify a {which}_key\"\\n            )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\string_run_evaluator.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def map(self, run: Run) -> Dict[str, str]:\\n        \"\"\"Maps the Run to a dictionary.\"\"\"\\n        if not run.outputs:\\n            raise ValueError(\\n                f\"Run with ID {run.id} lacks outputs required for evaluation.\"\\n                \" Ensure the Run has valid outputs.\"\\n            )\\n        if self.input_key is not None and self.input_key not in run.inputs:\\n            raise ValueError(\\n                f\"Run with ID {run.id} is missing the expected input key\"\\n                f\" \\'{self.input_key}\\'.\\\\nAvailable input keys in this Run\"\\n                f\"  are: {run.inputs.keys()}.\\\\nAdjust the evaluator\\'s\"\\n                f\" input_key or ensure your input data includes key\"\\n                f\" \\'{self.input_key}\\'.\"\\n            )\\n        elif self.prediction_key is not None and self.prediction_key not in run.outputs:\\n            available_keys = \", \".join(run.outputs.keys())\\n            raise ValueError(\\n                f\"Run with ID {run.id} doesn\\'t have the expected prediction key\"\\n                f\" \\'{self.prediction_key}\\'. Available prediction keys in this Run are:\"\\n                f\" {available_keys}. Adjust the evaluator\\'s prediction_key or\"\\n                \" ensure the Run object\\'s outputs the expected key.\"\\n            )\\n\\n        else:\\n            input_ = self._get_key(run.inputs, self.input_key, \"input\")\\n            prediction = self._get_key(run.outputs, self.prediction_key, \"prediction\")\\n            return {\\n                \"input\": input_,\\n                \"prediction\": prediction,\\n            }\\n\\n\\nclass ToolStringRunMapper(StringRunMapper):\\n    \"\"\"Map an input to the tool.\"\"\"\\n\\n    def map(self, run: Run) -> Dict[str, str]:\\n        if not run.outputs:\\n            raise ValueError(f\"Run {run.id} has no outputs to evaluate.\")\\n        return {\"input\": run.inputs[\"input\"], \"prediction\": run.outputs[\"output\"]}\\n\\n\\nclass StringExampleMapper(Serializable):\\n    \"\"\"Map an example, or row in the dataset, to the inputs of an evaluation.\"\"\"\\n\\n    reference_key: Optional[str] = None' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\string_run_evaluator.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='@property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"The keys to extract from the run.\"\"\"\\n        return [\"reference\"]\\n\\n    def serialize_chat_messages(self, messages: List[Dict]) -> str:\\n        \"\"\"Extract the input messages from the run.\"\"\"\\n        chat_messages = _get_messages_from_run_dict(messages)\\n        return get_buffer_string(chat_messages)\\n\\n    def map(self, example: Example) -> Dict[str, str]:\\n        \"\"\"Maps the Example, or dataset row to a dictionary.\"\"\"\\n        if not example.outputs:\\n            raise ValueError(\\n                f\"Example {example.id} has no outputs to use as a reference.\"\\n            )\\n        if self.reference_key is None:\\n            if len(example.outputs) > 1:\\n                raise ValueError(\\n                    f\"Example {example.id} has multiple outputs, so you must\"\\n                    \" specify a reference_key.\"\\n                )\\n            else:\\n                output = list(example.outputs.values())[0]\\n        elif self.reference_key not in example.outputs:\\n            raise ValueError(\\n                f\"Example {example.id} does not have reference key\"\\n                f\" {self.reference_key}.\"\\n            )\\n        else:\\n            output = example.outputs[self.reference_key]\\n        return {\\n            \"reference\": self.serialize_chat_messages([output])\\n            if isinstance(output, dict) and output.get(\"type\") and output.get(\"data\")\\n            else output\\n        }\\n\\n    def __call__(self, example: Example) -> Dict[str, str]:\\n        \"\"\"Maps the Run and Example to a dictionary.\"\"\"\\n        if not example.outputs:\\n            raise ValueError(\\n                f\"Example {example.id} has no outputs to use as areference label.\"\\n            )\\n        return self.map(example)\\n\\n\\nclass StringRunEvaluatorChain(Chain, RunEvaluator):\\n    \"\"\"Evaluate Run and optional examples.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\string_run_evaluator.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='run_mapper: StringRunMapper\\n    \"\"\"Maps the Run to a dictionary with \\'input\\' and \\'prediction\\' strings.\"\"\"\\n    example_mapper: Optional[StringExampleMapper] = None\\n    \"\"\"Maps the Example (dataset row) to a dictionary\\n    with a \\'reference\\' string.\"\"\"\\n    name: str\\n    \"\"\"The name of the evaluation metric.\"\"\"\\n    string_evaluator: StringEvaluator\\n    \"\"\"The evaluation chain.\"\"\"\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        return [\"run\", \"example\"]\\n\\n    @property\\n    def output_keys(self) -> List[str]:\\n        return [\"feedback\"]\\n\\n    def _prepare_input(self, inputs: Dict[str, Any]) -> Dict[str, str]:\\n        run: Run = inputs[\"run\"]\\n        example: Optional[Example] = inputs.get(\"example\")\\n        evaluate_strings_inputs = self.run_mapper(run)\\n        if not self.string_evaluator.requires_input:\\n            # Hide warning about unused input\\n            evaluate_strings_inputs.pop(\"input\", None)\\n        if example and self.example_mapper and self.string_evaluator.requires_reference:\\n            evaluate_strings_inputs.update(self.example_mapper(example))\\n        elif self.string_evaluator.requires_reference:\\n            raise ValueError(\\n                f\"Evaluator {self.name} requires an reference\"\\n                \" example from the dataset,\"\\n                f\" but none was provided for run {run.id}.\"\\n            )\\n        return evaluate_strings_inputs\\n\\n    def _prepare_output(self, output: Dict[str, Any]) -> Dict[str, Any]:\\n        evaluation_result = EvaluationResult(\\n            key=self.name, comment=output.get(\"reasoning\"), **output\\n        )\\n        if RUN_KEY in output:\\n            # TODO: Not currently surfaced. Update\\n            evaluation_result.evaluator_info[RUN_KEY] = output[RUN_KEY]\\n        return {\"feedback\": evaluation_result}' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\string_run_evaluator.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def _call(\\n        self,\\n        inputs: Dict[str, str],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        \"\"\"Call the evaluation chain.\"\"\"\\n        evaluate_strings_inputs = self._prepare_input(inputs)\\n        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\\n        callbacks = _run_manager.get_child()\\n        chain_output = self.string_evaluator.evaluate_strings(\\n            **evaluate_strings_inputs,\\n            callbacks=callbacks,\\n            include_run_info=True,\\n        )\\n        return self._prepare_output(chain_output)\\n\\n    async def _acall(\\n        self,\\n        inputs: Dict[str, str],\\n        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        \"\"\"Call the evaluation chain.\"\"\"\\n        evaluate_strings_inputs = self._prepare_input(inputs)\\n        _run_manager = run_manager or AsyncCallbackManagerForChainRun.get_noop_manager()\\n        callbacks = _run_manager.get_child()\\n        chain_output = await self.string_evaluator.aevaluate_strings(\\n            **evaluate_strings_inputs,\\n            callbacks=callbacks,\\n            include_run_info=True,\\n        )\\n        return self._prepare_output(chain_output)\\n\\n    def _prepare_evaluator_output(self, output: Dict[str, Any]) -> EvaluationResult:\\n        feedback: EvaluationResult = output[\"feedback\"]\\n        if RUN_KEY not in feedback.evaluator_info:\\n            feedback.evaluator_info[RUN_KEY] = output[RUN_KEY]\\n        return feedback' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\string_run_evaluator.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def evaluate_run(\\n        self, run: Run, example: Optional[Example] = None\\n    ) -> EvaluationResult:\\n        \"\"\"Evaluate an example.\"\"\"\\n        try:\\n            result = self({\"run\": run, \"example\": example}, include_run_info=True)\\n            return self._prepare_evaluator_output(result)\\n        except Exception as e:\\n            return EvaluationResult(\\n                key=self.string_evaluator.evaluation_name,\\n                comment=f\"Error evaluating run {run.id}: {e}\",\\n                # TODO: Add run ID once we can declare it via callbacks\\n            )\\n\\n    async def aevaluate_run(\\n        self, run: Run, example: Optional[Example] = None\\n    ) -> EvaluationResult:\\n        \"\"\"Evaluate an example.\"\"\"\\n        try:\\n            result = await self.acall(\\n                {\"run\": run, \"example\": example}, include_run_info=True\\n            )\\n            return self._prepare_evaluator_output(result)\\n        except Exception as e:\\n            return EvaluationResult(\\n                key=self.string_evaluator.evaluation_name,\\n                comment=f\"Error evaluating run {run.id}: {e}\",\\n            )\\n\\n    @classmethod\\n    def from_run_and_data_type(\\n        cls,\\n        evaluator: StringEvaluator,\\n        run_type: str,\\n        data_type: DataType,\\n        input_key: Optional[str] = None,\\n        prediction_key: Optional[str] = None,\\n        reference_key: Optional[str] = None,\\n        tags: Optional[List[str]] = None,\\n    ) -> StringRunEvaluatorChain:\\n        \"\"\"\\n        Create a StringRunEvaluatorChain from an evaluator and the run and dataset types.\\n\\n        This method provides an easy way to instantiate a StringRunEvaluatorChain, by\\n        taking an evaluator and information about the type of run and the data.\\n        The method supports LLM and chain runs.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\string_run_evaluator.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Args:\\n            evaluator (StringEvaluator): The string evaluator to use.\\n            run_type (str): The type of run being evaluated.\\n                Supported types are LLM and Chain.\\n            data_type (DataType): The type of dataset used in the run.\\n            input_key (str, optional): The key used to map the input from the run.\\n            prediction_key (str, optional): The key used to map the prediction from the run.\\n            reference_key (str, optional): The key used to map the reference from the dataset.\\n            tags (List[str], optional): List of tags to attach to the evaluation chain.\\n\\n        Returns:\\n            StringRunEvaluatorChain: The instantiated evaluation chain.\\n\\n        Raises:\\n            ValueError: If the run type is not supported, or if the evaluator requires a\\n                reference from the dataset but the reference key is not provided.\\n\\n        \"\"\"  # noqa: E501\\n\\n        # Configure how run inputs/predictions are passed to the evaluator\\n        if run_type == \"llm\":\\n            run_mapper: StringRunMapper = LLMStringRunMapper()\\n        elif run_type == \"chain\":\\n            run_mapper = ChainStringRunMapper(\\n                input_key=input_key, prediction_key=prediction_key\\n            )\\n        else:\\n            raise ValueError(\\n                f\"Unsupported run type {run_type}. Expected one of \\'llm\\' or \\'chain\\'.\"\\n            )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\string_run_evaluator.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='# Configure how example rows are fed as a reference string to the evaluator\\n        if (\\n            reference_key is not None\\n            or data_type in (DataType.llm, DataType.chat)\\n            or evaluator.requires_reference\\n        ):\\n            example_mapper = StringExampleMapper(reference_key=reference_key)\\n        elif evaluator.requires_reference:\\n            raise ValueError(\\n                f\"Evaluator {evaluator.evaluation_name} requires a reference\"\\n                \" example from the dataset. Please specify the reference key from\"\\n                \" amongst the dataset outputs keys.\"\\n            )\\n        else:\\n            example_mapper = None\\n        return cls(\\n            name=evaluator.evaluation_name,\\n            run_mapper=run_mapper,\\n            example_mapper=example_mapper,\\n            string_evaluator=evaluator,\\n            tags=tags,\\n        )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\string_run_evaluator.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"LangSmith evaluation utilities.\\n\\nThis module provides utilities for evaluating Chains and other language model\\napplications using LangChain evaluators and LangSmith.\\n\\nFor more information on the LangSmith API, see the `LangSmith API documentation <https://docs.smith.langchain.com/docs/>`_.\\n\\n**Example**\\n\\n.. code-block:: python\\n\\n    from langsmith import Client\\n    from langchain_community.chat_models import ChatOpenAI\\n    from langchain.chains import LLMChain\\n    from langchain.smith import EvaluatorType, RunEvalConfig, run_on_dataset\\n\\n    def construct_chain():\\n        llm = ChatOpenAI(temperature=0)\\n        chain = LLMChain.from_string(\\n            llm,\\n            \"What\\'s the answer to {your_input_key}\"\\n        )\\n        return chain\\n\\n    evaluation_config = RunEvalConfig(\\n        evaluators=[\\n            EvaluatorType.QA,  # \"Correctness\" against a reference answer\\n            EvaluatorType.EMBEDDING_DISTANCE,\\n            RunEvalConfig.Criteria(\"helpfulness\"),\\n            RunEvalConfig.Criteria({\\n                \"fifth-grader-score\": \"Do you have to be smarter than a fifth grader to answer this question?\"\\n            }),\\n        ]\\n    )\\n\\n    client = Client()\\n    run_on_dataset(\\n        client,\\n        \"<my_dataset_name>\",\\n        construct_chain,\\n        evaluation=evaluation_config\\n    )\\n\\n**Attributes**\\n\\n- ``arun_on_dataset``: Asynchronous function to evaluate a chain or other LangChain component over a dataset.\\n- ``run_on_dataset``: Function to evaluate a chain or other LangChain component over a dataset.\\n- ``RunEvalConfig``: Class representing the configuration for running evaluation.\\n- ``StringRunEvaluatorChain``: Class representing a string run evaluator chain.\\n- ``InputFormatError``: Exception raised when the input format is incorrect.\\n\\n\"\"\"  # noqa: E501' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain.smith.evaluation.config import RunEvalConfig\\nfrom langchain.smith.evaluation.runner_utils import (\\n    InputFormatError,\\n    arun_on_dataset,\\n    run_on_dataset,\\n)\\nfrom langchain.smith.evaluation.string_run_evaluator import StringRunEvaluatorChain\\n\\n__all__ = [\\n    \"InputFormatError\",\\n    \"arun_on_dataset\",\\n    \"run_on_dataset\",\\n    \"StringRunEvaluatorChain\",\\n    \"RunEvalConfig\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from typing import (\\n    Any,\\n    Callable,\\n    Iterator,\\n    List,\\n    Optional,\\n    Sequence,\\n    Tuple,\\n    TypeVar,\\n    Union,\\n)\\n\\nfrom langchain_core.stores import BaseStore\\n\\nK = TypeVar(\"K\")\\nV = TypeVar(\"V\")\\n\\n\\nclass EncoderBackedStore(BaseStore[K, V]):\\n    \"\"\"Wraps a store with key and value encoders/decoders.\\n\\n    Examples that uses JSON for encoding/decoding:\\n\\n    .. code-block:: python\\n\\n        import json\\n\\n        def key_encoder(key: int) -> str:\\n            return json.dumps(key)\\n\\n        def value_serializer(value: float) -> str:\\n            return json.dumps(value)\\n\\n        def value_deserializer(serialized_value: str) -> float:\\n            return json.loads(serialized_value)\\n\\n        # Create an instance of the abstract store\\n        abstract_store = MyCustomStore()\\n\\n        # Create an instance of the encoder-backed store\\n        store = EncoderBackedStore(\\n            store=abstract_store,\\n            key_encoder=key_encoder,\\n            value_serializer=value_serializer,\\n            value_deserializer=value_deserializer\\n        )\\n\\n        # Use the encoder-backed store methods\\n        store.mset([(1, 3.14), (2, 2.718)])\\n        values = store.mget([1, 2])  # Retrieves [3.14, 2.718]\\n        store.mdelete([1, 2])  # Deletes the keys 1 and 2\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        store: BaseStore[str, Any],\\n        key_encoder: Callable[[K], str],\\n        value_serializer: Callable[[V], bytes],\\n        value_deserializer: Callable[[Any], V],\\n    ) -> None:\\n        \"\"\"Initialize an EncodedStore.\"\"\"\\n        self.store = store\\n        self.key_encoder = key_encoder\\n        self.value_serializer = value_serializer\\n        self.value_deserializer = value_deserializer' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\storage\\\\encoder_backed.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def mget(self, keys: Sequence[K]) -> List[Optional[V]]:\\n        \"\"\"Get the values associated with the given keys.\"\"\"\\n        encoded_keys: List[str] = [self.key_encoder(key) for key in keys]\\n        values = self.store.mget(encoded_keys)\\n        return [\\n            self.value_deserializer(value) if value is not None else value\\n            for value in values\\n        ]\\n\\n    def mset(self, key_value_pairs: Sequence[Tuple[K, V]]) -> None:\\n        \"\"\"Set the values for the given keys.\"\"\"\\n        encoded_pairs = [\\n            (self.key_encoder(key), self.value_serializer(value))\\n            for key, value in key_value_pairs\\n        ]\\n        self.store.mset(encoded_pairs)\\n\\n    def mdelete(self, keys: Sequence[K]) -> None:\\n        \"\"\"Delete the given keys and their associated values.\"\"\"\\n        encoded_keys = [self.key_encoder(key) for key in keys]\\n        self.store.mdelete(encoded_keys)\\n\\n    def yield_keys(\\n        self, *, prefix: Optional[str] = None\\n    ) -> Union[Iterator[K], Iterator[str]]:\\n        \"\"\"Get an iterator over keys that match the given prefix.\"\"\"\\n        # For the time being this does not return K, but str\\n        # it\\'s for debugging purposes. Should fix this.\\n        yield from self.store.yield_keys(prefix=prefix)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\storage\\\\encoder_backed.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.storage.exceptions import InvalidKeyException\\n\\n__all__ = [\"InvalidKeyException\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\storage\\\\exceptions.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='import os\\nimport re\\nfrom pathlib import Path\\nfrom typing import Iterator, List, Optional, Sequence, Tuple, Union\\n\\nfrom langchain_core.stores import ByteStore\\n\\nfrom langchain.storage.exceptions import InvalidKeyException\\n\\n\\nclass LocalFileStore(ByteStore):\\n    \"\"\"BaseStore interface that works on the local file system.\\n\\n    Examples:\\n        Create a LocalFileStore instance and perform operations on it:\\n\\n        .. code-block:: python\\n\\n            from langchain.storage import LocalFileStore\\n\\n            # Instantiate the LocalFileStore with the root path\\n            file_store = LocalFileStore(\"/path/to/root\")\\n\\n            # Set values for keys\\n            file_store.mset([(\"key1\", b\"value1\"), (\"key2\", b\"value2\")])\\n\\n            # Get values for keys\\n            values = file_store.mget([\"key1\", \"key2\"])  # Returns [b\"value1\", b\"value2\"]\\n\\n            # Delete keys\\n            file_store.mdelete([\"key1\"])\\n\\n            # Iterate over keys\\n            for key in file_store.yield_keys():\\n                print(key)\\n\\n    \"\"\"\\n\\n    def __init__(self, root_path: Union[str, Path]) -> None:\\n        \"\"\"Implement the BaseStore interface for the local file system.\\n\\n        Args:\\n            root_path (Union[str, Path]): The root path of the file store. All keys are\\n                interpreted as paths relative to this root.\\n        \"\"\"\\n        self.root_path = Path(root_path).absolute()\\n\\n    def _get_full_path(self, key: str) -> Path:\\n        \"\"\"Get the full path for a given key relative to the root path.\\n\\n        Args:\\n            key (str): The key relative to the root path.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\storage\\\\file_system.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Returns:\\n            Path: The full path for the given key.\\n        \"\"\"\\n        if not re.match(r\"^[a-zA-Z0-9_.\\\\-/]+$\", key):\\n            raise InvalidKeyException(f\"Invalid characters in key: {key}\")\\n        full_path = os.path.abspath(self.root_path / key)\\n        common_path = os.path.commonpath([str(self.root_path), full_path])\\n        if common_path != str(self.root_path):\\n            raise InvalidKeyException(\\n                f\"Invalid key: {key}. Key should be relative to the full path.\"\\n                f\"{self.root_path} vs. {common_path} and full path of {full_path}\"\\n            )\\n\\n        return Path(full_path)\\n\\n    def mget(self, keys: Sequence[str]) -> List[Optional[bytes]]:\\n        \"\"\"Get the values associated with the given keys.\\n\\n        Args:\\n            keys: A sequence of keys.\\n\\n        Returns:\\n            A sequence of optional values associated with the keys.\\n            If a key is not found, the corresponding value will be None.\\n        \"\"\"\\n        values: List[Optional[bytes]] = []\\n        for key in keys:\\n            full_path = self._get_full_path(key)\\n            if full_path.exists():\\n                value = full_path.read_bytes()\\n                values.append(value)\\n            else:\\n                values.append(None)\\n        return values\\n\\n    def mset(self, key_value_pairs: Sequence[Tuple[str, bytes]]) -> None:\\n        \"\"\"Set the values for the given keys.\\n\\n        Args:\\n            key_value_pairs: A sequence of key-value pairs.\\n\\n        Returns:\\n            None\\n        \"\"\"\\n        for key, value in key_value_pairs:\\n            full_path = self._get_full_path(key)\\n            full_path.parent.mkdir(parents=True, exist_ok=True)\\n            full_path.write_bytes(value)\\n\\n    def mdelete(self, keys: Sequence[str]) -> None:\\n        \"\"\"Delete the given keys and their associated values.\\n\\n        Args:\\n            keys (Sequence[str]): A sequence of keys to delete.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\storage\\\\file_system.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Returns:\\n            None\\n        \"\"\"\\n        for key in keys:\\n            full_path = self._get_full_path(key)\\n            if full_path.exists():\\n                full_path.unlink()\\n\\n    def yield_keys(self, prefix: Optional[str] = None) -> Iterator[str]:\\n        \"\"\"Get an iterator over keys that match the given prefix.\\n\\n        Args:\\n            prefix (Optional[str]): The prefix to match.\\n\\n        Returns:\\n            Iterator[str]: An iterator over keys that match the given prefix.\\n        \"\"\"\\n        prefix_path = self._get_full_path(prefix) if prefix else self.root_path\\n        for file in prefix_path.rglob(\"*\"):\\n            if file.is_file():\\n                relative_path = file.relative_to(self.root_path)\\n                yield str(relative_path)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\storage\\\\file_system.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"In memory store that is not thread safe and has no eviction policy.\\n\\nThis is a simple implementation of the BaseStore using a dictionary that is useful\\nprimarily for unit testing purposes.\\n\"\"\"\\nfrom typing import (\\n    Any,\\n    Dict,\\n    Generic,\\n    Iterator,\\n    List,\\n    Optional,\\n    Sequence,\\n    Tuple,\\n    TypeVar,\\n)\\n\\nfrom langchain_core.stores import BaseStore\\n\\nV = TypeVar(\"V\")\\n\\n\\nclass InMemoryBaseStore(BaseStore[str, V], Generic[V]):\\n    \"\"\"In-memory implementation of the BaseStore using a dictionary.\\n\\n    Attributes:\\n        store (Dict[str, Any]): The underlying dictionary that stores\\n            the key-value pairs.\\n\\n    Examples:\\n\\n        .. code-block:: python\\n\\n            from langchain.storage import InMemoryStore\\n\\n            store = InMemoryStore()\\n            store.mset([(\\'key1\\', \\'value1\\'), (\\'key2\\', \\'value2\\')])\\n            store.mget([\\'key1\\', \\'key2\\'])\\n            # [\\'value1\\', \\'value2\\']\\n            store.mdelete([\\'key1\\'])\\n            list(store.yield_keys())\\n            # [\\'key2\\']\\n            list(store.yield_keys(prefix=\\'k\\'))\\n            # [\\'key2\\']\\n    \"\"\"\\n\\n    def __init__(self) -> None:\\n        \"\"\"Initialize an empty store.\"\"\"\\n        self.store: Dict[str, V] = {}\\n\\n    def mget(self, keys: Sequence[str]) -> List[Optional[V]]:\\n        \"\"\"Get the values associated with the given keys.\\n\\n        Args:\\n            keys (Sequence[str]): A sequence of keys.\\n\\n        Returns:\\n            A sequence of optional values associated with the keys.\\n            If a key is not found, the corresponding value will be None.\\n        \"\"\"\\n        return [self.store.get(key) for key in keys]\\n\\n    def mset(self, key_value_pairs: Sequence[Tuple[str, V]]) -> None:\\n        \"\"\"Set the values for the given keys.\\n\\n        Args:\\n            key_value_pairs (Sequence[Tuple[str, V]]): A sequence of key-value pairs.\\n\\n        Returns:\\n            None\\n        \"\"\"\\n        for key, value in key_value_pairs:\\n            self.store[key] = value' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\storage\\\\in_memory.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def mdelete(self, keys: Sequence[str]) -> None:\\n        \"\"\"Delete the given keys and their associated values.\\n\\n        Args:\\n            keys (Sequence[str]): A sequence of keys to delete.\\n        \"\"\"\\n        for key in keys:\\n            if key in self.store:\\n                del self.store[key]\\n\\n    def yield_keys(self, prefix: Optional[str] = None) -> Iterator[str]:\\n        \"\"\"Get an iterator over keys that match the given prefix.\\n\\n        Args:\\n            prefix (str, optional): The prefix to match. Defaults to None.\\n\\n        Returns:\\n            Iterator[str]: An iterator over keys that match the given prefix.\\n        \"\"\"\\n        if prefix is None:\\n            yield from self.store.keys()\\n        else:\\n            for key in self.store.keys():\\n                if key.startswith(prefix):\\n                    yield key\\n\\n\\nInMemoryStore = InMemoryBaseStore[Any]\\nInMemoryByteStore = InMemoryBaseStore[bytes]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\storage\\\\in_memory.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.storage.redis import RedisStore\\n\\n__all__ = [\"RedisStore\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\storage\\\\redis.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.storage.upstash_redis import (\\n    UpstashRedisByteStore,\\n    UpstashRedisStore,\\n)\\n\\n__all__ = [\"UpstashRedisStore\", \"UpstashRedisByteStore\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\storage\\\\upstash_redis.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Create a key-value store for any langchain serializable object.\"\"\"\\nfrom typing import Callable, Optional\\n\\nfrom langchain_core.documents import Document\\nfrom langchain_core.load import Serializable, dumps, loads\\nfrom langchain_core.stores import BaseStore, ByteStore\\n\\nfrom langchain.storage.encoder_backed import EncoderBackedStore\\n\\n\\ndef _dump_as_bytes(obj: Serializable) -> bytes:\\n    \"\"\"Return a bytes representation of a document.\"\"\"\\n    return dumps(obj).encode(\"utf-8\")\\n\\n\\ndef _dump_document_as_bytes(obj: Document) -> bytes:\\n    \"\"\"Return a bytes representation of a document.\"\"\"\\n    if not isinstance(obj, Document):\\n        raise TypeError(\"Expected a Document instance\")\\n    return dumps(obj).encode(\"utf-8\")\\n\\n\\ndef _load_document_from_bytes(serialized: bytes) -> Document:\\n    \"\"\"Return a document from a bytes representation.\"\"\"\\n    obj = loads(serialized.decode(\"utf-8\"))\\n    if not isinstance(obj, Document):\\n        raise TypeError(f\"Expected a Document instance. Got {type(obj)}\")\\n    return obj\\n\\n\\ndef _load_from_bytes(serialized: bytes) -> Serializable:\\n    \"\"\"Return a document from a bytes representation.\"\"\"\\n    return loads(serialized.decode(\"utf-8\"))\\n\\n\\ndef _identity(x: str) -> str:\\n    \"\"\"Return the same object.\"\"\"\\n    return x\\n\\n\\n# PUBLIC API\\n\\n\\ndef create_lc_store(\\n    store: ByteStore,\\n    *,\\n    key_encoder: Optional[Callable[[str], str]] = None,\\n) -> BaseStore[str, Serializable]:\\n    \"\"\"Create a store for langchain serializable objects from a bytes store.\\n\\n    Args:\\n        store: A bytes store to use as the underlying store.\\n        key_encoder: A function to encode keys; if None uses identity function.\\n\\n    Returns:\\n        A key-value store for documents.\\n    \"\"\"\\n    return EncoderBackedStore(\\n        store,\\n        key_encoder or _identity,\\n        _dump_as_bytes,\\n        _load_from_bytes,\\n    )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\storage\\\\_lc_store.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def create_kv_docstore(\\n    store: ByteStore,\\n    *,\\n    key_encoder: Optional[Callable[[str], str]] = None,\\n) -> BaseStore[str, Document]:\\n    \"\"\"Create a store for langchain Document objects from a bytes store.\\n\\n    This store does run time type checking to ensure that the values are\\n    Document objects.\\n\\n    Args:\\n        store: A bytes store to use as the underlying store.\\n        key_encoder: A function to encode keys; if None uses identity function.\\n\\n    Returns:\\n        A key-value store for documents.\\n    \"\"\"\\n    return EncoderBackedStore(\\n        store,\\n        key_encoder or _identity,\\n        _dump_document_as_bytes,\\n        _load_document_from_bytes,\\n    )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\storage\\\\_lc_store.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Implementations of key-value stores and storage helpers.\\n\\nModule provides implementations of various key-value stores that conform\\nto a simple key-value interface.\\n\\nThe primary goal of these storages is to support implementation of caching.\\n\"\"\"\\nimport warnings\\nfrom typing import Any\\n\\nfrom langchain_core._api import LangChainDeprecationWarning\\n\\nfrom langchain.storage._lc_store import create_kv_docstore, create_lc_store\\nfrom langchain.storage.encoder_backed import EncoderBackedStore\\nfrom langchain.storage.file_system import LocalFileStore\\nfrom langchain.storage.in_memory import InMemoryByteStore, InMemoryStore\\nfrom langchain.utils.interactive_env import is_interactive_env\\n\\n\\ndef __getattr__(name: str) -> Any:\\n    from langchain_community import storage\\n\\n    # If not in interactive env, raise warning.\\n    if not is_interactive_env():\\n        warnings.warn(\\n            \"Importing stores from langchain is deprecated. Importing from \"\\n            \"langchain will no longer be supported as of langchain==0.2.0. \"\\n            \"Please import from langchain-community instead:\\\\n\\\\n\"\\n            f\"`from langchain_community.storage import {name}`.\\\\n\\\\n\"\\n            \"To install langchain-community run `pip install -U langchain-community`.\",\\n            category=LangChainDeprecationWarning,\\n        )\\n\\n    return getattr(storage, name)\\n\\n\\n__all__ = [\\n    \"EncoderBackedStore\",\\n    \"InMemoryStore\",\\n    \"InMemoryByteStore\",\\n    \"LocalFileStore\",\\n    \"RedisStore\",\\n    \"create_lc_store\",\\n    \"create_kv_docstore\",\\n    \"UpstashRedisByteStore\",\\n    \"UpstashRedisStore\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\storage\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.tools import (\\n    BaseTool,\\n    SchemaAnnotationError,\\n    StructuredTool,\\n    Tool,\\n    ToolException,\\n    create_schema_from_function,\\n    tool,\\n)\\n\\n__all__ = [\\n    \"SchemaAnnotationError\",\\n    \"create_schema_from_function\",\\n    \"ToolException\",\\n    \"BaseTool\",\\n    \"Tool\",\\n    \"StructuredTool\",\\n    \"tool\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.convert_to_openai import format_tool_to_openai_function\\n\\n# For backwards compatibility\\n__all__ = [\"format_tool_to_openai_function\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\convert_to_openai.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.ifttt import IFTTTWebhook\\n\\n__all__ = [\"IFTTTWebhook\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\ifttt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.plugin import (\\n    AIPlugin,\\n    AIPluginTool,\\n    AIPluginToolSchema,\\n    ApiConfig,\\n)\\n\\n__all__ = [\\n    \"ApiConfig\",\\n    \"AIPlugin\",\\n    \"AIPluginToolSchema\",\\n    \"AIPluginTool\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\plugin.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Different methods for rendering Tools to be passed to LLMs.\\n\\nDepending on the LLM you are using and the prompting strategy you are using,\\nyou may want Tools to be rendered in a different way.\\nThis module contains various ways to render tools.\\n\"\"\"\\nfrom typing import List\\n\\n# For backwards compatibility\\nfrom langchain_community.tools.convert_to_openai import (\\n    format_tool_to_openai_function,\\n    format_tool_to_openai_tool,\\n)\\nfrom langchain_core.tools import BaseTool\\n\\n__all__ = [\\n    \"render_text_description\",\\n    \"render_text_description_and_args\",\\n    \"format_tool_to_openai_tool\",\\n    \"format_tool_to_openai_function\",\\n]\\n\\n\\ndef render_text_description(tools: List[BaseTool]) -> str:\\n    \"\"\"Render the tool name and description in plain text.\\n\\n    Output will be in the format of:\\n\\n    .. code-block:: markdown\\n\\n        search: This tool is used for search\\n        calculator: This tool is used for math\\n    \"\"\"\\n    return \"\\\\n\".join([f\"{tool.name}: {tool.description}\" for tool in tools])\\n\\n\\ndef render_text_description_and_args(tools: List[BaseTool]) -> str:\\n    \"\"\"Render the tool name, description, and args in plain text.\\n\\n    Output will be in the format of:\\n\\n    .. code-block:: markdown\\n\\n        search: This tool is used for search, args: {\"query\": {\"type\": \"string\"}}\\n        calculator: This tool is used for math, \\\\\\nargs: {\"expression\": {\"type\": \"string\"}}\\n    \"\"\"\\n    tool_strings = []\\n    for tool in tools:\\n        args_schema = str(tool.args)\\n        tool_strings.append(f\"{tool.name}: {tool.description}, args: {args_schema}\")\\n    return \"\\\\n\".join(tool_strings)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\render.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from functools import partial\\nfrom typing import Optional\\n\\nfrom langchain_core.prompts import BasePromptTemplate, PromptTemplate, format_document\\nfrom langchain_core.pydantic_v1 import BaseModel, Field\\nfrom langchain_core.retrievers import BaseRetriever\\n\\nfrom langchain.tools import Tool\\n\\n\\nclass RetrieverInput(BaseModel):\\n    \"\"\"Input to the retriever.\"\"\"\\n\\n    query: str = Field(description=\"query to look up in retriever\")\\n\\n\\ndef _get_relevant_documents(\\n    query: str,\\n    retriever: BaseRetriever,\\n    document_prompt: BasePromptTemplate,\\n    document_separator: str,\\n) -> str:\\n    docs = retriever.get_relevant_documents(query)\\n    return document_separator.join(\\n        format_document(doc, document_prompt) for doc in docs\\n    )\\n\\n\\nasync def _aget_relevant_documents(\\n    query: str,\\n    retriever: BaseRetriever,\\n    document_prompt: BasePromptTemplate,\\n    document_separator: str,\\n) -> str:\\n    docs = await retriever.aget_relevant_documents(query)\\n    return document_separator.join(\\n        format_document(doc, document_prompt) for doc in docs\\n    )\\n\\n\\ndef create_retriever_tool(\\n    retriever: BaseRetriever,\\n    name: str,\\n    description: str,\\n    *,\\n    document_prompt: Optional[BasePromptTemplate] = None,\\n    document_separator: str = \"\\\\n\\\\n\",\\n) -> Tool:\\n    \"\"\"Create a tool to do retrieval of documents.\\n\\n    Args:\\n        retriever: The retriever to use for the retrieval\\n        name: The name for the tool. This will be passed to the language model,\\n            so should be unique and somewhat descriptive.\\n        description: The description for the tool. This will be passed to the language\\n            model, so should be descriptive.' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\retriever.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='Returns:\\n        Tool class to pass to an agent\\n    \"\"\"\\n    document_prompt = document_prompt or PromptTemplate.from_template(\"{page_content}\")\\n    func = partial(\\n        _get_relevant_documents,\\n        retriever=retriever,\\n        document_prompt=document_prompt,\\n        document_separator=document_separator,\\n    )\\n    afunc = partial(\\n        _aget_relevant_documents,\\n        retriever=retriever,\\n        document_prompt=document_prompt,\\n        document_separator=document_separator,\\n    )\\n    return Tool(\\n        name=name,\\n        description=description,\\n        func=func,\\n        coroutine=afunc,\\n        args_schema=RetrieverInput,\\n    )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\retriever.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.yahoo_finance_news import YahooFinanceNewsTool\\n\\n__all__ = [\"YahooFinanceNewsTool\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\yahoo_finance_news.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"**Tools** are classes that an Agent uses to interact with the world.\\n\\nEach tool has a **description**. Agent uses the description to choose the right\\ntool for the job.\\n\\n**Class hierarchy:**\\n\\n.. code-block::\\n\\n    ToolMetaclass --> BaseTool --> <name>Tool  # Examples: AIPluginTool, BaseGraphQLTool\\n                                   <name>      # Examples: BraveSearch, HumanInputRun\\n\\n**Main helpers:**\\n\\n.. code-block::\\n\\n    CallbackManagerForToolRun, AsyncCallbackManagerForToolRun\\n\"\"\"\\nimport warnings\\nfrom typing import Any\\n\\nfrom langchain_core._api import LangChainDeprecationWarning\\nfrom langchain_core.tools import BaseTool, StructuredTool, Tool, tool\\n\\nfrom langchain.utils.interactive_env import is_interactive_env\\n\\n# Used for internal purposes\\n_DEPRECATED_TOOLS = {\"PythonAstREPLTool\", \"PythonREPLTool\"}\\n\\n\\ndef _import_python_tool_PythonAstREPLTool() -> Any:\\n    raise ImportError(\\n        \"This tool has been moved to langchain experiment. \"\\n        \"This tool has access to a python REPL. \"\\n        \"For best practices make sure to sandbox this tool. \"\\n        \"Read https://github.com/langchain-ai/langchain/blob/master/SECURITY.md \"\\n        \"To keep using this code as is, install langchain experimental and \"\\n        \"update relevant imports replacing \\'langchain\\' with \\'langchain_experimental\\'\"\\n    )\\n\\n\\ndef _import_python_tool_PythonREPLTool() -> Any:\\n    raise ImportError(\\n        \"This tool has been moved to langchain experiment. \"\\n        \"This tool has access to a python REPL. \"\\n        \"For best practices make sure to sandbox this tool. \"\\n        \"Read https://github.com/langchain-ai/langchain/blob/master/SECURITY.md \"\\n        \"To keep using this code as is, install langchain experimental and \"\\n        \"update relevant imports replacing \\'langchain\\' with \\'langchain_experimental\\'\"\\n    )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def __getattr__(name: str) -> Any:\\n    if name == \"PythonAstREPLTool\":\\n        return _import_python_tool_PythonAstREPLTool()\\n    elif name == \"PythonREPLTool\":\\n        return _import_python_tool_PythonREPLTool()\\n    else:\\n        from langchain_community import tools\\n\\n        # If not in interactive env, raise warning.\\n        if not is_interactive_env():\\n            warnings.warn(\\n                \"Importing tools from langchain is deprecated. Importing from \"\\n                \"langchain will no longer be supported as of langchain==0.2.0. \"\\n                \"Please import from langchain-community instead:\\\\n\\\\n\"\\n                f\"`from langchain_community.tools import {name}`.\\\\n\\\\n\"\\n                \"To install langchain-community run \"\\n                \"`pip install -U langchain-community`.\",\\n                category=LangChainDeprecationWarning,\\n            )\\n\\n        return getattr(tools, name)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='__all__ = [\\n    \"AINAppOps\",\\n    \"AINOwnerOps\",\\n    \"AINRuleOps\",\\n    \"AINTransfer\",\\n    \"AINValueOps\",\\n    \"AIPluginTool\",\\n    \"APIOperation\",\\n    \"ArxivQueryRun\",\\n    \"AzureCogsFormRecognizerTool\",\\n    \"AzureCogsImageAnalysisTool\",\\n    \"AzureCogsSpeech2TextTool\",\\n    \"AzureCogsText2SpeechTool\",\\n    \"AzureCogsTextAnalyticsHealthTool\",\\n    \"BaseGraphQLTool\",\\n    \"BaseRequestsTool\",\\n    \"BaseSQLDatabaseTool\",\\n    \"BaseSparkSQLTool\",\\n    \"BaseTool\",\\n    \"BearlyInterpreterTool\",\\n    \"BingSearchResults\",\\n    \"BingSearchRun\",\\n    \"BraveSearch\",\\n    \"ClickTool\",\\n    \"CopyFileTool\",\\n    \"CurrentWebPageTool\",\\n    \"DeleteFileTool\",\\n    \"DuckDuckGoSearchResults\",\\n    \"DuckDuckGoSearchRun\",\\n    \"E2BDataAnalysisTool\",\\n    \"EdenAiExplicitImageTool\",\\n    \"EdenAiObjectDetectionTool\",\\n    \"EdenAiParsingIDTool\",\\n    \"EdenAiParsingInvoiceTool\",\\n    \"EdenAiSpeechToTextTool\",\\n    \"EdenAiTextModerationTool\",\\n    \"EdenAiTextToSpeechTool\",\\n    \"EdenaiTool\",\\n    \"ElevenLabsText2SpeechTool\",\\n    \"ExtractHyperlinksTool\",\\n    \"ExtractTextTool\",\\n    \"FileSearchTool\",\\n    \"GetElementsTool\",\\n    \"GmailCreateDraft\",\\n    \"GmailGetMessage\",\\n    \"GmailGetThread\",\\n    \"GmailSearch\",\\n    \"GmailSendMessage\",\\n    \"GoogleCloudTextToSpeechTool\",\\n    \"GooglePlacesTool\",\\n    \"GoogleSearchResults\",\\n    \"GoogleSearchRun\",\\n    \"GoogleSerperResults\",\\n    \"GoogleSerperRun\",\\n    \"SearchAPIResults\",\\n    \"SearchAPIRun\",\\n    \"HumanInputRun\",\\n    \"IFTTTWebhook\",\\n    \"InfoPowerBITool\",\\n    \"InfoSQLDatabaseTool\",\\n    \"InfoSparkSQLTool\",\\n    \"JiraAction\",\\n    \"JsonGetValueTool\",\\n    \"JsonListKeysTool\",\\n    \"ListDirectoryTool\",\\n    \"ListPowerBITool\",\\n    \"ListSQLDatabaseTool\",\\n    \"ListSparkSQLTool\",\\n    \"MerriamWebsterQueryRun\",\\n    \"MetaphorSearchResults\",\\n    \"MoveFileTool\",\\n    \"NasaAction\",\\n    \"NavigateBackTool\",\\n    \"NavigateTool\",\\n    \"O365CreateDraftMessage\",\\n    \"O365SearchEmails\",\\n    \"O365SearchEvents\",\\n    \"O365SendEvent\",\\n    \"O365SendMessage\",\\n    \"OpenAPISpec\",\\n    \"OpenWeatherMapQueryRun\",\\n    \"PubmedQueryRun\",\\n    \"RedditSearchRun\",' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"QueryCheckerTool\",\\n    \"QueryPowerBITool\",\\n    \"QuerySQLCheckerTool\",\\n    \"QuerySQLDataBaseTool\",\\n    \"QuerySparkSQLTool\",\\n    \"ReadFileTool\",\\n    \"RequestsDeleteTool\",\\n    \"RequestsGetTool\",\\n    \"RequestsPatchTool\",\\n    \"RequestsPostTool\",\\n    \"RequestsPutTool\",\\n    \"SteamWebAPIQueryRun\",\\n    \"SceneXplainTool\",\\n    \"SearxSearchResults\",\\n    \"SearxSearchRun\",\\n    \"ShellTool\",\\n    \"SlackGetChannel\",\\n    \"SlackGetMessage\",\\n    \"SlackScheduleMessage\",\\n    \"SlackSendMessage\",\\n    \"SleepTool\",\\n    \"StdInInquireTool\",\\n    \"StackExchangeTool\",\\n    \"SteamshipImageGenerationTool\",\\n    \"StructuredTool\",\\n    \"Tool\",\\n    \"VectorStoreQATool\",\\n    \"VectorStoreQAWithSourcesTool\",\\n    \"WikipediaQueryRun\",\\n    \"WolframAlphaQueryRun\",\\n    \"WriteFileTool\",\\n    \"YahooFinanceNewsTool\",\\n    \"YouTubeSearchTool\",\\n    \"ZapierNLAListActions\",\\n    \"ZapierNLARunAction\",\\n    \"format_tool_to_openai_function\",\\n    \"tool\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.ainetwork.app import (\\n    AINAppOps,\\n    AppOperationType,\\n    AppSchema,\\n)\\n\\n__all__ = [\"AppOperationType\", \"AppSchema\", \"AINAppOps\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\ainetwork\\\\app.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.ainetwork.base import AINBaseTool, OperationType\\n\\n__all__ = [\"OperationType\", \"AINBaseTool\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\ainetwork\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.ainetwork.owner import AINOwnerOps, RuleSchema\\n\\n__all__ = [\"RuleSchema\", \"AINOwnerOps\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\ainetwork\\\\owner.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.ainetwork.rule import AINRuleOps, RuleSchema\\n\\n__all__ = [\"RuleSchema\", \"AINRuleOps\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\ainetwork\\\\rule.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.ainetwork.transfer import AINTransfer, TransferSchema\\n\\n__all__ = [\"TransferSchema\", \"AINTransfer\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\ainetwork\\\\transfer.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.ainetwork.value import AINValueOps, ValueSchema\\n\\n__all__ = [\"ValueSchema\", \"AINValueOps\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\ainetwork\\\\value.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.amadeus.base import AmadeusBaseTool\\n\\n__all__ = [\"AmadeusBaseTool\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\amadeus\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.amadeus.closest_airport import (\\n    AmadeusClosestAirport,\\n    ClosestAirportSchema,\\n)\\n\\n__all__ = [\"ClosestAirportSchema\", \"AmadeusClosestAirport\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\amadeus\\\\closest_airport.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.amadeus.flight_search import (\\n    AmadeusFlightSearch,\\n    FlightSearchSchema,\\n)\\n\\n__all__ = [\"FlightSearchSchema\", \"AmadeusFlightSearch\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\amadeus\\\\flight_search.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Amadeus tools.\"\"\"\\n\\nfrom langchain_community.tools.amadeus.closest_airport import AmadeusClosestAirport\\nfrom langchain_community.tools.amadeus.flight_search import AmadeusFlightSearch\\n\\n__all__ = [\\n    \"AmadeusClosestAirport\",\\n    \"AmadeusFlightSearch\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\amadeus\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.arxiv.tool import ArxivInput, ArxivQueryRun\\n\\n__all__ = [\"ArxivInput\", \"ArxivQueryRun\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\arxiv\\\\tool.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Arxiv API toolkit.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\arxiv\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.azure_cognitive_services.form_recognizer import (\\n    AzureCogsFormRecognizerTool,\\n)\\n\\n__all__ = [\"AzureCogsFormRecognizerTool\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\azure_cognitive_services\\\\form_recognizer.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.azure_cognitive_services.image_analysis import (\\n    AzureCogsImageAnalysisTool,\\n)\\n\\n__all__ = [\"AzureCogsImageAnalysisTool\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\azure_cognitive_services\\\\image_analysis.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.azure_cognitive_services.speech2text import (\\n    AzureCogsSpeech2TextTool,\\n)\\n\\n__all__ = [\"AzureCogsSpeech2TextTool\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\azure_cognitive_services\\\\speech2text.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.azure_cognitive_services.text2speech import (\\n    AzureCogsText2SpeechTool,\\n)\\n\\n__all__ = [\"AzureCogsText2SpeechTool\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\azure_cognitive_services\\\\text2speech.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.azure_cognitive_services.text_analytics_health import (\\n    AzureCogsTextAnalyticsHealthTool,\\n)\\n\\n__all__ = [\"AzureCogsTextAnalyticsHealthTool\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\azure_cognitive_services\\\\text_analytics_health.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Azure Cognitive Services Tools.\"\"\"\\n\\nfrom langchain_community.tools.azure_cognitive_services.form_recognizer import (\\n    AzureCogsFormRecognizerTool,\\n)\\nfrom langchain_community.tools.azure_cognitive_services.image_analysis import (\\n    AzureCogsImageAnalysisTool,\\n)\\nfrom langchain_community.tools.azure_cognitive_services.speech2text import (\\n    AzureCogsSpeech2TextTool,\\n)\\nfrom langchain_community.tools.azure_cognitive_services.text2speech import (\\n    AzureCogsText2SpeechTool,\\n)\\nfrom langchain_community.tools.azure_cognitive_services.text_analytics_health import (\\n    AzureCogsTextAnalyticsHealthTool,\\n)\\n\\n__all__ = [\\n    \"AzureCogsImageAnalysisTool\",\\n    \"AzureCogsFormRecognizerTool\",\\n    \"AzureCogsSpeech2TextTool\",\\n    \"AzureCogsText2SpeechTool\",\\n    \"AzureCogsTextAnalyticsHealthTool\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\azure_cognitive_services\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.bearly.tool import (\\n    BearlyInterpreterTool,\\n    BearlyInterpreterToolArguments,\\n    FileInfo,\\n)\\n\\n__all__ = [\\n    \"BearlyInterpreterToolArguments\",\\n    \"FileInfo\",\\n    \"BearlyInterpreterTool\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\bearly\\\\tool.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.bing_search.tool import BingSearchResults, BingSearchRun\\n\\n__all__ = [\"BingSearchRun\", \"BingSearchResults\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\bing_search\\\\tool.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Bing Search API toolkit.\"\"\"\\n\\nfrom langchain_community.tools.bing_search.tool import BingSearchResults, BingSearchRun\\n\\n__all__ = [\"BingSearchRun\", \"BingSearchResults\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\bing_search\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.brave_search.tool import BraveSearch\\n\\n__all__ = [\"BraveSearch\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\brave_search\\\\tool.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.clickup.tool import ClickupAction\\n\\n__all__ = [\"ClickupAction\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\clickup\\\\tool.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.dataforseo_api_search.tool import (\\n    DataForSeoAPISearchResults,\\n    DataForSeoAPISearchRun,\\n)\\n\\n__all__ = [\"DataForSeoAPISearchRun\", \"DataForSeoAPISearchResults\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\dataforseo_api_search\\\\tool.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.dataforseo_api_search.tool import (\\n    DataForSeoAPISearchResults,\\n    DataForSeoAPISearchRun,\\n)\\n\\n\"\"\"DataForSeo API Toolkit.\"\"\"\\n\"\"\"Tool for the DataForSeo SERP API.\"\"\"\\n\\n__all__ = [\"DataForSeoAPISearchRun\", \"DataForSeoAPISearchResults\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\dataforseo_api_search\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.ddg_search.tool import (\\n    DDGInput,\\n    DuckDuckGoSearchResults,\\n    DuckDuckGoSearchRun,\\n    DuckDuckGoSearchTool,\\n)\\n\\n__all__ = [\\n    \"DDGInput\",\\n    \"DuckDuckGoSearchRun\",\\n    \"DuckDuckGoSearchResults\",\\n    \"DuckDuckGoSearchTool\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\ddg_search\\\\tool.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"DuckDuckGo Search API toolkit.\"\"\"\\n\\nfrom langchain_community.tools.ddg_search.tool import DuckDuckGoSearchRun\\n\\n__all__ = [\"DuckDuckGoSearchRun\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\ddg_search\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.e2b_data_analysis.tool import (\\n    E2BDataAnalysisTool,\\n    E2BDataAnalysisToolArguments,\\n    UploadedFile,\\n)\\n\\n__all__ = [\\n    \"UploadedFile\",\\n    \"E2BDataAnalysisToolArguments\",\\n    \"E2BDataAnalysisTool\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\e2b_data_analysis\\\\tool.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.edenai.audio_speech_to_text import EdenAiSpeechToTextTool\\n\\n__all__ = [\"EdenAiSpeechToTextTool\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\edenai\\\\audio_speech_to_text.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.edenai.audio_text_to_speech import EdenAiTextToSpeechTool\\n\\n__all__ = [\"EdenAiTextToSpeechTool\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\edenai\\\\audio_text_to_speech.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.edenai.edenai_base_tool import EdenaiTool\\n\\n__all__ = [\"EdenaiTool\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\edenai\\\\edenai_base_tool.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.edenai.image_explicitcontent import (\\n    EdenAiExplicitImageTool,\\n)\\n\\n__all__ = [\"EdenAiExplicitImageTool\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\edenai\\\\image_explicitcontent.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.edenai.image_objectdetection import (\\n    EdenAiObjectDetectionTool,\\n)\\n\\n__all__ = [\"EdenAiObjectDetectionTool\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\edenai\\\\image_objectdetection.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.edenai.ocr_identityparser import EdenAiParsingIDTool\\n\\n__all__ = [\"EdenAiParsingIDTool\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\edenai\\\\ocr_identityparser.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.edenai.ocr_invoiceparser import EdenAiParsingInvoiceTool\\n\\n__all__ = [\"EdenAiParsingInvoiceTool\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\edenai\\\\ocr_invoiceparser.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.edenai.text_moderation import EdenAiTextModerationTool\\n\\n__all__ = [\"EdenAiTextModerationTool\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\edenai\\\\text_moderation.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Edenai Tools.\"\"\"\\nfrom langchain_community.tools.edenai.audio_speech_to_text import (\\n    EdenAiSpeechToTextTool,\\n)\\nfrom langchain_community.tools.edenai.audio_text_to_speech import (\\n    EdenAiTextToSpeechTool,\\n)\\nfrom langchain_community.tools.edenai.edenai_base_tool import EdenaiTool\\nfrom langchain_community.tools.edenai.image_explicitcontent import (\\n    EdenAiExplicitImageTool,\\n)\\nfrom langchain_community.tools.edenai.image_objectdetection import (\\n    EdenAiObjectDetectionTool,\\n)\\nfrom langchain_community.tools.edenai.ocr_identityparser import (\\n    EdenAiParsingIDTool,\\n)\\nfrom langchain_community.tools.edenai.ocr_invoiceparser import (\\n    EdenAiParsingInvoiceTool,\\n)\\nfrom langchain_community.tools.edenai.text_moderation import (\\n    EdenAiTextModerationTool,\\n)\\n\\n__all__ = [\\n    \"EdenAiExplicitImageTool\",\\n    \"EdenAiObjectDetectionTool\",\\n    \"EdenAiParsingIDTool\",\\n    \"EdenAiParsingInvoiceTool\",\\n    \"EdenAiTextToSpeechTool\",\\n    \"EdenAiSpeechToTextTool\",\\n    \"EdenAiTextModerationTool\",\\n    \"EdenaiTool\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\edenai\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.eleven_labs.models import ElevenLabsModel\\n\\n__all__ = [\"ElevenLabsModel\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\eleven_labs\\\\models.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.eleven_labs.text2speech import (\\n    ElevenLabsText2SpeechTool,\\n)\\n\\n__all__ = [\"ElevenLabsText2SpeechTool\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\eleven_labs\\\\text2speech.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Eleven Labs Services Tools.\"\"\"\\n\\nfrom langchain_community.tools.eleven_labs.text2speech import ElevenLabsText2SpeechTool\\n\\n__all__ = [\"ElevenLabsText2SpeechTool\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\eleven_labs\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.file_management.copy import CopyFileTool, FileCopyInput\\n\\n__all__ = [\"FileCopyInput\", \"CopyFileTool\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\file_management\\\\copy.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.file_management.delete import (\\n    DeleteFileTool,\\n    FileDeleteInput,\\n)\\n\\n__all__ = [\"FileDeleteInput\", \"DeleteFileTool\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\file_management\\\\delete.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.file_management.file_search import (\\n    FileSearchInput,\\n    FileSearchTool,\\n)\\n\\n__all__ = [\"FileSearchInput\", \"FileSearchTool\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\file_management\\\\file_search.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.file_management.list_dir import (\\n    DirectoryListingInput,\\n    ListDirectoryTool,\\n)\\n\\n__all__ = [\"DirectoryListingInput\", \"ListDirectoryTool\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\file_management\\\\list_dir.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.file_management.move import FileMoveInput, MoveFileTool\\n\\n__all__ = [\"FileMoveInput\", \"MoveFileTool\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\file_management\\\\move.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.file_management.read import ReadFileInput, ReadFileTool\\n\\n__all__ = [\"ReadFileInput\", \"ReadFileTool\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\file_management\\\\read.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.file_management.write import (\\n    WriteFileInput,\\n    WriteFileTool,\\n)\\n\\n__all__ = [\"WriteFileInput\", \"WriteFileTool\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\file_management\\\\write.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"File Management Tools.\"\"\"\\n\\nfrom langchain_community.tools.file_management.copy import CopyFileTool\\nfrom langchain_community.tools.file_management.delete import DeleteFileTool\\nfrom langchain_community.tools.file_management.file_search import FileSearchTool\\nfrom langchain_community.tools.file_management.list_dir import ListDirectoryTool\\nfrom langchain_community.tools.file_management.move import MoveFileTool\\nfrom langchain_community.tools.file_management.read import ReadFileTool\\nfrom langchain_community.tools.file_management.write import WriteFileTool\\n\\n__all__ = [\\n    \"CopyFileTool\",\\n    \"DeleteFileTool\",\\n    \"FileSearchTool\",\\n    \"MoveFileTool\",\\n    \"ReadFileTool\",\\n    \"WriteFileTool\",\\n    \"ListDirectoryTool\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\file_management\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.github.tool import GitHubAction\\n\\n__all__ = [\"GitHubAction\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\github\\\\tool.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\" GitHub Tool \"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\github\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.gitlab.tool import GitLabAction\\n\\n__all__ = [\"GitLabAction\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\gitlab\\\\tool.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\" GitLab Tool \"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\gitlab\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.gmail.base import GmailBaseTool\\n\\n__all__ = [\"GmailBaseTool\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\gmail\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.gmail.create_draft import (\\n    CreateDraftSchema,\\n    GmailCreateDraft,\\n)\\n\\n__all__ = [\"CreateDraftSchema\", \"GmailCreateDraft\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\gmail\\\\create_draft.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.gmail.get_message import (\\n    GmailGetMessage,\\n    SearchArgsSchema,\\n)\\n\\n__all__ = [\"SearchArgsSchema\", \"GmailGetMessage\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\gmail\\\\get_message.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.gmail.get_thread import GetThreadSchema, GmailGetThread\\n\\n__all__ = [\"GetThreadSchema\", \"GmailGetThread\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\gmail\\\\get_thread.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.gmail.search import (\\n    GmailSearch,\\n    Resource,\\n    SearchArgsSchema,\\n)\\n\\n__all__ = [\"Resource\", \"SearchArgsSchema\", \"GmailSearch\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\gmail\\\\search.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.gmail.send_message import (\\n    GmailSendMessage,\\n    SendMessageSchema,\\n)\\n\\n__all__ = [\"SendMessageSchema\", \"GmailSendMessage\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\gmail\\\\send_message.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Gmail tools.\"\"\"\\n\\nfrom langchain_community.tools.gmail.create_draft import GmailCreateDraft\\nfrom langchain_community.tools.gmail.get_message import GmailGetMessage\\nfrom langchain_community.tools.gmail.get_thread import GmailGetThread\\nfrom langchain_community.tools.gmail.search import GmailSearch\\nfrom langchain_community.tools.gmail.send_message import GmailSendMessage\\n\\n__all__ = [\\n    \"GmailCreateDraft\",\\n    \"GmailSendMessage\",\\n    \"GmailSearch\",\\n    \"GmailGetMessage\",\\n    \"GmailGetThread\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\gmail\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.golden_query.tool import GoldenQueryRun\\n\\n__all__ = [\"GoldenQueryRun\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\golden_query\\\\tool.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Golden API toolkit.\"\"\"\\n\\n\\nfrom langchain_community.tools.golden_query.tool import GoldenQueryRun\\n\\n__all__ = [\\n    \"GoldenQueryRun\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\golden_query\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.google_cloud.texttospeech import (\\n    GoogleCloudTextToSpeechTool,\\n)\\n\\n__all__ = [\\n    \"GoogleCloudTextToSpeechTool\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\google_cloud\\\\texttospeech.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Google Cloud Tools.\"\"\"\\n\\nfrom langchain_community.tools.google_cloud.texttospeech import (\\n    GoogleCloudTextToSpeechTool,\\n)\\n\\n__all__ = [\"GoogleCloudTextToSpeechTool\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\google_cloud\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.google_finance.tool import GoogleFinanceQueryRun\\n\\n__all__ = [\"GoogleFinanceQueryRun\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\google_finance\\\\tool.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Google Finance API Toolkit.\"\"\"\\n\\nfrom langchain_community.tools.google_finance.tool import GoogleFinanceQueryRun\\n\\n__all__ = [\"GoogleFinanceQueryRun\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\google_finance\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.google_jobs.tool import GoogleJobsQueryRun\\n\\n__all__ = [\"GoogleJobsQueryRun\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\google_jobs\\\\tool.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Google Jobs API Toolkit.\"\"\"\\n\\nfrom langchain_community.tools.google_jobs.tool import GoogleJobsQueryRun\\n\\n__all__ = [\"GoogleJobsQueryRun\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\google_jobs\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.google_lens.tool import GoogleLensQueryRun\\n\\n__all__ = [\"GoogleLensQueryRun\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\google_lens\\\\tool.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Google Lens API Toolkit.\"\"\"\\n\\nfrom langchain_community.tools.google_lens.tool import GoogleLensQueryRun\\n\\n__all__ = [\"GoogleLensQueryRun\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\google_lens\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.google_places.tool import (\\n    GooglePlacesSchema,\\n    GooglePlacesTool,\\n)\\n\\n__all__ = [\"GooglePlacesSchema\", \"GooglePlacesTool\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\google_places\\\\tool.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Google Places API Toolkit.\"\"\"\\n\\nfrom langchain_community.tools.google_places.tool import GooglePlacesTool\\n\\n__all__ = [\"GooglePlacesTool\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\google_places\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.google_scholar.tool import GoogleScholarQueryRun\\n\\n__all__ = [\"GoogleScholarQueryRun\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\google_scholar\\\\tool.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Google Scholar API Toolkit.\"\"\"\\n\\nfrom langchain_community.tools.google_scholar.tool import GoogleScholarQueryRun\\n\\n__all__ = [\"GoogleScholarQueryRun\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\google_scholar\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.google_search.tool import (\\n    GoogleSearchResults,\\n    GoogleSearchRun,\\n)\\n\\n__all__ = [\"GoogleSearchRun\", \"GoogleSearchResults\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\google_search\\\\tool.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Google Search API Toolkit.\"\"\"\\n\\nfrom langchain_community.tools.google_search.tool import (\\n    GoogleSearchResults,\\n    GoogleSearchRun,\\n)\\n\\n__all__ = [\"GoogleSearchRun\", \"GoogleSearchResults\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\google_search\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.google_serper.tool import (\\n    GoogleSerperResults,\\n    GoogleSerperRun,\\n)\\n\\n__all__ = [\"GoogleSerperRun\", \"GoogleSerperResults\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\google_serper\\\\tool.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.google_serper.tool import (\\n    GoogleSerperResults,\\n    GoogleSerperRun,\\n)\\n\\n\"\"\"Google Serper API Toolkit.\"\"\"\\n\"\"\"Tool for the Serer.dev Google Search API.\"\"\"\\n\\n__all__ = [\"GoogleSerperRun\", \"GoogleSerperResults\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\google_serper\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.google_trends.tool import GoogleTrendsQueryRun\\n\\n__all__ = [\"GoogleTrendsQueryRun\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\google_trends\\\\tool.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Google Trends API Toolkit.\"\"\"\\n\\nfrom langchain_community.tools.google_trends.tool import GoogleTrendsQueryRun\\n\\n__all__ = [\"GoogleTrendsQueryRun\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\google_trends\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.graphql.tool import BaseGraphQLTool\\n\\n__all__ = [\"BaseGraphQLTool\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\graphql\\\\tool.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Tools for interacting with a GraphQL API\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\graphql\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.human.tool import HumanInputRun\\n\\n__all__ = [\"HumanInputRun\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\human\\\\tool.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Tool for asking for human input.\"\"\"\\n\\nfrom langchain_community.tools.human.tool import HumanInputRun\\n\\n__all__ = [\"HumanInputRun\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\human\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.interaction.tool import StdInInquireTool\\n\\n__all__ = [\"StdInInquireTool\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\interaction\\\\tool.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Tools for interacting with the user.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\interaction\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.jira.tool import JiraAction\\n\\n__all__ = [\"JiraAction\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\jira\\\\tool.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Jira Tool.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\jira\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.json.tool import (\\n    JsonGetValueTool,\\n    JsonListKeysTool,\\n    JsonSpec,\\n)\\n\\n__all__ = [\"JsonSpec\", \"JsonListKeysTool\", \"JsonGetValueTool\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\json\\\\tool.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Tools for interacting with a JSON file.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\json\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.memorize.tool import Memorize, TrainableLLM\\n\\n__all__ = [\"TrainableLLM\", \"Memorize\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\memorize\\\\tool.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Unsupervised learning based memorization.\"\"\"\\n\\nfrom langchain_community.tools.memorize.tool import Memorize\\n\\n__all__ = [\"Memorize\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\memorize\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.merriam_webster.tool import MerriamWebsterQueryRun\\n\\n__all__ = [\"MerriamWebsterQueryRun\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\merriam_webster\\\\tool.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Merriam-Webster API toolkit.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\merriam_webster\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.metaphor_search.tool import MetaphorSearchResults\\n\\n__all__ = [\"MetaphorSearchResults\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\metaphor_search\\\\tool.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Metaphor Search API toolkit.\"\"\"\\n\\nfrom langchain_community.tools.metaphor_search.tool import MetaphorSearchResults\\n\\n__all__ = [\"MetaphorSearchResults\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\metaphor_search\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.multion.close_session import (\\n    CloseSessionSchema,\\n    MultionCloseSession,\\n)\\n\\n__all__ = [\"CloseSessionSchema\", \"MultionCloseSession\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\multion\\\\close_session.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.multion.create_session import (\\n    CreateSessionSchema,\\n    MultionCreateSession,\\n)\\n\\n__all__ = [\"CreateSessionSchema\", \"MultionCreateSession\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\multion\\\\create_session.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.multion.update_session import (\\n    MultionUpdateSession,\\n    UpdateSessionSchema,\\n)\\n\\n__all__ = [\"UpdateSessionSchema\", \"MultionUpdateSession\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\multion\\\\update_session.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"MutliOn Client API tools.\"\"\"\\nfrom langchain_community.tools.multion.close_session import MultionCloseSession\\nfrom langchain_community.tools.multion.create_session import MultionCreateSession\\nfrom langchain_community.tools.multion.update_session import MultionUpdateSession\\n\\n__all__ = [\"MultionCreateSession\", \"MultionUpdateSession\", \"MultionCloseSession\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\multion\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.nasa.tool import NasaAction\\n\\n__all__ = [\"NasaAction\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\nasa\\\\tool.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.nuclia.tool import NUASchema, NucliaUnderstandingAPI\\n\\n__all__ = [\"NUASchema\", \"NucliaUnderstandingAPI\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\nuclia\\\\tool.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.nuclia.tool import NucliaUnderstandingAPI\\n\\n__all__ = [\"NucliaUnderstandingAPI\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\nuclia\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.office365.base import O365BaseTool\\n\\n__all__ = [\"O365BaseTool\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\office365\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.office365.create_draft_message import (\\n    CreateDraftMessageSchema,\\n    O365CreateDraftMessage,\\n)\\n\\n__all__ = [\"CreateDraftMessageSchema\", \"O365CreateDraftMessage\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\office365\\\\create_draft_message.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.office365.events_search import (\\n    O365SearchEvents,\\n    SearchEventsInput,\\n)\\n\\n__all__ = [\"SearchEventsInput\", \"O365SearchEvents\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\office365\\\\events_search.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.office365.messages_search import (\\n    O365SearchEmails,\\n    SearchEmailsInput,\\n)\\n\\n__all__ = [\"SearchEmailsInput\", \"O365SearchEmails\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\office365\\\\messages_search.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.office365.send_event import (\\n    O365SendEvent,\\n    SendEventSchema,\\n)\\n\\n__all__ = [\"SendEventSchema\", \"O365SendEvent\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\office365\\\\send_event.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.office365.send_message import (\\n    O365SendMessage,\\n    SendMessageSchema,\\n)\\n\\n__all__ = [\"SendMessageSchema\", \"O365SendMessage\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\office365\\\\send_message.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"O365 tools.\"\"\"\\n\\nfrom langchain_community.tools.office365.create_draft_message import (\\n    O365CreateDraftMessage,\\n)\\nfrom langchain_community.tools.office365.events_search import O365SearchEvents\\nfrom langchain_community.tools.office365.messages_search import O365SearchEmails\\nfrom langchain_community.tools.office365.send_event import O365SendEvent\\nfrom langchain_community.tools.office365.send_message import O365SendMessage\\n\\n__all__ = [\\n    \"O365SearchEmails\",\\n    \"O365SearchEvents\",\\n    \"O365CreateDraftMessage\",\\n    \"O365SendMessage\",\\n    \"O365SendEvent\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\office365\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.openapi.utils.api_models import (\\n    INVALID_LOCATION_TEMPL,\\n    PRIMITIVE_TYPES,\\n    SCHEMA_TYPE,\\n    SUPPORTED_LOCATIONS,\\n    APIOperation,\\n    APIProperty,\\n    APIPropertyBase,\\n    APIPropertyLocation,\\n    APIRequestBody,\\n    APIRequestBodyProperty,\\n)\\n\\n__all__ = [\\n    \"PRIMITIVE_TYPES\",\\n    \"APIPropertyLocation\",\\n    \"SUPPORTED_LOCATIONS\",\\n    \"INVALID_LOCATION_TEMPL\",\\n    \"SCHEMA_TYPE\",\\n    \"APIPropertyBase\",\\n    \"APIProperty\",\\n    \"APIRequestBodyProperty\",\\n    \"APIRequestBody\",\\n    \"APIOperation\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\openapi\\\\utils\\\\api_models.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Utility functions for parsing an OpenAPI spec. Kept for backwards compat.\"\"\"\\nfrom langchain_community.utilities.openapi import HTTPVerb, OpenAPISpec\\n\\n__all__ = [\"HTTPVerb\", \"OpenAPISpec\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\openapi\\\\utils\\\\openapi_utils.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.openweathermap.tool import OpenWeatherMapQueryRun\\n\\n__all__ = [\"OpenWeatherMapQueryRun\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\openweathermap\\\\tool.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"OpenWeatherMap API toolkit.\"\"\"\\n\\n\\nfrom langchain_community.tools.openweathermap.tool import OpenWeatherMapQueryRun\\n\\n__all__ = [\\n    \"OpenWeatherMapQueryRun\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\openweathermap\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.playwright.base import (\\n    BaseBrowserTool,\\n)\\n\\n__all__ = [\"BaseBrowserTool\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\playwright\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.playwright.click import ClickTool, ClickToolInput\\n\\n__all__ = [\"ClickToolInput\", \"ClickTool\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\playwright\\\\click.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.playwright.current_page import CurrentWebPageTool\\n\\n__all__ = [\"CurrentWebPageTool\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\playwright\\\\current_page.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.playwright.extract_hyperlinks import (\\n    ExtractHyperlinksTool,\\n    ExtractHyperlinksToolInput,\\n)\\n\\n__all__ = [\"ExtractHyperlinksToolInput\", \"ExtractHyperlinksTool\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\playwright\\\\extract_hyperlinks.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.playwright.extract_text import ExtractTextTool\\n\\n__all__ = [\"ExtractTextTool\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\playwright\\\\extract_text.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.playwright.get_elements import (\\n    GetElementsTool,\\n    GetElementsToolInput,\\n)\\n\\n__all__ = [\"GetElementsToolInput\", \"GetElementsTool\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\playwright\\\\get_elements.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.playwright.navigate import (\\n    NavigateTool,\\n    NavigateToolInput,\\n)\\n\\n__all__ = [\"NavigateToolInput\", \"NavigateTool\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\playwright\\\\navigate.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.playwright.navigate_back import NavigateBackTool\\n\\n__all__ = [\"NavigateBackTool\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\playwright\\\\navigate_back.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Browser tools and toolkit.\"\"\"\\n\\nfrom langchain_community.tools.playwright.click import ClickTool\\nfrom langchain_community.tools.playwright.current_page import CurrentWebPageTool\\nfrom langchain_community.tools.playwright.extract_hyperlinks import (\\n    ExtractHyperlinksTool,\\n)\\nfrom langchain_community.tools.playwright.extract_text import ExtractTextTool\\nfrom langchain_community.tools.playwright.get_elements import GetElementsTool\\nfrom langchain_community.tools.playwright.navigate import NavigateTool\\nfrom langchain_community.tools.playwright.navigate_back import NavigateBackTool\\n\\n__all__ = [\\n    \"NavigateTool\",\\n    \"NavigateBackTool\",\\n    \"ExtractTextTool\",\\n    \"ExtractHyperlinksTool\",\\n    \"GetElementsTool\",\\n    \"ClickTool\",\\n    \"CurrentWebPageTool\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\playwright\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.powerbi.tool import (\\n    InfoPowerBITool,\\n    ListPowerBITool,\\n    QueryPowerBITool,\\n)\\n\\n__all__ = [\"QueryPowerBITool\", \"InfoPowerBITool\", \"ListPowerBITool\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\powerbi\\\\tool.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Tools for interacting with a PowerBI dataset.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\powerbi\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.pubmed.tool import PubmedQueryRun\\n\\n__all__ = [\"PubmedQueryRun\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\pubmed\\\\tool.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"PubMed API toolkit.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\pubmed\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from typing import Any\\n\\n\\ndef __getattr__(name: str = \"\") -> Any:\\n    raise ImportError(\\n        \"This tool has been moved to langchain experiment. \"\\n        \"This tool has access to a python REPL. \"\\n        \"For best practices make sure to sandbox this tool. \"\\n        \"Read https://github.com/langchain-ai/langchain/blob/master/SECURITY.md \"\\n        \"To keep using this code as is, install langchain experimental and \"\\n        \"update relevant imports replacing \\'langchain\\' with \\'langchain_experimental\\'\"\\n    )' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\python\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.reddit_search.tool import (\\n    RedditSearchRun,\\n    RedditSearchSchema,\\n)\\n\\n__all__ = [\"RedditSearchSchema\", \"RedditSearchRun\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\reddit_search\\\\tool.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.requests.tool import (\\n    BaseRequestsTool,\\n    RequestsDeleteTool,\\n    RequestsGetTool,\\n    RequestsPatchTool,\\n    RequestsPostTool,\\n    RequestsPutTool,\\n)\\n\\n__all__ = [\\n    \"BaseRequestsTool\",\\n    \"RequestsGetTool\",\\n    \"RequestsPostTool\",\\n    \"RequestsPatchTool\",\\n    \"RequestsPutTool\",\\n    \"RequestsDeleteTool\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\requests\\\\tool.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Tools for making requests to an API endpoint.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\requests\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.scenexplain.tool import SceneXplainInput, SceneXplainTool\\n\\n__all__ = [\"SceneXplainInput\", \"SceneXplainTool\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\scenexplain\\\\tool.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"SceneXplain API toolkit.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\scenexplain\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.searchapi.tool import SearchAPIResults, SearchAPIRun\\n\\n__all__ = [\"SearchAPIRun\", \"SearchAPIResults\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\searchapi\\\\tool.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.searchapi.tool import SearchAPIResults, SearchAPIRun\\n\\n\"\"\"SearchApi.io API Toolkit.\"\"\"\\n\"\"\"Tool for the SearchApi.io Google SERP API.\"\"\"\\n\\n__all__ = [\"SearchAPIResults\", \"SearchAPIRun\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\searchapi\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.searx_search.tool import (\\n    SearxSearchResults,\\n    SearxSearchRun,\\n)\\n\\n__all__ = [\"SearxSearchRun\", \"SearxSearchResults\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\searx_search\\\\tool.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.shell.tool import (\\n    ShellInput,\\n    ShellTool,\\n)\\n\\n__all__ = [\"ShellInput\", \"ShellTool\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\shell\\\\tool.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Shell tool.\"\"\"\\n\\nfrom langchain_community.tools.shell.tool import ShellTool\\n\\n__all__ = [\"ShellTool\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\shell\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.slack.base import SlackBaseTool\\n\\n__all__ = [\"SlackBaseTool\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\slack\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.slack.get_channel import SlackGetChannel\\n\\n__all__ = [\"SlackGetChannel\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\slack\\\\get_channel.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.slack.get_message import (\\n    SlackGetMessage,\\n    SlackGetMessageSchema,\\n)\\n\\n__all__ = [\"SlackGetMessageSchema\", \"SlackGetMessage\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\slack\\\\get_message.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.slack.schedule_message import (\\n    ScheduleMessageSchema,\\n    SlackScheduleMessage,\\n)\\n\\n__all__ = [\"ScheduleMessageSchema\", \"SlackScheduleMessage\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\slack\\\\schedule_message.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.slack.send_message import (\\n    SendMessageSchema,\\n    SlackSendMessage,\\n)\\n\\n__all__ = [\"SendMessageSchema\", \"SlackSendMessage\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\slack\\\\send_message.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Slack tools.\"\"\"\\n\\nfrom langchain_community.tools.slack.get_channel import SlackGetChannel\\nfrom langchain_community.tools.slack.get_message import SlackGetMessage\\nfrom langchain_community.tools.slack.schedule_message import SlackScheduleMessage\\nfrom langchain_community.tools.slack.send_message import SlackSendMessage\\n\\n__all__ = [\\n    \"SlackGetChannel\",\\n    \"SlackGetMessage\",\\n    \"SlackScheduleMessage\",\\n    \"SlackSendMessage\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\slack\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.sleep.tool import SleepInput, SleepTool\\n\\n__all__ = [\"SleepInput\", \"SleepTool\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\sleep\\\\tool.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Sleep tool.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\sleep\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.spark_sql.tool import (\\n    BaseSparkSQLTool,\\n    InfoSparkSQLTool,\\n    ListSparkSQLTool,\\n    QueryCheckerTool,\\n    QuerySparkSQLTool,\\n)\\n\\n__all__ = [\\n    \"BaseSparkSQLTool\",\\n    \"QuerySparkSQLTool\",\\n    \"InfoSparkSQLTool\",\\n    \"ListSparkSQLTool\",\\n    \"QueryCheckerTool\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\spark_sql\\\\tool.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Tools for interacting with Spark SQL.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\spark_sql\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.sql_database.prompt import QUERY_CHECKER\\n\\n__all__ = [\"QUERY_CHECKER\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\sql_database\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.sql_database.tool import (\\n    BaseSQLDatabaseTool,\\n    InfoSQLDatabaseTool,\\n    ListSQLDatabaseTool,\\n    QuerySQLCheckerTool,\\n    QuerySQLDataBaseTool,\\n)\\n\\n__all__ = [\\n    \"BaseSQLDatabaseTool\",\\n    \"QuerySQLDataBaseTool\",\\n    \"InfoSQLDatabaseTool\",\\n    \"ListSQLDatabaseTool\",\\n    \"QuerySQLCheckerTool\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\sql_database\\\\tool.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Tools for interacting with a SQL database.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\sql_database\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.stackexchange.tool import StackExchangeTool\\n\\n__all__ = [\"StackExchangeTool\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\stackexchange\\\\tool.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"StackExchange API toolkit.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\stackexchange\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.steam.tool import SteamWebAPIQueryRun\\n\\n__all__ = [\"SteamWebAPIQueryRun\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\steam\\\\tool.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Steam API toolkit\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\steam\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.steamship_image_generation.tool import (\\n    ModelName,\\n    SteamshipImageGenerationTool,\\n)\\n\\n__all__ = [\"ModelName\", \"SteamshipImageGenerationTool\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\steamship_image_generation\\\\tool.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Tool to generate an image.\"\"\"\\n\\nfrom langchain_community.tools.steamship_image_generation.tool import (\\n    SteamshipImageGenerationTool,\\n)\\n\\n__all__ = [\"SteamshipImageGenerationTool\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\steamship_image_generation\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.tavily_search.tool import (\\n    TavilyAnswer,\\n    TavilyInput,\\n    TavilySearchResults,\\n)\\n\\n__all__ = [\"TavilyInput\", \"TavilySearchResults\", \"TavilyAnswer\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\tavily_search\\\\tool.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Tavily Search API toolkit.\"\"\"\\n\\nfrom langchain_community.tools.tavily_search.tool import (\\n    TavilyAnswer,\\n    TavilySearchResults,\\n)\\n\\n__all__ = [\"TavilySearchResults\", \"TavilyAnswer\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\tavily_search\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.vectorstore.tool import (\\n    VectorStoreQATool,\\n    VectorStoreQAWithSourcesTool,\\n)\\n\\n__all__ = [\\n    \"VectorStoreQATool\",\\n    \"VectorStoreQAWithSourcesTool\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\vectorstore\\\\tool.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Simple tool wrapper around VectorDBQA chain.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\vectorstore\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.wikipedia.tool import WikipediaQueryRun\\n\\n__all__ = [\"WikipediaQueryRun\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\wikipedia\\\\tool.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Wikipedia API toolkit.\"\"\"' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\wikipedia\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.wolfram_alpha.tool import WolframAlphaQueryRun\\n\\n__all__ = [\"WolframAlphaQueryRun\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\wolfram_alpha\\\\tool.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Wolfram Alpha API toolkit.\"\"\"\\n\\n\\nfrom langchain_community.tools.wolfram_alpha.tool import WolframAlphaQueryRun\\n\\n__all__ = [\\n    \"WolframAlphaQueryRun\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\wolfram_alpha\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.youtube.search import YouTubeSearchTool\\n\\n__all__ = [\"YouTubeSearchTool\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\youtube\\\\search.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.tools.zapier.tool import (\\n    ZapierNLAListActions,\\n    ZapierNLARunAction,\\n)\\n\\n__all__ = [\"ZapierNLARunAction\", \"ZapierNLAListActions\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\zapier\\\\tool.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Zapier Tool.\"\"\"\\n\\nfrom langchain_community.tools.zapier.tool import (\\n    ZapierNLAListActions,\\n    ZapierNLARunAction,\\n)\\n\\n__all__ = [\\n    \"ZapierNLARunAction\",\\n    \"ZapierNLAListActions\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\zapier\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utilities.alpha_vantage import AlphaVantageAPIWrapper\\n\\n__all__ = [\"AlphaVantageAPIWrapper\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\alpha_vantage.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utilities.anthropic import (\\n    get_num_tokens_anthropic,\\n    get_token_ids_anthropic,\\n)\\n\\n__all__ = [\\n    \"get_num_tokens_anthropic\",\\n    \"get_token_ids_anthropic\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\anthropic.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utilities.apify import ApifyWrapper\\n\\n__all__ = [\"ApifyWrapper\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\apify.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utilities.arcee import (\\n    ArceeDocument,\\n    ArceeDocumentAdapter,\\n    ArceeDocumentSource,\\n    ArceeRoute,\\n    ArceeWrapper,\\n    DALMFilter,\\n    DALMFilterType,\\n)\\n\\n__all__ = [\\n    \"ArceeRoute\",\\n    \"DALMFilterType\",\\n    \"DALMFilter\",\\n    \"ArceeDocumentSource\",\\n    \"ArceeDocument\",\\n    \"ArceeDocumentAdapter\",\\n    \"ArceeWrapper\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\arcee.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utilities.arxiv import ArxivAPIWrapper\\n\\n__all__ = [\"ArxivAPIWrapper\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\arxiv.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Shims for asyncio features that may be missing from older python versions\"\"\"\\n\\nimport sys\\n\\nif sys.version_info[:2] < (3, 11):\\n    from async_timeout import timeout as asyncio_timeout\\nelse:\\n    from asyncio import timeout as asyncio_timeout\\n\\n\\n__all__ = [\"asyncio_timeout\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\asyncio.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utilities.awslambda import LambdaWrapper\\n\\n__all__ = [\"LambdaWrapper\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\awslambda.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utilities.bibtex import BibtexparserWrapper\\n\\n__all__ = [\"BibtexparserWrapper\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\bibtex.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utilities.bing_search import BingSearchAPIWrapper\\n\\n__all__ = [\"BingSearchAPIWrapper\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\bing_search.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utilities.brave_search import BraveSearchWrapper\\n\\n__all__ = [\"BraveSearchWrapper\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\brave_search.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utilities.clickup import (\\n    ClickupAPIWrapper,\\n    Component,\\n    CUList,\\n    Member,\\n    Space,\\n    Task,\\n    Team,\\n)\\n\\n__all__ = [\\n    \"Component\",\\n    \"Task\",\\n    \"CUList\",\\n    \"Member\",\\n    \"Team\",\\n    \"Space\",\\n    \"ClickupAPIWrapper\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\clickup.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\\n\\n__all__ = [\"DallEAPIWrapper\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\dalle_image_generator.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utilities.dataforseo_api_search import DataForSeoAPIWrapper\\n\\n__all__ = [\"DataForSeoAPIWrapper\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\dataforseo_api_search.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utilities.duckduckgo_search import DuckDuckGoSearchAPIWrapper\\n\\n__all__ = [\"DuckDuckGoSearchAPIWrapper\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\duckduckgo_search.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utilities.github import GitHubAPIWrapper\\n\\n__all__ = [\"GitHubAPIWrapper\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\github.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utilities.gitlab import GitLabAPIWrapper\\n\\n__all__ = [\"GitLabAPIWrapper\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\gitlab.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utilities.golden_query import (\\n    GoldenQueryAPIWrapper,\\n)\\n\\n__all__ = [\"GoldenQueryAPIWrapper\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\golden_query.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utilities.google_finance import GoogleFinanceAPIWrapper\\n\\n__all__ = [\"GoogleFinanceAPIWrapper\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\google_finance.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utilities.google_jobs import GoogleJobsAPIWrapper\\n\\n__all__ = [\"GoogleJobsAPIWrapper\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\google_jobs.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utilities.google_lens import GoogleLensAPIWrapper\\n\\n__all__ = [\"GoogleLensAPIWrapper\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\google_lens.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utilities.google_places_api import GooglePlacesAPIWrapper\\n\\n__all__ = [\"GooglePlacesAPIWrapper\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\google_places_api.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utilities.google_scholar import GoogleScholarAPIWrapper\\n\\n__all__ = [\"GoogleScholarAPIWrapper\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\google_scholar.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utilities.google_search import GoogleSearchAPIWrapper\\n\\n__all__ = [\"GoogleSearchAPIWrapper\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\google_search.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utilities.google_serper import GoogleSerperAPIWrapper\\n\\n__all__ = [\"GoogleSerperAPIWrapper\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\google_serper.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utilities.google_trends import GoogleTrendsAPIWrapper\\n\\n__all__ = [\"GoogleTrendsAPIWrapper\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\google_trends.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utilities.graphql import GraphQLAPIWrapper\\n\\n__all__ = [\"GraphQLAPIWrapper\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\graphql.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utilities.jira import JiraAPIWrapper\\n\\n__all__ = [\"JiraAPIWrapper\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\jira.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.utils.loading import try_load_from_hub\\n\\n# For backwards compatibility\\n__all__ = [\"try_load_from_hub\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\loading.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utilities.max_compute import MaxComputeAPIWrapper\\n\\n__all__ = [\"MaxComputeAPIWrapper\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\max_compute.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utilities.merriam_webster import (\\n    MerriamWebsterAPIWrapper,\\n)\\n\\n__all__ = [\\n    \"MerriamWebsterAPIWrapper\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\merriam_webster.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utilities.metaphor_search import (\\n    MetaphorSearchAPIWrapper,\\n)\\n\\n__all__ = [\"MetaphorSearchAPIWrapper\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\metaphor_search.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utilities.nasa import (\\n    NasaAPIWrapper,\\n)\\n\\n__all__ = [\"NasaAPIWrapper\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\nasa.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utilities.opaqueprompts import desanitize, sanitize\\n\\n__all__ = [\"sanitize\", \"desanitize\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\opaqueprompts.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utilities.openapi import HTTPVerb, OpenAPISpec\\n\\n__all__ = [\"HTTPVerb\", \"OpenAPISpec\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\openapi.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utilities.openweathermap import OpenWeatherMapAPIWrapper\\n\\n__all__ = [\"OpenWeatherMapAPIWrapper\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\openweathermap.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utilities.outline import (\\n    OutlineAPIWrapper,\\n)\\n\\n__all__ = [\"OutlineAPIWrapper\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\outline.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utilities.portkey import Portkey\\n\\n__all__ = [\"Portkey\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\portkey.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utilities.powerbi import (\\n    PowerBIDataset,\\n)\\n\\n__all__ = [\\n    \"PowerBIDataset\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\powerbi.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utilities.pubmed import PubMedAPIWrapper\\n\\n__all__ = [\"PubMedAPIWrapper\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\pubmed.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utilities.python import PythonREPL\\n\\n__all__ = [\"PythonREPL\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\python.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utilities.reddit_search import RedditSearchAPIWrapper\\n\\n__all__ = [\"RedditSearchAPIWrapper\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\reddit_search.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utilities.redis import (\\n    TokenEscaper,\\n    check_redis_module_exist,\\n    get_client,\\n)\\n\\n__all__ = [\\n    \"TokenEscaper\",\\n    \"check_redis_module_exist\",\\n    \"get_client\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\redis.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utilities.requests import (\\n    Requests,\\n    RequestsWrapper,\\n    TextRequestsWrapper,\\n)\\n\\n__all__ = [\"Requests\", \"TextRequestsWrapper\", \"RequestsWrapper\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\requests.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utilities.scenexplain import SceneXplainAPIWrapper\\n\\n__all__ = [\"SceneXplainAPIWrapper\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\scenexplain.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utilities.searchapi import SearchApiAPIWrapper\\n\\n__all__ = [\"SearchApiAPIWrapper\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\searchapi.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utilities.searx_search import (\\n    SearxResults,\\n    SearxSearchWrapper,\\n)\\n\\n__all__ = [\"SearxResults\", \"SearxSearchWrapper\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\searx_search.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utilities.serpapi import HiddenPrints, SerpAPIWrapper\\n\\n__all__ = [\"HiddenPrints\", \"SerpAPIWrapper\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\serpapi.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utilities.spark_sql import SparkSQL\\n\\n__all__ = [\"SparkSQL\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\spark_sql.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utilities.sql_database import (\\n    SQLDatabase,\\n    truncate_word,\\n)\\n\\n__all__ = [\"truncate_word\", \"SQLDatabase\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\sql_database.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utilities.stackexchange import StackExchangeAPIWrapper\\n\\n__all__ = [\"StackExchangeAPIWrapper\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\stackexchange.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utilities.steam import SteamWebAPIWrapper\\n\\n__all__ = [\"SteamWebAPIWrapper\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\steam.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utilities.tavily_search import (\\n    TavilySearchAPIWrapper,\\n)\\n\\n__all__ = [\"TavilySearchAPIWrapper\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\tavily_search.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utilities.tensorflow_datasets import TensorflowDatasets\\n\\n__all__ = [\"TensorflowDatasets\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\tensorflow_datasets.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utilities.twilio import TwilioAPIWrapper\\n\\n__all__ = [\"TwilioAPIWrapper\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\twilio.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utilities.vertexai import (\\n    create_retry_decorator,\\n    get_client_info,\\n    init_vertexai,\\n    raise_vertex_import_error,\\n)\\n\\n__all__ = [\\n    \"create_retry_decorator\",\\n    \"raise_vertex_import_error\",\\n    \"init_vertexai\",\\n    \"get_client_info\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\vertexai.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utilities.wikipedia import (\\n    WikipediaAPIWrapper,\\n)\\n\\n__all__ = [\"WikipediaAPIWrapper\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\wikipedia.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utilities.wolfram_alpha import WolframAlphaAPIWrapper\\n\\n__all__ = [\"WolframAlphaAPIWrapper\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\wolfram_alpha.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utilities.zapier import ZapierNLAWrapper\\n\\n__all__ = [\"ZapierNLAWrapper\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\zapier.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"**Utilities** are the integrations with third-part systems and packages.\\n\\nOther LangChain classes use **Utilities** to interact with third-part systems\\nand packages.\\n\"\"\"\\nimport warnings\\nfrom typing import Any\\n\\nfrom langchain_community.utilities.requests import (\\n    Requests,\\n    RequestsWrapper,\\n    TextRequestsWrapper,\\n)\\nfrom langchain_core._api import LangChainDeprecationWarning\\n\\nfrom langchain.utils.interactive_env import is_interactive_env\\n\\n\\ndef __getattr__(name: str) -> Any:\\n    from langchain_community import utilities\\n\\n    # If not in interactive env, raise warning.\\n    if not is_interactive_env():\\n        warnings.warn(\\n            \"Importing this utility from langchain is deprecated. Importing it from \"\\n            \"langchain will no longer be supported as of langchain==0.2.0. \"\\n            \"Please import from langchain-community instead:\\\\n\\\\n\"\\n            f\"`from langchain_community.utilities import {name}`.\\\\n\\\\n\"\\n            \"To install langchain-community run `pip install -U langchain-community`.\",\\n            category=LangChainDeprecationWarning,\\n        )\\n\\n    return getattr(utilities, name)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='__all__ = [\\n    \"AlphaVantageAPIWrapper\",\\n    \"ApifyWrapper\",\\n    \"ArceeWrapper\",\\n    \"ArxivAPIWrapper\",\\n    \"BibtexparserWrapper\",\\n    \"BingSearchAPIWrapper\",\\n    \"BraveSearchWrapper\",\\n    \"DuckDuckGoSearchAPIWrapper\",\\n    \"GoldenQueryAPIWrapper\",\\n    \"GoogleFinanceAPIWrapper\",\\n    \"GoogleLensAPIWrapper\",\\n    \"GoogleJobsAPIWrapper\",\\n    \"GooglePlacesAPIWrapper\",\\n    \"GoogleScholarAPIWrapper\",\\n    \"GoogleTrendsAPIWrapper\",\\n    \"GoogleSearchAPIWrapper\",\\n    \"GoogleSerperAPIWrapper\",\\n    \"GraphQLAPIWrapper\",\\n    \"JiraAPIWrapper\",\\n    \"LambdaWrapper\",\\n    \"MaxComputeAPIWrapper\",\\n    \"MerriamWebsterAPIWrapper\",\\n    \"MetaphorSearchAPIWrapper\",\\n    \"NasaAPIWrapper\",\\n    \"OpenWeatherMapAPIWrapper\",\\n    \"OutlineAPIWrapper\",\\n    \"Portkey\",\\n    \"PowerBIDataset\",\\n    \"PubMedAPIWrapper\",\\n    \"PythonREPL\",\\n    \"Requests\",\\n    \"RequestsWrapper\",\\n    \"SteamWebAPIWrapper\",\\n    \"SQLDatabase\",\\n    \"SceneXplainAPIWrapper\",\\n    \"SearchApiAPIWrapper\",\\n    \"SearxSearchWrapper\",\\n    \"SerpAPIWrapper\",\\n    \"SparkSQL\",\\n    \"StackExchangeAPIWrapper\",\\n    \"TensorflowDatasets\",\\n    \"TextRequestsWrapper\",\\n    \"TwilioAPIWrapper\",\\n    \"WikipediaAPIWrapper\",\\n    \"WolframAlphaAPIWrapper\",\\n    \"ZapierNLAWrapper\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.utils.aiter import NoLock, Tee, py_anext\\n\\n__all__ = [\"py_anext\", \"NoLock\", \"Tee\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utils\\\\aiter.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.utils.env import get_from_dict_or_env, get_from_env\\n\\n__all__ = [\"get_from_dict_or_env\", \"get_from_env\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utils\\\\env.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utils.ernie_functions import (\\n    FunctionDescription,\\n    ToolDescription,\\n    convert_pydantic_to_ernie_function,\\n    convert_pydantic_to_ernie_tool,\\n)\\n\\n__all__ = [\\n    \"FunctionDescription\",\\n    \"ToolDescription\",\\n    \"convert_pydantic_to_ernie_function\",\\n    \"convert_pydantic_to_ernie_tool\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utils\\\\ernie_functions.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.utils.formatting import StrictFormatter\\n\\n__all__ = [\"StrictFormatter\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utils\\\\formatting.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.utils.html import (\\n    DEFAULT_LINK_REGEX,\\n    PREFIXES_TO_IGNORE,\\n    PREFIXES_TO_IGNORE_REGEX,\\n    SUFFIXES_TO_IGNORE,\\n    SUFFIXES_TO_IGNORE_REGEX,\\n    extract_sub_links,\\n    find_all_links,\\n)\\n\\n__all__ = [\\n    \"PREFIXES_TO_IGNORE\",\\n    \"SUFFIXES_TO_IGNORE\",\\n    \"SUFFIXES_TO_IGNORE_REGEX\",\\n    \"PREFIXES_TO_IGNORE_REGEX\",\\n    \"DEFAULT_LINK_REGEX\",\\n    \"find_all_links\",\\n    \"extract_sub_links\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utils\\\\html.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.utils.input import (\\n    get_bolded_text,\\n    get_color_mapping,\\n    get_colored_text,\\n    print_text,\\n)\\n\\n__all__ = [\"get_color_mapping\", \"get_colored_text\", \"get_bolded_text\", \"print_text\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utils\\\\input.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='def is_interactive_env() -> bool:\\n    \"\"\"Determine if running within IPython or Jupyter.\"\"\"\\n    import sys\\n\\n    return hasattr(sys, \"ps2\")' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utils\\\\interactive_env.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.utils.iter import NoLock, Tee, batch_iterate, tee_peer\\n\\n__all__ = [\"NoLock\", \"tee_peer\", \"Tee\", \"batch_iterate\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utils\\\\iter.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.utils.json_schema import (\\n    _dereference_refs_helper,\\n    _infer_skip_keys,\\n    _retrieve_ref,\\n    dereference_refs,\\n)\\n\\n__all__ = [\\n    \"_retrieve_ref\",\\n    \"_dereference_refs_helper\",\\n    \"_infer_skip_keys\",\\n    \"dereference_refs\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utils\\\\json_schema.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.utils.loading import try_load_from_hub\\n\\n__all__ = [\"try_load_from_hub\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utils\\\\loading.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utils.math import (\\n    Matrix,\\n    cosine_similarity,\\n    cosine_similarity_top_k,\\n)\\n\\n__all__ = [\"Matrix\", \"cosine_similarity\", \"cosine_similarity_top_k\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utils\\\\math.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utils.openai import is_openai_v1\\n\\n__all__ = [\"is_openai_v1\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utils\\\\openai.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.utils.openai_functions import (\\n    FunctionDescription,\\n    ToolDescription,\\n    convert_pydantic_to_openai_function,\\n    convert_pydantic_to_openai_tool,\\n)\\n\\n__all__ = [\\n    \"FunctionDescription\",\\n    \"ToolDescription\",\\n    \"convert_pydantic_to_openai_function\",\\n    \"convert_pydantic_to_openai_tool\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utils\\\\openai_functions.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.utils.pydantic import get_pydantic_major_version\\n\\n__all__ = [\"get_pydantic_major_version\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utils\\\\pydantic.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.utils.strings import comma_list, stringify_dict, stringify_value\\n\\n__all__ = [\"stringify_value\", \"stringify_dict\", \"comma_list\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utils\\\\strings.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.utils.utils import (\\n    build_extra_kwargs,\\n    check_package_version,\\n    convert_to_secret_str,\\n    get_pydantic_field_names,\\n    guard_import,\\n    mock_now,\\n    raise_for_status_with_text,\\n    xor_args,\\n)\\n\\n__all__ = [\\n    \"xor_args\",\\n    \"raise_for_status_with_text\",\\n    \"mock_now\",\\n    \"guard_import\",\\n    \"check_package_version\",\\n    \"get_pydantic_field_names\",\\n    \"build_extra_kwargs\",\\n    \"convert_to_secret_str\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utils\\\\utils.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"\\n**Utility functions** for LangChain.\\n\\nThese functions do not depend on any other LangChain module.\\n\"\"\"\\n\\nfrom langchain_core.utils.formatting import StrictFormatter, formatter\\nfrom langchain_core.utils.input import (\\n    get_bolded_text,\\n    get_color_mapping,\\n    get_colored_text,\\n    print_text,\\n)\\nfrom langchain_core.utils.utils import (\\n    check_package_version,\\n    convert_to_secret_str,\\n    get_pydantic_field_names,\\n    guard_import,\\n    mock_now,\\n    raise_for_status_with_text,\\n    xor_args,\\n)\\n\\nfrom langchain.utils.env import get_from_dict_or_env, get_from_env\\nfrom langchain.utils.math import cosine_similarity, cosine_similarity_top_k\\nfrom langchain.utils.strings import comma_list, stringify_dict, stringify_value\\n\\n__all__ = [\\n    \"StrictFormatter\",\\n    \"check_package_version\",\\n    \"comma_list\",\\n    \"convert_to_secret_str\",\\n    \"cosine_similarity\",\\n    \"cosine_similarity_top_k\",\\n    \"formatter\",\\n    \"get_bolded_text\",\\n    \"get_color_mapping\",\\n    \"get_colored_text\",\\n    \"get_from_dict_or_env\",\\n    \"get_from_env\",\\n    \"get_pydantic_field_names\",\\n    \"guard_import\",\\n    \"mock_now\",\\n    \"print_text\",\\n    \"raise_for_status_with_text\",\\n    \"stringify_dict\",\\n    \"stringify_value\",\\n    \"xor_args\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\utils\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.alibabacloud_opensearch import (\\n    AlibabaCloudOpenSearch,\\n    AlibabaCloudOpenSearchSettings,\\n)\\n\\n__all__ = [\\n    \"AlibabaCloudOpenSearchSettings\",\\n    \"AlibabaCloudOpenSearch\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\alibabacloud_opensearch.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.analyticdb import (\\n    AnalyticDB,\\n)\\n\\n__all__ = [\\n    \"AnalyticDB\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\analyticdb.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.annoy import (\\n    Annoy,\\n)\\n\\n__all__ = [\"Annoy\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\annoy.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.astradb import (\\n    AstraDB,\\n)\\n\\n__all__ = [\\n    \"AstraDB\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\astradb.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.atlas import AtlasDB\\n\\n__all__ = [\"AtlasDB\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\atlas.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.awadb import AwaDB\\n\\n__all__ = [\"AwaDB\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\awadb.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.azuresearch import (\\n    AzureSearch,\\n    AzureSearchVectorStoreRetriever,\\n)\\n\\n__all__ = [\\n    \"AzureSearch\",\\n    \"AzureSearchVectorStoreRetriever\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\azuresearch.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.azure_cosmos_db import (\\n    AzureCosmosDBVectorSearch,\\n    CosmosDBDocumentType,\\n    CosmosDBSimilarityType,\\n)\\n\\n__all__ = [\\n    \"CosmosDBSimilarityType\",\\n    \"CosmosDBDocumentType\",\\n    \"AzureCosmosDBVectorSearch\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\azure_cosmos_db.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.bageldb import (\\n    Bagel,\\n)\\n\\n__all__ = [\"Bagel\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\bageldb.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.baiducloud_vector_search import BESVectorStore\\n\\n__all__ = [\"BESVectorStore\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\baiducloud_vector_search.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core.vectorstores import VectorStore, VectorStoreRetriever\\n\\n__all__ = [\"VectorStore\", \"VectorStoreRetriever\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.cassandra import CVST, Cassandra\\n\\n__all__ = [\"CVST\", \"Cassandra\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\cassandra.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.chroma import (\\n    Chroma,\\n)\\n\\n__all__ = [\"Chroma\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\chroma.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.clarifai import Clarifai\\n\\n__all__ = [\"Clarifai\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\clarifai.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.clickhouse import (\\n    Clickhouse,\\n    ClickhouseSettings,\\n)\\n\\n__all__ = [\"ClickhouseSettings\", \"Clickhouse\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\clickhouse.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.dashvector import DashVector\\n\\n__all__ = [\"DashVector\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\dashvector.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.databricks_vector_search import (\\n    DatabricksVectorSearch,\\n)\\n\\n__all__ = [\"DatabricksVectorSearch\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\databricks_vector_search.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.deeplake import DeepLake\\n\\n__all__ = [\"DeepLake\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\deeplake.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.dingo import Dingo\\n\\n__all__ = [\"Dingo\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\dingo.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.elasticsearch import (\\n    ApproxRetrievalStrategy,\\n    BaseRetrievalStrategy,\\n    ElasticsearchStore,\\n    ExactRetrievalStrategy,\\n    SparseRetrievalStrategy,\\n)\\n\\n__all__ = [\\n    \"BaseRetrievalStrategy\",\\n    \"ApproxRetrievalStrategy\",\\n    \"ExactRetrievalStrategy\",\\n    \"SparseRetrievalStrategy\",\\n    \"ElasticsearchStore\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\elasticsearch.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.elastic_vector_search import (\\n    ElasticKnnSearch,\\n    ElasticVectorSearch,\\n)\\n\\n__all__ = [\\n    \"ElasticVectorSearch\",\\n    \"ElasticKnnSearch\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\elastic_vector_search.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.epsilla import Epsilla\\n\\n__all__ = [\"Epsilla\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\epsilla.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.faiss import (\\n    FAISS,\\n)\\n\\n__all__ = [\"FAISS\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\faiss.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.hippo import Hippo\\n\\n__all__ = [\"Hippo\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\hippo.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.hologres import (\\n    Hologres,\\n)\\n\\n__all__ = [\"Hologres\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\hologres.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.lancedb import LanceDB\\n\\n__all__ = [\"LanceDB\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\lancedb.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.llm_rails import LLMRails, LLMRailsRetriever\\n\\n__all__ = [\"LLMRails\", \"LLMRailsRetriever\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\llm_rails.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.marqo import Marqo\\n\\n__all__ = [\"Marqo\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\marqo.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.matching_engine import MatchingEngine\\n\\n__all__ = [\"MatchingEngine\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\matching_engine.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.meilisearch import Meilisearch\\n\\n__all__ = [\"Meilisearch\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\meilisearch.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.milvus import Milvus\\n\\n__all__ = [\"Milvus\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\milvus.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.momento_vector_index import (\\n    MomentoVectorIndex,\\n)\\n\\n__all__ = [\"MomentoVectorIndex\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\momento_vector_index.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.mongodb_atlas import (\\n    MongoDBAtlasVectorSearch,\\n    MongoDBDocumentType,\\n)\\n\\n__all__ = [\\n    \"MongoDBDocumentType\",\\n    \"MongoDBAtlasVectorSearch\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\mongodb_atlas.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.myscale import (\\n    MyScale,\\n    MyScaleSettings,\\n    MyScaleWithoutJSON,\\n)\\n\\n__all__ = [\"MyScaleSettings\", \"MyScale\", \"MyScaleWithoutJSON\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\myscale.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.neo4j_vector import (\\n    Neo4jVector,\\n    SearchType,\\n)\\n\\n__all__ = [\\n    \"SearchType\",\\n    \"Neo4jVector\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\neo4j_vector.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.nucliadb import NucliaDB\\n\\n__all__ = [\"NucliaDB\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\nucliadb.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.opensearch_vector_search import (\\n    OpenSearchVectorSearch,\\n)\\n\\n__all__ = [\\n    \"OpenSearchVectorSearch\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\opensearch_vector_search.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.pgembedding import (\\n    CollectionStore,\\n    EmbeddingStore,\\n    PGEmbedding,\\n    QueryResult,\\n)\\n\\n__all__ = [\\n    \"CollectionStore\",\\n    \"EmbeddingStore\",\\n    \"QueryResult\",\\n    \"PGEmbedding\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\pgembedding.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.pgvector import (\\n    DistanceStrategy,\\n    PGVector,\\n)\\n\\n__all__ = [\\n    \"DistanceStrategy\",\\n    \"PGVector\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\pgvector.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.pgvecto_rs import PGVecto_rs\\n\\n__all__ = [\"PGVecto_rs\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\pgvecto_rs.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.pinecone import Pinecone\\n\\n__all__ = [\"Pinecone\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\pinecone.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.qdrant import (\\n    Qdrant,\\n    QdrantException,\\n)\\n\\n__all__ = [\"QdrantException\", \"Qdrant\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\qdrant.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.rocksetdb import Rockset\\n\\n__all__ = [\"Rockset\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\rocksetdb.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.scann import (\\n    ScaNN,\\n)\\n\\n__all__ = [\"ScaNN\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\scann.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.semadb import SemaDB\\n\\n__all__ = [\"SemaDB\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\semadb.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.singlestoredb import (\\n    SingleStoreDB,\\n    SingleStoreDBRetriever,\\n)\\n\\n__all__ = [\"SingleStoreDB\", \"SingleStoreDBRetriever\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\singlestoredb.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.sklearn import (\\n    BaseSerializer,\\n    BsonSerializer,\\n    JsonSerializer,\\n    ParquetSerializer,\\n    SKLearnVectorStore,\\n    SKLearnVectorStoreException,\\n)\\n\\n__all__ = [\\n    \"BaseSerializer\",\\n    \"JsonSerializer\",\\n    \"BsonSerializer\",\\n    \"ParquetSerializer\",\\n    \"SKLearnVectorStoreException\",\\n    \"SKLearnVectorStore\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\sklearn.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.sqlitevss import SQLiteVSS\\n\\n__all__ = [\"SQLiteVSS\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\sqlitevss.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.starrocks import (\\n    StarRocks,\\n    StarRocksSettings,\\n)\\n\\n__all__ = [\\n    \"StarRocksSettings\",\\n    \"StarRocks\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\starrocks.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.supabase import SupabaseVectorStore\\n\\n__all__ = [\"SupabaseVectorStore\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\supabase.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.tair import Tair\\n\\n__all__ = [\"Tair\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\tair.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.tencentvectordb import (\\n    ConnectionParams,\\n    IndexParams,\\n    TencentVectorDB,\\n)\\n\\n__all__ = [\"ConnectionParams\", \"IndexParams\", \"TencentVectorDB\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\tencentvectordb.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.tigris import Tigris\\n\\n__all__ = [\"Tigris\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\tigris.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.tiledb import (\\n    TileDB,\\n)\\n\\n__all__ = [\\n    \"TileDB\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\tiledb.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.timescalevector import (\\n    TimescaleVector,\\n)\\n\\n__all__ = [\\n    \"TimescaleVector\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\timescalevector.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.typesense import Typesense\\n\\n__all__ = [\"Typesense\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\typesense.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.usearch import USearch\\n\\n__all__ = [\"USearch\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\usearch.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.utils import (\\n    DistanceStrategy,\\n    filter_complex_metadata,\\n    maximal_marginal_relevance,\\n)\\n\\n__all__ = [\"DistanceStrategy\", \"maximal_marginal_relevance\", \"filter_complex_metadata\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\utils.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.vald import Vald\\n\\n__all__ = [\"Vald\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\vald.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.vearch import Vearch\\n\\n__all__ = [\"Vearch\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\vearch.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.vectara import Vectara, VectaraRetriever\\n\\n__all__ = [\"Vectara\", \"VectaraRetriever\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\vectara.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.vespa import VespaStore\\n\\n__all__ = [\"VespaStore\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\vespa.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.weaviate import (\\n    Weaviate,\\n)\\n\\n__all__ = [\\n    \"Weaviate\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\weaviate.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.xata import XataVectorStore\\n\\n__all__ = [\"XataVectorStore\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\xata.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.yellowbrick import Yellowbrick\\n\\n__all__ = [\"Yellowbrick\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\yellowbrick.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.zep import CollectionConfig, ZepVectorStore\\n\\n__all__ = [\"CollectionConfig\", \"ZepVectorStore\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\zep.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.zilliz import Zilliz\\n\\n__all__ = [\"Zilliz\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\zilliz.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"**Vector store** stores embedded data and performs vector search.\\n\\nOne of the most common ways to store and search over unstructured data is to\\nembed it and store the resulting embedding vectors, and then query the store\\nand retrieve the data that are \\'most similar\\' to the embedded query.\\n\\n**Class hierarchy:**\\n\\n.. code-block::\\n\\n    VectorStore --> <name>  # Examples: Annoy, FAISS, Milvus\\n\\n    BaseRetriever --> VectorStoreRetriever --> <name>Retriever  # Example: VespaRetriever\\n\\n**Main helpers:**\\n\\n.. code-block::\\n\\n    Embeddings, Document\\n\"\"\"  # noqa: E501\\nimport warnings\\nfrom typing import Any\\n\\nfrom langchain_core._api import LangChainDeprecationWarning\\nfrom langchain_core.vectorstores import VectorStore\\n\\nfrom langchain.utils.interactive_env import is_interactive_env\\n\\n\\ndef __getattr__(name: str) -> Any:\\n    from langchain_community import vectorstores\\n\\n    # If not in interactive env, raise warning.\\n    if not is_interactive_env():\\n        warnings.warn(\\n            \"Importing vector stores from langchain is deprecated. Importing from \"\\n            \"langchain will no longer be supported as of langchain==0.2.0. \"\\n            \"Please import from langchain-community instead:\\\\n\\\\n\"\\n            f\"`from langchain_community.vectorstores import {name}`.\\\\n\\\\n\"\\n            \"To install langchain-community run `pip install -U langchain-community`.\",\\n            category=LangChainDeprecationWarning,\\n        )\\n\\n    return getattr(vectorstores, name)' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='__all__ = [\\n    \"AlibabaCloudOpenSearch\",\\n    \"AlibabaCloudOpenSearchSettings\",\\n    \"AnalyticDB\",\\n    \"Annoy\",\\n    \"AtlasDB\",\\n    \"AwaDB\",\\n    \"AzureSearch\",\\n    \"Bagel\",\\n    \"Cassandra\",\\n    \"AstraDB\",\\n    \"Chroma\",\\n    \"Clarifai\",\\n    \"Clickhouse\",\\n    \"ClickhouseSettings\",\\n    \"DashVector\",\\n    \"DatabricksVectorSearch\",\\n    \"DeepLake\",\\n    \"Dingo\",\\n    \"DocArrayHnswSearch\",\\n    \"DocArrayInMemorySearch\",\\n    \"ElasticKnnSearch\",\\n    \"ElasticVectorSearch\",\\n    \"ElasticsearchStore\",\\n    \"Epsilla\",\\n    \"FAISS\",\\n    \"Hologres\",\\n    \"LanceDB\",\\n    \"LLMRails\",\\n    \"Marqo\",\\n    \"MatchingEngine\",\\n    \"Meilisearch\",\\n    \"Milvus\",\\n    \"MomentoVectorIndex\",\\n    \"MongoDBAtlasVectorSearch\",\\n    \"MyScale\",\\n    \"MyScaleSettings\",\\n    \"Neo4jVector\",\\n    \"OpenSearchVectorSearch\",\\n    \"PGEmbedding\",\\n    \"PGVector\",\\n    \"Pinecone\",\\n    \"Qdrant\",\\n    \"Redis\",\\n    \"Rockset\",\\n    \"SKLearnVectorStore\",\\n    \"ScaNN\",\\n    \"SemaDB\",\\n    \"SingleStoreDB\",\\n    \"SQLiteVSS\",\\n    \"StarRocks\",\\n    \"SupabaseVectorStore\",\\n    \"Tair\",\\n    \"TileDB\",\\n    \"Tigris\",\\n    \"TimescaleVector\",\\n    \"Typesense\",\\n    \"USearch\",\\n    \"Vald\",\\n    \"Vearch\",\\n    \"Vectara\",\\n    \"VespaStore\",\\n    \"Weaviate\",\\n    \"Yellowbrick\",\\n    \"ZepVectorStore\",\\n    \"Zilliz\",\\n    \"TencentVectorDB\",\\n    \"AzureCosmosDBVectorSearch\",\\n    \"VectorStore\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.docarray.base import (\\n    DocArrayIndex,\\n)\\n\\n__all__ = [\"DocArrayIndex\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\docarray\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.docarray.hnsw import DocArrayHnswSearch\\n\\n__all__ = [\"DocArrayHnswSearch\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\docarray\\\\hnsw.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.docarray.in_memory import DocArrayInMemorySearch\\n\\n__all__ = [\"DocArrayInMemorySearch\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\docarray\\\\in_memory.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.docarray.hnsw import DocArrayHnswSearch\\nfrom langchain_community.vectorstores.docarray.in_memory import DocArrayInMemorySearch\\n\\n__all__ = [\\n    \"DocArrayHnswSearch\",\\n    \"DocArrayInMemorySearch\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\docarray\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.redis.base import (\\n    Redis,\\n    RedisVectorStoreRetriever,\\n    check_index_exists,\\n)\\n\\n__all__ = [\\n    \"check_index_exists\",\\n    \"Redis\",\\n    \"RedisVectorStoreRetriever\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\redis\\\\base.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.redis.filters import (\\n    RedisFilter,\\n    RedisFilterExpression,\\n    RedisFilterField,\\n    RedisFilterOperator,\\n    RedisNum,\\n    RedisTag,\\n    RedisText,\\n    check_operator_misuse,\\n)\\n\\n__all__ = [\\n    \"RedisFilterOperator\",\\n    \"RedisFilter\",\\n    \"RedisFilterField\",\\n    \"check_operator_misuse\",\\n    \"RedisTag\",\\n    \"RedisNum\",\\n    \"RedisText\",\\n    \"RedisFilterExpression\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\redis\\\\filters.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_community.vectorstores.redis.schema import (\\n    FlatVectorField,\\n    HNSWVectorField,\\n    NumericFieldSchema,\\n    RedisDistanceMetric,\\n    RedisField,\\n    RedisModel,\\n    RedisVectorField,\\n    TagFieldSchema,\\n    TextFieldSchema,\\n    read_schema,\\n)\\n\\n__all__ = [\\n    \"RedisDistanceMetric\",\\n    \"RedisField\",\\n    \"TextFieldSchema\",\\n    \"TagFieldSchema\",\\n    \"NumericFieldSchema\",\\n    \"RedisVectorField\",\\n    \"FlatVectorField\",\\n    \"HNSWVectorField\",\\n    \"RedisModel\",\\n    \"read_schema\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\redis\\\\schema.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from .base import Redis, RedisVectorStoreRetriever\\nfrom .filters import (\\n    RedisFilter,\\n    RedisNum,\\n    RedisTag,\\n    RedisText,\\n)\\n\\n__all__ = [\\n    \"Redis\",\\n    \"RedisFilter\",\\n    \"RedisTag\",\\n    \"RedisText\",\\n    \"RedisNum\",\\n    \"RedisVectorStoreRetriever\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\vectorstores\\\\redis\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core._api.deprecation import (\\n    LangChainDeprecationWarning,\\n    LangChainPendingDeprecationWarning,\\n    deprecated,\\n    suppress_langchain_deprecation_warning,\\n    surface_langchain_deprecation_warnings,\\n    warn_deprecated,\\n)\\n\\n__all__ = [\\n    \"LangChainDeprecationWarning\",\\n    \"LangChainPendingDeprecationWarning\",\\n    \"deprecated\",\\n    \"suppress_langchain_deprecation_warning\",\\n    \"warn_deprecated\",\\n    \"surface_langchain_deprecation_warnings\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\_api\\\\deprecation.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='from langchain_core._api.path import as_import_path, get_relative_path\\n\\n__all__ = [\"get_relative_path\", \"as_import_path\"]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\_api\\\\path.py', 'language': <Language.PYTHON: 'python'>}\n",
      "page_content='\"\"\"Helper functions for managing the LangChain API.\\n\\nThis module is only relevant for LangChain developers, not for users.\\n\\n.. warning::\\n\\n    This module and its submodules are for internal use only.  Do not use them\\n    in your own code.  We may change the API at any time with no warning.\\n\\n\"\"\"\\n\\nfrom .deprecation import (\\n    LangChainDeprecationWarning,\\n    deprecated,\\n    suppress_langchain_deprecation_warning,\\n    surface_langchain_deprecation_warnings,\\n    warn_deprecated,\\n)\\n\\n__all__ = [\\n    \"deprecated\",\\n    \"LangChainDeprecationWarning\",\\n    \"suppress_langchain_deprecation_warning\",\\n    \"surface_langchain_deprecation_warnings\",\\n    \"warn_deprecated\",\\n]' metadata={'source': 'langchain-library\\\\libs\\\\langchain\\\\langchain\\\\_api\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}\n"
     ]
    }
   ],
   "source": [
    "# Take a look at your split texts\n",
    "for text in texts:\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `SentenceTransformerEmbeddings` & `Chroma` to power Retrieval\n",
    "\n",
    "To keep our activity private, we'll use a free, open-source embeddings model hosted on the [Hugging Face](https://huggingface.co/).\n",
    "\n",
    "`all-MiniLM-L6-v2` is a solid default choice due to its compact size, efficiency, and effectiveness in generating meaningful sentence embeddings. You may find more models [here](https://huggingface.co/models?pipeline_tag=sentence-similarity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dae\\.vscode\\Software\\Build-AI\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\dae\\.vscode\\Software\\Build-AI\\.venv\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Chroma Database\n",
    "Now we need to create our database. We can do so in a single line to pass in documents, set our embeddings model, and choose a directory to save our database to persistent storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma \n",
    "\n",
    "# load it into Chroma\n",
    "db = Chroma.from_documents(\n",
    "    texts, \n",
    "    embedding_function, \n",
    "    persist_directory=\"./chroma_db\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Retriever\n",
    "We'll use our new `db` as our retriever using `as_retriever()` from the LangChain library.\n",
    "\n",
    "I encourage you to read [this](https://api.python.langchain.com/en/stable/vectorstores/langchain_community.vectorstores.chroma.Chroma.html?highlight=chroma%20as_retriever#langchain_community.vectorstores.chroma.Chroma.as_retriever \"Learn more about 'as_retriever'\") Python API documentation from LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the retriever\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"mmr\", # or \"similarity\"\n",
    "    search_kwargs={\"k\": 32,}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the retriever\n",
    "retrieved_docs = retriever.invoke(\n",
    "    \"Jina Embeddings\"\n",
    ")\n",
    "\n",
    "# Print the first retrieved doc\n",
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We've successfully used the retriever to get relevant context from our database!\n",
    "\n",
    "Now let's create a chain to generate answers based on what context we retrieve.\n",
    "\n",
    "We'll start by calling another open-source model from the Hugging Face model hub. \n",
    "\n",
    "LangChain provides a wrapper, `HuggingFaceHub` to quickly initialize models for text generation or text2text generation.\n",
    "- Note: You will need a HuggingFace Access Token, which you can get [here](https://huggingface.co/settings/tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "\n",
    "# Define the LLM for summarization\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"google/flan-t5-base\",\n",
    "    model_kwargs={\"temperature\": 0.7, \"max_length\": 500},\n",
    "    huggingfacehub_api_token=\"\",\n",
    "    task=\"text-generation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If the HuggingFace API isn't available, use the following cell to load GPT-3.5 as a drop-in replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use OpenAI since HF endpoint doesn't seem to like the length of my connection time\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-1106\", temperature=0.7, openai_api_key=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "# Initialize Agent memory\n",
    "memory = ConversationSummaryMemory(\n",
    "    llm=llm,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "# Initialize the ConversationalRetrievalChain\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How can I initialize a ReAct agent?\"\n",
    "result = qa(question)\n",
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> **Question**: What is the class hierarchy? \n",
      "\n",
      "**Answer**: The class hierarchy includes the following structure:\n",
      "\n",
      "- Chain --> <name>Chain  # Examples: LLMChain, MapReduceChain, RouterChain\n",
      "\n",
      "This structure demonstrates that Chains are easily reusable components linked together, allowing for the encoding of a sequence of calls to components like models, document retrievers, and other Chains. \n",
      "\n",
      "-> **Question**: What classes are derived from the Chain class? \n",
      "\n",
      "**Answer**: The following classes are derived from the `Chain` class:\n",
      "1. `RouterChain`\n",
      "2. `MultiRouteChain`\n",
      "3. `SequentialChain`\n",
      "4. `TransformChain`\n",
      "5. `LLMRouterChain`\n",
      "6. `LLMMathChain`\n",
      "\n",
      "These are all part of the `langchain.chains` module and are used for various purposes such as routing, transformation, and language model-based operations. \n",
      "\n",
      "-> **Question**: What one improvement do you propose in code in relation to the class hierarchy for the Chain class? \n",
      "\n",
      "**Answer**: The class hierarchy for the `Chain` class seems to be consistent with the inheritance structure. The code includes an abstract class `Chain` that serves as the base class for creating structured sequences of calls to components. Then, it is being inherited by other classes such as `RouterChain`, `MultiRouteChain`, and `SequentialChain` to implement specific functionality.\n",
      "\n",
      "No significant improvements are necessary based on the provided code snippets. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"What is the class hierarchy?\",\n",
    "    \"What classes are derived from the Chain class?\",\n",
    "    \"What one improvement do you propose in code in relation to the class hierarchy for the Chain class?\",\n",
    "]\n",
    "\n",
    "# Iterate over the list of questions and print the answers\n",
    "for question in questions:\n",
    "    result = qa(question)\n",
    "    print(f\"-> **Question**: {question} \\n\")\n",
    "    print(f\"**Answer**: {result['answer']} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatPromptTemplate\n",
    "\n",
    "Since our LLM is a chat model, we can make use of all 'Chat' related functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "# Create your Prompt Template by using `PromptTemplate`\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "Use three sentences maximum and keep the answer as concise as possible. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "\n",
    "# Use the LangChain Hub to load a premade prompt\n",
    "# Uncomment the following, comment out the above, \\\n",
    "    # to use the Hub\n",
    "\n",
    "#from langchain import hub\n",
    "\n",
    "#QA_CHAIN_PROMPT = hub.pull(\"rlm/rag-prompt-default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_text': 'You can initialize a ReAct agent using the `create_react_agent` method, which takes a language model (LLM) and a sequence of tools as input and returns an agent executor. The agent executor is used to load an agent executor given tools and LLM.'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Docs\n",
    "question = \"How can I initialize a ReAct agent?\"\n",
    "docs = retriever.get_relevant_documents(question)\n",
    "\n",
    "# Chain\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=QA_CHAIN_PROMPT)\n",
    "\n",
    "# Run\n",
    "chain({\"input_documents\": docs, \"question\": question}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapping Up\n",
    "In this notebook, you've learned how to use the `Langchain` library to create a question-answering system. \n",
    "\n",
    "You've seen how to load a question-answering chain and how to create a prompt template for the system. You've also learned about the importance of keeping answers concise and not making up answers when the system doesn't know the answer. Furthermore, you've explored an alternative way of pulling a default prompt from the `Langchain` hub. \n",
    "\n",
    "While we didn't apply an output parser in this notebook, you've gained a solid foundation in setting up a question-answering system with `Langchain`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
